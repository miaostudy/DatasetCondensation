nohup: ignoring input
2024-10-30 14:49:10: eval_it_pool: [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000, 18000, 20000]
Downloading https://zenodo.org/records/10519652/files/organamnist.npz?download=1 to /data/users/xiongyuxuan/.medmnist/organamnist.npz
  0%|          | 0/38247708 [00:00<?, ?it/s]  0%|          | 32768/38247708 [00:00<03:36, 176534.22it/s]  0%|          | 65536/38247708 [00:00<03:37, 175919.34it/s]  0%|          | 98304/38247708 [00:00<03:37, 175751.41it/s]  1%|          | 196608/38247708 [00:00<02:01, 313861.43it/s]  1%|          | 294912/38247708 [00:00<01:37, 389859.76it/s]  1%|          | 393216/38247708 [00:01<01:26, 435738.94it/s]  1%|▏         | 491520/38247708 [00:01<01:21, 464956.56it/s]  2%|▏         | 589824/38247708 [00:01<01:17, 483922.21it/s]  2%|▏         | 688128/38247708 [00:01<01:15, 496558.28it/s]  2%|▏         | 786432/38247708 [00:01<01:14, 505736.10it/s]  3%|▎         | 1048576/38247708 [00:02<01:02, 597381.66it/s]  4%|▍         | 1507328/38247708 [00:02<00:34, 1063366.51it/s]  4%|▍         | 1703936/38247708 [00:02<00:37, 986780.87it/s]   6%|▌         | 2293760/38247708 [00:02<00:21, 1671904.33it/s]  7%|▋         | 2588672/38247708 [00:02<00:21, 1645840.44it/s]  8%|▊         | 2916352/38247708 [00:03<00:21, 1676885.40it/s]  8%|▊         | 3244032/38247708 [00:03<00:20, 1699447.63it/s]  9%|▉         | 3571712/38247708 [00:03<00:20, 1715767.08it/s] 10%|█         | 3899392/38247708 [00:03<00:19, 1728221.41it/s] 11%|█         | 4259840/38247708 [00:03<00:19, 1785963.55it/s] 12%|█▏        | 4587520/38247708 [00:04<00:18, 1778812.13it/s] 13%|█▎        | 4947968/38247708 [00:04<00:18, 1822118.91it/s] 14%|█▍        | 5308416/38247708 [00:04<00:17, 1853732.16it/s] 15%|█▍        | 5668864/38247708 [00:04<00:17, 1875222.52it/s] 16%|█▌        | 6029312/38247708 [00:04<00:17, 1891672.77it/s] 17%|█▋        | 6389760/38247708 [00:05<00:16, 1903426.90it/s] 18%|█▊        | 6750208/38247708 [00:05<00:15, 2043790.61it/s] 19%|█▊        | 7077888/38247708 [00:05<00:13, 2285721.59it/s] 19%|█▉        | 7340032/38247708 [00:05<00:15, 2031072.11it/s] 20%|█▉        | 7569408/38247708 [00:05<00:16, 1905673.12it/s] 21%|██        | 7864320/38247708 [00:05<00:16, 1799520.88it/s] 22%|██▏       | 8224768/38247708 [00:05<00:14, 2010224.94it/s] 22%|██▏       | 8486912/38247708 [00:06<00:13, 2127324.21it/s] 23%|██▎       | 8716288/38247708 [00:06<00:14, 2004419.84it/s] 23%|██▎       | 8978432/38247708 [00:06<00:16, 1794234.10it/s] 24%|██▍       | 9338880/38247708 [00:06<00:14, 1994833.79it/s] 25%|██▌       | 9732096/38247708 [00:06<00:15, 1884575.60it/s] 26%|██▋       | 10092544/38247708 [00:06<00:14, 1901828.14it/s] 27%|██▋       | 10485760/38247708 [00:07<00:14, 1960512.98it/s] 28%|██▊       | 10846208/38247708 [00:07<00:14, 1954341.49it/s] 29%|██▉       | 11239424/38247708 [00:07<00:13, 1996990.81it/s] 30%|███       | 11599872/38247708 [00:07<00:13, 1977843.53it/s] 31%|███▏      | 11993088/38247708 [00:07<00:13, 2012876.67it/s] 32%|███▏      | 12353536/38247708 [00:08<00:13, 1990746.27it/s] 33%|███▎      | 12746752/38247708 [00:08<00:12, 2019312.51it/s] 34%|███▍      | 13107200/38247708 [00:08<00:12, 1997197.27it/s] 35%|███▌      | 13467648/38247708 [00:08<00:12, 1986479.43it/s] 36%|███▌      | 13860864/38247708 [00:08<00:12, 2011675.52it/s] 37%|███▋      | 14221312/38247708 [00:08<00:12, 1996981.79it/s] 38%|███▊      | 14614528/38247708 [00:09<00:11, 2018867.07it/s] 39%|███▉      | 14974976/38247708 [00:09<00:11, 2002719.83it/s] 40%|████      | 15368192/38247708 [00:09<00:11, 2023054.48it/s] 41%|████      | 15761408/38247708 [00:09<00:10, 2045874.11it/s] 42%|████▏     | 16121856/38247708 [00:09<00:10, 2015728.11it/s] 43%|████▎     | 16515072/38247708 [00:10<00:10, 2039281.19it/s] 44%|████▍     | 16908288/38247708 [00:10<00:10, 2058386.00it/s] 45%|████▌     | 17268736/38247708 [00:10<00:10, 2020725.45it/s] 46%|████▌     | 17661952/38247708 [00:10<00:10, 2044409.68it/s] 47%|████▋     | 18055168/38247708 [00:10<00:09, 2061413.04it/s] 48%|████▊     | 18448384/38247708 [00:11<00:09, 2076277.46it/s] 49%|████▉     | 18841600/38247708 [00:11<00:09, 2084717.71it/s] 50%|█████     | 19234816/38247708 [00:11<00:09, 2097299.01it/s] 51%|█████▏    | 19660800/38247708 [00:11<00:08, 2143143.27it/s] 52%|█████▏    | 20054016/38247708 [00:11<00:08, 2139367.58it/s] 54%|█████▎    | 20480000/38247708 [00:11<00:08, 2146268.55it/s] 55%|█████▍    | 20905984/38247708 [00:12<00:07, 2203807.48it/s] 56%|█████▌    | 21331968/38247708 [00:12<00:07, 2226776.91it/s] 57%|█████▋    | 21757952/38247708 [00:12<00:07, 2260271.70it/s] 58%|█████▊    | 22216704/38247708 [00:12<00:06, 2301296.82it/s] 59%|█████▉    | 22675456/38247708 [00:12<00:06, 2346642.42it/s] 60%|██████    | 23134208/38247708 [00:13<00:06, 2379678.29it/s] 62%|██████▏   | 23592960/38247708 [00:13<00:06, 2405147.94it/s] 63%|██████▎   | 24084480/38247708 [00:13<00:05, 2442551.12it/s] 64%|██████▍   | 24576000/38247708 [00:13<00:05, 2524126.81it/s] 66%|██████▌   | 25067520/38247708 [00:13<00:05, 2568039.99it/s] 67%|██████▋   | 25559040/38247708 [00:14<00:04, 2595787.90it/s] 68%|██████▊   | 26083328/38247708 [00:14<00:04, 2659413.46it/s] 70%|██████▉   | 26640384/38247708 [00:14<00:04, 2730311.95it/s] 71%|███████   | 27197440/38247708 [00:14<00:03, 2804004.81it/s] 73%|███████▎  | 27754496/38247708 [00:14<00:03, 2882675.32it/s] 74%|███████▍  | 28344320/38247708 [00:14<00:03, 2962336.00it/s] 76%|███████▌  | 28966912/38247708 [00:15<00:03, 3045682.60it/s] 77%|███████▋  | 29589504/38247708 [00:15<00:02, 3130051.78it/s] 79%|███████▉  | 30212096/38247708 [00:15<00:02, 3227246.58it/s] 81%|████████  | 30867456/38247708 [00:15<00:02, 3312936.30it/s] 83%|████████▎ | 31555584/38247708 [00:15<00:01, 3421385.27it/s] 84%|████████▍ | 32243712/38247708 [00:15<00:01, 4058771.91it/s] 86%|████████▌ | 32702464/38247708 [00:16<00:01, 3825418.69it/s] 87%|████████▋ | 33128448/38247708 [00:16<00:01, 3605206.05it/s] 88%|████████▊ | 33751040/38247708 [00:16<00:01, 3564896.15it/s] 90%|█████████ | 34504704/38247708 [00:16<00:01, 3726854.71it/s] 92%|█████████▏| 35291136/38247708 [00:16<00:00, 3892852.36it/s] 94%|█████████▍| 36077568/38247708 [00:16<00:00, 4680891.48it/s] 96%|█████████▌| 36601856/38247708 [00:17<00:00, 4780842.92it/s] 97%|█████████▋| 37126144/38247708 [00:17<00:00, 4094741.99it/s] 99%|█████████▉| 37847040/38247708 [00:17<00:00, 4102053.95it/s]100%|██████████| 38247708/38247708 [00:17<00:00, 2198252.20it/s]
2024-10-30 14:49:30: 
================== Exp 0 ==================
 
2024-10-30 14:49:30: Hyper-parameters: 
{'dataset': 'OrganAMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': '/data/users/xiongyuxuan/DD/OBJDD/DatasetCondensation-master/data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f5f8402db20>, 'dsa': True, 'logger': <Logger DM_IPC=10_Model_ConvNet_Data_OrganAMNIST (INFO)>}
2024-10-30 14:49:30: Evaluation model pool: ['ConvNet']
2024-10-30 14:49:35: class c = 0: 1956 real images
2024-10-30 14:49:35: class c = 1: 1390 real images
2024-10-30 14:49:35: class c = 2: 1357 real images
2024-10-30 14:49:35: class c = 3: 1474 real images
2024-10-30 14:49:35: class c = 4: 3963 real images
2024-10-30 14:49:35: class c = 5: 3817 real images
2024-10-30 14:49:35: class c = 6: 6164 real images
2024-10-30 14:49:35: class c = 7: 3919 real images
2024-10-30 14:49:35: class c = 8: 3929 real images
2024-10-30 14:49:35: class c = 9: 3031 real images
2024-10-30 14:49:35: class c = 10: 3561 real images
2024-10-30 14:49:35: real images channel 0, mean = 0.4680, std = 0.2974
main_DM.py:120: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
main_DM.py:120: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975312/work/torch/csrc/utils/tensor_new.cpp:230.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-10-30 14:49:35: initialize synthetic data from random real images
2024-10-30 14:49:35: [2024-10-30 14:49:35] training begins
2024-10-30 14:49:35: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-10-30 14:49:35: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 14:49:35: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975312/work/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 14:51:19: Evaluate 5 random ConvNet, ACCmean = 0.6915 ACCstd = 0.0035
-------------------------
2024-10-30 14:51:19: Evaluate 5 random ConvNet, SENmean = 0.6850 SENstd = 0.0057
-------------------------
2024-10-30 14:51:19: Evaluate 5 random ConvNet, SPEmean = 0.9690 SPEstd = 0.0003
-------------------------
2024-10-30 14:51:19: Evaluate 5 random ConvNet, F!mean = 0.6717 F!std = 0.0057
-------------------------
2024-10-30 14:51:19: Evaluate 5 random ConvNet, mean = 0.6915 std = 0.0035
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 14:51:19: [2024-10-30 14:51:19] iter = 00000, loss = 16.3652
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 14:51:22: [2024-10-30 14:51:22] iter = 00010, loss = 5.1101
2024-10-30 14:51:25: [2024-10-30 14:51:25] iter = 00020, loss = 3.4662
2024-10-30 14:51:28: [2024-10-30 14:51:28] iter = 00030, loss = 3.1930
2024-10-30 14:51:31: [2024-10-30 14:51:31] iter = 00040, loss = 3.5085
2024-10-30 14:51:33: [2024-10-30 14:51:33] iter = 00050, loss = 3.5076
2024-10-30 14:51:35: [2024-10-30 14:51:35] iter = 00060, loss = 2.9806
2024-10-30 14:51:38: [2024-10-30 14:51:38] iter = 00070, loss = 2.8446
2024-10-30 14:51:40: [2024-10-30 14:51:40] iter = 00080, loss = 2.8307
2024-10-30 14:51:43: [2024-10-30 14:51:43] iter = 00090, loss = 3.3715
2024-10-30 14:51:46: [2024-10-30 14:51:46] iter = 00100, loss = 2.5329
2024-10-30 14:51:48: [2024-10-30 14:51:48] iter = 00110, loss = 2.5200
2024-10-30 14:51:51: [2024-10-30 14:51:51] iter = 00120, loss = 2.6424
2024-10-30 14:51:54: [2024-10-30 14:51:54] iter = 00130, loss = 2.8835
2024-10-30 14:51:57: [2024-10-30 14:51:57] iter = 00140, loss = 2.8919
2024-10-30 14:52:01: [2024-10-30 14:52:01] iter = 00150, loss = 2.8522
2024-10-30 14:52:03: [2024-10-30 14:52:03] iter = 00160, loss = 2.3223
2024-10-30 14:52:05: [2024-10-30 14:52:05] iter = 00170, loss = 2.6270
2024-10-30 14:52:06: [2024-10-30 14:52:06] iter = 00180, loss = 2.7259
2024-10-30 14:52:07: [2024-10-30 14:52:07] iter = 00190, loss = 2.1560
2024-10-30 14:52:09: [2024-10-30 14:52:09] iter = 00200, loss = 2.3205
2024-10-30 14:52:10: [2024-10-30 14:52:10] iter = 00210, loss = 2.1120
2024-10-30 14:52:11: [2024-10-30 14:52:11] iter = 00220, loss = 3.4396
2024-10-30 14:52:13: [2024-10-30 14:52:13] iter = 00230, loss = 2.6592
2024-10-30 14:52:16: [2024-10-30 14:52:16] iter = 00240, loss = 3.0304
2024-10-30 14:52:19: [2024-10-30 14:52:19] iter = 00250, loss = 2.7557
2024-10-30 14:52:22: [2024-10-30 14:52:22] iter = 00260, loss = 2.4926
2024-10-30 14:52:24: [2024-10-30 14:52:24] iter = 00270, loss = 3.0467
2024-10-30 14:52:26: [2024-10-30 14:52:26] iter = 00280, loss = 2.3862
2024-10-30 14:52:29: [2024-10-30 14:52:29] iter = 00290, loss = 2.3921
2024-10-30 14:52:31: [2024-10-30 14:52:31] iter = 00300, loss = 2.0876
2024-10-30 14:52:34: [2024-10-30 14:52:34] iter = 00310, loss = 2.3937
2024-10-30 14:52:36: [2024-10-30 14:52:36] iter = 00320, loss = 2.7213
2024-10-30 14:52:40: [2024-10-30 14:52:40] iter = 00330, loss = 2.4744
2024-10-30 14:52:42: [2024-10-30 14:52:42] iter = 00340, loss = 2.2137
2024-10-30 14:52:44: [2024-10-30 14:52:44] iter = 00350, loss = 3.0973
2024-10-30 14:52:46: [2024-10-30 14:52:46] iter = 00360, loss = 2.6579
2024-10-30 14:52:49: [2024-10-30 14:52:49] iter = 00370, loss = 2.8109
2024-10-30 14:52:51: [2024-10-30 14:52:51] iter = 00380, loss = 2.3274
2024-10-30 14:52:53: [2024-10-30 14:52:53] iter = 00390, loss = 2.3450
2024-10-30 14:52:56: [2024-10-30 14:52:56] iter = 00400, loss = 2.3240
2024-10-30 14:52:58: [2024-10-30 14:52:58] iter = 00410, loss = 2.3249
2024-10-30 14:53:00: [2024-10-30 14:53:00] iter = 00420, loss = 2.9842
2024-10-30 14:53:02: [2024-10-30 14:53:02] iter = 00430, loss = 2.7325
2024-10-30 14:53:05: [2024-10-30 14:53:05] iter = 00440, loss = 2.2012
2024-10-30 14:53:07: [2024-10-30 14:53:07] iter = 00450, loss = 2.3640
2024-10-30 14:53:09: [2024-10-30 14:53:09] iter = 00460, loss = 2.9923
2024-10-30 14:53:11: [2024-10-30 14:53:11] iter = 00470, loss = 2.4495
2024-10-30 14:53:13: [2024-10-30 14:53:13] iter = 00480, loss = 2.3961
2024-10-30 14:53:16: [2024-10-30 14:53:16] iter = 00490, loss = 2.0630
2024-10-30 14:53:19: [2024-10-30 14:53:18] iter = 00500, loss = 2.5015
2024-10-30 14:53:21: [2024-10-30 14:53:21] iter = 00510, loss = 2.2176
2024-10-30 14:53:22: [2024-10-30 14:53:22] iter = 00520, loss = 2.1791
2024-10-30 14:53:25: [2024-10-30 14:53:25] iter = 00530, loss = 3.3592
2024-10-30 14:53:27: [2024-10-30 14:53:27] iter = 00540, loss = 2.0610
2024-10-30 14:53:29: [2024-10-30 14:53:29] iter = 00550, loss = 2.1226
2024-10-30 14:53:32: [2024-10-30 14:53:32] iter = 00560, loss = 2.2784
2024-10-30 14:53:35: [2024-10-30 14:53:35] iter = 00570, loss = 2.5076
2024-10-30 14:53:37: [2024-10-30 14:53:37] iter = 00580, loss = 3.1710
2024-10-30 14:53:40: [2024-10-30 14:53:40] iter = 00590, loss = 2.8533
2024-10-30 14:53:41: [2024-10-30 14:53:41] iter = 00600, loss = 2.6684
2024-10-30 14:53:43: [2024-10-30 14:53:43] iter = 00610, loss = 3.4449
2024-10-30 14:53:45: [2024-10-30 14:53:45] iter = 00620, loss = 3.5961
2024-10-30 14:53:47: [2024-10-30 14:53:47] iter = 00630, loss = 2.7265
2024-10-30 14:53:49: [2024-10-30 14:53:49] iter = 00640, loss = 3.5683
2024-10-30 14:53:51: [2024-10-30 14:53:51] iter = 00650, loss = 2.4280
2024-10-30 14:53:53: [2024-10-30 14:53:53] iter = 00660, loss = 2.3684
2024-10-30 14:53:55: [2024-10-30 14:53:55] iter = 00670, loss = 2.5158
2024-10-30 14:53:57: [2024-10-30 14:53:57] iter = 00680, loss = 4.4069
2024-10-30 14:53:58: [2024-10-30 14:53:58] iter = 00690, loss = 2.1234
2024-10-30 14:54:01: [2024-10-30 14:54:01] iter = 00700, loss = 2.1838
2024-10-30 14:54:03: [2024-10-30 14:54:03] iter = 00710, loss = 2.2090
2024-10-30 14:54:06: [2024-10-30 14:54:05] iter = 00720, loss = 2.4150
2024-10-30 14:54:08: [2024-10-30 14:54:08] iter = 00730, loss = 2.5346
2024-10-30 14:54:10: [2024-10-30 14:54:10] iter = 00740, loss = 2.2293
2024-10-30 14:54:12: [2024-10-30 14:54:12] iter = 00750, loss = 2.7646
2024-10-30 14:54:14: [2024-10-30 14:54:14] iter = 00760, loss = 3.8838
2024-10-30 14:54:15: [2024-10-30 14:54:15] iter = 00770, loss = 2.3000
2024-10-30 14:54:18: [2024-10-30 14:54:18] iter = 00780, loss = 2.4235
2024-10-30 14:54:20: [2024-10-30 14:54:20] iter = 00790, loss = 2.5362
2024-10-30 14:54:23: [2024-10-30 14:54:23] iter = 00800, loss = 2.0918
2024-10-30 14:54:25: [2024-10-30 14:54:25] iter = 00810, loss = 2.0409
2024-10-30 14:54:27: [2024-10-30 14:54:27] iter = 00820, loss = 2.1979
2024-10-30 14:54:30: [2024-10-30 14:54:30] iter = 00830, loss = 2.7147
2024-10-30 14:54:32: [2024-10-30 14:54:32] iter = 00840, loss = 2.2675
2024-10-30 14:54:35: [2024-10-30 14:54:35] iter = 00850, loss = 2.2073
2024-10-30 14:54:37: [2024-10-30 14:54:37] iter = 00860, loss = 2.2450
2024-10-30 14:54:39: [2024-10-30 14:54:39] iter = 00870, loss = 2.4172
2024-10-30 14:54:41: [2024-10-30 14:54:41] iter = 00880, loss = 2.2265
2024-10-30 14:54:43: [2024-10-30 14:54:43] iter = 00890, loss = 2.0984
2024-10-30 14:54:45: [2024-10-30 14:54:45] iter = 00900, loss = 2.7176
2024-10-30 14:54:48: [2024-10-30 14:54:48] iter = 00910, loss = 2.7190
2024-10-30 14:54:50: [2024-10-30 14:54:50] iter = 00920, loss = 1.8726
2024-10-30 14:54:52: [2024-10-30 14:54:52] iter = 00930, loss = 2.1707
2024-10-30 14:54:54: [2024-10-30 14:54:54] iter = 00940, loss = 2.2555
2024-10-30 14:54:57: [2024-10-30 14:54:57] iter = 00950, loss = 2.3815
2024-10-30 14:54:59: [2024-10-30 14:54:59] iter = 00960, loss = 2.9453
2024-10-30 14:55:01: [2024-10-30 14:55:01] iter = 00970, loss = 2.6556
2024-10-30 14:55:03: [2024-10-30 14:55:03] iter = 00980, loss = 11.2282
2024-10-30 14:55:05: [2024-10-30 14:55:05] iter = 00990, loss = 2.4872
2024-10-30 14:55:07: [2024-10-30 14:55:07] iter = 01000, loss = 2.6780
2024-10-30 14:55:10: [2024-10-30 14:55:10] iter = 01010, loss = 2.4930
2024-10-30 14:55:12: [2024-10-30 14:55:12] iter = 01020, loss = 2.1160
2024-10-30 14:55:14: [2024-10-30 14:55:14] iter = 01030, loss = 3.2734
2024-10-30 14:55:16: [2024-10-30 14:55:16] iter = 01040, loss = 4.5213
2024-10-30 14:55:18: [2024-10-30 14:55:18] iter = 01050, loss = 2.6260
2024-10-30 14:55:20: [2024-10-30 14:55:20] iter = 01060, loss = 2.8848
2024-10-30 14:55:22: [2024-10-30 14:55:22] iter = 01070, loss = 2.1446
2024-10-30 14:55:23: [2024-10-30 14:55:23] iter = 01080, loss = 2.0222
2024-10-30 14:55:25: [2024-10-30 14:55:25] iter = 01090, loss = 3.0300
2024-10-30 14:55:27: [2024-10-30 14:55:27] iter = 01100, loss = 2.3544
2024-10-30 14:55:29: [2024-10-30 14:55:29] iter = 01110, loss = 2.4717
2024-10-30 14:55:31: [2024-10-30 14:55:31] iter = 01120, loss = 2.6892
2024-10-30 14:55:33: [2024-10-30 14:55:33] iter = 01130, loss = 3.8659
2024-10-30 14:55:35: [2024-10-30 14:55:35] iter = 01140, loss = 2.7924
2024-10-30 14:55:36: [2024-10-30 14:55:36] iter = 01150, loss = 1.9978
2024-10-30 14:55:39: [2024-10-30 14:55:39] iter = 01160, loss = 2.2677
2024-10-30 14:55:40: [2024-10-30 14:55:40] iter = 01170, loss = 2.0045
2024-10-30 14:55:42: [2024-10-30 14:55:42] iter = 01180, loss = 2.4055
2024-10-30 14:55:45: [2024-10-30 14:55:45] iter = 01190, loss = 4.2942
2024-10-30 14:55:47: [2024-10-30 14:55:47] iter = 01200, loss = 2.5511
2024-10-30 14:55:50: [2024-10-30 14:55:50] iter = 01210, loss = 6.9834
2024-10-30 14:55:53: [2024-10-30 14:55:53] iter = 01220, loss = 2.0203
2024-10-30 14:55:55: [2024-10-30 14:55:55] iter = 01230, loss = 1.8967
2024-10-30 14:55:58: [2024-10-30 14:55:58] iter = 01240, loss = 2.2230
2024-10-30 14:56:01: [2024-10-30 14:56:01] iter = 01250, loss = 2.2372
2024-10-30 14:56:03: [2024-10-30 14:56:03] iter = 01260, loss = 2.7024
2024-10-30 14:56:06: [2024-10-30 14:56:06] iter = 01270, loss = 4.9368
2024-10-30 14:56:09: [2024-10-30 14:56:09] iter = 01280, loss = 3.2842
2024-10-30 14:56:12: [2024-10-30 14:56:12] iter = 01290, loss = 2.0687
2024-10-30 14:56:16: [2024-10-30 14:56:16] iter = 01300, loss = 3.0800
2024-10-30 14:56:19: [2024-10-30 14:56:19] iter = 01310, loss = 2.3243
2024-10-30 14:56:23: [2024-10-30 14:56:23] iter = 01320, loss = 2.1790
2024-10-30 14:56:27: [2024-10-30 14:56:27] iter = 01330, loss = 2.2684
2024-10-30 14:56:32: [2024-10-30 14:56:32] iter = 01340, loss = 2.3359
2024-10-30 14:56:36: [2024-10-30 14:56:36] iter = 01350, loss = 4.2238
2024-10-30 14:56:40: [2024-10-30 14:56:40] iter = 01360, loss = 2.2011
2024-10-30 14:56:44: [2024-10-30 14:56:44] iter = 01370, loss = 2.7265
2024-10-30 14:56:49: [2024-10-30 14:56:49] iter = 01380, loss = 2.1303
2024-10-30 14:56:54: [2024-10-30 14:56:54] iter = 01390, loss = 2.1560
2024-10-30 14:56:59: [2024-10-30 14:56:59] iter = 01400, loss = 2.4765
2024-10-30 14:57:03: [2024-10-30 14:57:03] iter = 01410, loss = 3.2956
2024-10-30 14:57:09: [2024-10-30 14:57:09] iter = 01420, loss = 1.8376
2024-10-30 14:57:13: [2024-10-30 14:57:13] iter = 01430, loss = 1.8999
2024-10-30 14:57:17: [2024-10-30 14:57:17] iter = 01440, loss = 1.7316
2024-10-30 14:57:20: [2024-10-30 14:57:20] iter = 01450, loss = 2.3921
2024-10-30 14:57:25: [2024-10-30 14:57:25] iter = 01460, loss = 2.3018
2024-10-30 14:57:29: [2024-10-30 14:57:29] iter = 01470, loss = 2.1380
2024-10-30 14:57:34: [2024-10-30 14:57:34] iter = 01480, loss = 2.0696
2024-10-30 14:57:37: [2024-10-30 14:57:37] iter = 01490, loss = 2.2748
2024-10-30 14:57:40: [2024-10-30 14:57:40] iter = 01500, loss = 2.1604
2024-10-30 14:57:45: [2024-10-30 14:57:45] iter = 01510, loss = 3.5630
2024-10-30 14:57:48: [2024-10-30 14:57:48] iter = 01520, loss = 2.1391
2024-10-30 14:57:52: [2024-10-30 14:57:52] iter = 01530, loss = 2.4269
2024-10-30 14:57:57: [2024-10-30 14:57:57] iter = 01540, loss = 2.2158
2024-10-30 14:58:01: [2024-10-30 14:58:01] iter = 01550, loss = 2.5525
2024-10-30 14:58:06: [2024-10-30 14:58:06] iter = 01560, loss = 2.4894
2024-10-30 14:58:12: [2024-10-30 14:58:12] iter = 01570, loss = 2.5576
2024-10-30 14:58:17: [2024-10-30 14:58:17] iter = 01580, loss = 2.3033
2024-10-30 14:58:21: [2024-10-30 14:58:21] iter = 01590, loss = 2.0135
2024-10-30 14:58:26: [2024-10-30 14:58:26] iter = 01600, loss = 2.1114
2024-10-30 14:58:30: [2024-10-30 14:58:30] iter = 01610, loss = 2.3247
2024-10-30 14:58:34: [2024-10-30 14:58:34] iter = 01620, loss = 2.1786
2024-10-30 14:58:38: [2024-10-30 14:58:38] iter = 01630, loss = 2.2583
2024-10-30 14:58:44: [2024-10-30 14:58:44] iter = 01640, loss = 2.2111
2024-10-30 14:58:48: [2024-10-30 14:58:48] iter = 01650, loss = 2.2652
2024-10-30 14:58:53: [2024-10-30 14:58:53] iter = 01660, loss = 3.1780
2024-10-30 14:58:58: [2024-10-30 14:58:58] iter = 01670, loss = 3.1561
2024-10-30 14:59:03: [2024-10-30 14:59:03] iter = 01680, loss = 2.2320
2024-10-30 14:59:07: [2024-10-30 14:59:07] iter = 01690, loss = 3.4140
2024-10-30 14:59:12: [2024-10-30 14:59:12] iter = 01700, loss = 2.8975
2024-10-30 14:59:17: [2024-10-30 14:59:17] iter = 01710, loss = 2.5933
2024-10-30 14:59:23: [2024-10-30 14:59:23] iter = 01720, loss = 1.9507
2024-10-30 14:59:28: [2024-10-30 14:59:28] iter = 01730, loss = 1.8713
2024-10-30 14:59:33: [2024-10-30 14:59:33] iter = 01740, loss = 2.4319
2024-10-30 14:59:38: [2024-10-30 14:59:38] iter = 01750, loss = 2.6771
2024-10-30 14:59:42: [2024-10-30 14:59:42] iter = 01760, loss = 2.2378
2024-10-30 14:59:46: [2024-10-30 14:59:46] iter = 01770, loss = 2.3135
2024-10-30 14:59:51: [2024-10-30 14:59:51] iter = 01780, loss = 2.2238
2024-10-30 14:59:56: [2024-10-30 14:59:55] iter = 01790, loss = 2.3054
2024-10-30 15:00:00: [2024-10-30 15:00:00] iter = 01800, loss = 1.9461
2024-10-30 15:00:05: [2024-10-30 15:00:05] iter = 01810, loss = 2.8790
2024-10-30 15:00:09: [2024-10-30 15:00:09] iter = 01820, loss = 3.9109
2024-10-30 15:00:14: [2024-10-30 15:00:14] iter = 01830, loss = 2.5163
2024-10-30 15:00:19: [2024-10-30 15:00:19] iter = 01840, loss = 2.8396
2024-10-30 15:00:23: [2024-10-30 15:00:23] iter = 01850, loss = 2.2552
2024-10-30 15:00:28: [2024-10-30 15:00:28] iter = 01860, loss = 2.2398
2024-10-30 15:00:31: [2024-10-30 15:00:31] iter = 01870, loss = 2.7165
2024-10-30 15:00:35: [2024-10-30 15:00:35] iter = 01880, loss = 3.3891
2024-10-30 15:00:40: [2024-10-30 15:00:40] iter = 01890, loss = 1.8480
2024-10-30 15:00:43: [2024-10-30 15:00:43] iter = 01900, loss = 2.2463
2024-10-30 15:00:47: [2024-10-30 15:00:47] iter = 01910, loss = 3.0967
2024-10-30 15:00:51: [2024-10-30 15:00:51] iter = 01920, loss = 2.1382
2024-10-30 15:00:55: [2024-10-30 15:00:55] iter = 01930, loss = 2.9133
2024-10-30 15:00:59: [2024-10-30 15:00:59] iter = 01940, loss = 5.6530
2024-10-30 15:01:03: [2024-10-30 15:01:03] iter = 01950, loss = 2.1993
2024-10-30 15:01:06: [2024-10-30 15:01:06] iter = 01960, loss = 2.9002
2024-10-30 15:01:11: [2024-10-30 15:01:11] iter = 01970, loss = 2.2435
2024-10-30 15:01:15: [2024-10-30 15:01:15] iter = 01980, loss = 2.8656
2024-10-30 15:01:19: [2024-10-30 15:01:19] iter = 01990, loss = 2.4090
2024-10-30 15:01:23: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
2024-10-30 15:01:23: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 15:01:23: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 83736}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 15:03:59: Evaluate 5 random ConvNet, ACCmean = 0.7865 ACCstd = 0.0073
-------------------------
2024-10-30 15:03:59: Evaluate 5 random ConvNet, SENmean = 0.7801 SENstd = 0.0062
-------------------------
2024-10-30 15:03:59: Evaluate 5 random ConvNet, SPEmean = 0.9786 SPEstd = 0.0007
-------------------------
2024-10-30 15:03:59: Evaluate 5 random ConvNet, F!mean = 0.7694 F!std = 0.0068
-------------------------
2024-10-30 15:03:59: Evaluate 5 random ConvNet, mean = 0.7865 std = 0.0073
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 15:03:59: [2024-10-30 15:03:59] iter = 02000, loss = 1.8836
2024-10-30 15:04:03: [2024-10-30 15:04:03] iter = 02010, loss = 2.4378
2024-10-30 15:04:07: [2024-10-30 15:04:07] iter = 02020, loss = 3.0274
2024-10-30 15:04:10: [2024-10-30 15:04:10] iter = 02030, loss = 3.0907
2024-10-30 15:04:13: [2024-10-30 15:04:13] iter = 02040, loss = 2.3597
2024-10-30 15:04:17: [2024-10-30 15:04:17] iter = 02050, loss = 4.1172
2024-10-30 15:04:21: [2024-10-30 15:04:21] iter = 02060, loss = 4.5025
2024-10-30 15:04:26: [2024-10-30 15:04:26] iter = 02070, loss = 2.2573
2024-10-30 15:04:29: [2024-10-30 15:04:29] iter = 02080, loss = 2.4502
2024-10-30 15:04:32: [2024-10-30 15:04:32] iter = 02090, loss = 2.3171
2024-10-30 15:04:36: [2024-10-30 15:04:36] iter = 02100, loss = 2.2504
2024-10-30 15:04:39: [2024-10-30 15:04:39] iter = 02110, loss = 1.6676
2024-10-30 15:04:42: [2024-10-30 15:04:42] iter = 02120, loss = 2.0756
2024-10-30 15:04:46: [2024-10-30 15:04:46] iter = 02130, loss = 1.8362
2024-10-30 15:04:50: [2024-10-30 15:04:50] iter = 02140, loss = 2.4294
2024-10-30 15:04:55: [2024-10-30 15:04:55] iter = 02150, loss = 3.1581
2024-10-30 15:04:59: [2024-10-30 15:04:59] iter = 02160, loss = 2.0765
2024-10-30 15:05:03: [2024-10-30 15:05:03] iter = 02170, loss = 5.5724
2024-10-30 15:05:07: [2024-10-30 15:05:07] iter = 02180, loss = 3.3712
2024-10-30 15:05:11: [2024-10-30 15:05:11] iter = 02190, loss = 2.5431
2024-10-30 15:05:15: [2024-10-30 15:05:15] iter = 02200, loss = 2.1756
2024-10-30 15:05:18: [2024-10-30 15:05:18] iter = 02210, loss = 2.1609
2024-10-30 15:05:22: [2024-10-30 15:05:22] iter = 02220, loss = 2.2230
2024-10-30 15:05:27: [2024-10-30 15:05:27] iter = 02230, loss = 2.5696
2024-10-30 15:05:30: [2024-10-30 15:05:30] iter = 02240, loss = 1.8972
2024-10-30 15:05:33: [2024-10-30 15:05:33] iter = 02250, loss = 1.7938
2024-10-30 15:05:37: [2024-10-30 15:05:37] iter = 02260, loss = 2.4260
2024-10-30 15:05:42: [2024-10-30 15:05:42] iter = 02270, loss = 2.6302
2024-10-30 15:05:46: [2024-10-30 15:05:46] iter = 02280, loss = 2.0862
2024-10-30 15:05:49: [2024-10-30 15:05:49] iter = 02290, loss = 2.8397
2024-10-30 15:05:53: [2024-10-30 15:05:53] iter = 02300, loss = 1.9469
2024-10-30 15:05:56: [2024-10-30 15:05:56] iter = 02310, loss = 2.5964
2024-10-30 15:06:00: [2024-10-30 15:06:00] iter = 02320, loss = 2.3655
2024-10-30 15:06:04: [2024-10-30 15:06:04] iter = 02330, loss = 3.0379
2024-10-30 15:06:07: [2024-10-30 15:06:07] iter = 02340, loss = 1.9871
2024-10-30 15:06:12: [2024-10-30 15:06:12] iter = 02350, loss = 2.0849
2024-10-30 15:06:16: [2024-10-30 15:06:16] iter = 02360, loss = 1.8980
2024-10-30 15:06:20: [2024-10-30 15:06:20] iter = 02370, loss = 2.5054
2024-10-30 15:06:23: [2024-10-30 15:06:23] iter = 02380, loss = 2.1828
2024-10-30 15:06:27: [2024-10-30 15:06:27] iter = 02390, loss = 2.1435
2024-10-30 15:06:30: [2024-10-30 15:06:30] iter = 02400, loss = 2.4029
2024-10-30 15:06:34: [2024-10-30 15:06:34] iter = 02410, loss = 2.8171
2024-10-30 15:06:37: [2024-10-30 15:06:37] iter = 02420, loss = 2.1964
2024-10-30 15:06:41: [2024-10-30 15:06:41] iter = 02430, loss = 2.7520
2024-10-30 15:06:45: [2024-10-30 15:06:45] iter = 02440, loss = 2.5188
2024-10-30 15:06:49: [2024-10-30 15:06:49] iter = 02450, loss = 2.6986
2024-10-30 15:06:52: [2024-10-30 15:06:52] iter = 02460, loss = 2.5468
2024-10-30 15:06:54: [2024-10-30 15:06:54] iter = 02470, loss = 1.9827
2024-10-30 15:06:58: [2024-10-30 15:06:58] iter = 02480, loss = 2.0016
2024-10-30 15:07:02: [2024-10-30 15:07:02] iter = 02490, loss = 2.5323
2024-10-30 15:07:05: [2024-10-30 15:07:05] iter = 02500, loss = 3.1399
2024-10-30 15:07:08: [2024-10-30 15:07:08] iter = 02510, loss = 2.2567
2024-10-30 15:07:12: [2024-10-30 15:07:12] iter = 02520, loss = 3.1706
2024-10-30 15:07:16: [2024-10-30 15:07:16] iter = 02530, loss = 2.0548
2024-10-30 15:07:20: [2024-10-30 15:07:20] iter = 02540, loss = 3.6712
2024-10-30 15:07:23: [2024-10-30 15:07:23] iter = 02550, loss = 2.3524
2024-10-30 15:07:26: [2024-10-30 15:07:26] iter = 02560, loss = 2.5491
2024-10-30 15:07:29: [2024-10-30 15:07:29] iter = 02570, loss = 2.2012
2024-10-30 15:07:33: [2024-10-30 15:07:33] iter = 02580, loss = 2.1363
2024-10-30 15:07:36: [2024-10-30 15:07:36] iter = 02590, loss = 2.1025
2024-10-30 15:07:40: [2024-10-30 15:07:40] iter = 02600, loss = 2.5089
2024-10-30 15:07:43: [2024-10-30 15:07:43] iter = 02610, loss = 2.5842
2024-10-30 15:07:46: [2024-10-30 15:07:46] iter = 02620, loss = 1.9799
2024-10-30 15:07:50: [2024-10-30 15:07:50] iter = 02630, loss = 2.8818
2024-10-30 15:07:54: [2024-10-30 15:07:54] iter = 02640, loss = 2.4218
2024-10-30 15:07:58: [2024-10-30 15:07:58] iter = 02650, loss = 6.2479
2024-10-30 15:08:02: [2024-10-30 15:08:02] iter = 02660, loss = 2.2024
2024-10-30 15:08:06: [2024-10-30 15:08:06] iter = 02670, loss = 2.1964
2024-10-30 15:08:08: [2024-10-30 15:08:08] iter = 02680, loss = 2.1515
2024-10-30 15:08:11: [2024-10-30 15:08:11] iter = 02690, loss = 2.1213
2024-10-30 15:08:14: [2024-10-30 15:08:14] iter = 02700, loss = 1.9405
2024-10-30 15:08:17: [2024-10-30 15:08:17] iter = 02710, loss = 5.9621
2024-10-30 15:08:22: [2024-10-30 15:08:22] iter = 02720, loss = 2.4573
2024-10-30 15:08:25: [2024-10-30 15:08:25] iter = 02730, loss = 2.1673
2024-10-30 15:08:29: [2024-10-30 15:08:29] iter = 02740, loss = 3.6136
2024-10-30 15:08:33: [2024-10-30 15:08:33] iter = 02750, loss = 1.9576
2024-10-30 15:08:36: [2024-10-30 15:08:36] iter = 02760, loss = 2.2465
2024-10-30 15:08:40: [2024-10-30 15:08:40] iter = 02770, loss = 2.3123
2024-10-30 15:08:43: [2024-10-30 15:08:43] iter = 02780, loss = 2.0641
2024-10-30 15:08:47: [2024-10-30 15:08:47] iter = 02790, loss = 2.2371
2024-10-30 15:08:50: [2024-10-30 15:08:50] iter = 02800, loss = 3.0218
2024-10-30 15:08:53: [2024-10-30 15:08:53] iter = 02810, loss = 2.0259
2024-10-30 15:08:57: [2024-10-30 15:08:57] iter = 02820, loss = 2.3693
2024-10-30 15:09:00: [2024-10-30 15:09:00] iter = 02830, loss = 3.2149
2024-10-30 15:09:03: [2024-10-30 15:09:03] iter = 02840, loss = 3.7012
2024-10-30 15:09:07: [2024-10-30 15:09:07] iter = 02850, loss = 2.1280
2024-10-30 15:09:11: [2024-10-30 15:09:11] iter = 02860, loss = 3.2789
2024-10-30 15:09:15: [2024-10-30 15:09:15] iter = 02870, loss = 2.3449
2024-10-30 15:09:19: [2024-10-30 15:09:19] iter = 02880, loss = 2.7991
2024-10-30 15:09:24: [2024-10-30 15:09:24] iter = 02890, loss = 2.2506
2024-10-30 15:09:28: [2024-10-30 15:09:28] iter = 02900, loss = 2.5814
2024-10-30 15:09:33: [2024-10-30 15:09:33] iter = 02910, loss = 3.2932
2024-10-30 15:09:37: [2024-10-30 15:09:37] iter = 02920, loss = 2.0555
2024-10-30 15:09:41: [2024-10-30 15:09:41] iter = 02930, loss = 1.9725
2024-10-30 15:09:45: [2024-10-30 15:09:45] iter = 02940, loss = 2.0214
2024-10-30 15:09:48: [2024-10-30 15:09:48] iter = 02950, loss = 3.0023
2024-10-30 15:09:51: [2024-10-30 15:09:51] iter = 02960, loss = 2.1751
2024-10-30 15:09:55: [2024-10-30 15:09:55] iter = 02970, loss = 2.1633
2024-10-30 15:09:59: [2024-10-30 15:09:59] iter = 02980, loss = 2.1164
2024-10-30 15:10:03: [2024-10-30 15:10:03] iter = 02990, loss = 2.2874
2024-10-30 15:10:06: [2024-10-30 15:10:06] iter = 03000, loss = 2.0115
2024-10-30 15:10:10: [2024-10-30 15:10:10] iter = 03010, loss = 2.4384
2024-10-30 15:10:13: [2024-10-30 15:10:13] iter = 03020, loss = 2.4616
2024-10-30 15:10:17: [2024-10-30 15:10:17] iter = 03030, loss = 2.3556
2024-10-30 15:10:21: [2024-10-30 15:10:21] iter = 03040, loss = 2.0372
2024-10-30 15:10:26: [2024-10-30 15:10:26] iter = 03050, loss = 2.1514
2024-10-30 15:10:30: [2024-10-30 15:10:30] iter = 03060, loss = 2.9218
2024-10-30 15:10:34: [2024-10-30 15:10:34] iter = 03070, loss = 2.6043
2024-10-30 15:10:37: [2024-10-30 15:10:37] iter = 03080, loss = 1.7995
2024-10-30 15:10:41: [2024-10-30 15:10:41] iter = 03090, loss = 2.4112
2024-10-30 15:10:45: [2024-10-30 15:10:45] iter = 03100, loss = 2.3863
2024-10-30 15:10:49: [2024-10-30 15:10:49] iter = 03110, loss = 4.4003
2024-10-30 15:10:53: [2024-10-30 15:10:53] iter = 03120, loss = 2.6883
2024-10-30 15:10:57: [2024-10-30 15:10:57] iter = 03130, loss = 2.0158
2024-10-30 15:11:02: [2024-10-30 15:11:02] iter = 03140, loss = 2.1956
2024-10-30 15:11:07: [2024-10-30 15:11:07] iter = 03150, loss = 2.2528
2024-10-30 15:11:12: [2024-10-30 15:11:12] iter = 03160, loss = 1.8081
2024-10-30 15:11:16: [2024-10-30 15:11:16] iter = 03170, loss = 2.2926
2024-10-30 15:11:20: [2024-10-30 15:11:20] iter = 03180, loss = 2.2954
2024-10-30 15:11:23: [2024-10-30 15:11:23] iter = 03190, loss = 2.4474
2024-10-30 15:11:27: [2024-10-30 15:11:27] iter = 03200, loss = 3.2500
2024-10-30 15:11:30: [2024-10-30 15:11:30] iter = 03210, loss = 2.7454
2024-10-30 15:11:35: [2024-10-30 15:11:35] iter = 03220, loss = 3.0229
2024-10-30 15:11:39: [2024-10-30 15:11:39] iter = 03230, loss = 3.4840
2024-10-30 15:11:43: [2024-10-30 15:11:43] iter = 03240, loss = 1.8205
2024-10-30 15:11:47: [2024-10-30 15:11:47] iter = 03250, loss = 2.2457
2024-10-30 15:11:50: [2024-10-30 15:11:50] iter = 03260, loss = 2.6278
2024-10-30 15:11:53: [2024-10-30 15:11:53] iter = 03270, loss = 2.0028
2024-10-30 15:11:57: [2024-10-30 15:11:57] iter = 03280, loss = 2.2937
2024-10-30 15:12:01: [2024-10-30 15:12:01] iter = 03290, loss = 1.6675
2024-10-30 15:12:05: [2024-10-30 15:12:05] iter = 03300, loss = 2.1168
2024-10-30 15:12:08: [2024-10-30 15:12:08] iter = 03310, loss = 1.9901
2024-10-30 15:12:12: [2024-10-30 15:12:12] iter = 03320, loss = 1.9853
2024-10-30 15:12:16: [2024-10-30 15:12:16] iter = 03330, loss = 2.3321
2024-10-30 15:12:20: [2024-10-30 15:12:20] iter = 03340, loss = 2.0410
2024-10-30 15:12:25: [2024-10-30 15:12:25] iter = 03350, loss = 1.8404
2024-10-30 15:12:29: [2024-10-30 15:12:29] iter = 03360, loss = 2.1999
2024-10-30 15:12:33: [2024-10-30 15:12:33] iter = 03370, loss = 2.0924
2024-10-30 15:12:36: [2024-10-30 15:12:36] iter = 03380, loss = 2.5709
2024-10-30 15:12:40: [2024-10-30 15:12:40] iter = 03390, loss = 1.9349
2024-10-30 15:12:44: [2024-10-30 15:12:44] iter = 03400, loss = 2.5189
2024-10-30 15:12:48: [2024-10-30 15:12:48] iter = 03410, loss = 2.1993
2024-10-30 15:12:52: [2024-10-30 15:12:52] iter = 03420, loss = 1.9446
2024-10-30 15:12:57: [2024-10-30 15:12:57] iter = 03430, loss = 2.5919
2024-10-30 15:13:01: [2024-10-30 15:13:01] iter = 03440, loss = 4.6898
2024-10-30 15:13:06: [2024-10-30 15:13:06] iter = 03450, loss = 2.3468
2024-10-30 15:13:09: [2024-10-30 15:13:09] iter = 03460, loss = 1.9562
2024-10-30 15:13:12: [2024-10-30 15:13:12] iter = 03470, loss = 3.7994
2024-10-30 15:13:14: [2024-10-30 15:13:14] iter = 03480, loss = 2.4324
2024-10-30 15:13:16: [2024-10-30 15:13:16] iter = 03490, loss = 1.6415
2024-10-30 15:13:19: [2024-10-30 15:13:19] iter = 03500, loss = 2.1345
2024-10-30 15:13:23: [2024-10-30 15:13:23] iter = 03510, loss = 2.3562
2024-10-30 15:13:27: [2024-10-30 15:13:27] iter = 03520, loss = 2.2080
2024-10-30 15:13:31: [2024-10-30 15:13:31] iter = 03530, loss = 2.0715
2024-10-30 15:13:34: [2024-10-30 15:13:34] iter = 03540, loss = 2.7508
2024-10-30 15:13:38: [2024-10-30 15:13:38] iter = 03550, loss = 1.9071
2024-10-30 15:13:42: [2024-10-30 15:13:42] iter = 03560, loss = 2.0583
2024-10-30 15:13:45: [2024-10-30 15:13:45] iter = 03570, loss = 2.3092
2024-10-30 15:13:48: [2024-10-30 15:13:48] iter = 03580, loss = 2.3049
2024-10-30 15:13:51: [2024-10-30 15:13:51] iter = 03590, loss = 2.8734
2024-10-30 15:13:55: [2024-10-30 15:13:55] iter = 03600, loss = 2.3408
2024-10-30 15:13:58: [2024-10-30 15:13:58] iter = 03610, loss = 2.2688
2024-10-30 15:14:02: [2024-10-30 15:14:02] iter = 03620, loss = 2.1615
2024-10-30 15:14:07: [2024-10-30 15:14:07] iter = 03630, loss = 2.4281
2024-10-30 15:14:11: [2024-10-30 15:14:11] iter = 03640, loss = 2.0628
2024-10-30 15:14:15: [2024-10-30 15:14:15] iter = 03650, loss = 2.5171
2024-10-30 15:14:19: [2024-10-30 15:14:19] iter = 03660, loss = 2.5601
2024-10-30 15:14:22: [2024-10-30 15:14:22] iter = 03670, loss = 2.5280
2024-10-30 15:14:26: [2024-10-30 15:14:26] iter = 03680, loss = 1.9726
2024-10-30 15:14:29: [2024-10-30 15:14:29] iter = 03690, loss = 2.6566
2024-10-30 15:14:33: [2024-10-30 15:14:33] iter = 03700, loss = 1.8167
2024-10-30 15:14:38: [2024-10-30 15:14:38] iter = 03710, loss = 2.2083
2024-10-30 15:14:42: [2024-10-30 15:14:42] iter = 03720, loss = 2.0356
2024-10-30 15:14:47: [2024-10-30 15:14:47] iter = 03730, loss = 2.3982
2024-10-30 15:14:51: [2024-10-30 15:14:51] iter = 03740, loss = 2.0135
2024-10-30 15:14:55: [2024-10-30 15:14:55] iter = 03750, loss = 1.8873
2024-10-30 15:14:59: [2024-10-30 15:14:59] iter = 03760, loss = 2.3223
2024-10-30 15:15:04: [2024-10-30 15:15:04] iter = 03770, loss = 1.8446
2024-10-30 15:15:08: [2024-10-30 15:15:08] iter = 03780, loss = 1.9361
2024-10-30 15:15:13: [2024-10-30 15:15:13] iter = 03790, loss = 2.4110
2024-10-30 15:15:17: [2024-10-30 15:15:17] iter = 03800, loss = 2.6136
2024-10-30 15:15:21: [2024-10-30 15:15:21] iter = 03810, loss = 3.3169
2024-10-30 15:15:26: [2024-10-30 15:15:26] iter = 03820, loss = 3.2808
2024-10-30 15:15:30: [2024-10-30 15:15:30] iter = 03830, loss = 2.5864
2024-10-30 15:15:35: [2024-10-30 15:15:35] iter = 03840, loss = 2.9065
2024-10-30 15:15:38: [2024-10-30 15:15:38] iter = 03850, loss = 2.3939
2024-10-30 15:15:43: [2024-10-30 15:15:43] iter = 03860, loss = 3.6381
2024-10-30 15:15:46: [2024-10-30 15:15:46] iter = 03870, loss = 2.4894
2024-10-30 15:15:51: [2024-10-30 15:15:51] iter = 03880, loss = 2.7951
2024-10-30 15:15:55: [2024-10-30 15:15:55] iter = 03890, loss = 2.0269
2024-10-30 15:15:59: [2024-10-30 15:15:59] iter = 03900, loss = 2.8588
2024-10-30 15:16:02: [2024-10-30 15:16:02] iter = 03910, loss = 1.7916
2024-10-30 15:16:05: [2024-10-30 15:16:05] iter = 03920, loss = 4.8677
2024-10-30 15:16:08: [2024-10-30 15:16:08] iter = 03930, loss = 1.8572
2024-10-30 15:16:10: [2024-10-30 15:16:10] iter = 03940, loss = 2.2576
2024-10-30 15:16:12: [2024-10-30 15:16:12] iter = 03950, loss = 3.3300
2024-10-30 15:16:14: [2024-10-30 15:16:14] iter = 03960, loss = 2.4420
2024-10-30 15:16:17: [2024-10-30 15:16:17] iter = 03970, loss = 2.3704
2024-10-30 15:16:20: [2024-10-30 15:16:20] iter = 03980, loss = 1.8334
2024-10-30 15:16:23: [2024-10-30 15:16:23] iter = 03990, loss = 2.1820
2024-10-30 15:16:26: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
2024-10-30 15:16:26: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 15:16:26: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 86297}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 15:18:46: Evaluate 5 random ConvNet, ACCmean = 0.7829 ACCstd = 0.0046
-------------------------
2024-10-30 15:18:46: Evaluate 5 random ConvNet, SENmean = 0.7819 SENstd = 0.0037
-------------------------
2024-10-30 15:18:46: Evaluate 5 random ConvNet, SPEmean = 0.9782 SPEstd = 0.0005
-------------------------
2024-10-30 15:18:46: Evaluate 5 random ConvNet, F!mean = 0.7684 F!std = 0.0041
-------------------------
2024-10-30 15:18:46: Evaluate 5 random ConvNet, mean = 0.7829 std = 0.0046
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 15:18:46: [2024-10-30 15:18:46] iter = 04000, loss = 2.4299
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 15:18:49: [2024-10-30 15:18:49] iter = 04010, loss = 2.3532
2024-10-30 15:18:53: [2024-10-30 15:18:53] iter = 04020, loss = 2.3073
2024-10-30 15:18:55: [2024-10-30 15:18:55] iter = 04030, loss = 3.1128
2024-10-30 15:18:58: [2024-10-30 15:18:58] iter = 04040, loss = 4.4443
2024-10-30 15:19:03: [2024-10-30 15:19:03] iter = 04050, loss = 2.0746
2024-10-30 15:19:06: [2024-10-30 15:19:06] iter = 04060, loss = 2.2341
2024-10-30 15:19:10: [2024-10-30 15:19:10] iter = 04070, loss = 2.3618
2024-10-30 15:19:14: [2024-10-30 15:19:14] iter = 04080, loss = 1.8437
2024-10-30 15:19:17: [2024-10-30 15:19:17] iter = 04090, loss = 1.9737
2024-10-30 15:19:21: [2024-10-30 15:19:21] iter = 04100, loss = 2.5327
2024-10-30 15:19:25: [2024-10-30 15:19:25] iter = 04110, loss = 2.9824
2024-10-30 15:19:29: [2024-10-30 15:19:29] iter = 04120, loss = 2.1059
2024-10-30 15:19:32: [2024-10-30 15:19:32] iter = 04130, loss = 2.7398
2024-10-30 15:19:36: [2024-10-30 15:19:36] iter = 04140, loss = 2.1005
2024-10-30 15:19:40: [2024-10-30 15:19:40] iter = 04150, loss = 3.1805
2024-10-30 15:19:44: [2024-10-30 15:19:44] iter = 04160, loss = 2.3671
2024-10-30 15:19:48: [2024-10-30 15:19:48] iter = 04170, loss = 2.4938
2024-10-30 15:19:51: [2024-10-30 15:19:51] iter = 04180, loss = 2.4840
2024-10-30 15:19:56: [2024-10-30 15:19:56] iter = 04190, loss = 3.1080
2024-10-30 15:20:00: [2024-10-30 15:20:00] iter = 04200, loss = 2.1874
2024-10-30 15:20:03: [2024-10-30 15:20:03] iter = 04210, loss = 2.2539
2024-10-30 15:20:07: [2024-10-30 15:20:07] iter = 04220, loss = 2.8020
2024-10-30 15:20:12: [2024-10-30 15:20:12] iter = 04230, loss = 2.0109
2024-10-30 15:20:16: [2024-10-30 15:20:16] iter = 04240, loss = 8.8461
2024-10-30 15:20:21: [2024-10-30 15:20:21] iter = 04250, loss = 2.4818
2024-10-30 15:20:25: [2024-10-30 15:20:25] iter = 04260, loss = 2.3614
2024-10-30 15:20:29: [2024-10-30 15:20:29] iter = 04270, loss = 7.4176
2024-10-30 15:20:33: [2024-10-30 15:20:33] iter = 04280, loss = 3.3629
2024-10-30 15:20:36: [2024-10-30 15:20:36] iter = 04290, loss = 6.5057
2024-10-30 15:20:40: [2024-10-30 15:20:40] iter = 04300, loss = 2.4826
2024-10-30 15:20:44: [2024-10-30 15:20:44] iter = 04310, loss = 3.2798
2024-10-30 15:20:48: [2024-10-30 15:20:48] iter = 04320, loss = 2.2753
2024-10-30 15:20:52: [2024-10-30 15:20:52] iter = 04330, loss = 2.7261
2024-10-30 15:20:56: [2024-10-30 15:20:56] iter = 04340, loss = 1.8801
2024-10-30 15:20:59: [2024-10-30 15:20:59] iter = 04350, loss = 2.2464
2024-10-30 15:21:03: [2024-10-30 15:21:03] iter = 04360, loss = 2.6852
2024-10-30 15:21:07: [2024-10-30 15:21:07] iter = 04370, loss = 2.0793
2024-10-30 15:21:11: [2024-10-30 15:21:11] iter = 04380, loss = 3.9518
2024-10-30 15:21:16: [2024-10-30 15:21:16] iter = 04390, loss = 2.0611
2024-10-30 15:21:20: [2024-10-30 15:21:20] iter = 04400, loss = 2.2993
2024-10-30 15:21:23: [2024-10-30 15:21:23] iter = 04410, loss = 2.0131
2024-10-30 15:21:26: [2024-10-30 15:21:26] iter = 04420, loss = 1.8856
2024-10-30 15:21:31: [2024-10-30 15:21:31] iter = 04430, loss = 2.1990
2024-10-30 15:21:35: [2024-10-30 15:21:35] iter = 04440, loss = 2.6950
2024-10-30 15:21:40: [2024-10-30 15:21:40] iter = 04450, loss = 4.1195
2024-10-30 15:21:43: [2024-10-30 15:21:43] iter = 04460, loss = 2.8976
2024-10-30 15:21:47: [2024-10-30 15:21:47] iter = 04470, loss = 2.0444
2024-10-30 15:21:53: [2024-10-30 15:21:53] iter = 04480, loss = 2.1463
2024-10-30 15:21:57: [2024-10-30 15:21:57] iter = 04490, loss = 2.1749
2024-10-30 15:22:01: [2024-10-30 15:22:00] iter = 04500, loss = 2.4545
2024-10-30 15:22:04: [2024-10-30 15:22:04] iter = 04510, loss = 2.0079
2024-10-30 15:22:07: [2024-10-30 15:22:07] iter = 04520, loss = 1.8561
2024-10-30 15:22:10: [2024-10-30 15:22:10] iter = 04530, loss = 3.5701
2024-10-30 15:22:14: [2024-10-30 15:22:14] iter = 04540, loss = 2.3766
2024-10-30 15:22:19: [2024-10-30 15:22:19] iter = 04550, loss = 2.1137
2024-10-30 15:22:22: [2024-10-30 15:22:22] iter = 04560, loss = 2.1564
2024-10-30 15:22:26: [2024-10-30 15:22:26] iter = 04570, loss = 1.9565
2024-10-30 15:22:31: [2024-10-30 15:22:31] iter = 04580, loss = 3.6233
2024-10-30 15:22:34: [2024-10-30 15:22:34] iter = 04590, loss = 2.0368
2024-10-30 15:22:39: [2024-10-30 15:22:39] iter = 04600, loss = 3.0066
2024-10-30 15:22:43: [2024-10-30 15:22:43] iter = 04610, loss = 2.3485
2024-10-30 15:22:47: [2024-10-30 15:22:47] iter = 04620, loss = 1.8664
2024-10-30 15:22:51: [2024-10-30 15:22:51] iter = 04630, loss = 2.2513
2024-10-30 15:22:54: [2024-10-30 15:22:54] iter = 04640, loss = 2.6895
2024-10-30 15:22:59: [2024-10-30 15:22:59] iter = 04650, loss = 1.8237
2024-10-30 15:23:03: [2024-10-30 15:23:03] iter = 04660, loss = 3.1782
2024-10-30 15:23:07: [2024-10-30 15:23:07] iter = 04670, loss = 2.3116
2024-10-30 15:23:11: [2024-10-30 15:23:11] iter = 04680, loss = 1.9875
2024-10-30 15:23:15: [2024-10-30 15:23:15] iter = 04690, loss = 2.2058
2024-10-30 15:23:18: [2024-10-30 15:23:18] iter = 04700, loss = 1.9696
2024-10-30 15:23:22: [2024-10-30 15:23:22] iter = 04710, loss = 2.6394
2024-10-30 15:23:26: [2024-10-30 15:23:26] iter = 04720, loss = 1.9347
2024-10-30 15:23:30: [2024-10-30 15:23:30] iter = 04730, loss = 2.3988
2024-10-30 15:23:34: [2024-10-30 15:23:34] iter = 04740, loss = 1.8700
2024-10-30 15:23:37: [2024-10-30 15:23:37] iter = 04750, loss = 2.4399
2024-10-30 15:23:41: [2024-10-30 15:23:41] iter = 04760, loss = 2.0959
2024-10-30 15:23:44: [2024-10-30 15:23:44] iter = 04770, loss = 2.5279
2024-10-30 15:23:46: [2024-10-30 15:23:46] iter = 04780, loss = 2.6245
2024-10-30 15:23:50: [2024-10-30 15:23:50] iter = 04790, loss = 3.4787
2024-10-30 15:23:53: [2024-10-30 15:23:53] iter = 04800, loss = 2.3558
2024-10-30 15:23:56: [2024-10-30 15:23:56] iter = 04810, loss = 1.8920
2024-10-30 15:23:59: [2024-10-30 15:23:59] iter = 04820, loss = 3.6540
2024-10-30 15:24:02: [2024-10-30 15:24:02] iter = 04830, loss = 2.4622
2024-10-30 15:24:06: [2024-10-30 15:24:06] iter = 04840, loss = 2.0251
2024-10-30 15:24:09: [2024-10-30 15:24:09] iter = 04850, loss = 3.6819
2024-10-30 15:24:12: [2024-10-30 15:24:12] iter = 04860, loss = 2.7581
2024-10-30 15:24:16: [2024-10-30 15:24:16] iter = 04870, loss = 2.1949
2024-10-30 15:24:19: [2024-10-30 15:24:19] iter = 04880, loss = 2.4134
2024-10-30 15:24:24: [2024-10-30 15:24:24] iter = 04890, loss = 3.4734
2024-10-30 15:24:28: [2024-10-30 15:24:28] iter = 04900, loss = 3.3926
2024-10-30 15:24:32: [2024-10-30 15:24:32] iter = 04910, loss = 2.3491
2024-10-30 15:24:36: [2024-10-30 15:24:36] iter = 04920, loss = 2.3600
2024-10-30 15:24:41: [2024-10-30 15:24:41] iter = 04930, loss = 2.1210
2024-10-30 15:24:45: [2024-10-30 15:24:45] iter = 04940, loss = 2.6137
2024-10-30 15:24:49: [2024-10-30 15:24:49] iter = 04950, loss = 4.5361
2024-10-30 15:24:53: [2024-10-30 15:24:53] iter = 04960, loss = 2.0711
2024-10-30 15:24:56: [2024-10-30 15:24:56] iter = 04970, loss = 1.8244
2024-10-30 15:25:00: [2024-10-30 15:25:00] iter = 04980, loss = 2.2278
2024-10-30 15:25:03: [2024-10-30 15:25:03] iter = 04990, loss = 2.0236
2024-10-30 15:25:06: [2024-10-30 15:25:06] iter = 05000, loss = 2.4960
2024-10-30 15:25:11: [2024-10-30 15:25:11] iter = 05010, loss = 2.8710
2024-10-30 15:25:15: [2024-10-30 15:25:15] iter = 05020, loss = 2.3875
2024-10-30 15:25:18: [2024-10-30 15:25:18] iter = 05030, loss = 2.2068
2024-10-30 15:25:21: [2024-10-30 15:25:21] iter = 05040, loss = 2.4314
2024-10-30 15:25:24: [2024-10-30 15:25:24] iter = 05050, loss = 2.5124
2024-10-30 15:25:28: [2024-10-30 15:25:28] iter = 05060, loss = 2.1367
2024-10-30 15:25:33: [2024-10-30 15:25:33] iter = 05070, loss = 3.0596
2024-10-30 15:25:37: [2024-10-30 15:25:37] iter = 05080, loss = 2.9400
2024-10-30 15:25:41: [2024-10-30 15:25:41] iter = 05090, loss = 2.3146
2024-10-30 15:25:45: [2024-10-30 15:25:45] iter = 05100, loss = 2.3312
2024-10-30 15:25:50: [2024-10-30 15:25:50] iter = 05110, loss = 2.5248
2024-10-30 15:25:54: [2024-10-30 15:25:54] iter = 05120, loss = 3.5690
2024-10-30 15:25:59: [2024-10-30 15:25:59] iter = 05130, loss = 4.3053
2024-10-30 15:26:02: [2024-10-30 15:26:02] iter = 05140, loss = 2.0539
2024-10-30 15:26:06: [2024-10-30 15:26:06] iter = 05150, loss = 2.2344
2024-10-30 15:26:10: [2024-10-30 15:26:10] iter = 05160, loss = 1.9113
2024-10-30 15:26:14: [2024-10-30 15:26:14] iter = 05170, loss = 1.9975
2024-10-30 15:26:19: [2024-10-30 15:26:19] iter = 05180, loss = 2.1531
2024-10-30 15:26:23: [2024-10-30 15:26:23] iter = 05190, loss = 2.5157
2024-10-30 15:26:27: [2024-10-30 15:26:27] iter = 05200, loss = 2.6140
2024-10-30 15:26:31: [2024-10-30 15:26:31] iter = 05210, loss = 2.2146
2024-10-30 15:26:35: [2024-10-30 15:26:35] iter = 05220, loss = 2.1752
2024-10-30 15:26:39: [2024-10-30 15:26:39] iter = 05230, loss = 2.5978
2024-10-30 15:26:44: [2024-10-30 15:26:44] iter = 05240, loss = 2.5998
2024-10-30 15:26:48: [2024-10-30 15:26:48] iter = 05250, loss = 2.0127
2024-10-30 15:26:51: [2024-10-30 15:26:51] iter = 05260, loss = 2.4473
2024-10-30 15:26:56: [2024-10-30 15:26:56] iter = 05270, loss = 1.7991
2024-10-30 15:27:01: [2024-10-30 15:27:01] iter = 05280, loss = 2.3626
2024-10-30 15:27:04: [2024-10-30 15:27:04] iter = 05290, loss = 2.6397
2024-10-30 15:27:09: [2024-10-30 15:27:09] iter = 05300, loss = 2.4830
2024-10-30 15:27:13: [2024-10-30 15:27:13] iter = 05310, loss = 1.8899
2024-10-30 15:27:17: [2024-10-30 15:27:17] iter = 05320, loss = 2.2940
2024-10-30 15:27:21: [2024-10-30 15:27:21] iter = 05330, loss = 3.1521
2024-10-30 15:27:25: [2024-10-30 15:27:25] iter = 05340, loss = 2.1386
2024-10-30 15:27:29: [2024-10-30 15:27:29] iter = 05350, loss = 3.4641
2024-10-30 15:27:32: [2024-10-30 15:27:32] iter = 05360, loss = 2.9086
2024-10-30 15:27:36: [2024-10-30 15:27:36] iter = 05370, loss = 2.3080
2024-10-30 15:27:40: [2024-10-30 15:27:40] iter = 05380, loss = 2.3993
2024-10-30 15:27:45: [2024-10-30 15:27:45] iter = 05390, loss = 2.3612
2024-10-30 15:27:49: [2024-10-30 15:27:49] iter = 05400, loss = 2.7713
2024-10-30 15:27:52: [2024-10-30 15:27:52] iter = 05410, loss = 1.9420
2024-10-30 15:27:57: [2024-10-30 15:27:57] iter = 05420, loss = 3.6957
2024-10-30 15:28:00: [2024-10-30 15:28:00] iter = 05430, loss = 1.9656
2024-10-30 15:28:04: [2024-10-30 15:28:04] iter = 05440, loss = 1.8336
2024-10-30 15:28:08: [2024-10-30 15:28:08] iter = 05450, loss = 2.0490
2024-10-30 15:28:12: [2024-10-30 15:28:12] iter = 05460, loss = 2.1756
2024-10-30 15:28:16: [2024-10-30 15:28:16] iter = 05470, loss = 2.2139
2024-10-30 15:28:19: [2024-10-30 15:28:19] iter = 05480, loss = 3.0931
2024-10-30 15:28:23: [2024-10-30 15:28:23] iter = 05490, loss = 2.2207
2024-10-30 15:28:26: [2024-10-30 15:28:26] iter = 05500, loss = 3.9302
2024-10-30 15:28:31: [2024-10-30 15:28:31] iter = 05510, loss = 1.8384
2024-10-30 15:28:35: [2024-10-30 15:28:35] iter = 05520, loss = 2.0605
2024-10-30 15:28:39: [2024-10-30 15:28:39] iter = 05530, loss = 2.0848
2024-10-30 15:28:43: [2024-10-30 15:28:43] iter = 05540, loss = 1.8352
2024-10-30 15:28:46: [2024-10-30 15:28:46] iter = 05550, loss = 2.2382
2024-10-30 15:28:50: [2024-10-30 15:28:50] iter = 05560, loss = 1.9174
2024-10-30 15:28:53: [2024-10-30 15:28:53] iter = 05570, loss = 2.3215
2024-10-30 15:28:57: [2024-10-30 15:28:57] iter = 05580, loss = 1.9239
2024-10-30 15:29:00: [2024-10-30 15:29:00] iter = 05590, loss = 2.1890
2024-10-30 15:29:03: [2024-10-30 15:29:03] iter = 05600, loss = 2.3140
2024-10-30 15:29:07: [2024-10-30 15:29:07] iter = 05610, loss = 2.3203
2024-10-30 15:29:11: [2024-10-30 15:29:11] iter = 05620, loss = 2.0225
2024-10-30 15:29:15: [2024-10-30 15:29:15] iter = 05630, loss = 2.6546
2024-10-30 15:29:18: [2024-10-30 15:29:18] iter = 05640, loss = 2.9649
2024-10-30 15:29:22: [2024-10-30 15:29:22] iter = 05650, loss = 2.3746
2024-10-30 15:29:26: [2024-10-30 15:29:26] iter = 05660, loss = 2.9678
2024-10-30 15:29:29: [2024-10-30 15:29:29] iter = 05670, loss = 2.6861
2024-10-30 15:29:33: [2024-10-30 15:29:33] iter = 05680, loss = 2.2633
2024-10-30 15:29:37: [2024-10-30 15:29:37] iter = 05690, loss = 2.1167
2024-10-30 15:29:42: [2024-10-30 15:29:42] iter = 05700, loss = 2.5265
2024-10-30 15:29:46: [2024-10-30 15:29:46] iter = 05710, loss = 2.2352
2024-10-30 15:29:50: [2024-10-30 15:29:50] iter = 05720, loss = 2.1300
2024-10-30 15:29:54: [2024-10-30 15:29:54] iter = 05730, loss = 2.1474
2024-10-30 15:29:58: [2024-10-30 15:29:58] iter = 05740, loss = 2.2593
2024-10-30 15:30:01: [2024-10-30 15:30:01] iter = 05750, loss = 2.1876
2024-10-30 15:30:04: [2024-10-30 15:30:04] iter = 05760, loss = 2.5621
2024-10-30 15:30:08: [2024-10-30 15:30:08] iter = 05770, loss = 2.3664
2024-10-30 15:30:12: [2024-10-30 15:30:12] iter = 05780, loss = 2.5633
2024-10-30 15:30:16: [2024-10-30 15:30:16] iter = 05790, loss = 2.5759
2024-10-30 15:30:20: [2024-10-30 15:30:20] iter = 05800, loss = 1.8038
2024-10-30 15:30:22: [2024-10-30 15:30:22] iter = 05810, loss = 1.5308
2024-10-30 15:30:25: [2024-10-30 15:30:25] iter = 05820, loss = 2.1066
2024-10-30 15:30:29: [2024-10-30 15:30:29] iter = 05830, loss = 7.8483
2024-10-30 15:30:32: [2024-10-30 15:30:32] iter = 05840, loss = 3.9433
2024-10-30 15:30:35: [2024-10-30 15:30:35] iter = 05850, loss = 2.3467
2024-10-30 15:30:39: [2024-10-30 15:30:39] iter = 05860, loss = 1.9285
2024-10-30 15:30:42: [2024-10-30 15:30:42] iter = 05870, loss = 2.7446
2024-10-30 15:30:45: [2024-10-30 15:30:45] iter = 05880, loss = 2.1325
2024-10-30 15:30:50: [2024-10-30 15:30:50] iter = 05890, loss = 2.5000
2024-10-30 15:30:54: [2024-10-30 15:30:54] iter = 05900, loss = 2.8950
2024-10-30 15:30:58: [2024-10-30 15:30:58] iter = 05910, loss = 2.1821
2024-10-30 15:31:01: [2024-10-30 15:31:01] iter = 05920, loss = 2.2286
2024-10-30 15:31:06: [2024-10-30 15:31:06] iter = 05930, loss = 2.5945
2024-10-30 15:31:09: [2024-10-30 15:31:09] iter = 05940, loss = 2.0305
2024-10-30 15:31:12: [2024-10-30 15:31:12] iter = 05950, loss = 1.8011
2024-10-30 15:31:16: [2024-10-30 15:31:16] iter = 05960, loss = 3.0735
2024-10-30 15:31:20: [2024-10-30 15:31:20] iter = 05970, loss = 2.4257
2024-10-30 15:31:24: [2024-10-30 15:31:24] iter = 05980, loss = 2.8735
2024-10-30 15:31:29: [2024-10-30 15:31:29] iter = 05990, loss = 2.9986
2024-10-30 15:31:33: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
2024-10-30 15:31:33: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 15:31:33: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 93543}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 15:33:51: Evaluate 5 random ConvNet, ACCmean = 0.7815 ACCstd = 0.0033
-------------------------
2024-10-30 15:33:51: Evaluate 5 random ConvNet, SENmean = 0.7748 SENstd = 0.0034
-------------------------
2024-10-30 15:33:51: Evaluate 5 random ConvNet, SPEmean = 0.9781 SPEstd = 0.0003
-------------------------
2024-10-30 15:33:51: Evaluate 5 random ConvNet, F!mean = 0.7651 F!std = 0.0037
-------------------------
2024-10-30 15:33:51: Evaluate 5 random ConvNet, mean = 0.7815 std = 0.0033
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 15:33:52: [2024-10-30 15:33:52] iter = 06000, loss = 2.7122
2024-10-30 15:33:56: [2024-10-30 15:33:56] iter = 06010, loss = 2.3681
2024-10-30 15:33:59: [2024-10-30 15:33:59] iter = 06020, loss = 2.3868
2024-10-30 15:34:03: [2024-10-30 15:34:03] iter = 06030, loss = 2.0342
2024-10-30 15:34:07: [2024-10-30 15:34:07] iter = 06040, loss = 1.9265
2024-10-30 15:34:11: [2024-10-30 15:34:11] iter = 06050, loss = 2.7695
2024-10-30 15:34:15: [2024-10-30 15:34:15] iter = 06060, loss = 1.9254
2024-10-30 15:34:17: [2024-10-30 15:34:17] iter = 06070, loss = 2.4694
2024-10-30 15:34:20: [2024-10-30 15:34:20] iter = 06080, loss = 2.5795
2024-10-30 15:34:23: [2024-10-30 15:34:23] iter = 06090, loss = 2.2221
2024-10-30 15:34:27: [2024-10-30 15:34:27] iter = 06100, loss = 1.7166
2024-10-30 15:34:31: [2024-10-30 15:34:31] iter = 06110, loss = 2.4400
2024-10-30 15:34:34: [2024-10-30 15:34:34] iter = 06120, loss = 2.1417
2024-10-30 15:34:38: [2024-10-30 15:34:38] iter = 06130, loss = 2.4282
2024-10-30 15:34:42: [2024-10-30 15:34:42] iter = 06140, loss = 2.0915
2024-10-30 15:34:45: [2024-10-30 15:34:45] iter = 06150, loss = 2.4661
2024-10-30 15:34:50: [2024-10-30 15:34:50] iter = 06160, loss = 1.7631
2024-10-30 15:34:54: [2024-10-30 15:34:54] iter = 06170, loss = 2.0399
2024-10-30 15:34:58: [2024-10-30 15:34:58] iter = 06180, loss = 2.7300
2024-10-30 15:35:01: [2024-10-30 15:35:01] iter = 06190, loss = 2.7867
2024-10-30 15:35:05: [2024-10-30 15:35:05] iter = 06200, loss = 1.9817
2024-10-30 15:35:08: [2024-10-30 15:35:08] iter = 06210, loss = 1.9719
2024-10-30 15:35:12: [2024-10-30 15:35:12] iter = 06220, loss = 2.0876
2024-10-30 15:35:15: [2024-10-30 15:35:15] iter = 06230, loss = 2.8955
2024-10-30 15:35:20: [2024-10-30 15:35:20] iter = 06240, loss = 2.1851
2024-10-30 15:35:23: [2024-10-30 15:35:23] iter = 06250, loss = 1.7108
2024-10-30 15:35:27: [2024-10-30 15:35:27] iter = 06260, loss = 2.3197
2024-10-30 15:35:30: [2024-10-30 15:35:30] iter = 06270, loss = 1.8700
2024-10-30 15:35:33: [2024-10-30 15:35:33] iter = 06280, loss = 1.8636
2024-10-30 15:35:37: [2024-10-30 15:35:37] iter = 06290, loss = 2.4142
2024-10-30 15:35:40: [2024-10-30 15:35:40] iter = 06300, loss = 3.5302
2024-10-30 15:35:44: [2024-10-30 15:35:44] iter = 06310, loss = 2.5205
2024-10-30 15:35:48: [2024-10-30 15:35:48] iter = 06320, loss = 2.3250
2024-10-30 15:35:52: [2024-10-30 15:35:52] iter = 06330, loss = 2.5022
2024-10-30 15:35:55: [2024-10-30 15:35:55] iter = 06340, loss = 2.2594
2024-10-30 15:35:58: [2024-10-30 15:35:58] iter = 06350, loss = 3.7695
2024-10-30 15:36:02: [2024-10-30 15:36:02] iter = 06360, loss = 1.7873
2024-10-30 15:36:05: [2024-10-30 15:36:05] iter = 06370, loss = 2.1053
2024-10-30 15:36:09: [2024-10-30 15:36:09] iter = 06380, loss = 5.3604
2024-10-30 15:36:13: [2024-10-30 15:36:13] iter = 06390, loss = 2.3362
2024-10-30 15:36:17: [2024-10-30 15:36:17] iter = 06400, loss = 2.3859
2024-10-30 15:36:20: [2024-10-30 15:36:20] iter = 06410, loss = 2.0425
2024-10-30 15:36:24: [2024-10-30 15:36:24] iter = 06420, loss = 2.2329
2024-10-30 15:36:28: [2024-10-30 15:36:28] iter = 06430, loss = 2.6318
2024-10-30 15:36:32: [2024-10-30 15:36:32] iter = 06440, loss = 4.5581
2024-10-30 15:36:37: [2024-10-30 15:36:37] iter = 06450, loss = 1.8238
2024-10-30 15:36:41: [2024-10-30 15:36:41] iter = 06460, loss = 2.5698
2024-10-30 15:36:44: [2024-10-30 15:36:44] iter = 06470, loss = 2.3287
2024-10-30 15:36:48: [2024-10-30 15:36:48] iter = 06480, loss = 2.6398
2024-10-30 15:36:52: [2024-10-30 15:36:52] iter = 06490, loss = 3.9394
2024-10-30 15:36:56: [2024-10-30 15:36:56] iter = 06500, loss = 2.1723
2024-10-30 15:36:59: [2024-10-30 15:36:59] iter = 06510, loss = 2.0027
2024-10-30 15:37:03: [2024-10-30 15:37:03] iter = 06520, loss = 2.9679
2024-10-30 15:37:07: [2024-10-30 15:37:07] iter = 06530, loss = 1.9023
2024-10-30 15:37:11: [2024-10-30 15:37:11] iter = 06540, loss = 2.8052
2024-10-30 15:37:14: [2024-10-30 15:37:14] iter = 06550, loss = 2.1933
2024-10-30 15:37:19: [2024-10-30 15:37:19] iter = 06560, loss = 2.2502
2024-10-30 15:37:21: [2024-10-30 15:37:21] iter = 06570, loss = 2.2526
2024-10-30 15:37:23: [2024-10-30 15:37:23] iter = 06580, loss = 2.3730
2024-10-30 15:37:26: [2024-10-30 15:37:26] iter = 06590, loss = 2.5451
2024-10-30 15:37:30: [2024-10-30 15:37:30] iter = 06600, loss = 1.9420
2024-10-30 15:37:33: [2024-10-30 15:37:33] iter = 06610, loss = 2.2457
2024-10-30 15:37:37: [2024-10-30 15:37:37] iter = 06620, loss = 1.9225
2024-10-30 15:37:40: [2024-10-30 15:37:40] iter = 06630, loss = 2.3465
2024-10-30 15:37:43: [2024-10-30 15:37:43] iter = 06640, loss = 1.9828
2024-10-30 15:37:47: [2024-10-30 15:37:47] iter = 06650, loss = 1.9749
2024-10-30 15:37:50: [2024-10-30 15:37:50] iter = 06660, loss = 2.5479
2024-10-30 15:37:54: [2024-10-30 15:37:54] iter = 06670, loss = 2.6668
2024-10-30 15:37:57: [2024-10-30 15:37:57] iter = 06680, loss = 2.3321
2024-10-30 15:38:01: [2024-10-30 15:38:01] iter = 06690, loss = 1.8423
2024-10-30 15:38:05: [2024-10-30 15:38:05] iter = 06700, loss = 2.8052
2024-10-30 15:38:09: [2024-10-30 15:38:09] iter = 06710, loss = 2.3872
2024-10-30 15:38:14: [2024-10-30 15:38:14] iter = 06720, loss = 2.0762
2024-10-30 15:38:19: [2024-10-30 15:38:19] iter = 06730, loss = 2.0327
2024-10-30 15:38:23: [2024-10-30 15:38:23] iter = 06740, loss = 2.8182
2024-10-30 15:38:26: [2024-10-30 15:38:26] iter = 06750, loss = 3.0898
2024-10-30 15:38:29: [2024-10-30 15:38:29] iter = 06760, loss = 1.9085
2024-10-30 15:38:32: [2024-10-30 15:38:32] iter = 06770, loss = 6.5208
2024-10-30 15:38:36: [2024-10-30 15:38:36] iter = 06780, loss = 2.2309
2024-10-30 15:38:39: [2024-10-30 15:38:39] iter = 06790, loss = 2.0764
2024-10-30 15:38:42: [2024-10-30 15:38:42] iter = 06800, loss = 1.8291
2024-10-30 15:38:46: [2024-10-30 15:38:46] iter = 06810, loss = 2.6006
2024-10-30 15:38:50: [2024-10-30 15:38:50] iter = 06820, loss = 3.3360
2024-10-30 15:38:53: [2024-10-30 15:38:53] iter = 06830, loss = 4.4023
2024-10-30 15:38:57: [2024-10-30 15:38:57] iter = 06840, loss = 3.0496
2024-10-30 15:39:01: [2024-10-30 15:39:01] iter = 06850, loss = 2.2575
2024-10-30 15:39:05: [2024-10-30 15:39:05] iter = 06860, loss = 2.2105
2024-10-30 15:39:09: [2024-10-30 15:39:09] iter = 06870, loss = 2.7143
2024-10-30 15:39:12: [2024-10-30 15:39:12] iter = 06880, loss = 1.8206
2024-10-30 15:39:16: [2024-10-30 15:39:16] iter = 06890, loss = 1.9195
2024-10-30 15:39:20: [2024-10-30 15:39:20] iter = 06900, loss = 2.2469
2024-10-30 15:39:24: [2024-10-30 15:39:24] iter = 06910, loss = 2.2371
2024-10-30 15:39:28: [2024-10-30 15:39:28] iter = 06920, loss = 2.4161
2024-10-30 15:39:32: [2024-10-30 15:39:32] iter = 06930, loss = 2.5611
2024-10-30 15:39:35: [2024-10-30 15:39:35] iter = 06940, loss = 2.1901
2024-10-30 15:39:39: [2024-10-30 15:39:39] iter = 06950, loss = 2.5597
2024-10-30 15:39:43: [2024-10-30 15:39:43] iter = 06960, loss = 3.2002
2024-10-30 15:39:47: [2024-10-30 15:39:47] iter = 06970, loss = 2.1484
2024-10-30 15:39:51: [2024-10-30 15:39:51] iter = 06980, loss = 3.1263
2024-10-30 15:39:54: [2024-10-30 15:39:54] iter = 06990, loss = 2.2610
2024-10-30 15:39:58: [2024-10-30 15:39:58] iter = 07000, loss = 2.0610
2024-10-30 15:40:03: [2024-10-30 15:40:03] iter = 07010, loss = 3.0794
2024-10-30 15:40:07: [2024-10-30 15:40:07] iter = 07020, loss = 2.1884
2024-10-30 15:40:10: [2024-10-30 15:40:10] iter = 07030, loss = 3.1031
2024-10-30 15:40:15: [2024-10-30 15:40:15] iter = 07040, loss = 2.8365
2024-10-30 15:40:18: [2024-10-30 15:40:18] iter = 07050, loss = 2.6017
2024-10-30 15:40:22: [2024-10-30 15:40:22] iter = 07060, loss = 2.7000
2024-10-30 15:40:25: [2024-10-30 15:40:25] iter = 07070, loss = 2.2401
2024-10-30 15:40:29: [2024-10-30 15:40:29] iter = 07080, loss = 2.0049
2024-10-30 15:40:32: [2024-10-30 15:40:32] iter = 07090, loss = 2.5025
2024-10-30 15:40:36: [2024-10-30 15:40:36] iter = 07100, loss = 2.2078
2024-10-30 15:40:40: [2024-10-30 15:40:40] iter = 07110, loss = 1.7120
2024-10-30 15:40:43: [2024-10-30 15:40:43] iter = 07120, loss = 2.3528
2024-10-30 15:40:47: [2024-10-30 15:40:47] iter = 07130, loss = 1.9018
2024-10-30 15:40:50: [2024-10-30 15:40:50] iter = 07140, loss = 2.0922
2024-10-30 15:40:54: [2024-10-30 15:40:54] iter = 07150, loss = 1.8359
2024-10-30 15:40:57: [2024-10-30 15:40:57] iter = 07160, loss = 2.0345
2024-10-30 15:41:01: [2024-10-30 15:41:01] iter = 07170, loss = 2.6622
2024-10-30 15:41:05: [2024-10-30 15:41:05] iter = 07180, loss = 2.1596
2024-10-30 15:41:09: [2024-10-30 15:41:09] iter = 07190, loss = 2.0532
2024-10-30 15:41:12: [2024-10-30 15:41:12] iter = 07200, loss = 1.9725
2024-10-30 15:41:16: [2024-10-30 15:41:16] iter = 07210, loss = 1.7021
2024-10-30 15:41:19: [2024-10-30 15:41:19] iter = 07220, loss = 2.4007
2024-10-30 15:41:23: [2024-10-30 15:41:23] iter = 07230, loss = 1.9189
2024-10-30 15:41:27: [2024-10-30 15:41:27] iter = 07240, loss = 2.5167
2024-10-30 15:41:30: [2024-10-30 15:41:30] iter = 07250, loss = 1.9990
2024-10-30 15:41:34: [2024-10-30 15:41:34] iter = 07260, loss = 2.4921
2024-10-30 15:41:38: [2024-10-30 15:41:38] iter = 07270, loss = 2.8370
2024-10-30 15:41:42: [2024-10-30 15:41:42] iter = 07280, loss = 3.6165
2024-10-30 15:41:47: [2024-10-30 15:41:47] iter = 07290, loss = 1.7967
2024-10-30 15:41:51: [2024-10-30 15:41:51] iter = 07300, loss = 2.3641
2024-10-30 15:41:54: [2024-10-30 15:41:54] iter = 07310, loss = 2.9578
2024-10-30 15:41:57: [2024-10-30 15:41:57] iter = 07320, loss = 3.1222
2024-10-30 15:42:01: [2024-10-30 15:42:01] iter = 07330, loss = 1.8265
2024-10-30 15:42:04: [2024-10-30 15:42:04] iter = 07340, loss = 3.5603
2024-10-30 15:42:07: [2024-10-30 15:42:07] iter = 07350, loss = 2.1435
2024-10-30 15:42:11: [2024-10-30 15:42:11] iter = 07360, loss = 2.9105
2024-10-30 15:42:14: [2024-10-30 15:42:14] iter = 07370, loss = 2.0389
2024-10-30 15:42:18: [2024-10-30 15:42:18] iter = 07380, loss = 2.5840
2024-10-30 15:42:21: [2024-10-30 15:42:21] iter = 07390, loss = 2.1560
2024-10-30 15:42:25: [2024-10-30 15:42:25] iter = 07400, loss = 2.2145
2024-10-30 15:42:29: [2024-10-30 15:42:29] iter = 07410, loss = 2.8998
2024-10-30 15:42:32: [2024-10-30 15:42:32] iter = 07420, loss = 2.0388
2024-10-30 15:42:36: [2024-10-30 15:42:36] iter = 07430, loss = 2.8671
2024-10-30 15:42:40: [2024-10-30 15:42:40] iter = 07440, loss = 2.2486
2024-10-30 15:42:44: [2024-10-30 15:42:44] iter = 07450, loss = 2.5953
2024-10-30 15:42:48: [2024-10-30 15:42:48] iter = 07460, loss = 2.6219
2024-10-30 15:42:52: [2024-10-30 15:42:52] iter = 07470, loss = 2.2028
2024-10-30 15:42:56: [2024-10-30 15:42:56] iter = 07480, loss = 2.6565
2024-10-30 15:42:59: [2024-10-30 15:42:59] iter = 07490, loss = 2.9822
2024-10-30 15:43:01: [2024-10-30 15:43:01] iter = 07500, loss = 2.5781
2024-10-30 15:43:05: [2024-10-30 15:43:05] iter = 07510, loss = 2.3669
2024-10-30 15:43:09: [2024-10-30 15:43:09] iter = 07520, loss = 2.2938
2024-10-30 15:43:13: [2024-10-30 15:43:13] iter = 07530, loss = 2.2748
2024-10-30 15:43:16: [2024-10-30 15:43:16] iter = 07540, loss = 1.7532
2024-10-30 15:43:21: [2024-10-30 15:43:21] iter = 07550, loss = 3.6814
2024-10-30 15:43:25: [2024-10-30 15:43:25] iter = 07560, loss = 2.3900
2024-10-30 15:43:29: [2024-10-30 15:43:29] iter = 07570, loss = 2.3016
2024-10-30 15:43:32: [2024-10-30 15:43:32] iter = 07580, loss = 1.6981
2024-10-30 15:43:35: [2024-10-30 15:43:35] iter = 07590, loss = 2.8851
2024-10-30 15:43:39: [2024-10-30 15:43:39] iter = 07600, loss = 3.9028
2024-10-30 15:43:42: [2024-10-30 15:43:42] iter = 07610, loss = 2.0192
2024-10-30 15:43:46: [2024-10-30 15:43:46] iter = 07620, loss = 2.4905
2024-10-30 15:43:49: [2024-10-30 15:43:49] iter = 07630, loss = 3.4815
2024-10-30 15:43:53: [2024-10-30 15:43:53] iter = 07640, loss = 2.4809
2024-10-30 15:43:57: [2024-10-30 15:43:57] iter = 07650, loss = 1.9991
2024-10-30 15:44:00: [2024-10-30 15:44:00] iter = 07660, loss = 3.8289
2024-10-30 15:44:04: [2024-10-30 15:44:04] iter = 07670, loss = 2.0242
2024-10-30 15:44:07: [2024-10-30 15:44:07] iter = 07680, loss = 2.2330
2024-10-30 15:44:11: [2024-10-30 15:44:11] iter = 07690, loss = 2.3744
2024-10-30 15:44:15: [2024-10-30 15:44:15] iter = 07700, loss = 2.8461
2024-10-30 15:44:19: [2024-10-30 15:44:19] iter = 07710, loss = 2.6109
2024-10-30 15:44:23: [2024-10-30 15:44:23] iter = 07720, loss = 1.7961
2024-10-30 15:44:27: [2024-10-30 15:44:27] iter = 07730, loss = 1.7467
2024-10-30 15:44:32: [2024-10-30 15:44:32] iter = 07740, loss = 1.9898
2024-10-30 15:44:36: [2024-10-30 15:44:36] iter = 07750, loss = 2.4325
2024-10-30 15:44:40: [2024-10-30 15:44:40] iter = 07760, loss = 2.5511
2024-10-30 15:44:44: [2024-10-30 15:44:44] iter = 07770, loss = 2.2084
2024-10-30 15:44:47: [2024-10-30 15:44:47] iter = 07780, loss = 2.9357
2024-10-30 15:44:51: [2024-10-30 15:44:51] iter = 07790, loss = 3.3722
2024-10-30 15:44:54: [2024-10-30 15:44:54] iter = 07800, loss = 2.4564
2024-10-30 15:44:58: [2024-10-30 15:44:58] iter = 07810, loss = 2.2784
2024-10-30 15:45:02: [2024-10-30 15:45:02] iter = 07820, loss = 2.1694
2024-10-30 15:45:06: [2024-10-30 15:45:06] iter = 07830, loss = 2.0791
2024-10-30 15:45:10: [2024-10-30 15:45:10] iter = 07840, loss = 2.4716
2024-10-30 15:45:14: [2024-10-30 15:45:14] iter = 07850, loss = 2.8938
2024-10-30 15:45:18: [2024-10-30 15:45:18] iter = 07860, loss = 2.2018
2024-10-30 15:45:23: [2024-10-30 15:45:23] iter = 07870, loss = 1.9219
2024-10-30 15:45:27: [2024-10-30 15:45:27] iter = 07880, loss = 2.1543
2024-10-30 15:45:31: [2024-10-30 15:45:31] iter = 07890, loss = 2.1184
2024-10-30 15:45:35: [2024-10-30 15:45:35] iter = 07900, loss = 2.1573
2024-10-30 15:45:39: [2024-10-30 15:45:39] iter = 07910, loss = 1.7332
2024-10-30 15:45:40: [2024-10-30 15:45:40] iter = 07920, loss = 2.2273
2024-10-30 15:45:45: [2024-10-30 15:45:45] iter = 07930, loss = 2.5134
2024-10-30 15:45:48: [2024-10-30 15:45:48] iter = 07940, loss = 2.4356
2024-10-30 15:45:52: [2024-10-30 15:45:52] iter = 07950, loss = 2.3392
2024-10-30 15:45:57: [2024-10-30 15:45:57] iter = 07960, loss = 2.7506
2024-10-30 15:46:01: [2024-10-30 15:46:01] iter = 07970, loss = 2.1890
2024-10-30 15:46:05: [2024-10-30 15:46:05] iter = 07980, loss = 2.2319
2024-10-30 15:46:09: [2024-10-30 15:46:09] iter = 07990, loss = 2.1758
2024-10-30 15:46:12: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
2024-10-30 15:46:12: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 15:46:12: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 72833}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 15:48:46: Evaluate 5 random ConvNet, ACCmean = 0.7815 ACCstd = 0.0062
-------------------------
2024-10-30 15:48:46: Evaluate 5 random ConvNet, SENmean = 0.7828 SENstd = 0.0068
-------------------------
2024-10-30 15:48:46: Evaluate 5 random ConvNet, SPEmean = 0.9781 SPEstd = 0.0006
-------------------------
2024-10-30 15:48:46: Evaluate 5 random ConvNet, F!mean = 0.7693 F!std = 0.0070
-------------------------
2024-10-30 15:48:46: Evaluate 5 random ConvNet, mean = 0.7815 std = 0.0062
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 15:48:46: [2024-10-30 15:48:46] iter = 08000, loss = 1.6555
2024-10-30 15:48:49: [2024-10-30 15:48:49] iter = 08010, loss = 1.8976
2024-10-30 15:48:53: [2024-10-30 15:48:53] iter = 08020, loss = 2.4756
2024-10-30 15:48:56: [2024-10-30 15:48:56] iter = 08030, loss = 2.9565
2024-10-30 15:49:00: [2024-10-30 15:49:00] iter = 08040, loss = 1.9684
2024-10-30 15:49:04: [2024-10-30 15:49:04] iter = 08050, loss = 1.9474
2024-10-30 15:49:07: [2024-10-30 15:49:07] iter = 08060, loss = 1.9475
2024-10-30 15:49:11: [2024-10-30 15:49:11] iter = 08070, loss = 2.9333
2024-10-30 15:49:14: [2024-10-30 15:49:14] iter = 08080, loss = 2.1686
2024-10-30 15:49:18: [2024-10-30 15:49:18] iter = 08090, loss = 1.9123
2024-10-30 15:49:23: [2024-10-30 15:49:23] iter = 08100, loss = 2.2247
2024-10-30 15:49:28: [2024-10-30 15:49:28] iter = 08110, loss = 1.8085
2024-10-30 15:49:32: [2024-10-30 15:49:32] iter = 08120, loss = 2.0116
2024-10-30 15:49:37: [2024-10-30 15:49:37] iter = 08130, loss = 2.2381
2024-10-30 15:49:41: [2024-10-30 15:49:41] iter = 08140, loss = 2.1179
2024-10-30 15:49:46: [2024-10-30 15:49:46] iter = 08150, loss = 1.8966
2024-10-30 15:49:50: [2024-10-30 15:49:50] iter = 08160, loss = 2.4420
2024-10-30 15:49:55: [2024-10-30 15:49:55] iter = 08170, loss = 1.8243
2024-10-30 15:49:59: [2024-10-30 15:49:59] iter = 08180, loss = 2.0851
2024-10-30 15:50:04: [2024-10-30 15:50:04] iter = 08190, loss = 2.1689
2024-10-30 15:50:08: [2024-10-30 15:50:08] iter = 08200, loss = 2.8307
2024-10-30 15:50:13: [2024-10-30 15:50:13] iter = 08210, loss = 2.7271
2024-10-30 15:50:17: [2024-10-30 15:50:17] iter = 08220, loss = 2.3902
2024-10-30 15:50:21: [2024-10-30 15:50:21] iter = 08230, loss = 2.9039
2024-10-30 15:50:25: [2024-10-30 15:50:25] iter = 08240, loss = 2.0746
2024-10-30 15:50:28: [2024-10-30 15:50:28] iter = 08250, loss = 2.3765
2024-10-30 15:50:34: [2024-10-30 15:50:34] iter = 08260, loss = 2.2682
2024-10-30 15:50:38: [2024-10-30 15:50:38] iter = 08270, loss = 2.0808
2024-10-30 15:50:43: [2024-10-30 15:50:43] iter = 08280, loss = 2.5416
2024-10-30 15:50:47: [2024-10-30 15:50:47] iter = 08290, loss = 2.0531
2024-10-30 15:50:50: [2024-10-30 15:50:50] iter = 08300, loss = 1.8582
2024-10-30 15:50:54: [2024-10-30 15:50:54] iter = 08310, loss = 1.8736
2024-10-30 15:50:58: [2024-10-30 15:50:58] iter = 08320, loss = 1.7832
2024-10-30 15:51:02: [2024-10-30 15:51:02] iter = 08330, loss = 1.8239
2024-10-30 15:51:06: [2024-10-30 15:51:06] iter = 08340, loss = 2.9482
2024-10-30 15:51:10: [2024-10-30 15:51:10] iter = 08350, loss = 2.3787
2024-10-30 15:51:14: [2024-10-30 15:51:14] iter = 08360, loss = 1.8508
2024-10-30 15:51:18: [2024-10-30 15:51:18] iter = 08370, loss = 1.8539
2024-10-30 15:51:21: [2024-10-30 15:51:21] iter = 08380, loss = 2.1375
2024-10-30 15:51:25: [2024-10-30 15:51:25] iter = 08390, loss = 2.3674
2024-10-30 15:51:29: [2024-10-30 15:51:29] iter = 08400, loss = 1.8293
2024-10-30 15:51:33: [2024-10-30 15:51:33] iter = 08410, loss = 1.8670
2024-10-30 15:51:37: [2024-10-30 15:51:37] iter = 08420, loss = 2.4865
2024-10-30 15:51:40: [2024-10-30 15:51:40] iter = 08430, loss = 2.1789
2024-10-30 15:51:44: [2024-10-30 15:51:44] iter = 08440, loss = 1.9500
2024-10-30 15:51:48: [2024-10-30 15:51:48] iter = 08450, loss = 2.2692
2024-10-30 15:51:52: [2024-10-30 15:51:52] iter = 08460, loss = 2.0400
2024-10-30 15:51:56: [2024-10-30 15:51:56] iter = 08470, loss = 2.1918
2024-10-30 15:51:59: [2024-10-30 15:51:59] iter = 08480, loss = 2.6616
2024-10-30 15:52:02: [2024-10-30 15:52:02] iter = 08490, loss = 2.2793
2024-10-30 15:52:07: [2024-10-30 15:52:07] iter = 08500, loss = 1.8647
2024-10-30 15:52:11: [2024-10-30 15:52:11] iter = 08510, loss = 2.0482
2024-10-30 15:52:15: [2024-10-30 15:52:15] iter = 08520, loss = 2.9118
2024-10-30 15:52:19: [2024-10-30 15:52:19] iter = 08530, loss = 2.3525
2024-10-30 15:52:23: [2024-10-30 15:52:23] iter = 08540, loss = 1.7902
2024-10-30 15:52:27: [2024-10-30 15:52:27] iter = 08550, loss = 2.0780
2024-10-30 15:52:31: [2024-10-30 15:52:31] iter = 08560, loss = 1.9690
2024-10-30 15:52:34: [2024-10-30 15:52:34] iter = 08570, loss = 1.9899
2024-10-30 15:52:39: [2024-10-30 15:52:39] iter = 08580, loss = 4.1380
2024-10-30 15:52:42: [2024-10-30 15:52:42] iter = 08590, loss = 2.0601
2024-10-30 15:52:45: [2024-10-30 15:52:45] iter = 08600, loss = 2.1641
2024-10-30 15:52:47: [2024-10-30 15:52:47] iter = 08610, loss = 1.9902
2024-10-30 15:52:51: [2024-10-30 15:52:51] iter = 08620, loss = 2.3393
2024-10-30 15:52:54: [2024-10-30 15:52:54] iter = 08630, loss = 1.9757
2024-10-30 15:52:57: [2024-10-30 15:52:57] iter = 08640, loss = 2.4649
2024-10-30 15:53:00: [2024-10-30 15:53:00] iter = 08650, loss = 2.5849
2024-10-30 15:53:04: [2024-10-30 15:53:04] iter = 08660, loss = 2.7370
2024-10-30 15:53:07: [2024-10-30 15:53:07] iter = 08670, loss = 3.0347
2024-10-30 15:53:10: [2024-10-30 15:53:10] iter = 08680, loss = 1.9280
2024-10-30 15:53:14: [2024-10-30 15:53:14] iter = 08690, loss = 3.2536
2024-10-30 15:53:18: [2024-10-30 15:53:18] iter = 08700, loss = 1.8313
2024-10-30 15:53:21: [2024-10-30 15:53:21] iter = 08710, loss = 2.8543
2024-10-30 15:53:25: [2024-10-30 15:53:25] iter = 08720, loss = 2.1806
2024-10-30 15:53:29: [2024-10-30 15:53:29] iter = 08730, loss = 3.9018
2024-10-30 15:53:32: [2024-10-30 15:53:32] iter = 08740, loss = 2.1089
2024-10-30 15:53:37: [2024-10-30 15:53:37] iter = 08750, loss = 2.0247
2024-10-30 15:53:41: [2024-10-30 15:53:41] iter = 08760, loss = 3.2850
2024-10-30 15:53:44: [2024-10-30 15:53:44] iter = 08770, loss = 2.3216
2024-10-30 15:53:48: [2024-10-30 15:53:48] iter = 08780, loss = 1.9601
2024-10-30 15:53:53: [2024-10-30 15:53:53] iter = 08790, loss = 7.1496
2024-10-30 15:53:57: [2024-10-30 15:53:57] iter = 08800, loss = 1.8879
2024-10-30 15:54:01: [2024-10-30 15:54:01] iter = 08810, loss = 2.4729
2024-10-30 15:54:04: [2024-10-30 15:54:04] iter = 08820, loss = 2.3568
2024-10-30 15:54:09: [2024-10-30 15:54:09] iter = 08830, loss = 2.1404
2024-10-30 15:54:12: [2024-10-30 15:54:12] iter = 08840, loss = 2.8170
2024-10-30 15:54:16: [2024-10-30 15:54:16] iter = 08850, loss = 1.8844
2024-10-30 15:54:19: [2024-10-30 15:54:19] iter = 08860, loss = 1.8511
2024-10-30 15:54:24: [2024-10-30 15:54:24] iter = 08870, loss = 1.8455
2024-10-30 15:54:28: [2024-10-30 15:54:28] iter = 08880, loss = 2.3405
2024-10-30 15:54:32: [2024-10-30 15:54:32] iter = 08890, loss = 2.0348
2024-10-30 15:54:36: [2024-10-30 15:54:36] iter = 08900, loss = 3.2821
2024-10-30 15:54:40: [2024-10-30 15:54:40] iter = 08910, loss = 3.6198
2024-10-30 15:54:44: [2024-10-30 15:54:44] iter = 08920, loss = 2.1557
2024-10-30 15:54:47: [2024-10-30 15:54:47] iter = 08930, loss = 2.7871
2024-10-30 15:54:51: [2024-10-30 15:54:51] iter = 08940, loss = 2.4193
2024-10-30 15:54:55: [2024-10-30 15:54:55] iter = 08950, loss = 2.2325
2024-10-30 15:54:59: [2024-10-30 15:54:59] iter = 08960, loss = 3.0193
2024-10-30 15:55:04: [2024-10-30 15:55:04] iter = 08970, loss = 2.7680
2024-10-30 15:55:08: [2024-10-30 15:55:08] iter = 08980, loss = 2.2032
2024-10-30 15:55:11: [2024-10-30 15:55:11] iter = 08990, loss = 2.6748
2024-10-30 15:55:15: [2024-10-30 15:55:15] iter = 09000, loss = 3.8201
2024-10-30 15:55:19: [2024-10-30 15:55:19] iter = 09010, loss = 1.9126
2024-10-30 15:55:24: [2024-10-30 15:55:24] iter = 09020, loss = 3.1327
2024-10-30 15:55:27: [2024-10-30 15:55:27] iter = 09030, loss = 4.0027
2024-10-30 15:55:31: [2024-10-30 15:55:31] iter = 09040, loss = 2.2929
2024-10-30 15:55:37: [2024-10-30 15:55:37] iter = 09050, loss = 2.8771
2024-10-30 15:55:40: [2024-10-30 15:55:40] iter = 09060, loss = 1.9518
2024-10-30 15:55:44: [2024-10-30 15:55:44] iter = 09070, loss = 2.6690
2024-10-30 15:55:47: [2024-10-30 15:55:47] iter = 09080, loss = 2.3249
2024-10-30 15:55:51: [2024-10-30 15:55:51] iter = 09090, loss = 3.8386
2024-10-30 15:55:55: [2024-10-30 15:55:55] iter = 09100, loss = 1.9495
2024-10-30 15:55:58: [2024-10-30 15:55:58] iter = 09110, loss = 2.0765
2024-10-30 15:56:01: [2024-10-30 15:56:01] iter = 09120, loss = 2.3604
2024-10-30 15:56:06: [2024-10-30 15:56:06] iter = 09130, loss = 2.2659
2024-10-30 15:56:10: [2024-10-30 15:56:10] iter = 09140, loss = 2.1088
2024-10-30 15:56:14: [2024-10-30 15:56:14] iter = 09150, loss = 3.9099
2024-10-30 15:56:18: [2024-10-30 15:56:18] iter = 09160, loss = 2.1842
2024-10-30 15:56:23: [2024-10-30 15:56:23] iter = 09170, loss = 2.2879
2024-10-30 15:56:27: [2024-10-30 15:56:27] iter = 09180, loss = 2.6555
2024-10-30 15:56:32: [2024-10-30 15:56:32] iter = 09190, loss = 2.1050
2024-10-30 15:56:35: [2024-10-30 15:56:35] iter = 09200, loss = 3.0352
2024-10-30 15:56:39: [2024-10-30 15:56:39] iter = 09210, loss = 2.2437
2024-10-30 15:56:42: [2024-10-30 15:56:42] iter = 09220, loss = 1.8501
2024-10-30 15:56:46: [2024-10-30 15:56:46] iter = 09230, loss = 2.5850
2024-10-30 15:56:50: [2024-10-30 15:56:50] iter = 09240, loss = 2.4464
2024-10-30 15:56:53: [2024-10-30 15:56:53] iter = 09250, loss = 2.5373
2024-10-30 15:56:57: [2024-10-30 15:56:57] iter = 09260, loss = 2.5469
2024-10-30 15:57:01: [2024-10-30 15:57:01] iter = 09270, loss = 2.9639
2024-10-30 15:57:05: [2024-10-30 15:57:05] iter = 09280, loss = 2.7687
2024-10-30 15:57:09: [2024-10-30 15:57:09] iter = 09290, loss = 1.9041
2024-10-30 15:57:13: [2024-10-30 15:57:13] iter = 09300, loss = 2.2134
2024-10-30 15:57:17: [2024-10-30 15:57:17] iter = 09310, loss = 1.9131
2024-10-30 15:57:21: [2024-10-30 15:57:21] iter = 09320, loss = 6.0291
2024-10-30 15:57:26: [2024-10-30 15:57:26] iter = 09330, loss = 1.8533
2024-10-30 15:57:30: [2024-10-30 15:57:30] iter = 09340, loss = 2.0913
2024-10-30 15:57:34: [2024-10-30 15:57:34] iter = 09350, loss = 1.9033
2024-10-30 15:57:38: [2024-10-30 15:57:38] iter = 09360, loss = 2.2460
2024-10-30 15:57:42: [2024-10-30 15:57:42] iter = 09370, loss = 2.6686
2024-10-30 15:57:46: [2024-10-30 15:57:46] iter = 09380, loss = 2.6019
2024-10-30 15:57:50: [2024-10-30 15:57:50] iter = 09390, loss = 2.1268
2024-10-30 15:57:54: [2024-10-30 15:57:54] iter = 09400, loss = 2.0013
2024-10-30 15:57:58: [2024-10-30 15:57:58] iter = 09410, loss = 2.9670
2024-10-30 15:58:02: [2024-10-30 15:58:02] iter = 09420, loss = 2.2870
2024-10-30 15:58:06: [2024-10-30 15:58:06] iter = 09430, loss = 2.3856
2024-10-30 15:58:09: [2024-10-30 15:58:09] iter = 09440, loss = 2.5261
2024-10-30 15:58:13: [2024-10-30 15:58:13] iter = 09450, loss = 2.4158
2024-10-30 15:58:17: [2024-10-30 15:58:17] iter = 09460, loss = 3.3713
2024-10-30 15:58:22: [2024-10-30 15:58:22] iter = 09470, loss = 2.6545
2024-10-30 15:58:25: [2024-10-30 15:58:25] iter = 09480, loss = 3.0910
2024-10-30 15:58:28: [2024-10-30 15:58:28] iter = 09490, loss = 2.2978
2024-10-30 15:58:32: [2024-10-30 15:58:32] iter = 09500, loss = 2.4640
2024-10-30 15:58:35: [2024-10-30 15:58:35] iter = 09510, loss = 2.7092
2024-10-30 15:58:39: [2024-10-30 15:58:39] iter = 09520, loss = 1.9596
2024-10-30 15:58:43: [2024-10-30 15:58:43] iter = 09530, loss = 5.1075
2024-10-30 15:58:47: [2024-10-30 15:58:47] iter = 09540, loss = 1.9447
2024-10-30 15:58:51: [2024-10-30 15:58:51] iter = 09550, loss = 2.5736
2024-10-30 15:58:54: [2024-10-30 15:58:54] iter = 09560, loss = 7.7864
2024-10-30 15:58:58: [2024-10-30 15:58:58] iter = 09570, loss = 2.1188
2024-10-30 15:59:01: [2024-10-30 15:59:01] iter = 09580, loss = 2.2551
2024-10-30 15:59:04: [2024-10-30 15:59:04] iter = 09590, loss = 1.7655
2024-10-30 15:59:08: [2024-10-30 15:59:08] iter = 09600, loss = 1.7247
2024-10-30 15:59:12: [2024-10-30 15:59:12] iter = 09610, loss = 2.2160
2024-10-30 15:59:17: [2024-10-30 15:59:17] iter = 09620, loss = 2.5210
2024-10-30 15:59:21: [2024-10-30 15:59:21] iter = 09630, loss = 6.3822
2024-10-30 15:59:25: [2024-10-30 15:59:25] iter = 09640, loss = 2.5061
2024-10-30 15:59:29: [2024-10-30 15:59:29] iter = 09650, loss = 2.2282
2024-10-30 15:59:33: [2024-10-30 15:59:33] iter = 09660, loss = 2.3595
2024-10-30 15:59:37: [2024-10-30 15:59:37] iter = 09670, loss = 4.5310
2024-10-30 15:59:40: [2024-10-30 15:59:40] iter = 09680, loss = 2.0605
2024-10-30 15:59:45: [2024-10-30 15:59:45] iter = 09690, loss = 2.9890
2024-10-30 15:59:49: [2024-10-30 15:59:49] iter = 09700, loss = 2.2626
2024-10-30 15:59:53: [2024-10-30 15:59:53] iter = 09710, loss = 3.0850
2024-10-30 15:59:56: [2024-10-30 15:59:56] iter = 09720, loss = 2.4833
2024-10-30 15:59:59: [2024-10-30 15:59:59] iter = 09730, loss = 4.0753
2024-10-30 16:00:03: [2024-10-30 16:00:03] iter = 09740, loss = 1.7913
2024-10-30 16:00:06: [2024-10-30 16:00:06] iter = 09750, loss = 2.0276
2024-10-30 16:00:10: [2024-10-30 16:00:10] iter = 09760, loss = 3.3011
2024-10-30 16:00:15: [2024-10-30 16:00:15] iter = 09770, loss = 2.2082
2024-10-30 16:00:18: [2024-10-30 16:00:18] iter = 09780, loss = 2.4705
2024-10-30 16:00:21: [2024-10-30 16:00:21] iter = 09790, loss = 2.1014
2024-10-30 16:00:25: [2024-10-30 16:00:25] iter = 09800, loss = 2.2932
2024-10-30 16:00:28: [2024-10-30 16:00:28] iter = 09810, loss = 2.6885
2024-10-30 16:00:32: [2024-10-30 16:00:32] iter = 09820, loss = 2.1465
2024-10-30 16:00:37: [2024-10-30 16:00:37] iter = 09830, loss = 2.7172
2024-10-30 16:00:40: [2024-10-30 16:00:40] iter = 09840, loss = 1.9535
2024-10-30 16:00:43: [2024-10-30 16:00:43] iter = 09850, loss = 2.2301
2024-10-30 16:00:47: [2024-10-30 16:00:47] iter = 09860, loss = 3.0552
2024-10-30 16:00:51: [2024-10-30 16:00:51] iter = 09870, loss = 2.1293
2024-10-30 16:00:55: [2024-10-30 16:00:55] iter = 09880, loss = 2.0971
2024-10-30 16:00:58: [2024-10-30 16:00:58] iter = 09890, loss = 2.5477
2024-10-30 16:01:02: [2024-10-30 16:01:02] iter = 09900, loss = 2.6668
2024-10-30 16:01:05: [2024-10-30 16:01:05] iter = 09910, loss = 2.3828
2024-10-30 16:01:10: [2024-10-30 16:01:10] iter = 09920, loss = 1.8400
2024-10-30 16:01:15: [2024-10-30 16:01:15] iter = 09930, loss = 2.4313
2024-10-30 16:01:19: [2024-10-30 16:01:19] iter = 09940, loss = 2.6281
2024-10-30 16:01:23: [2024-10-30 16:01:23] iter = 09950, loss = 3.2212
2024-10-30 16:01:27: [2024-10-30 16:01:27] iter = 09960, loss = 2.0748
2024-10-30 16:01:31: [2024-10-30 16:01:31] iter = 09970, loss = 1.7499
2024-10-30 16:01:36: [2024-10-30 16:01:36] iter = 09980, loss = 2.1146
2024-10-30 16:01:39: [2024-10-30 16:01:39] iter = 09990, loss = 2.5926
2024-10-30 16:01:43: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
2024-10-30 16:01:43: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 16:01:43: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 3437}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 16:04:07: Evaluate 5 random ConvNet, ACCmean = 0.7763 ACCstd = 0.0036
-------------------------
2024-10-30 16:04:07: Evaluate 5 random ConvNet, SENmean = 0.7780 SENstd = 0.0035
-------------------------
2024-10-30 16:04:07: Evaluate 5 random ConvNet, SPEmean = 0.9776 SPEstd = 0.0003
-------------------------
2024-10-30 16:04:07: Evaluate 5 random ConvNet, F!mean = 0.7634 F!std = 0.0038
-------------------------
2024-10-30 16:04:07: Evaluate 5 random ConvNet, mean = 0.7763 std = 0.0036
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 16:04:08: [2024-10-30 16:04:08] iter = 10000, loss = 1.8320
2024-10-30 16:04:11: [2024-10-30 16:04:11] iter = 10010, loss = 2.0992
2024-10-30 16:04:14: [2024-10-30 16:04:14] iter = 10020, loss = 2.6365
2024-10-30 16:04:18: [2024-10-30 16:04:18] iter = 10030, loss = 2.1050
2024-10-30 16:04:23: [2024-10-30 16:04:23] iter = 10040, loss = 2.3372
2024-10-30 16:04:26: [2024-10-30 16:04:26] iter = 10050, loss = 2.1177
2024-10-30 16:04:30: [2024-10-30 16:04:30] iter = 10060, loss = 1.9858
2024-10-30 16:04:34: [2024-10-30 16:04:34] iter = 10070, loss = 2.0698
2024-10-30 16:04:39: [2024-10-30 16:04:39] iter = 10080, loss = 2.0251
2024-10-30 16:04:43: [2024-10-30 16:04:43] iter = 10090, loss = 1.9598
2024-10-30 16:04:48: [2024-10-30 16:04:48] iter = 10100, loss = 6.3483
2024-10-30 16:04:52: [2024-10-30 16:04:52] iter = 10110, loss = 2.0421
2024-10-30 16:04:55: [2024-10-30 16:04:55] iter = 10120, loss = 2.0899
2024-10-30 16:04:59: [2024-10-30 16:04:59] iter = 10130, loss = 2.0828
2024-10-30 16:05:02: [2024-10-30 16:05:02] iter = 10140, loss = 1.8334
2024-10-30 16:05:06: [2024-10-30 16:05:06] iter = 10150, loss = 2.5111
2024-10-30 16:05:10: [2024-10-30 16:05:10] iter = 10160, loss = 2.6293
2024-10-30 16:05:14: [2024-10-30 16:05:14] iter = 10170, loss = 2.4868
2024-10-30 16:05:18: [2024-10-30 16:05:18] iter = 10180, loss = 2.2684
2024-10-30 16:05:21: [2024-10-30 16:05:21] iter = 10190, loss = 2.1584
2024-10-30 16:05:25: [2024-10-30 16:05:25] iter = 10200, loss = 2.5634
2024-10-30 16:05:29: [2024-10-30 16:05:29] iter = 10210, loss = 1.8439
2024-10-30 16:05:33: [2024-10-30 16:05:33] iter = 10220, loss = 2.1347
2024-10-30 16:05:36: [2024-10-30 16:05:36] iter = 10230, loss = 1.7878
2024-10-30 16:05:41: [2024-10-30 16:05:41] iter = 10240, loss = 2.5123
2024-10-30 16:05:44: [2024-10-30 16:05:44] iter = 10250, loss = 2.5854
2024-10-30 16:05:48: [2024-10-30 16:05:48] iter = 10260, loss = 2.8341
2024-10-30 16:05:52: [2024-10-30 16:05:52] iter = 10270, loss = 1.9908
2024-10-30 16:05:55: [2024-10-30 16:05:55] iter = 10280, loss = 2.2005
2024-10-30 16:05:59: [2024-10-30 16:05:59] iter = 10290, loss = 2.2893
2024-10-30 16:06:03: [2024-10-30 16:06:03] iter = 10300, loss = 2.7540
2024-10-30 16:06:06: [2024-10-30 16:06:06] iter = 10310, loss = 2.2860
2024-10-30 16:06:09: [2024-10-30 16:06:09] iter = 10320, loss = 8.7271
2024-10-30 16:06:13: [2024-10-30 16:06:13] iter = 10330, loss = 2.2309
2024-10-30 16:06:17: [2024-10-30 16:06:17] iter = 10340, loss = 2.0460
2024-10-30 16:06:21: [2024-10-30 16:06:21] iter = 10350, loss = 2.0196
2024-10-30 16:06:25: [2024-10-30 16:06:25] iter = 10360, loss = 2.0900
2024-10-30 16:06:28: [2024-10-30 16:06:28] iter = 10370, loss = 2.1067
2024-10-30 16:06:31: [2024-10-30 16:06:31] iter = 10380, loss = 2.2795
2024-10-30 16:06:36: [2024-10-30 16:06:36] iter = 10390, loss = 2.1524
2024-10-30 16:06:39: [2024-10-30 16:06:39] iter = 10400, loss = 1.8215
2024-10-30 16:06:43: [2024-10-30 16:06:43] iter = 10410, loss = 3.4591
2024-10-30 16:06:47: [2024-10-30 16:06:47] iter = 10420, loss = 2.2623
2024-10-30 16:06:51: [2024-10-30 16:06:51] iter = 10430, loss = 1.9713
2024-10-30 16:06:55: [2024-10-30 16:06:55] iter = 10440, loss = 2.6544
2024-10-30 16:06:59: [2024-10-30 16:06:59] iter = 10450, loss = 2.1557
2024-10-30 16:07:03: [2024-10-30 16:07:03] iter = 10460, loss = 2.4247
2024-10-30 16:07:06: [2024-10-30 16:07:06] iter = 10470, loss = 2.5307
2024-10-30 16:07:10: [2024-10-30 16:07:10] iter = 10480, loss = 3.0466
2024-10-30 16:07:14: [2024-10-30 16:07:14] iter = 10490, loss = 2.0935
2024-10-30 16:07:19: [2024-10-30 16:07:19] iter = 10500, loss = 2.3833
2024-10-30 16:07:22: [2024-10-30 16:07:22] iter = 10510, loss = 2.6157
2024-10-30 16:07:26: [2024-10-30 16:07:26] iter = 10520, loss = 4.3096
2024-10-30 16:07:30: [2024-10-30 16:07:30] iter = 10530, loss = 1.8652
2024-10-30 16:07:34: [2024-10-30 16:07:34] iter = 10540, loss = 2.1037
2024-10-30 16:07:37: [2024-10-30 16:07:37] iter = 10550, loss = 2.9047
2024-10-30 16:07:41: [2024-10-30 16:07:41] iter = 10560, loss = 3.7412
2024-10-30 16:07:45: [2024-10-30 16:07:45] iter = 10570, loss = 1.9546
2024-10-30 16:07:48: [2024-10-30 16:07:48] iter = 10580, loss = 2.9492
2024-10-30 16:07:53: [2024-10-30 16:07:53] iter = 10590, loss = 1.9557
2024-10-30 16:07:57: [2024-10-30 16:07:57] iter = 10600, loss = 3.3203
2024-10-30 16:08:01: [2024-10-30 16:08:01] iter = 10610, loss = 1.9632
2024-10-30 16:08:04: [2024-10-30 16:08:04] iter = 10620, loss = 1.9840
2024-10-30 16:08:09: [2024-10-30 16:08:09] iter = 10630, loss = 9.3113
2024-10-30 16:08:12: [2024-10-30 16:08:12] iter = 10640, loss = 2.2506
2024-10-30 16:08:16: [2024-10-30 16:08:16] iter = 10650, loss = 1.8627
2024-10-30 16:08:20: [2024-10-30 16:08:20] iter = 10660, loss = 2.1755
2024-10-30 16:08:23: [2024-10-30 16:08:23] iter = 10670, loss = 1.9689
2024-10-30 16:08:26: [2024-10-30 16:08:26] iter = 10680, loss = 2.1238
2024-10-30 16:08:30: [2024-10-30 16:08:30] iter = 10690, loss = 2.5851
2024-10-30 16:08:34: [2024-10-30 16:08:34] iter = 10700, loss = 2.4251
2024-10-30 16:08:38: [2024-10-30 16:08:38] iter = 10710, loss = 1.9384
2024-10-30 16:08:41: [2024-10-30 16:08:41] iter = 10720, loss = 3.5118
2024-10-30 16:08:44: [2024-10-30 16:08:44] iter = 10730, loss = 2.6806
2024-10-30 16:08:47: [2024-10-30 16:08:47] iter = 10740, loss = 2.0467
2024-10-30 16:08:51: [2024-10-30 16:08:51] iter = 10750, loss = 1.9808
2024-10-30 16:08:54: [2024-10-30 16:08:54] iter = 10760, loss = 2.3273
2024-10-30 16:08:58: [2024-10-30 16:08:58] iter = 10770, loss = 2.2787
2024-10-30 16:09:01: [2024-10-30 16:09:01] iter = 10780, loss = 2.1415
2024-10-30 16:09:05: [2024-10-30 16:09:05] iter = 10790, loss = 1.9280
2024-10-30 16:09:08: [2024-10-30 16:09:08] iter = 10800, loss = 2.0558
2024-10-30 16:09:12: [2024-10-30 16:09:12] iter = 10810, loss = 2.2310
2024-10-30 16:09:16: [2024-10-30 16:09:16] iter = 10820, loss = 1.9895
2024-10-30 16:09:20: [2024-10-30 16:09:20] iter = 10830, loss = 2.1967
2024-10-30 16:09:24: [2024-10-30 16:09:24] iter = 10840, loss = 2.0007
2024-10-30 16:09:28: [2024-10-30 16:09:28] iter = 10850, loss = 10.9672
2024-10-30 16:09:31: [2024-10-30 16:09:31] iter = 10860, loss = 2.4415
2024-10-30 16:09:34: [2024-10-30 16:09:34] iter = 10870, loss = 2.5590
2024-10-30 16:09:37: [2024-10-30 16:09:37] iter = 10880, loss = 1.9863
2024-10-30 16:09:40: [2024-10-30 16:09:40] iter = 10890, loss = 2.6235
2024-10-30 16:09:44: [2024-10-30 16:09:44] iter = 10900, loss = 2.0050
2024-10-30 16:09:47: [2024-10-30 16:09:47] iter = 10910, loss = 2.9666
2024-10-30 16:09:51: [2024-10-30 16:09:51] iter = 10920, loss = 3.3422
2024-10-30 16:09:55: [2024-10-30 16:09:55] iter = 10930, loss = 2.1088
2024-10-30 16:09:59: [2024-10-30 16:09:59] iter = 10940, loss = 2.2265
2024-10-30 16:10:03: [2024-10-30 16:10:03] iter = 10950, loss = 3.0803
2024-10-30 16:10:08: [2024-10-30 16:10:08] iter = 10960, loss = 2.3417
2024-10-30 16:10:12: [2024-10-30 16:10:12] iter = 10970, loss = 1.9396
2024-10-30 16:10:15: [2024-10-30 16:10:15] iter = 10980, loss = 3.7962
2024-10-30 16:10:18: [2024-10-30 16:10:18] iter = 10990, loss = 3.7489
2024-10-30 16:10:21: [2024-10-30 16:10:21] iter = 11000, loss = 3.2673
2024-10-30 16:10:25: [2024-10-30 16:10:25] iter = 11010, loss = 1.9654
2024-10-30 16:10:30: [2024-10-30 16:10:30] iter = 11020, loss = 2.0809
2024-10-30 16:10:34: [2024-10-30 16:10:34] iter = 11030, loss = 2.0661
2024-10-30 16:10:37: [2024-10-30 16:10:37] iter = 11040, loss = 2.3999
2024-10-30 16:10:41: [2024-10-30 16:10:41] iter = 11050, loss = 2.2021
2024-10-30 16:10:44: [2024-10-30 16:10:44] iter = 11060, loss = 2.1435
2024-10-30 16:10:48: [2024-10-30 16:10:48] iter = 11070, loss = 2.0277
2024-10-30 16:10:51: [2024-10-30 16:10:51] iter = 11080, loss = 2.6048
2024-10-30 16:10:54: [2024-10-30 16:10:54] iter = 11090, loss = 3.8766
2024-10-30 16:10:58: [2024-10-30 16:10:58] iter = 11100, loss = 2.2803
2024-10-30 16:11:02: [2024-10-30 16:11:02] iter = 11110, loss = 2.7953
2024-10-30 16:11:06: [2024-10-30 16:11:06] iter = 11120, loss = 1.8423
2024-10-30 16:11:09: [2024-10-30 16:11:09] iter = 11130, loss = 2.0139
2024-10-30 16:11:12: [2024-10-30 16:11:12] iter = 11140, loss = 1.9255
2024-10-30 16:11:16: [2024-10-30 16:11:16] iter = 11150, loss = 1.8452
2024-10-30 16:11:20: [2024-10-30 16:11:20] iter = 11160, loss = 2.1514
2024-10-30 16:11:23: [2024-10-30 16:11:23] iter = 11170, loss = 2.6256
2024-10-30 16:11:26: [2024-10-30 16:11:26] iter = 11180, loss = 2.9780
2024-10-30 16:11:30: [2024-10-30 16:11:30] iter = 11190, loss = 1.8468
2024-10-30 16:11:33: [2024-10-30 16:11:33] iter = 11200, loss = 1.8274
2024-10-30 16:11:36: [2024-10-30 16:11:36] iter = 11210, loss = 2.3580
2024-10-30 16:11:39: [2024-10-30 16:11:39] iter = 11220, loss = 1.8893
2024-10-30 16:11:42: [2024-10-30 16:11:42] iter = 11230, loss = 2.3643
2024-10-30 16:11:45: [2024-10-30 16:11:45] iter = 11240, loss = 2.5390
2024-10-30 16:11:50: [2024-10-30 16:11:50] iter = 11250, loss = 2.3082
2024-10-30 16:11:55: [2024-10-30 16:11:55] iter = 11260, loss = 2.6641
2024-10-30 16:11:59: [2024-10-30 16:11:59] iter = 11270, loss = 1.9499
2024-10-30 16:12:02: [2024-10-30 16:12:02] iter = 11280, loss = 2.0987
2024-10-30 16:12:07: [2024-10-30 16:12:07] iter = 11290, loss = 1.9068
2024-10-30 16:12:11: [2024-10-30 16:12:11] iter = 11300, loss = 2.5730
2024-10-30 16:12:14: [2024-10-30 16:12:14] iter = 11310, loss = 4.2355
2024-10-30 16:12:16: [2024-10-30 16:12:16] iter = 11320, loss = 2.5406
2024-10-30 16:12:19: [2024-10-30 16:12:19] iter = 11330, loss = 1.7701
2024-10-30 16:12:24: [2024-10-30 16:12:24] iter = 11340, loss = 2.3799
2024-10-30 16:12:28: [2024-10-30 16:12:28] iter = 11350, loss = 2.3634
2024-10-30 16:12:33: [2024-10-30 16:12:33] iter = 11360, loss = 2.0697
2024-10-30 16:12:37: [2024-10-30 16:12:37] iter = 11370, loss = 2.2391
2024-10-30 16:12:40: [2024-10-30 16:12:40] iter = 11380, loss = 3.1292
2024-10-30 16:12:43: [2024-10-30 16:12:43] iter = 11390, loss = 2.6373
2024-10-30 16:12:47: [2024-10-30 16:12:47] iter = 11400, loss = 1.9595
2024-10-30 16:12:51: [2024-10-30 16:12:51] iter = 11410, loss = 1.6472
2024-10-30 16:12:55: [2024-10-30 16:12:55] iter = 11420, loss = 4.8991
2024-10-30 16:12:59: [2024-10-30 16:12:59] iter = 11430, loss = 2.1149
2024-10-30 16:13:03: [2024-10-30 16:13:03] iter = 11440, loss = 2.7705
2024-10-30 16:13:07: [2024-10-30 16:13:07] iter = 11450, loss = 3.2988
2024-10-30 16:13:11: [2024-10-30 16:13:11] iter = 11460, loss = 2.0351
2024-10-30 16:13:14: [2024-10-30 16:13:14] iter = 11470, loss = 2.0623
2024-10-30 16:13:19: [2024-10-30 16:13:19] iter = 11480, loss = 2.1077
2024-10-30 16:13:23: [2024-10-30 16:13:23] iter = 11490, loss = 2.1139
2024-10-30 16:13:26: [2024-10-30 16:13:26] iter = 11500, loss = 2.1051
2024-10-30 16:13:30: [2024-10-30 16:13:30] iter = 11510, loss = 2.7118
2024-10-30 16:13:32: [2024-10-30 16:13:32] iter = 11520, loss = 3.8716
2024-10-30 16:13:36: [2024-10-30 16:13:36] iter = 11530, loss = 2.1480
2024-10-30 16:13:40: [2024-10-30 16:13:40] iter = 11540, loss = 1.9978
2024-10-30 16:13:43: [2024-10-30 16:13:43] iter = 11550, loss = 3.1407
2024-10-30 16:13:47: [2024-10-30 16:13:47] iter = 11560, loss = 3.1039
2024-10-30 16:13:51: [2024-10-30 16:13:51] iter = 11570, loss = 4.9345
2024-10-30 16:13:55: [2024-10-30 16:13:55] iter = 11580, loss = 2.2741
2024-10-30 16:13:58: [2024-10-30 16:13:58] iter = 11590, loss = 2.4085
2024-10-30 16:14:01: [2024-10-30 16:14:01] iter = 11600, loss = 1.9176
2024-10-30 16:14:04: [2024-10-30 16:14:04] iter = 11610, loss = 4.1018
2024-10-30 16:14:07: [2024-10-30 16:14:07] iter = 11620, loss = 1.8804
2024-10-30 16:14:11: [2024-10-30 16:14:10] iter = 11630, loss = 2.4803
2024-10-30 16:14:15: [2024-10-30 16:14:15] iter = 11640, loss = 1.7091
2024-10-30 16:14:20: [2024-10-30 16:14:20] iter = 11650, loss = 2.6323
2024-10-30 16:14:24: [2024-10-30 16:14:24] iter = 11660, loss = 2.5255
2024-10-30 16:14:28: [2024-10-30 16:14:28] iter = 11670, loss = 2.1485
2024-10-30 16:14:32: [2024-10-30 16:14:32] iter = 11680, loss = 2.1615
2024-10-30 16:14:35: [2024-10-30 16:14:35] iter = 11690, loss = 2.1856
2024-10-30 16:14:39: [2024-10-30 16:14:39] iter = 11700, loss = 2.9984
2024-10-30 16:14:44: [2024-10-30 16:14:44] iter = 11710, loss = 1.9989
2024-10-30 16:14:47: [2024-10-30 16:14:47] iter = 11720, loss = 2.8191
2024-10-30 16:14:51: [2024-10-30 16:14:51] iter = 11730, loss = 2.0458
2024-10-30 16:14:54: [2024-10-30 16:14:54] iter = 11740, loss = 2.5051
2024-10-30 16:14:58: [2024-10-30 16:14:58] iter = 11750, loss = 5.5158
2024-10-30 16:15:02: [2024-10-30 16:15:02] iter = 11760, loss = 2.2477
2024-10-30 16:15:05: [2024-10-30 16:15:05] iter = 11770, loss = 3.3058
2024-10-30 16:15:10: [2024-10-30 16:15:10] iter = 11780, loss = 2.0642
2024-10-30 16:15:14: [2024-10-30 16:15:14] iter = 11790, loss = 2.0399
2024-10-30 16:15:17: [2024-10-30 16:15:17] iter = 11800, loss = 2.0920
2024-10-30 16:15:21: [2024-10-30 16:15:21] iter = 11810, loss = 2.7331
2024-10-30 16:15:24: [2024-10-30 16:15:24] iter = 11820, loss = 2.4796
2024-10-30 16:15:29: [2024-10-30 16:15:29] iter = 11830, loss = 2.0928
2024-10-30 16:15:32: [2024-10-30 16:15:32] iter = 11840, loss = 2.0656
2024-10-30 16:15:36: [2024-10-30 16:15:36] iter = 11850, loss = 2.2430
2024-10-30 16:15:40: [2024-10-30 16:15:40] iter = 11860, loss = 2.1581
2024-10-30 16:15:43: [2024-10-30 16:15:43] iter = 11870, loss = 2.4363
2024-10-30 16:15:48: [2024-10-30 16:15:48] iter = 11880, loss = 2.2813
2024-10-30 16:15:52: [2024-10-30 16:15:52] iter = 11890, loss = 2.0472
2024-10-30 16:15:56: [2024-10-30 16:15:56] iter = 11900, loss = 1.9377
2024-10-30 16:16:00: [2024-10-30 16:16:00] iter = 11910, loss = 2.2059
2024-10-30 16:16:03: [2024-10-30 16:16:03] iter = 11920, loss = 2.2160
2024-10-30 16:16:08: [2024-10-30 16:16:08] iter = 11930, loss = 2.0548
2024-10-30 16:16:12: [2024-10-30 16:16:12] iter = 11940, loss = 2.4185
2024-10-30 16:16:16: [2024-10-30 16:16:16] iter = 11950, loss = 3.6183
2024-10-30 16:16:19: [2024-10-30 16:16:19] iter = 11960, loss = 1.9909
2024-10-30 16:16:24: [2024-10-30 16:16:24] iter = 11970, loss = 1.8688
2024-10-30 16:16:28: [2024-10-30 16:16:28] iter = 11980, loss = 2.0071
2024-10-30 16:16:32: [2024-10-30 16:16:32] iter = 11990, loss = 1.9267
2024-10-30 16:16:35: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
2024-10-30 16:16:35: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 16:16:35: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 95822}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 16:19:14: Evaluate 5 random ConvNet, ACCmean = 0.7820 ACCstd = 0.0051
-------------------------
2024-10-30 16:19:14: Evaluate 5 random ConvNet, SENmean = 0.7809 SENstd = 0.0044
-------------------------
2024-10-30 16:19:14: Evaluate 5 random ConvNet, SPEmean = 0.9781 SPEstd = 0.0005
-------------------------
2024-10-30 16:19:14: Evaluate 5 random ConvNet, F!mean = 0.7701 F!std = 0.0049
-------------------------
2024-10-30 16:19:14: Evaluate 5 random ConvNet, mean = 0.7820 std = 0.0051
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 16:19:15: [2024-10-30 16:19:15] iter = 12000, loss = 1.8910
2024-10-30 16:19:19: [2024-10-30 16:19:19] iter = 12010, loss = 2.5130
2024-10-30 16:19:24: [2024-10-30 16:19:24] iter = 12020, loss = 2.1302
2024-10-30 16:19:28: [2024-10-30 16:19:28] iter = 12030, loss = 2.2773
2024-10-30 16:19:31: [2024-10-30 16:19:31] iter = 12040, loss = 1.9512
2024-10-30 16:19:34: [2024-10-30 16:19:34] iter = 12050, loss = 1.9587
2024-10-30 16:19:37: [2024-10-30 16:19:37] iter = 12060, loss = 2.7173
2024-10-30 16:19:41: [2024-10-30 16:19:41] iter = 12070, loss = 2.8182
2024-10-30 16:19:44: [2024-10-30 16:19:44] iter = 12080, loss = 2.9267
2024-10-30 16:19:48: [2024-10-30 16:19:48] iter = 12090, loss = 2.5327
2024-10-30 16:19:51: [2024-10-30 16:19:51] iter = 12100, loss = 2.5003
2024-10-30 16:19:56: [2024-10-30 16:19:56] iter = 12110, loss = 2.3657
2024-10-30 16:20:00: [2024-10-30 16:20:00] iter = 12120, loss = 1.9357
2024-10-30 16:20:04: [2024-10-30 16:20:04] iter = 12130, loss = 1.8947
2024-10-30 16:20:08: [2024-10-30 16:20:08] iter = 12140, loss = 2.1812
2024-10-30 16:20:11: [2024-10-30 16:20:11] iter = 12150, loss = 2.2229
2024-10-30 16:20:15: [2024-10-30 16:20:15] iter = 12160, loss = 2.1500
2024-10-30 16:20:19: [2024-10-30 16:20:19] iter = 12170, loss = 3.3559
2024-10-30 16:20:23: [2024-10-30 16:20:23] iter = 12180, loss = 2.0771
2024-10-30 16:20:26: [2024-10-30 16:20:26] iter = 12190, loss = 2.1380
2024-10-30 16:20:31: [2024-10-30 16:20:31] iter = 12200, loss = 3.3120
2024-10-30 16:20:35: [2024-10-30 16:20:35] iter = 12210, loss = 2.5826
2024-10-30 16:20:38: [2024-10-30 16:20:38] iter = 12220, loss = 1.8633
2024-10-30 16:20:42: [2024-10-30 16:20:42] iter = 12230, loss = 3.0521
2024-10-30 16:20:45: [2024-10-30 16:20:45] iter = 12240, loss = 2.0075
2024-10-30 16:20:48: [2024-10-30 16:20:48] iter = 12250, loss = 1.9293
2024-10-30 16:20:52: [2024-10-30 16:20:52] iter = 12260, loss = 2.6625
2024-10-30 16:20:55: [2024-10-30 16:20:55] iter = 12270, loss = 1.8013
2024-10-30 16:20:58: [2024-10-30 16:20:58] iter = 12280, loss = 3.6146
2024-10-30 16:21:02: [2024-10-30 16:21:02] iter = 12290, loss = 2.1717
2024-10-30 16:21:07: [2024-10-30 16:21:07] iter = 12300, loss = 1.8924
2024-10-30 16:21:11: [2024-10-30 16:21:11] iter = 12310, loss = 2.1561
2024-10-30 16:21:15: [2024-10-30 16:21:15] iter = 12320, loss = 2.1950
2024-10-30 16:21:19: [2024-10-30 16:21:19] iter = 12330, loss = 2.3872
2024-10-30 16:21:23: [2024-10-30 16:21:23] iter = 12340, loss = 3.4349
2024-10-30 16:21:27: [2024-10-30 16:21:27] iter = 12350, loss = 3.2323
2024-10-30 16:21:30: [2024-10-30 16:21:30] iter = 12360, loss = 2.2208
2024-10-30 16:21:35: [2024-10-30 16:21:35] iter = 12370, loss = 2.1052
2024-10-30 16:21:38: [2024-10-30 16:21:38] iter = 12380, loss = 2.5828
2024-10-30 16:21:42: [2024-10-30 16:21:42] iter = 12390, loss = 1.9998
2024-10-30 16:21:46: [2024-10-30 16:21:46] iter = 12400, loss = 2.0861
2024-10-30 16:21:50: [2024-10-30 16:21:50] iter = 12410, loss = 2.1474
2024-10-30 16:21:53: [2024-10-30 16:21:53] iter = 12420, loss = 2.1691
2024-10-30 16:21:56: [2024-10-30 16:21:56] iter = 12430, loss = 2.2795
2024-10-30 16:21:59: [2024-10-30 16:21:59] iter = 12440, loss = 2.4276
2024-10-30 16:22:03: [2024-10-30 16:22:03] iter = 12450, loss = 2.0433
2024-10-30 16:22:06: [2024-10-30 16:22:06] iter = 12460, loss = 2.3763
2024-10-30 16:22:11: [2024-10-30 16:22:11] iter = 12470, loss = 1.7881
2024-10-30 16:22:14: [2024-10-30 16:22:14] iter = 12480, loss = 2.1804
2024-10-30 16:22:18: [2024-10-30 16:22:18] iter = 12490, loss = 2.7399
2024-10-30 16:22:22: [2024-10-30 16:22:22] iter = 12500, loss = 2.1390
2024-10-30 16:22:26: [2024-10-30 16:22:26] iter = 12510, loss = 2.1246
2024-10-30 16:22:30: [2024-10-30 16:22:29] iter = 12520, loss = 3.5642
2024-10-30 16:22:34: [2024-10-30 16:22:34] iter = 12530, loss = 2.9288
2024-10-30 16:22:39: [2024-10-30 16:22:39] iter = 12540, loss = 2.1041
2024-10-30 16:22:44: [2024-10-30 16:22:44] iter = 12550, loss = 2.2544
2024-10-30 16:22:48: [2024-10-30 16:22:48] iter = 12560, loss = 2.0589
2024-10-30 16:22:52: [2024-10-30 16:22:52] iter = 12570, loss = 2.6559
2024-10-30 16:22:57: [2024-10-30 16:22:57] iter = 12580, loss = 2.1807
2024-10-30 16:23:01: [2024-10-30 16:23:01] iter = 12590, loss = 2.1591
2024-10-30 16:23:06: [2024-10-30 16:23:06] iter = 12600, loss = 2.2799
2024-10-30 16:23:10: [2024-10-30 16:23:10] iter = 12610, loss = 1.9857
2024-10-30 16:23:14: [2024-10-30 16:23:14] iter = 12620, loss = 2.6463
2024-10-30 16:23:20: [2024-10-30 16:23:20] iter = 12630, loss = 2.1469
2024-10-30 16:23:24: [2024-10-30 16:23:24] iter = 12640, loss = 2.1809
2024-10-30 16:23:28: [2024-10-30 16:23:28] iter = 12650, loss = 3.2767
2024-10-30 16:23:31: [2024-10-30 16:23:31] iter = 12660, loss = 2.9543
2024-10-30 16:23:36: [2024-10-30 16:23:36] iter = 12670, loss = 1.9858
2024-10-30 16:23:40: [2024-10-30 16:23:40] iter = 12680, loss = 1.8251
2024-10-30 16:23:45: [2024-10-30 16:23:45] iter = 12690, loss = 2.6289
2024-10-30 16:23:49: [2024-10-30 16:23:49] iter = 12700, loss = 2.7063
2024-10-30 16:23:54: [2024-10-30 16:23:54] iter = 12710, loss = 2.0315
2024-10-30 16:23:58: [2024-10-30 16:23:58] iter = 12720, loss = 2.7418
2024-10-30 16:24:02: [2024-10-30 16:24:02] iter = 12730, loss = 2.0025
2024-10-30 16:24:07: [2024-10-30 16:24:07] iter = 12740, loss = 2.5975
2024-10-30 16:24:11: [2024-10-30 16:24:11] iter = 12750, loss = 2.2994
2024-10-30 16:24:16: [2024-10-30 16:24:16] iter = 12760, loss = 2.0727
2024-10-30 16:24:20: [2024-10-30 16:24:20] iter = 12770, loss = 2.0997
2024-10-30 16:24:24: [2024-10-30 16:24:24] iter = 12780, loss = 2.1077
2024-10-30 16:24:28: [2024-10-30 16:24:28] iter = 12790, loss = 2.1819
2024-10-30 16:24:32: [2024-10-30 16:24:32] iter = 12800, loss = 2.8779
2024-10-30 16:24:36: [2024-10-30 16:24:36] iter = 12810, loss = 2.6795
2024-10-30 16:24:40: [2024-10-30 16:24:40] iter = 12820, loss = 2.7014
2024-10-30 16:24:45: [2024-10-30 16:24:45] iter = 12830, loss = 1.8407
2024-10-30 16:24:49: [2024-10-30 16:24:49] iter = 12840, loss = 2.1203
2024-10-30 16:24:53: [2024-10-30 16:24:53] iter = 12850, loss = 2.0711
2024-10-30 16:24:57: [2024-10-30 16:24:57] iter = 12860, loss = 2.0349
2024-10-30 16:25:01: [2024-10-30 16:25:01] iter = 12870, loss = 2.3491
2024-10-30 16:25:05: [2024-10-30 16:25:05] iter = 12880, loss = 2.2727
2024-10-30 16:25:09: [2024-10-30 16:25:09] iter = 12890, loss = 2.1342
2024-10-30 16:25:14: [2024-10-30 16:25:14] iter = 12900, loss = 2.1452
2024-10-30 16:25:16: [2024-10-30 16:25:16] iter = 12910, loss = 2.0955
2024-10-30 16:25:19: [2024-10-30 16:25:19] iter = 12920, loss = 1.9809
2024-10-30 16:25:22: [2024-10-30 16:25:22] iter = 12930, loss = 3.0014
2024-10-30 16:25:25: [2024-10-30 16:25:25] iter = 12940, loss = 2.1079
2024-10-30 16:25:30: [2024-10-30 16:25:30] iter = 12950, loss = 2.5920
2024-10-30 16:25:34: [2024-10-30 16:25:34] iter = 12960, loss = 2.9110
2024-10-30 16:25:38: [2024-10-30 16:25:38] iter = 12970, loss = 2.2646
2024-10-30 16:25:43: [2024-10-30 16:25:43] iter = 12980, loss = 2.1505
2024-10-30 16:25:47: [2024-10-30 16:25:47] iter = 12990, loss = 2.2972
2024-10-30 16:25:51: [2024-10-30 16:25:51] iter = 13000, loss = 2.2561
2024-10-30 16:25:55: [2024-10-30 16:25:55] iter = 13010, loss = 3.3122
2024-10-30 16:25:59: [2024-10-30 16:25:59] iter = 13020, loss = 2.3764
2024-10-30 16:26:03: [2024-10-30 16:26:03] iter = 13030, loss = 2.0096
2024-10-30 16:26:08: [2024-10-30 16:26:08] iter = 13040, loss = 2.3632
2024-10-30 16:26:11: [2024-10-30 16:26:11] iter = 13050, loss = 1.9915
2024-10-30 16:26:15: [2024-10-30 16:26:15] iter = 13060, loss = 2.2729
2024-10-30 16:26:18: [2024-10-30 16:26:18] iter = 13070, loss = 1.9292
2024-10-30 16:26:22: [2024-10-30 16:26:22] iter = 13080, loss = 3.1019
2024-10-30 16:26:27: [2024-10-30 16:26:27] iter = 13090, loss = 2.1738
2024-10-30 16:26:32: [2024-10-30 16:26:32] iter = 13100, loss = 5.0046
2024-10-30 16:26:36: [2024-10-30 16:26:36] iter = 13110, loss = 2.5600
2024-10-30 16:26:40: [2024-10-30 16:26:40] iter = 13120, loss = 1.5356
2024-10-30 16:26:44: [2024-10-30 16:26:44] iter = 13130, loss = 2.0300
2024-10-30 16:26:48: [2024-10-30 16:26:48] iter = 13140, loss = 2.2733
2024-10-30 16:26:52: [2024-10-30 16:26:52] iter = 13150, loss = 1.9022
2024-10-30 16:26:57: [2024-10-30 16:26:57] iter = 13160, loss = 2.3653
2024-10-30 16:27:00: [2024-10-30 16:27:00] iter = 13170, loss = 2.2068
2024-10-30 16:27:05: [2024-10-30 16:27:05] iter = 13180, loss = 2.0479
2024-10-30 16:27:09: [2024-10-30 16:27:09] iter = 13190, loss = 2.7438
2024-10-30 16:27:14: [2024-10-30 16:27:14] iter = 13200, loss = 4.9699
2024-10-30 16:27:18: [2024-10-30 16:27:18] iter = 13210, loss = 2.2163
2024-10-30 16:27:22: [2024-10-30 16:27:22] iter = 13220, loss = 2.2205
2024-10-30 16:27:26: [2024-10-30 16:27:26] iter = 13230, loss = 2.3606
2024-10-30 16:27:30: [2024-10-30 16:27:30] iter = 13240, loss = 4.3977
2024-10-30 16:27:35: [2024-10-30 16:27:35] iter = 13250, loss = 2.1357
2024-10-30 16:27:39: [2024-10-30 16:27:39] iter = 13260, loss = 2.3819
2024-10-30 16:27:44: [2024-10-30 16:27:44] iter = 13270, loss = 2.0779
2024-10-30 16:27:48: [2024-10-30 16:27:48] iter = 13280, loss = 2.9903
2024-10-30 16:27:52: [2024-10-30 16:27:52] iter = 13290, loss = 1.9673
2024-10-30 16:27:56: [2024-10-30 16:27:56] iter = 13300, loss = 2.1220
2024-10-30 16:28:01: [2024-10-30 16:28:01] iter = 13310, loss = 2.2658
2024-10-30 16:28:05: [2024-10-30 16:28:05] iter = 13320, loss = 2.3439
2024-10-30 16:28:09: [2024-10-30 16:28:09] iter = 13330, loss = 2.0591
2024-10-30 16:28:13: [2024-10-30 16:28:13] iter = 13340, loss = 2.8581
2024-10-30 16:28:16: [2024-10-30 16:28:16] iter = 13350, loss = 1.8833
2024-10-30 16:28:21: [2024-10-30 16:28:21] iter = 13360, loss = 3.2721
2024-10-30 16:28:25: [2024-10-30 16:28:25] iter = 13370, loss = 1.8869
2024-10-30 16:28:29: [2024-10-30 16:28:29] iter = 13380, loss = 2.6334
2024-10-30 16:28:33: [2024-10-30 16:28:33] iter = 13390, loss = 2.1315
2024-10-30 16:28:37: [2024-10-30 16:28:37] iter = 13400, loss = 1.8766
2024-10-30 16:28:42: [2024-10-30 16:28:42] iter = 13410, loss = 3.4696
2024-10-30 16:28:45: [2024-10-30 16:28:45] iter = 13420, loss = 2.3763
2024-10-30 16:28:48: [2024-10-30 16:28:48] iter = 13430, loss = 2.2572
2024-10-30 16:28:52: [2024-10-30 16:28:52] iter = 13440, loss = 1.9911
2024-10-30 16:28:57: [2024-10-30 16:28:57] iter = 13450, loss = 2.6493
2024-10-30 16:29:02: [2024-10-30 16:29:02] iter = 13460, loss = 2.7177
2024-10-30 16:29:06: [2024-10-30 16:29:05] iter = 13470, loss = 1.8793
2024-10-30 16:29:10: [2024-10-30 16:29:10] iter = 13480, loss = 1.8500
2024-10-30 16:29:14: [2024-10-30 16:29:14] iter = 13490, loss = 1.8042
2024-10-30 16:29:18: [2024-10-30 16:29:18] iter = 13500, loss = 1.9657
2024-10-30 16:29:22: [2024-10-30 16:29:22] iter = 13510, loss = 3.0595
2024-10-30 16:29:27: [2024-10-30 16:29:27] iter = 13520, loss = 2.0590
2024-10-30 16:29:32: [2024-10-30 16:29:32] iter = 13530, loss = 2.1070
2024-10-30 16:29:36: [2024-10-30 16:29:36] iter = 13540, loss = 2.1164
2024-10-30 16:29:39: [2024-10-30 16:29:39] iter = 13550, loss = 4.1368
2024-10-30 16:29:42: [2024-10-30 16:29:42] iter = 13560, loss = 1.7164
2024-10-30 16:29:46: [2024-10-30 16:29:46] iter = 13570, loss = 2.2601
2024-10-30 16:29:49: [2024-10-30 16:29:49] iter = 13580, loss = 3.2101
2024-10-30 16:29:52: [2024-10-30 16:29:52] iter = 13590, loss = 2.2736
2024-10-30 16:29:56: [2024-10-30 16:29:56] iter = 13600, loss = 2.4625
2024-10-30 16:30:01: [2024-10-30 16:30:01] iter = 13610, loss = 1.8283
2024-10-30 16:30:05: [2024-10-30 16:30:05] iter = 13620, loss = 2.4665
2024-10-30 16:30:09: [2024-10-30 16:30:09] iter = 13630, loss = 4.8753
2024-10-30 16:30:14: [2024-10-30 16:30:14] iter = 13640, loss = 2.0118
2024-10-30 16:30:17: [2024-10-30 16:30:17] iter = 13650, loss = 2.8431
2024-10-30 16:30:20: [2024-10-30 16:30:20] iter = 13660, loss = 1.9972
2024-10-30 16:30:24: [2024-10-30 16:30:24] iter = 13670, loss = 1.9381
2024-10-30 16:30:28: [2024-10-30 16:30:28] iter = 13680, loss = 2.4138
2024-10-30 16:30:32: [2024-10-30 16:30:32] iter = 13690, loss = 2.3697
2024-10-30 16:30:36: [2024-10-30 16:30:36] iter = 13700, loss = 2.2951
2024-10-30 16:30:40: [2024-10-30 16:30:40] iter = 13710, loss = 1.9369
2024-10-30 16:30:43: [2024-10-30 16:30:43] iter = 13720, loss = 2.2578
2024-10-30 16:30:46: [2024-10-30 16:30:46] iter = 13730, loss = 3.9840
2024-10-30 16:30:49: [2024-10-30 16:30:49] iter = 13740, loss = 2.0212
2024-10-30 16:30:52: [2024-10-30 16:30:52] iter = 13750, loss = 2.6972
2024-10-30 16:30:57: [2024-10-30 16:30:56] iter = 13760, loss = 4.5774
2024-10-30 16:31:01: [2024-10-30 16:31:01] iter = 13770, loss = 2.3338
2024-10-30 16:31:06: [2024-10-30 16:31:06] iter = 13780, loss = 2.5536
2024-10-30 16:31:10: [2024-10-30 16:31:10] iter = 13790, loss = 2.7433
2024-10-30 16:31:15: [2024-10-30 16:31:15] iter = 13800, loss = 2.4440
2024-10-30 16:31:19: [2024-10-30 16:31:19] iter = 13810, loss = 3.2598
2024-10-30 16:31:23: [2024-10-30 16:31:23] iter = 13820, loss = 2.3047
2024-10-30 16:31:27: [2024-10-30 16:31:27] iter = 13830, loss = 2.3662
2024-10-30 16:31:32: [2024-10-30 16:31:32] iter = 13840, loss = 1.9956
2024-10-30 16:31:36: [2024-10-30 16:31:36] iter = 13850, loss = 2.6775
2024-10-30 16:31:40: [2024-10-30 16:31:40] iter = 13860, loss = 2.4913
2024-10-30 16:31:45: [2024-10-30 16:31:45] iter = 13870, loss = 2.6254
2024-10-30 16:31:49: [2024-10-30 16:31:49] iter = 13880, loss = 2.4593
2024-10-30 16:31:52: [2024-10-30 16:31:52] iter = 13890, loss = 1.8318
2024-10-30 16:31:56: [2024-10-30 16:31:56] iter = 13900, loss = 2.0767
2024-10-30 16:31:59: [2024-10-30 16:31:59] iter = 13910, loss = 1.5626
2024-10-30 16:32:03: [2024-10-30 16:32:03] iter = 13920, loss = 1.8746
2024-10-30 16:32:08: [2024-10-30 16:32:08] iter = 13930, loss = 1.9690
2024-10-30 16:32:13: [2024-10-30 16:32:13] iter = 13940, loss = 1.9942
2024-10-30 16:32:18: [2024-10-30 16:32:18] iter = 13950, loss = 2.5849
2024-10-30 16:32:22: [2024-10-30 16:32:22] iter = 13960, loss = 2.1297
2024-10-30 16:32:26: [2024-10-30 16:32:26] iter = 13970, loss = 1.9605
2024-10-30 16:32:30: [2024-10-30 16:32:30] iter = 13980, loss = 2.0302
2024-10-30 16:32:35: [2024-10-30 16:32:35] iter = 13990, loss = 2.0455
2024-10-30 16:32:39: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
2024-10-30 16:32:39: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 16:32:39: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 59066}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 16:35:27: Evaluate 5 random ConvNet, ACCmean = 0.7868 ACCstd = 0.0032
-------------------------
2024-10-30 16:35:27: Evaluate 5 random ConvNet, SENmean = 0.7831 SENstd = 0.0024
-------------------------
2024-10-30 16:35:27: Evaluate 5 random ConvNet, SPEmean = 0.9787 SPEstd = 0.0003
-------------------------
2024-10-30 16:35:27: Evaluate 5 random ConvNet, F!mean = 0.7691 F!std = 0.0029
-------------------------
2024-10-30 16:35:27: Evaluate 5 random ConvNet, mean = 0.7868 std = 0.0032
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 16:35:28: [2024-10-30 16:35:28] iter = 14000, loss = 4.0431
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 16:35:32: [2024-10-30 16:35:32] iter = 14010, loss = 2.2621
2024-10-30 16:35:36: [2024-10-30 16:35:36] iter = 14020, loss = 2.8168
2024-10-30 16:35:40: [2024-10-30 16:35:40] iter = 14030, loss = 7.5237
2024-10-30 16:35:45: [2024-10-30 16:35:45] iter = 14040, loss = 1.9103
2024-10-30 16:35:49: [2024-10-30 16:35:49] iter = 14050, loss = 2.2706
2024-10-30 16:35:53: [2024-10-30 16:35:53] iter = 14060, loss = 2.5417
2024-10-30 16:35:57: [2024-10-30 16:35:57] iter = 14070, loss = 2.4110
2024-10-30 16:36:01: [2024-10-30 16:36:01] iter = 14080, loss = 2.2269
2024-10-30 16:36:05: [2024-10-30 16:36:05] iter = 14090, loss = 1.9081
2024-10-30 16:36:10: [2024-10-30 16:36:10] iter = 14100, loss = 2.1771
2024-10-30 16:36:13: [2024-10-30 16:36:13] iter = 14110, loss = 1.9966
2024-10-30 16:36:16: [2024-10-30 16:36:16] iter = 14120, loss = 7.4375
2024-10-30 16:36:21: [2024-10-30 16:36:21] iter = 14130, loss = 2.2468
2024-10-30 16:36:26: [2024-10-30 16:36:26] iter = 14140, loss = 2.8335
2024-10-30 16:36:31: [2024-10-30 16:36:31] iter = 14150, loss = 2.2174
2024-10-30 16:36:35: [2024-10-30 16:36:35] iter = 14160, loss = 3.5797
2024-10-30 16:36:39: [2024-10-30 16:36:39] iter = 14170, loss = 2.0768
2024-10-30 16:36:43: [2024-10-30 16:36:43] iter = 14180, loss = 2.4316
2024-10-30 16:36:47: [2024-10-30 16:36:47] iter = 14190, loss = 2.3706
2024-10-30 16:36:51: [2024-10-30 16:36:51] iter = 14200, loss = 2.6870
2024-10-30 16:36:55: [2024-10-30 16:36:55] iter = 14210, loss = 2.3660
2024-10-30 16:37:00: [2024-10-30 16:37:00] iter = 14220, loss = 2.6851
2024-10-30 16:37:05: [2024-10-30 16:37:05] iter = 14230, loss = 2.1796
2024-10-30 16:37:09: [2024-10-30 16:37:09] iter = 14240, loss = 2.3018
2024-10-30 16:37:14: [2024-10-30 16:37:14] iter = 14250, loss = 1.8423
2024-10-30 16:37:19: [2024-10-30 16:37:19] iter = 14260, loss = 2.0733
2024-10-30 16:37:24: [2024-10-30 16:37:24] iter = 14270, loss = 1.9672
2024-10-30 16:37:29: [2024-10-30 16:37:29] iter = 14280, loss = 3.3610
2024-10-30 16:37:33: [2024-10-30 16:37:33] iter = 14290, loss = 2.2372
2024-10-30 16:37:37: [2024-10-30 16:37:37] iter = 14300, loss = 2.1309
2024-10-30 16:37:42: [2024-10-30 16:37:42] iter = 14310, loss = 1.9585
2024-10-30 16:37:46: [2024-10-30 16:37:46] iter = 14320, loss = 3.6214
2024-10-30 16:37:49: [2024-10-30 16:37:49] iter = 14330, loss = 5.3150
2024-10-30 16:37:53: [2024-10-30 16:37:53] iter = 14340, loss = 2.3971
2024-10-30 16:37:57: [2024-10-30 16:37:57] iter = 14350, loss = 2.1478
2024-10-30 16:38:01: [2024-10-30 16:38:01] iter = 14360, loss = 2.6601
2024-10-30 16:38:06: [2024-10-30 16:38:06] iter = 14370, loss = 2.4288
2024-10-30 16:38:10: [2024-10-30 16:38:10] iter = 14380, loss = 3.0172
2024-10-30 16:38:14: [2024-10-30 16:38:14] iter = 14390, loss = 2.4859
2024-10-30 16:38:18: [2024-10-30 16:38:18] iter = 14400, loss = 2.5629
2024-10-30 16:38:23: [2024-10-30 16:38:23] iter = 14410, loss = 1.8493
2024-10-30 16:38:27: [2024-10-30 16:38:27] iter = 14420, loss = 2.4962
2024-10-30 16:38:31: [2024-10-30 16:38:31] iter = 14430, loss = 1.9714
2024-10-30 16:38:36: [2024-10-30 16:38:36] iter = 14440, loss = 2.0599
2024-10-30 16:38:41: [2024-10-30 16:38:41] iter = 14450, loss = 2.5372
2024-10-30 16:38:45: [2024-10-30 16:38:45] iter = 14460, loss = 2.8028
2024-10-30 16:38:49: [2024-10-30 16:38:49] iter = 14470, loss = 2.1686
2024-10-30 16:38:53: [2024-10-30 16:38:53] iter = 14480, loss = 2.3194
2024-10-30 16:38:58: [2024-10-30 16:38:58] iter = 14490, loss = 1.8926
2024-10-30 16:39:02: [2024-10-30 16:39:02] iter = 14500, loss = 2.3214
2024-10-30 16:39:06: [2024-10-30 16:39:06] iter = 14510, loss = 2.1342
2024-10-30 16:39:11: [2024-10-30 16:39:11] iter = 14520, loss = 2.1475
2024-10-30 16:39:15: [2024-10-30 16:39:15] iter = 14530, loss = 2.7270
2024-10-30 16:39:19: [2024-10-30 16:39:19] iter = 14540, loss = 2.0213
2024-10-30 16:39:24: [2024-10-30 16:39:24] iter = 14550, loss = 2.9275
2024-10-30 16:39:28: [2024-10-30 16:39:28] iter = 14560, loss = 2.6475
2024-10-30 16:39:32: [2024-10-30 16:39:32] iter = 14570, loss = 2.2747
2024-10-30 16:39:37: [2024-10-30 16:39:37] iter = 14580, loss = 2.4580
2024-10-30 16:39:41: [2024-10-30 16:39:41] iter = 14590, loss = 1.7773
2024-10-30 16:39:46: [2024-10-30 16:39:46] iter = 14600, loss = 2.0523
2024-10-30 16:39:51: [2024-10-30 16:39:51] iter = 14610, loss = 3.1029
2024-10-30 16:39:55: [2024-10-30 16:39:55] iter = 14620, loss = 1.9043
2024-10-30 16:40:00: [2024-10-30 16:40:00] iter = 14630, loss = 2.1518
2024-10-30 16:40:04: [2024-10-30 16:40:04] iter = 14640, loss = 1.9676
2024-10-30 16:40:11: [2024-10-30 16:40:11] iter = 14650, loss = 2.1759
2024-10-30 16:40:16: [2024-10-30 16:40:16] iter = 14660, loss = 3.4251
2024-10-30 16:40:20: [2024-10-30 16:40:20] iter = 14670, loss = 1.9281
2024-10-30 16:40:24: [2024-10-30 16:40:24] iter = 14680, loss = 3.1977
2024-10-30 16:40:29: [2024-10-30 16:40:29] iter = 14690, loss = 2.0641
2024-10-30 16:40:33: [2024-10-30 16:40:33] iter = 14700, loss = 2.6799
2024-10-30 16:40:38: [2024-10-30 16:40:38] iter = 14710, loss = 2.1324
2024-10-30 16:40:42: [2024-10-30 16:40:42] iter = 14720, loss = 2.1597
2024-10-30 16:40:47: [2024-10-30 16:40:47] iter = 14730, loss = 2.0406
2024-10-30 16:40:51: [2024-10-30 16:40:51] iter = 14740, loss = 2.0853
2024-10-30 16:40:56: [2024-10-30 16:40:56] iter = 14750, loss = 2.9703
2024-10-30 16:40:59: [2024-10-30 16:40:59] iter = 14760, loss = 2.2421
2024-10-30 16:41:04: [2024-10-30 16:41:04] iter = 14770, loss = 2.1061
2024-10-30 16:41:08: [2024-10-30 16:41:08] iter = 14780, loss = 2.2514
2024-10-30 16:41:12: [2024-10-30 16:41:12] iter = 14790, loss = 2.3853
2024-10-30 16:41:16: [2024-10-30 16:41:16] iter = 14800, loss = 1.9545
2024-10-30 16:41:20: [2024-10-30 16:41:20] iter = 14810, loss = 2.0064
2024-10-30 16:41:24: [2024-10-30 16:41:24] iter = 14820, loss = 1.8202
2024-10-30 16:41:28: [2024-10-30 16:41:28] iter = 14830, loss = 2.8533
2024-10-30 16:41:31: [2024-10-30 16:41:31] iter = 14840, loss = 2.3139
2024-10-30 16:41:36: [2024-10-30 16:41:36] iter = 14850, loss = 2.2676
2024-10-30 16:41:40: [2024-10-30 16:41:40] iter = 14860, loss = 2.6991
2024-10-30 16:41:44: [2024-10-30 16:41:44] iter = 14870, loss = 1.6926
2024-10-30 16:41:48: [2024-10-30 16:41:48] iter = 14880, loss = 2.3706
2024-10-30 16:41:52: [2024-10-30 16:41:52] iter = 14890, loss = 2.2528
2024-10-30 16:41:57: [2024-10-30 16:41:57] iter = 14900, loss = 1.8769
2024-10-30 16:42:01: [2024-10-30 16:42:01] iter = 14910, loss = 1.6850
2024-10-30 16:42:05: [2024-10-30 16:42:05] iter = 14920, loss = 2.0125
2024-10-30 16:42:08: [2024-10-30 16:42:08] iter = 14930, loss = 2.4476
2024-10-30 16:42:12: [2024-10-30 16:42:12] iter = 14940, loss = 2.3522
2024-10-30 16:42:17: [2024-10-30 16:42:17] iter = 14950, loss = 2.1341
2024-10-30 16:42:21: [2024-10-30 16:42:21] iter = 14960, loss = 1.9957
2024-10-30 16:42:25: [2024-10-30 16:42:25] iter = 14970, loss = 2.0871
2024-10-30 16:42:30: [2024-10-30 16:42:30] iter = 14980, loss = 2.5219
2024-10-30 16:42:34: [2024-10-30 16:42:34] iter = 14990, loss = 2.0031
2024-10-30 16:42:39: [2024-10-30 16:42:39] iter = 15000, loss = 2.3311
2024-10-30 16:42:42: [2024-10-30 16:42:42] iter = 15010, loss = 2.5538
2024-10-30 16:42:46: [2024-10-30 16:42:46] iter = 15020, loss = 2.1626
2024-10-30 16:42:50: [2024-10-30 16:42:50] iter = 15030, loss = 3.1818
2024-10-30 16:42:54: [2024-10-30 16:42:54] iter = 15040, loss = 3.4185
2024-10-30 16:42:58: [2024-10-30 16:42:58] iter = 15050, loss = 2.4112
2024-10-30 16:43:03: [2024-10-30 16:43:03] iter = 15060, loss = 5.9470
2024-10-30 16:43:06: [2024-10-30 16:43:06] iter = 15070, loss = 1.8941
2024-10-30 16:43:10: [2024-10-30 16:43:10] iter = 15080, loss = 2.1230
2024-10-30 16:43:14: [2024-10-30 16:43:14] iter = 15090, loss = 1.8152
2024-10-30 16:43:18: [2024-10-30 16:43:18] iter = 15100, loss = 2.6322
2024-10-30 16:43:23: [2024-10-30 16:43:23] iter = 15110, loss = 2.2513
2024-10-30 16:43:28: [2024-10-30 16:43:28] iter = 15120, loss = 2.2810
2024-10-30 16:43:32: [2024-10-30 16:43:32] iter = 15130, loss = 2.1058
2024-10-30 16:43:35: [2024-10-30 16:43:35] iter = 15140, loss = 2.2879
2024-10-30 16:43:39: [2024-10-30 16:43:39] iter = 15150, loss = 2.2759
2024-10-30 16:43:43: [2024-10-30 16:43:43] iter = 15160, loss = 2.7657
2024-10-30 16:43:49: [2024-10-30 16:43:49] iter = 15170, loss = 2.3542
2024-10-30 16:43:53: [2024-10-30 16:43:53] iter = 15180, loss = 2.3419
2024-10-30 16:43:57: [2024-10-30 16:43:57] iter = 15190, loss = 2.3034
2024-10-30 16:44:02: [2024-10-30 16:44:02] iter = 15200, loss = 2.5702
2024-10-30 16:44:07: [2024-10-30 16:44:07] iter = 15210, loss = 1.8658
2024-10-30 16:44:11: [2024-10-30 16:44:11] iter = 15220, loss = 2.6063
2024-10-30 16:44:15: [2024-10-30 16:44:15] iter = 15230, loss = 1.9857
2024-10-30 16:44:20: [2024-10-30 16:44:20] iter = 15240, loss = 2.1013
2024-10-30 16:44:23: [2024-10-30 16:44:23] iter = 15250, loss = 1.9833
2024-10-30 16:44:26: [2024-10-30 16:44:26] iter = 15260, loss = 1.9626
2024-10-30 16:44:30: [2024-10-30 16:44:30] iter = 15270, loss = 2.6778
2024-10-30 16:44:34: [2024-10-30 16:44:34] iter = 15280, loss = 2.4482
2024-10-30 16:44:38: [2024-10-30 16:44:38] iter = 15290, loss = 2.1481
2024-10-30 16:44:42: [2024-10-30 16:44:42] iter = 15300, loss = 1.9103
2024-10-30 16:44:47: [2024-10-30 16:44:47] iter = 15310, loss = 2.3713
2024-10-30 16:44:51: [2024-10-30 16:44:51] iter = 15320, loss = 2.2088
2024-10-30 16:44:56: [2024-10-30 16:44:56] iter = 15330, loss = 1.8077
2024-10-30 16:45:01: [2024-10-30 16:45:01] iter = 15340, loss = 2.7929
2024-10-30 16:45:05: [2024-10-30 16:45:05] iter = 15350, loss = 2.1666
2024-10-30 16:45:10: [2024-10-30 16:45:10] iter = 15360, loss = 1.6708
2024-10-30 16:45:14: [2024-10-30 16:45:14] iter = 15370, loss = 7.4519
2024-10-30 16:45:19: [2024-10-30 16:45:19] iter = 15380, loss = 2.7343
2024-10-30 16:45:23: [2024-10-30 16:45:23] iter = 15390, loss = 1.9670
2024-10-30 16:45:27: [2024-10-30 16:45:27] iter = 15400, loss = 1.8533
2024-10-30 16:45:31: [2024-10-30 16:45:31] iter = 15410, loss = 2.0078
2024-10-30 16:45:35: [2024-10-30 16:45:35] iter = 15420, loss = 2.4931
2024-10-30 16:45:38: [2024-10-30 16:45:38] iter = 15430, loss = 3.9739
2024-10-30 16:45:43: [2024-10-30 16:45:43] iter = 15440, loss = 2.0566
2024-10-30 16:45:48: [2024-10-30 16:45:48] iter = 15450, loss = 2.4597
2024-10-30 16:45:53: [2024-10-30 16:45:53] iter = 15460, loss = 2.2709
2024-10-30 16:45:56: [2024-10-30 16:45:56] iter = 15470, loss = 2.2007
2024-10-30 16:45:59: [2024-10-30 16:45:59] iter = 15480, loss = 2.1480
2024-10-30 16:46:03: [2024-10-30 16:46:03] iter = 15490, loss = 2.1999
2024-10-30 16:46:07: [2024-10-30 16:46:07] iter = 15500, loss = 2.1106
2024-10-30 16:46:11: [2024-10-30 16:46:11] iter = 15510, loss = 3.0599
2024-10-30 16:46:14: [2024-10-30 16:46:14] iter = 15520, loss = 4.2626
2024-10-30 16:46:18: [2024-10-30 16:46:18] iter = 15530, loss = 2.4120
2024-10-30 16:46:23: [2024-10-30 16:46:23] iter = 15540, loss = 2.0756
2024-10-30 16:46:28: [2024-10-30 16:46:28] iter = 15550, loss = 2.1351
2024-10-30 16:46:32: [2024-10-30 16:46:32] iter = 15560, loss = 1.9359
2024-10-30 16:46:36: [2024-10-30 16:46:36] iter = 15570, loss = 2.6354
2024-10-30 16:46:39: [2024-10-30 16:46:39] iter = 15580, loss = 2.8867
2024-10-30 16:46:42: [2024-10-30 16:46:42] iter = 15590, loss = 2.0927
2024-10-30 16:46:46: [2024-10-30 16:46:46] iter = 15600, loss = 2.0581
2024-10-30 16:46:50: [2024-10-30 16:46:50] iter = 15610, loss = 2.7743
2024-10-30 16:46:54: [2024-10-30 16:46:54] iter = 15620, loss = 1.6395
2024-10-30 16:46:58: [2024-10-30 16:46:58] iter = 15630, loss = 2.2997
2024-10-30 16:47:02: [2024-10-30 16:47:02] iter = 15640, loss = 1.8266
2024-10-30 16:47:06: [2024-10-30 16:47:06] iter = 15650, loss = 2.2997
2024-10-30 16:47:09: [2024-10-30 16:47:09] iter = 15660, loss = 1.9802
2024-10-30 16:47:12: [2024-10-30 16:47:12] iter = 15670, loss = 2.1425
2024-10-30 16:47:17: [2024-10-30 16:47:17] iter = 15680, loss = 2.1298
2024-10-30 16:47:21: [2024-10-30 16:47:21] iter = 15690, loss = 2.0410
2024-10-30 16:47:24: [2024-10-30 16:47:24] iter = 15700, loss = 1.9790
2024-10-30 16:47:29: [2024-10-30 16:47:29] iter = 15710, loss = 2.3817
2024-10-30 16:47:33: [2024-10-30 16:47:33] iter = 15720, loss = 3.1388
2024-10-30 16:47:37: [2024-10-30 16:47:37] iter = 15730, loss = 1.9020
2024-10-30 16:47:40: [2024-10-30 16:47:40] iter = 15740, loss = 2.8031
2024-10-30 16:47:44: [2024-10-30 16:47:44] iter = 15750, loss = 2.0599
2024-10-30 16:47:48: [2024-10-30 16:47:48] iter = 15760, loss = 2.6881
2024-10-30 16:47:52: [2024-10-30 16:47:52] iter = 15770, loss = 2.9771
2024-10-30 16:47:57: [2024-10-30 16:47:57] iter = 15780, loss = 1.8767
2024-10-30 16:48:01: [2024-10-30 16:48:01] iter = 15790, loss = 5.3224
2024-10-30 16:48:06: [2024-10-30 16:48:06] iter = 15800, loss = 2.1588
2024-10-30 16:48:10: [2024-10-30 16:48:10] iter = 15810, loss = 2.2635
2024-10-30 16:48:14: [2024-10-30 16:48:14] iter = 15820, loss = 2.0607
2024-10-30 16:48:19: [2024-10-30 16:48:19] iter = 15830, loss = 2.1483
2024-10-30 16:48:23: [2024-10-30 16:48:23] iter = 15840, loss = 3.1463
2024-10-30 16:48:28: [2024-10-30 16:48:28] iter = 15850, loss = 2.5411
2024-10-30 16:48:32: [2024-10-30 16:48:32] iter = 15860, loss = 9.4371
2024-10-30 16:48:35: [2024-10-30 16:48:35] iter = 15870, loss = 2.0745
2024-10-30 16:48:40: [2024-10-30 16:48:40] iter = 15880, loss = 1.9906
2024-10-30 16:48:44: [2024-10-30 16:48:44] iter = 15890, loss = 2.5415
2024-10-30 16:48:49: [2024-10-30 16:48:49] iter = 15900, loss = 2.4411
2024-10-30 16:48:52: [2024-10-30 16:48:52] iter = 15910, loss = 2.0168
2024-10-30 16:48:56: [2024-10-30 16:48:56] iter = 15920, loss = 6.0656
2024-10-30 16:48:59: [2024-10-30 16:48:59] iter = 15930, loss = 2.0518
2024-10-30 16:49:03: [2024-10-30 16:49:03] iter = 15940, loss = 2.1632
2024-10-30 16:49:07: [2024-10-30 16:49:07] iter = 15950, loss = 1.8290
2024-10-30 16:49:12: [2024-10-30 16:49:12] iter = 15960, loss = 2.8708
2024-10-30 16:49:17: [2024-10-30 16:49:17] iter = 15970, loss = 1.9878
2024-10-30 16:49:21: [2024-10-30 16:49:21] iter = 15980, loss = 2.8704
2024-10-30 16:49:24: [2024-10-30 16:49:24] iter = 15990, loss = 1.8886
2024-10-30 16:49:29: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
2024-10-30 16:49:29: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 16:49:29: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 69019}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 16:52:20: Evaluate 5 random ConvNet, ACCmean = 0.7897 ACCstd = 0.0017
-------------------------
2024-10-30 16:52:20: Evaluate 5 random ConvNet, SENmean = 0.7871 SENstd = 0.0019
-------------------------
2024-10-30 16:52:20: Evaluate 5 random ConvNet, SPEmean = 0.9789 SPEstd = 0.0002
-------------------------
2024-10-30 16:52:20: Evaluate 5 random ConvNet, F!mean = 0.7757 F!std = 0.0017
-------------------------
2024-10-30 16:52:20: Evaluate 5 random ConvNet, mean = 0.7897 std = 0.0017
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 16:52:21: [2024-10-30 16:52:21] iter = 16000, loss = 2.3054
2024-10-30 16:52:24: [2024-10-30 16:52:24] iter = 16010, loss = 2.1620
2024-10-30 16:52:28: [2024-10-30 16:52:28] iter = 16020, loss = 2.3513
2024-10-30 16:52:32: [2024-10-30 16:52:32] iter = 16030, loss = 2.7563
2024-10-30 16:52:36: [2024-10-30 16:52:36] iter = 16040, loss = 1.9491
2024-10-30 16:52:41: [2024-10-30 16:52:41] iter = 16050, loss = 2.0307
2024-10-30 16:52:44: [2024-10-30 16:52:44] iter = 16060, loss = 2.0175
2024-10-30 16:52:48: [2024-10-30 16:52:48] iter = 16070, loss = 2.6207
2024-10-30 16:52:51: [2024-10-30 16:52:51] iter = 16080, loss = 1.9513
2024-10-30 16:52:57: [2024-10-30 16:52:57] iter = 16090, loss = 2.5773
2024-10-30 16:53:01: [2024-10-30 16:53:01] iter = 16100, loss = 3.0935
2024-10-30 16:53:04: [2024-10-30 16:53:04] iter = 16110, loss = 2.1396
2024-10-30 16:53:09: [2024-10-30 16:53:09] iter = 16120, loss = 2.0129
2024-10-30 16:53:12: [2024-10-30 16:53:12] iter = 16130, loss = 2.1122
2024-10-30 16:53:18: [2024-10-30 16:53:18] iter = 16140, loss = 1.8261
2024-10-30 16:53:22: [2024-10-30 16:53:22] iter = 16150, loss = 5.2898
2024-10-30 16:53:26: [2024-10-30 16:53:26] iter = 16160, loss = 1.6674
2024-10-30 16:53:30: [2024-10-30 16:53:30] iter = 16170, loss = 2.8191
2024-10-30 16:53:34: [2024-10-30 16:53:34] iter = 16180, loss = 2.0168
2024-10-30 16:53:38: [2024-10-30 16:53:38] iter = 16190, loss = 1.8955
2024-10-30 16:53:43: [2024-10-30 16:53:43] iter = 16200, loss = 2.2269
2024-10-30 16:53:46: [2024-10-30 16:53:46] iter = 16210, loss = 2.3207
2024-10-30 16:53:50: [2024-10-30 16:53:50] iter = 16220, loss = 2.0894
2024-10-30 16:53:54: [2024-10-30 16:53:54] iter = 16230, loss = 2.5997
2024-10-30 16:53:58: [2024-10-30 16:53:58] iter = 16240, loss = 2.2903
2024-10-30 16:54:03: [2024-10-30 16:54:03] iter = 16250, loss = 2.5841
2024-10-30 16:54:07: [2024-10-30 16:54:07] iter = 16260, loss = 1.9248
2024-10-30 16:54:10: [2024-10-30 16:54:10] iter = 16270, loss = 2.5191
2024-10-30 16:54:15: [2024-10-30 16:54:15] iter = 16280, loss = 3.4394
2024-10-30 16:54:19: [2024-10-30 16:54:19] iter = 16290, loss = 3.0428
2024-10-30 16:54:24: [2024-10-30 16:54:24] iter = 16300, loss = 2.2176
2024-10-30 16:54:28: [2024-10-30 16:54:28] iter = 16310, loss = 1.8669
2024-10-30 16:54:33: [2024-10-30 16:54:33] iter = 16320, loss = 2.7272
2024-10-30 16:54:36: [2024-10-30 16:54:36] iter = 16330, loss = 1.9682
2024-10-30 16:54:40: [2024-10-30 16:54:40] iter = 16340, loss = 3.4176
2024-10-30 16:54:44: [2024-10-30 16:54:44] iter = 16350, loss = 2.2756
2024-10-30 16:54:47: [2024-10-30 16:54:47] iter = 16360, loss = 1.9016
2024-10-30 16:54:51: [2024-10-30 16:54:51] iter = 16370, loss = 3.3555
2024-10-30 16:54:55: [2024-10-30 16:54:55] iter = 16380, loss = 2.7172
2024-10-30 16:55:00: [2024-10-30 16:55:00] iter = 16390, loss = 1.9013
2024-10-30 16:55:03: [2024-10-30 16:55:03] iter = 16400, loss = 2.0673
2024-10-30 16:55:08: [2024-10-30 16:55:08] iter = 16410, loss = 1.8439
2024-10-30 16:55:12: [2024-10-30 16:55:12] iter = 16420, loss = 2.1482
2024-10-30 16:55:16: [2024-10-30 16:55:16] iter = 16430, loss = 2.3956
2024-10-30 16:55:20: [2024-10-30 16:55:20] iter = 16440, loss = 2.2732
2024-10-30 16:55:25: [2024-10-30 16:55:25] iter = 16450, loss = 1.8710
2024-10-30 16:55:29: [2024-10-30 16:55:29] iter = 16460, loss = 2.0888
2024-10-30 16:55:34: [2024-10-30 16:55:34] iter = 16470, loss = 2.4850
2024-10-30 16:55:38: [2024-10-30 16:55:38] iter = 16480, loss = 2.0889
2024-10-30 16:55:41: [2024-10-30 16:55:41] iter = 16490, loss = 3.0423
2024-10-30 16:55:46: [2024-10-30 16:55:46] iter = 16500, loss = 2.6035
2024-10-30 16:55:50: [2024-10-30 16:55:50] iter = 16510, loss = 2.3559
2024-10-30 16:55:54: [2024-10-30 16:55:54] iter = 16520, loss = 2.0650
2024-10-30 16:55:58: [2024-10-30 16:55:58] iter = 16530, loss = 2.4862
2024-10-30 16:56:02: [2024-10-30 16:56:02] iter = 16540, loss = 2.2147
2024-10-30 16:56:06: [2024-10-30 16:56:06] iter = 16550, loss = 2.7276
2024-10-30 16:56:10: [2024-10-30 16:56:10] iter = 16560, loss = 2.2142
2024-10-30 16:56:14: [2024-10-30 16:56:14] iter = 16570, loss = 1.7958
2024-10-30 16:56:18: [2024-10-30 16:56:18] iter = 16580, loss = 2.0932
2024-10-30 16:56:23: [2024-10-30 16:56:23] iter = 16590, loss = 2.0833
2024-10-30 16:56:27: [2024-10-30 16:56:27] iter = 16600, loss = 3.0021
2024-10-30 16:56:30: [2024-10-30 16:56:30] iter = 16610, loss = 2.8615
2024-10-30 16:56:34: [2024-10-30 16:56:34] iter = 16620, loss = 1.8628
2024-10-30 16:56:38: [2024-10-30 16:56:38] iter = 16630, loss = 2.1630
2024-10-30 16:56:43: [2024-10-30 16:56:43] iter = 16640, loss = 1.8812
2024-10-30 16:56:49: [2024-10-30 16:56:49] iter = 16650, loss = 2.0702
2024-10-30 16:56:53: [2024-10-30 16:56:53] iter = 16660, loss = 1.7207
2024-10-30 16:56:56: [2024-10-30 16:56:56] iter = 16670, loss = 1.9750
2024-10-30 16:57:00: [2024-10-30 16:57:00] iter = 16680, loss = 1.8600
2024-10-30 16:57:04: [2024-10-30 16:57:04] iter = 16690, loss = 2.9666
2024-10-30 16:57:08: [2024-10-30 16:57:08] iter = 16700, loss = 3.9525
2024-10-30 16:57:12: [2024-10-30 16:57:12] iter = 16710, loss = 1.9027
2024-10-30 16:57:16: [2024-10-30 16:57:16] iter = 16720, loss = 1.7669
2024-10-30 16:57:20: [2024-10-30 16:57:20] iter = 16730, loss = 2.7965
2024-10-30 16:57:25: [2024-10-30 16:57:25] iter = 16740, loss = 2.5234
2024-10-30 16:57:29: [2024-10-30 16:57:29] iter = 16750, loss = 2.0853
2024-10-30 16:57:33: [2024-10-30 16:57:33] iter = 16760, loss = 2.0067
2024-10-30 16:57:37: [2024-10-30 16:57:37] iter = 16770, loss = 2.7687
2024-10-30 16:57:42: [2024-10-30 16:57:42] iter = 16780, loss = 1.8940
2024-10-30 16:57:46: [2024-10-30 16:57:46] iter = 16790, loss = 2.8847
2024-10-30 16:57:50: [2024-10-30 16:57:50] iter = 16800, loss = 2.8574
2024-10-30 16:57:54: [2024-10-30 16:57:54] iter = 16810, loss = 2.2888
2024-10-30 16:57:59: [2024-10-30 16:57:59] iter = 16820, loss = 2.3735
2024-10-30 16:58:03: [2024-10-30 16:58:03] iter = 16830, loss = 2.9841
2024-10-30 16:58:07: [2024-10-30 16:58:07] iter = 16840, loss = 2.1211
2024-10-30 16:58:11: [2024-10-30 16:58:11] iter = 16850, loss = 8.0526
2024-10-30 16:58:15: [2024-10-30 16:58:15] iter = 16860, loss = 3.3055
2024-10-30 16:58:20: [2024-10-30 16:58:20] iter = 16870, loss = 3.5508
2024-10-30 16:58:24: [2024-10-30 16:58:24] iter = 16880, loss = 1.6584
2024-10-30 16:58:28: [2024-10-30 16:58:28] iter = 16890, loss = 2.0792
2024-10-30 16:58:32: [2024-10-30 16:58:32] iter = 16900, loss = 5.4037
2024-10-30 16:58:36: [2024-10-30 16:58:36] iter = 16910, loss = 2.3217
2024-10-30 16:58:41: [2024-10-30 16:58:41] iter = 16920, loss = 1.8832
2024-10-30 16:58:45: [2024-10-30 16:58:45] iter = 16930, loss = 2.6101
2024-10-30 16:58:50: [2024-10-30 16:58:50] iter = 16940, loss = 2.1556
2024-10-30 16:58:56: [2024-10-30 16:58:56] iter = 16950, loss = 2.1438
2024-10-30 16:59:01: [2024-10-30 16:59:01] iter = 16960, loss = 2.2980
2024-10-30 16:59:05: [2024-10-30 16:59:05] iter = 16970, loss = 2.4243
2024-10-30 16:59:08: [2024-10-30 16:59:08] iter = 16980, loss = 2.1282
2024-10-30 16:59:12: [2024-10-30 16:59:12] iter = 16990, loss = 2.4066
2024-10-30 16:59:15: [2024-10-30 16:59:15] iter = 17000, loss = 1.9056
2024-10-30 16:59:19: [2024-10-30 16:59:19] iter = 17010, loss = 3.1156
2024-10-30 16:59:22: [2024-10-30 16:59:22] iter = 17020, loss = 2.0101
2024-10-30 16:59:27: [2024-10-30 16:59:27] iter = 17030, loss = 3.8504
2024-10-30 16:59:31: [2024-10-30 16:59:31] iter = 17040, loss = 1.8081
2024-10-30 16:59:35: [2024-10-30 16:59:35] iter = 17050, loss = 3.9118
2024-10-30 16:59:40: [2024-10-30 16:59:40] iter = 17060, loss = 2.0953
2024-10-30 16:59:46: [2024-10-30 16:59:46] iter = 17070, loss = 2.0335
2024-10-30 16:59:51: [2024-10-30 16:59:51] iter = 17080, loss = 3.4009
2024-10-30 16:59:56: [2024-10-30 16:59:56] iter = 17090, loss = 3.5936
2024-10-30 17:00:00: [2024-10-30 17:00:00] iter = 17100, loss = 2.4800
2024-10-30 17:00:05: [2024-10-30 17:00:05] iter = 17110, loss = 1.8839
2024-10-30 17:00:10: [2024-10-30 17:00:10] iter = 17120, loss = 1.9180
2024-10-30 17:00:14: [2024-10-30 17:00:14] iter = 17130, loss = 3.0564
2024-10-30 17:00:18: [2024-10-30 17:00:18] iter = 17140, loss = 2.0084
2024-10-30 17:00:21: [2024-10-30 17:00:21] iter = 17150, loss = 2.6221
2024-10-30 17:00:26: [2024-10-30 17:00:26] iter = 17160, loss = 1.9713
2024-10-30 17:00:30: [2024-10-30 17:00:30] iter = 17170, loss = 2.0896
2024-10-30 17:00:34: [2024-10-30 17:00:34] iter = 17180, loss = 2.3649
2024-10-30 17:00:38: [2024-10-30 17:00:38] iter = 17190, loss = 1.8483
2024-10-30 17:00:42: [2024-10-30 17:00:42] iter = 17200, loss = 2.3530
2024-10-30 17:00:47: [2024-10-30 17:00:47] iter = 17210, loss = 1.6974
2024-10-30 17:00:51: [2024-10-30 17:00:51] iter = 17220, loss = 2.1195
2024-10-30 17:00:55: [2024-10-30 17:00:55] iter = 17230, loss = 1.7623
2024-10-30 17:00:59: [2024-10-30 17:00:59] iter = 17240, loss = 1.8490
2024-10-30 17:01:03: [2024-10-30 17:01:03] iter = 17250, loss = 2.2086
2024-10-30 17:01:07: [2024-10-30 17:01:07] iter = 17260, loss = 2.2792
2024-10-30 17:01:11: [2024-10-30 17:01:11] iter = 17270, loss = 2.2682
2024-10-30 17:01:16: [2024-10-30 17:01:16] iter = 17280, loss = 2.0654
2024-10-30 17:01:22: [2024-10-30 17:01:22] iter = 17290, loss = 2.2838
2024-10-30 17:01:26: [2024-10-30 17:01:26] iter = 17300, loss = 2.5723
2024-10-30 17:01:30: [2024-10-30 17:01:30] iter = 17310, loss = 2.4013
2024-10-30 17:01:35: [2024-10-30 17:01:35] iter = 17320, loss = 2.2630
2024-10-30 17:01:40: [2024-10-30 17:01:40] iter = 17330, loss = 2.3053
2024-10-30 17:01:44: [2024-10-30 17:01:44] iter = 17340, loss = 1.8626
2024-10-30 17:01:49: [2024-10-30 17:01:49] iter = 17350, loss = 3.3724
2024-10-30 17:01:52: [2024-10-30 17:01:52] iter = 17360, loss = 3.6138
2024-10-30 17:01:57: [2024-10-30 17:01:57] iter = 17370, loss = 2.3603
2024-10-30 17:02:01: [2024-10-30 17:02:01] iter = 17380, loss = 2.7709
2024-10-30 17:02:06: [2024-10-30 17:02:06] iter = 17390, loss = 2.2045
2024-10-30 17:02:11: [2024-10-30 17:02:11] iter = 17400, loss = 2.4858
2024-10-30 17:02:16: [2024-10-30 17:02:16] iter = 17410, loss = 2.2897
2024-10-30 17:02:20: [2024-10-30 17:02:20] iter = 17420, loss = 2.6071
2024-10-30 17:02:24: [2024-10-30 17:02:24] iter = 17430, loss = 2.1585
2024-10-30 17:02:27: [2024-10-30 17:02:27] iter = 17440, loss = 2.2421
2024-10-30 17:02:32: [2024-10-30 17:02:32] iter = 17450, loss = 3.5101
2024-10-30 17:02:36: [2024-10-30 17:02:36] iter = 17460, loss = 3.1044
2024-10-30 17:02:41: [2024-10-30 17:02:41] iter = 17470, loss = 2.9805
2024-10-30 17:02:45: [2024-10-30 17:02:45] iter = 17480, loss = 2.2066
2024-10-30 17:02:49: [2024-10-30 17:02:49] iter = 17490, loss = 1.9671
2024-10-30 17:02:54: [2024-10-30 17:02:54] iter = 17500, loss = 1.9297
2024-10-30 17:02:59: [2024-10-30 17:02:59] iter = 17510, loss = 2.1954
2024-10-30 17:03:04: [2024-10-30 17:03:04] iter = 17520, loss = 2.1338
2024-10-30 17:03:08: [2024-10-30 17:03:08] iter = 17530, loss = 1.7742
2024-10-30 17:03:14: [2024-10-30 17:03:14] iter = 17540, loss = 1.6674
2024-10-30 17:03:18: [2024-10-30 17:03:18] iter = 17550, loss = 1.9372
2024-10-30 17:03:23: [2024-10-30 17:03:23] iter = 17560, loss = 2.1217
2024-10-30 17:03:28: [2024-10-30 17:03:28] iter = 17570, loss = 2.3132
2024-10-30 17:03:32: [2024-10-30 17:03:32] iter = 17580, loss = 1.8637
2024-10-30 17:03:35: [2024-10-30 17:03:35] iter = 17590, loss = 2.6421
2024-10-30 17:03:40: [2024-10-30 17:03:40] iter = 17600, loss = 2.9874
2024-10-30 17:03:44: [2024-10-30 17:03:44] iter = 17610, loss = 2.2687
2024-10-30 17:03:48: [2024-10-30 17:03:48] iter = 17620, loss = 2.3033
2024-10-30 17:03:53: [2024-10-30 17:03:53] iter = 17630, loss = 2.1774
2024-10-30 17:03:57: [2024-10-30 17:03:57] iter = 17640, loss = 2.3673
2024-10-30 17:04:02: [2024-10-30 17:04:02] iter = 17650, loss = 2.0628
2024-10-30 17:04:06: [2024-10-30 17:04:06] iter = 17660, loss = 3.6920
2024-10-30 17:04:11: [2024-10-30 17:04:11] iter = 17670, loss = 2.7829
2024-10-30 17:04:14: [2024-10-30 17:04:14] iter = 17680, loss = 1.8739
2024-10-30 17:04:18: [2024-10-30 17:04:18] iter = 17690, loss = 2.6774
2024-10-30 17:04:21: [2024-10-30 17:04:21] iter = 17700, loss = 2.9702
2024-10-30 17:04:25: [2024-10-30 17:04:25] iter = 17710, loss = 2.1670
2024-10-30 17:04:29: [2024-10-30 17:04:29] iter = 17720, loss = 2.2361
2024-10-30 17:04:33: [2024-10-30 17:04:33] iter = 17730, loss = 2.1773
2024-10-30 17:04:38: [2024-10-30 17:04:38] iter = 17740, loss = 2.4813
2024-10-30 17:04:42: [2024-10-30 17:04:42] iter = 17750, loss = 2.5583
2024-10-30 17:04:46: [2024-10-30 17:04:46] iter = 17760, loss = 2.0582
2024-10-30 17:04:50: [2024-10-30 17:04:50] iter = 17770, loss = 1.8535
2024-10-30 17:04:55: [2024-10-30 17:04:55] iter = 17780, loss = 1.7713
2024-10-30 17:04:59: [2024-10-30 17:04:59] iter = 17790, loss = 2.3492
2024-10-30 17:05:03: [2024-10-30 17:05:03] iter = 17800, loss = 5.4327
2024-10-30 17:05:08: [2024-10-30 17:05:08] iter = 17810, loss = 2.0845
2024-10-30 17:05:12: [2024-10-30 17:05:12] iter = 17820, loss = 2.7715
2024-10-30 17:05:17: [2024-10-30 17:05:17] iter = 17830, loss = 2.1904
2024-10-30 17:05:22: [2024-10-30 17:05:22] iter = 17840, loss = 2.2284
2024-10-30 17:05:27: [2024-10-30 17:05:27] iter = 17850, loss = 1.9588
2024-10-30 17:05:31: [2024-10-30 17:05:31] iter = 17860, loss = 1.7271
2024-10-30 17:05:36: [2024-10-30 17:05:36] iter = 17870, loss = 2.1411
2024-10-30 17:05:40: [2024-10-30 17:05:40] iter = 17880, loss = 2.3658
2024-10-30 17:05:44: [2024-10-30 17:05:44] iter = 17890, loss = 2.1989
2024-10-30 17:05:48: [2024-10-30 17:05:48] iter = 17900, loss = 1.9191
2024-10-30 17:05:51: [2024-10-30 17:05:51] iter = 17910, loss = 2.3156
2024-10-30 17:05:54: [2024-10-30 17:05:54] iter = 17920, loss = 1.7797
2024-10-30 17:05:57: [2024-10-30 17:05:57] iter = 17930, loss = 2.0729
2024-10-30 17:06:01: [2024-10-30 17:06:01] iter = 17940, loss = 1.7162
2024-10-30 17:06:06: [2024-10-30 17:06:06] iter = 17950, loss = 1.9868
2024-10-30 17:06:10: [2024-10-30 17:06:10] iter = 17960, loss = 1.9781
2024-10-30 17:06:14: [2024-10-30 17:06:14] iter = 17970, loss = 2.0710
2024-10-30 17:06:18: [2024-10-30 17:06:18] iter = 17980, loss = 2.0153
2024-10-30 17:06:22: [2024-10-30 17:06:22] iter = 17990, loss = 1.9702
2024-10-30 17:06:26: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
2024-10-30 17:06:26: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 17:06:26: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 85941}
Using downloaded and verified file: /data/users/xiongyuxuan/.medmnist/organamnist.npz
Loaded the dataset:OrganAMNIST
[2024-10-30 14:49:56] Evaluate_00: epoch = 1000 train time = 19 s train loss = 0.001474 train acc = 1.0000, test acc = 0.6869, test_sen =0.6805, test_spe =0.9685, test_f1 =0.6672
[2024-10-30 14:50:14] Evaluate_01: epoch = 1000 train time = 17 s train loss = 0.001491 train acc = 1.0000, test acc = 0.6918, test_sen =0.6839, test_spe =0.9691, test_f1 =0.6706
[2024-10-30 14:50:36] Evaluate_02: epoch = 1000 train time = 21 s train loss = 0.042908 train acc = 1.0000, test acc = 0.6885, test_sen =0.6792, test_spe =0.9688, test_f1 =0.6663
[2024-10-30 14:50:58] Evaluate_03: epoch = 1000 train time = 19 s train loss = 0.012075 train acc = 1.0000, test acc = 0.6934, test_sen =0.6856, test_spe =0.9692, test_f1 =0.6720
[2024-10-30 14:51:19] Evaluate_04: epoch = 1000 train time = 20 s train loss = 0.001542 train acc = 1.0000, test acc = 0.6969, test_sen =0.6955, test_spe =0.9695, test_f1 =0.6822
[2024-10-30 15:01:53] Evaluate_00: epoch = 1000 train time = 27 s train loss = 0.008503 train acc = 1.0000, test acc = 0.7831, test_sen =0.7771, test_spe =0.9782, test_f1 =0.7650
[2024-10-30 15:02:26] Evaluate_01: epoch = 1000 train time = 31 s train loss = 0.002874 train acc = 1.0000, test acc = 0.7922, test_sen =0.7862, test_spe =0.9792, test_f1 =0.7737
[2024-10-30 15:02:57] Evaluate_02: epoch = 1000 train time = 28 s train loss = 0.008564 train acc = 1.0000, test acc = 0.7806, test_sen =0.7751, test_spe =0.9780, test_f1 =0.7647
[2024-10-30 15:03:27] Evaluate_03: epoch = 1000 train time = 28 s train loss = 0.007961 train acc = 1.0000, test acc = 0.7978, test_sen =0.7889, test_spe =0.9797, test_f1 =0.7808
[2024-10-30 15:03:59] Evaluate_04: epoch = 1000 train time = 29 s train loss = 0.004207 train acc = 1.0000, test acc = 0.7788, test_sen =0.7734, test_spe =0.9778, test_f1 =0.7630
[2024-10-30 15:16:55] Evaluate_00: epoch = 1000 train time = 27 s train loss = 0.006375 train acc = 1.0000, test acc = 0.7784, test_sen =0.7773, test_spe =0.9778, test_f1 =0.7642
[2024-10-30 15:17:25] Evaluate_01: epoch = 1000 train time = 27 s train loss = 0.005450 train acc = 1.0000, test acc = 0.7847, test_sen =0.7827, test_spe =0.9784, test_f1 =0.7698
[2024-10-30 15:17:52] Evaluate_02: epoch = 1000 train time = 25 s train loss = 0.031565 train acc = 1.0000, test acc = 0.7910, test_sen =0.7882, test_spe =0.9790, test_f1 =0.7756
[2024-10-30 15:18:22] Evaluate_03: epoch = 1000 train time = 27 s train loss = 0.002467 train acc = 1.0000, test acc = 0.7812, test_sen =0.7820, test_spe =0.9781, test_f1 =0.7672
[2024-10-30 15:18:46] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.007239 train acc = 1.0000, test acc = 0.7791, test_sen =0.7792, test_spe =0.9779, test_f1 =0.7652
[2024-10-30 15:32:01] Evaluate_00: epoch = 1000 train time = 25 s train loss = 0.020318 train acc = 1.0000, test acc = 0.7792, test_sen =0.7732, test_spe =0.9779, test_f1 =0.7622
[2024-10-30 15:32:32] Evaluate_01: epoch = 1000 train time = 28 s train loss = 0.002518 train acc = 1.0000, test acc = 0.7837, test_sen =0.7795, test_spe =0.9783, test_f1 =0.7685
[2024-10-30 15:32:58] Evaluate_02: epoch = 1000 train time = 24 s train loss = 0.003395 train acc = 1.0000, test acc = 0.7764, test_sen =0.7694, test_spe =0.9775, test_f1 =0.7593
[2024-10-30 15:33:24] Evaluate_03: epoch = 1000 train time = 24 s train loss = 0.004473 train acc = 1.0000, test acc = 0.7830, test_sen =0.7756, test_spe =0.9782, test_f1 =0.7684
[2024-10-30 15:33:51] Evaluate_04: epoch = 1000 train time = 25 s train loss = 0.035462 train acc = 1.0000, test acc = 0.7852, test_sen =0.7762, test_spe =0.9785, test_f1 =0.7672
[2024-10-30 15:46:42] Evaluate_00: epoch = 1000 train time = 27 s train loss = 0.020899 train acc = 1.0000, test acc = 0.7762, test_sen =0.7784, test_spe =0.9776, test_f1 =0.7630
[2024-10-30 15:47:11] Evaluate_01: epoch = 1000 train time = 26 s train loss = 0.018766 train acc = 1.0000, test acc = 0.7864, test_sen =0.7869, test_spe =0.9786, test_f1 =0.7741
[2024-10-30 15:47:40] Evaluate_02: epoch = 1000 train time = 27 s train loss = 0.004395 train acc = 1.0000, test acc = 0.7910, test_sen =0.7942, test_spe =0.9790, test_f1 =0.7808
[2024-10-30 15:48:12] Evaluate_03: epoch = 1000 train time = 29 s train loss = 0.002882 train acc = 1.0000, test acc = 0.7746, test_sen =0.7763, test_spe =0.9774, test_f1 =0.7634
[2024-10-30 15:48:46] Evaluate_04: epoch = 1000 train time = 31 s train loss = 0.003372 train acc = 1.0000, test acc = 0.7794, test_sen =0.7782, test_spe =0.9778, test_f1 =0.7652
[2024-10-30 16:02:10] Evaluate_00: epoch = 1000 train time = 25 s train loss = 0.003294 train acc = 1.0000, test acc = 0.7747, test_sen =0.7766, test_spe =0.9775, test_f1 =0.7615
[2024-10-30 16:02:39] Evaluate_01: epoch = 1000 train time = 26 s train loss = 0.023031 train acc = 1.0000, test acc = 0.7828, test_sen =0.7849, test_spe =0.9783, test_f1 =0.7708
[2024-10-30 16:03:08] Evaluate_02: epoch = 1000 train time = 26 s train loss = 0.016420 train acc = 1.0000, test acc = 0.7721, test_sen =0.7748, test_spe =0.9772, test_f1 =0.7595
[2024-10-30 16:03:40] Evaluate_03: epoch = 1000 train time = 29 s train loss = 0.030286 train acc = 1.0000, test acc = 0.7758, test_sen =0.7766, test_spe =0.9776, test_f1 =0.7630
[2024-10-30 16:04:07] Evaluate_04: epoch = 1000 train time = 25 s train loss = 0.030066 train acc = 1.0000, test acc = 0.7761, test_sen =0.7769, test_spe =0.9776, test_f1 =0.7624
[2024-10-30 16:17:06] Evaluate_00: epoch = 1000 train time = 27 s train loss = 0.003761 train acc = 1.0000, test acc = 0.7902, test_sen =0.7879, test_spe =0.9789, test_f1 =0.7777
[2024-10-30 16:17:37] Evaluate_01: epoch = 1000 train time = 29 s train loss = 0.003989 train acc = 1.0000, test acc = 0.7807, test_sen =0.7791, test_spe =0.9780, test_f1 =0.7682
[2024-10-30 16:18:12] Evaluate_02: epoch = 1000 train time = 32 s train loss = 0.003695 train acc = 1.0000, test acc = 0.7760, test_sen =0.7752, test_spe =0.9775, test_f1 =0.7658
[2024-10-30 16:18:44] Evaluate_03: epoch = 1000 train time = 30 s train loss = 0.004080 train acc = 1.0000, test acc = 0.7780, test_sen =0.7788, test_spe =0.9777, test_f1 =0.7649
[2024-10-30 16:19:14] Evaluate_04: epoch = 1000 train time = 27 s train loss = 0.004525 train acc = 1.0000, test acc = 0.7848, test_sen =0.7837, test_spe =0.9783, test_f1 =0.7739
[2024-10-30 16:33:11] Evaluate_00: epoch = 1000 train time = 29 s train loss = 0.003943 train acc = 1.0000, test acc = 0.7834, test_sen =0.7812, test_spe =0.9783, test_f1 =0.7649
[2024-10-30 16:33:41] Evaluate_01: epoch = 1000 train time = 28 s train loss = 0.027750 train acc = 1.0000, test acc = 0.7863, test_sen =0.7853, test_spe =0.9786, test_f1 =0.7698
[2024-10-30 16:34:13] Evaluate_02: epoch = 1000 train time = 28 s train loss = 0.006974 train acc = 1.0000, test acc = 0.7888, test_sen =0.7844, test_spe =0.9789, test_f1 =0.7715
[2024-10-30 16:34:50] Evaluate_03: epoch = 1000 train time = 35 s train loss = 0.007947 train acc = 1.0000, test acc = 0.7918, test_sen =0.7852, test_spe =0.9792, test_f1 =0.7727
[2024-10-30 16:35:27] Evaluate_04: epoch = 1000 train time = 34 s train loss = 0.003326 train acc = 1.0000, test acc = 0.7837, test_sen =0.7792, test_spe =0.9784, test_f1 =0.7667
[2024-10-30 16:50:03] Evaluate_00: epoch = 1000 train time = 31 s train loss = 0.004327 train acc = 1.0000, test acc = 0.7899, test_sen =0.7857, test_spe =0.9788, test_f1 =0.7779
[2024-10-30 16:50:35] Evaluate_01: epoch = 1000 train time = 30 s train loss = 0.027109 train acc = 1.0000, test acc = 0.7904, test_sen =0.7880, test_spe =0.9790, test_f1 =0.7759
[2024-10-30 16:51:12] Evaluate_02: epoch = 1000 train time = 33 s train loss = 0.008607 train acc = 1.0000, test acc = 0.7894, test_sen =0.7885, test_spe =0.9788, test_f1 =0.7762
[2024-10-30 16:51:44] Evaluate_03: epoch = 1000 train time = 29 s train loss = 0.006192 train acc = 1.0000, test acc = 0.7921, test_sen =0.7890, test_spe =0.9791, test_f1 =0.7761
[2024-10-30 16:52:20] Evaluate_04: epoch = 1000 train time = 33 s train loss = 0.012397 train acc = 1.0000, test acc = 0.7868, test_sen =0.7840, test_spe =0.9786, test_f1 =0.7726
[2024-10-30 17:06:58] Evaluate_00: epoch = 1000 train time = 30 s train loss = 0.003571 train acc = 1.0000, test acc = 0.7814, test_sen =0.7845, test_spe =0.9781, test_f1 =0.7694/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:09:16: Evaluate 5 random ConvNet, ACCmean = 0.7760 ACCstd = 0.0040
-------------------------
2024-10-30 17:09:16: Evaluate 5 random ConvNet, SENmean = 0.7783 SENstd = 0.0050
-------------------------
2024-10-30 17:09:16: Evaluate 5 random ConvNet, SPEmean = 0.9776 SPEstd = 0.0004
-------------------------
2024-10-30 17:09:16: Evaluate 5 random ConvNet, F!mean = 0.7635 F!std = 0.0046
-------------------------
2024-10-30 17:09:16: Evaluate 5 random ConvNet, mean = 0.7760 std = 0.0040
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:09:17: [2024-10-30 17:09:17] iter = 18000, loss = 2.1286
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:09:21: [2024-10-30 17:09:21] iter = 18010, loss = 2.7056
2024-10-30 17:09:25: [2024-10-30 17:09:25] iter = 18020, loss = 2.1168
2024-10-30 17:09:30: [2024-10-30 17:09:30] iter = 18030, loss = 2.0938
2024-10-30 17:09:34: [2024-10-30 17:09:34] iter = 18040, loss = 1.9166
2024-10-30 17:09:38: [2024-10-30 17:09:38] iter = 18050, loss = 1.8172
2024-10-30 17:09:42: [2024-10-30 17:09:42] iter = 18060, loss = 2.1349
2024-10-30 17:09:44: [2024-10-30 17:09:44] iter = 18070, loss = 1.6032
2024-10-30 17:09:48: [2024-10-30 17:09:48] iter = 18080, loss = 1.9218
2024-10-30 17:09:52: [2024-10-30 17:09:52] iter = 18090, loss = 2.3915
2024-10-30 17:09:57: [2024-10-30 17:09:57] iter = 18100, loss = 2.0256
2024-10-30 17:10:01: [2024-10-30 17:10:01] iter = 18110, loss = 2.2747
2024-10-30 17:10:05: [2024-10-30 17:10:05] iter = 18120, loss = 2.3702
2024-10-30 17:10:08: [2024-10-30 17:10:08] iter = 18130, loss = 1.9226
2024-10-30 17:10:13: [2024-10-30 17:10:13] iter = 18140, loss = 1.7915
2024-10-30 17:10:17: [2024-10-30 17:10:17] iter = 18150, loss = 2.9643
2024-10-30 17:10:21: [2024-10-30 17:10:21] iter = 18160, loss = 2.2870
2024-10-30 17:10:25: [2024-10-30 17:10:25] iter = 18170, loss = 2.5929
2024-10-30 17:10:28: [2024-10-30 17:10:28] iter = 18180, loss = 2.0914
2024-10-30 17:10:32: [2024-10-30 17:10:32] iter = 18190, loss = 1.7073
2024-10-30 17:10:36: [2024-10-30 17:10:36] iter = 18200, loss = 1.9510
2024-10-30 17:10:41: [2024-10-30 17:10:41] iter = 18210, loss = 1.9808
2024-10-30 17:10:44: [2024-10-30 17:10:44] iter = 18220, loss = 2.4426
2024-10-30 17:10:48: [2024-10-30 17:10:48] iter = 18230, loss = 2.1759
2024-10-30 17:10:52: [2024-10-30 17:10:52] iter = 18240, loss = 2.0523
2024-10-30 17:10:57: [2024-10-30 17:10:57] iter = 18250, loss = 2.7050
2024-10-30 17:11:00: [2024-10-30 17:11:00] iter = 18260, loss = 1.7718
2024-10-30 17:11:05: [2024-10-30 17:11:05] iter = 18270, loss = 1.7867
2024-10-30 17:11:10: [2024-10-30 17:11:10] iter = 18280, loss = 1.9725
2024-10-30 17:11:15: [2024-10-30 17:11:15] iter = 18290, loss = 2.1690
2024-10-30 17:11:19: [2024-10-30 17:11:19] iter = 18300, loss = 2.5106
2024-10-30 17:11:24: [2024-10-30 17:11:24] iter = 18310, loss = 1.8991
2024-10-30 17:11:28: [2024-10-30 17:11:28] iter = 18320, loss = 3.1457
2024-10-30 17:11:32: [2024-10-30 17:11:32] iter = 18330, loss = 1.8834
2024-10-30 17:11:36: [2024-10-30 17:11:36] iter = 18340, loss = 1.8274
2024-10-30 17:11:40: [2024-10-30 17:11:40] iter = 18350, loss = 1.8485
2024-10-30 17:11:45: [2024-10-30 17:11:45] iter = 18360, loss = 2.2408
2024-10-30 17:11:49: [2024-10-30 17:11:49] iter = 18370, loss = 1.9266
2024-10-30 17:11:54: [2024-10-30 17:11:54] iter = 18380, loss = 2.3646
2024-10-30 17:11:59: [2024-10-30 17:11:59] iter = 18390, loss = 2.4623
2024-10-30 17:12:04: [2024-10-30 17:12:04] iter = 18400, loss = 4.3163
2024-10-30 17:12:07: [2024-10-30 17:12:07] iter = 18410, loss = 2.0244
2024-10-30 17:12:12: [2024-10-30 17:12:12] iter = 18420, loss = 2.0411
2024-10-30 17:12:15: [2024-10-30 17:12:15] iter = 18430, loss = 2.4217
2024-10-30 17:12:20: [2024-10-30 17:12:20] iter = 18440, loss = 3.0720
2024-10-30 17:12:24: [2024-10-30 17:12:24] iter = 18450, loss = 2.3323
2024-10-30 17:12:28: [2024-10-30 17:12:27] iter = 18460, loss = 2.0787
2024-10-30 17:12:31: [2024-10-30 17:12:31] iter = 18470, loss = 1.9201
2024-10-30 17:12:36: [2024-10-30 17:12:36] iter = 18480, loss = 1.8088
2024-10-30 17:12:41: [2024-10-30 17:12:41] iter = 18490, loss = 1.8124
2024-10-30 17:12:45: [2024-10-30 17:12:45] iter = 18500, loss = 6.4978
2024-10-30 17:12:50: [2024-10-30 17:12:50] iter = 18510, loss = 1.8498
2024-10-30 17:12:54: [2024-10-30 17:12:54] iter = 18520, loss = 2.1352
2024-10-30 17:12:58: [2024-10-30 17:12:58] iter = 18530, loss = 1.9877
2024-10-30 17:13:02: [2024-10-30 17:13:02] iter = 18540, loss = 1.8659
2024-10-30 17:13:06: [2024-10-30 17:13:06] iter = 18550, loss = 2.1283
2024-10-30 17:13:10: [2024-10-30 17:13:10] iter = 18560, loss = 1.8628
2024-10-30 17:13:15: [2024-10-30 17:13:15] iter = 18570, loss = 1.8679
2024-10-30 17:13:20: [2024-10-30 17:13:20] iter = 18580, loss = 1.8898
2024-10-30 17:13:24: [2024-10-30 17:13:24] iter = 18590, loss = 2.2991
2024-10-30 17:13:30: [2024-10-30 17:13:30] iter = 18600, loss = 3.0080
2024-10-30 17:13:34: [2024-10-30 17:13:34] iter = 18610, loss = 1.8872
2024-10-30 17:13:40: [2024-10-30 17:13:40] iter = 18620, loss = 1.8004
2024-10-30 17:13:44: [2024-10-30 17:13:44] iter = 18630, loss = 2.0931
2024-10-30 17:13:48: [2024-10-30 17:13:48] iter = 18640, loss = 1.8436
2024-10-30 17:13:53: [2024-10-30 17:13:53] iter = 18650, loss = 2.7885
2024-10-30 17:13:57: [2024-10-30 17:13:57] iter = 18660, loss = 2.0178
2024-10-30 17:14:01: [2024-10-30 17:14:01] iter = 18670, loss = 3.0539
2024-10-30 17:14:05: [2024-10-30 17:14:05] iter = 18680, loss = 2.8500
2024-10-30 17:14:08: [2024-10-30 17:14:08] iter = 18690, loss = 4.3101
2024-10-30 17:14:12: [2024-10-30 17:14:12] iter = 18700, loss = 2.5482
2024-10-30 17:14:16: [2024-10-30 17:14:16] iter = 18710, loss = 1.9817
2024-10-30 17:14:20: [2024-10-30 17:14:20] iter = 18720, loss = 5.5373
2024-10-30 17:14:24: [2024-10-30 17:14:24] iter = 18730, loss = 2.2291
2024-10-30 17:14:28: [2024-10-30 17:14:28] iter = 18740, loss = 2.4693
2024-10-30 17:14:31: [2024-10-30 17:14:31] iter = 18750, loss = 2.7916
2024-10-30 17:14:36: [2024-10-30 17:14:36] iter = 18760, loss = 2.2339
2024-10-30 17:14:39: [2024-10-30 17:14:39] iter = 18770, loss = 2.6800
2024-10-30 17:14:43: [2024-10-30 17:14:43] iter = 18780, loss = 3.4016
2024-10-30 17:14:47: [2024-10-30 17:14:47] iter = 18790, loss = 2.0257
2024-10-30 17:14:51: [2024-10-30 17:14:51] iter = 18800, loss = 2.8122
2024-10-30 17:14:55: [2024-10-30 17:14:55] iter = 18810, loss = 2.1878
2024-10-30 17:14:59: [2024-10-30 17:14:59] iter = 18820, loss = 2.7471
2024-10-30 17:15:03: [2024-10-30 17:15:03] iter = 18830, loss = 2.3152
2024-10-30 17:15:07: [2024-10-30 17:15:07] iter = 18840, loss = 7.3718
2024-10-30 17:15:13: [2024-10-30 17:15:13] iter = 18850, loss = 3.2910
2024-10-30 17:15:17: [2024-10-30 17:15:17] iter = 18860, loss = 4.7061
2024-10-30 17:15:22: [2024-10-30 17:15:22] iter = 18870, loss = 2.0599
2024-10-30 17:15:26: [2024-10-30 17:15:26] iter = 18880, loss = 2.1004
2024-10-30 17:15:30: [2024-10-30 17:15:30] iter = 18890, loss = 2.2975
2024-10-30 17:15:34: [2024-10-30 17:15:34] iter = 18900, loss = 2.0044
2024-10-30 17:15:37: [2024-10-30 17:15:37] iter = 18910, loss = 2.8831
2024-10-30 17:15:41: [2024-10-30 17:15:41] iter = 18920, loss = 2.8770
2024-10-30 17:15:44: [2024-10-30 17:15:44] iter = 18930, loss = 2.1603
2024-10-30 17:15:47: [2024-10-30 17:15:47] iter = 18940, loss = 1.9486
2024-10-30 17:15:51: [2024-10-30 17:15:51] iter = 18950, loss = 2.0030
2024-10-30 17:15:55: [2024-10-30 17:15:55] iter = 18960, loss = 1.8981
2024-10-30 17:15:59: [2024-10-30 17:15:59] iter = 18970, loss = 2.0642
2024-10-30 17:16:04: [2024-10-30 17:16:04] iter = 18980, loss = 2.5396
2024-10-30 17:16:08: [2024-10-30 17:16:08] iter = 18990, loss = 1.7913
2024-10-30 17:16:12: [2024-10-30 17:16:12] iter = 19000, loss = 1.6398
2024-10-30 17:16:16: [2024-10-30 17:16:16] iter = 19010, loss = 2.6910
2024-10-30 17:16:20: [2024-10-30 17:16:20] iter = 19020, loss = 2.4116
2024-10-30 17:16:26: [2024-10-30 17:16:26] iter = 19030, loss = 3.3759
2024-10-30 17:16:31: [2024-10-30 17:16:31] iter = 19040, loss = 2.1640
2024-10-30 17:16:35: [2024-10-30 17:16:35] iter = 19050, loss = 2.6438
2024-10-30 17:16:40: [2024-10-30 17:16:40] iter = 19060, loss = 3.1081
2024-10-30 17:16:45: [2024-10-30 17:16:45] iter = 19070, loss = 1.7886
2024-10-30 17:16:49: [2024-10-30 17:16:49] iter = 19080, loss = 2.1730
2024-10-30 17:16:53: [2024-10-30 17:16:53] iter = 19090, loss = 2.0702
2024-10-30 17:16:58: [2024-10-30 17:16:58] iter = 19100, loss = 2.9553
2024-10-30 17:17:03: [2024-10-30 17:17:03] iter = 19110, loss = 2.4033
2024-10-30 17:17:07: [2024-10-30 17:17:07] iter = 19120, loss = 2.2864
2024-10-30 17:17:11: [2024-10-30 17:17:11] iter = 19130, loss = 2.3146
2024-10-30 17:17:15: [2024-10-30 17:17:15] iter = 19140, loss = 2.3855
2024-10-30 17:17:19: [2024-10-30 17:17:19] iter = 19150, loss = 2.3559
2024-10-30 17:17:24: [2024-10-30 17:17:24] iter = 19160, loss = 2.2036
2024-10-30 17:17:29: [2024-10-30 17:17:29] iter = 19170, loss = 2.3529
2024-10-30 17:17:32: [2024-10-30 17:17:32] iter = 19180, loss = 1.8650
2024-10-30 17:17:37: [2024-10-30 17:17:37] iter = 19190, loss = 9.9916
2024-10-30 17:17:41: [2024-10-30 17:17:41] iter = 19200, loss = 1.6986
2024-10-30 17:17:45: [2024-10-30 17:17:45] iter = 19210, loss = 2.1317
2024-10-30 17:17:49: [2024-10-30 17:17:49] iter = 19220, loss = 1.9223
2024-10-30 17:17:53: [2024-10-30 17:17:53] iter = 19230, loss = 2.1288
2024-10-30 17:17:56: [2024-10-30 17:17:56] iter = 19240, loss = 3.4373
2024-10-30 17:18:00: [2024-10-30 17:18:00] iter = 19250, loss = 2.0709
2024-10-30 17:18:04: [2024-10-30 17:18:04] iter = 19260, loss = 2.4213
2024-10-30 17:18:09: [2024-10-30 17:18:09] iter = 19270, loss = 2.6137
2024-10-30 17:18:15: [2024-10-30 17:18:15] iter = 19280, loss = 2.6215
2024-10-30 17:18:19: [2024-10-30 17:18:19] iter = 19290, loss = 3.3284
2024-10-30 17:18:24: [2024-10-30 17:18:24] iter = 19300, loss = 2.0339
2024-10-30 17:18:29: [2024-10-30 17:18:29] iter = 19310, loss = 2.3706
2024-10-30 17:18:33: [2024-10-30 17:18:33] iter = 19320, loss = 2.4254
2024-10-30 17:18:38: [2024-10-30 17:18:38] iter = 19330, loss = 2.8166
2024-10-30 17:18:42: [2024-10-30 17:18:42] iter = 19340, loss = 1.9706
2024-10-30 17:18:46: [2024-10-30 17:18:46] iter = 19350, loss = 2.2143
2024-10-30 17:18:51: [2024-10-30 17:18:51] iter = 19360, loss = 2.0170
2024-10-30 17:18:56: [2024-10-30 17:18:56] iter = 19370, loss = 1.7394
2024-10-30 17:19:01: [2024-10-30 17:19:01] iter = 19380, loss = 4.2211
2024-10-30 17:19:06: [2024-10-30 17:19:06] iter = 19390, loss = 2.5578
2024-10-30 17:19:10: [2024-10-30 17:19:10] iter = 19400, loss = 2.1148
2024-10-30 17:19:16: [2024-10-30 17:19:16] iter = 19410, loss = 2.1267
2024-10-30 17:19:19: [2024-10-30 17:19:19] iter = 19420, loss = 2.1369
2024-10-30 17:19:24: [2024-10-30 17:19:24] iter = 19430, loss = 1.9469
2024-10-30 17:19:29: [2024-10-30 17:19:29] iter = 19440, loss = 2.8805
2024-10-30 17:19:33: [2024-10-30 17:19:33] iter = 19450, loss = 2.5547
2024-10-30 17:19:37: [2024-10-30 17:19:37] iter = 19460, loss = 1.8999
2024-10-30 17:19:42: [2024-10-30 17:19:42] iter = 19470, loss = 3.0767
2024-10-30 17:19:46: [2024-10-30 17:19:46] iter = 19480, loss = 2.2051
2024-10-30 17:19:51: [2024-10-30 17:19:51] iter = 19490, loss = 2.0626
2024-10-30 17:19:57: [2024-10-30 17:19:57] iter = 19500, loss = 2.3468
2024-10-30 17:20:01: [2024-10-30 17:20:01] iter = 19510, loss = 2.5894
2024-10-30 17:20:05: [2024-10-30 17:20:05] iter = 19520, loss = 1.7822
2024-10-30 17:20:09: [2024-10-30 17:20:09] iter = 19530, loss = 2.3591
2024-10-30 17:20:14: [2024-10-30 17:20:14] iter = 19540, loss = 2.0454
2024-10-30 17:20:18: [2024-10-30 17:20:18] iter = 19550, loss = 7.0338
2024-10-30 17:20:23: [2024-10-30 17:20:23] iter = 19560, loss = 2.2103
2024-10-30 17:20:29: [2024-10-30 17:20:29] iter = 19570, loss = 2.0346
2024-10-30 17:20:33: [2024-10-30 17:20:33] iter = 19580, loss = 1.9853
2024-10-30 17:20:38: [2024-10-30 17:20:38] iter = 19590, loss = 1.7868
2024-10-30 17:20:42: [2024-10-30 17:20:42] iter = 19600, loss = 1.9836
2024-10-30 17:20:46: [2024-10-30 17:20:46] iter = 19610, loss = 2.7393
2024-10-30 17:20:51: [2024-10-30 17:20:51] iter = 19620, loss = 2.0849
2024-10-30 17:20:56: [2024-10-30 17:20:56] iter = 19630, loss = 3.1493
2024-10-30 17:21:00: [2024-10-30 17:21:00] iter = 19640, loss = 2.2056
2024-10-30 17:21:04: [2024-10-30 17:21:04] iter = 19650, loss = 2.1243
2024-10-30 17:21:07: [2024-10-30 17:21:07] iter = 19660, loss = 1.8174
2024-10-30 17:21:11: [2024-10-30 17:21:11] iter = 19670, loss = 2.8391
2024-10-30 17:21:15: [2024-10-30 17:21:15] iter = 19680, loss = 3.5536
2024-10-30 17:21:21: [2024-10-30 17:21:21] iter = 19690, loss = 1.9791
2024-10-30 17:21:25: [2024-10-30 17:21:25] iter = 19700, loss = 1.7715
2024-10-30 17:21:30: [2024-10-30 17:21:30] iter = 19710, loss = 2.0420
2024-10-30 17:21:35: [2024-10-30 17:21:35] iter = 19720, loss = 2.3914
2024-10-30 17:21:37: [2024-10-30 17:21:37] iter = 19730, loss = 5.4082
2024-10-30 17:21:42: [2024-10-30 17:21:42] iter = 19740, loss = 2.2420
2024-10-30 17:21:46: [2024-10-30 17:21:46] iter = 19750, loss = 2.0683
2024-10-30 17:21:51: [2024-10-30 17:21:51] iter = 19760, loss = 2.5133
2024-10-30 17:21:56: [2024-10-30 17:21:56] iter = 19770, loss = 2.4466
2024-10-30 17:22:00: [2024-10-30 17:22:00] iter = 19780, loss = 2.1412
2024-10-30 17:22:03: [2024-10-30 17:22:03] iter = 19790, loss = 2.3228
2024-10-30 17:22:07: [2024-10-30 17:22:07] iter = 19800, loss = 2.0717
2024-10-30 17:22:11: [2024-10-30 17:22:11] iter = 19810, loss = 1.8834
2024-10-30 17:22:15: [2024-10-30 17:22:15] iter = 19820, loss = 3.7968
2024-10-30 17:22:20: [2024-10-30 17:22:20] iter = 19830, loss = 2.7200
2024-10-30 17:22:24: [2024-10-30 17:22:24] iter = 19840, loss = 2.1326
2024-10-30 17:22:28: [2024-10-30 17:22:28] iter = 19850, loss = 2.5141
2024-10-30 17:22:32: [2024-10-30 17:22:32] iter = 19860, loss = 2.5683
2024-10-30 17:22:36: [2024-10-30 17:22:36] iter = 19870, loss = 2.2670
2024-10-30 17:22:40: [2024-10-30 17:22:40] iter = 19880, loss = 2.5241
2024-10-30 17:22:44: [2024-10-30 17:22:44] iter = 19890, loss = 2.3364
2024-10-30 17:22:48: [2024-10-30 17:22:48] iter = 19900, loss = 2.4967
2024-10-30 17:22:53: [2024-10-30 17:22:53] iter = 19910, loss = 2.3679
2024-10-30 17:22:56: [2024-10-30 17:22:56] iter = 19920, loss = 2.2253
2024-10-30 17:22:59: [2024-10-30 17:22:59] iter = 19930, loss = 2.1951
2024-10-30 17:23:03: [2024-10-30 17:23:03] iter = 19940, loss = 1.8398
2024-10-30 17:23:08: [2024-10-30 17:23:08] iter = 19950, loss = 2.1993
2024-10-30 17:23:12: [2024-10-30 17:23:12] iter = 19960, loss = 3.5510
2024-10-30 17:23:16: [2024-10-30 17:23:16] iter = 19970, loss = 2.2320
2024-10-30 17:23:19: [2024-10-30 17:23:19] iter = 19980, loss = 1.9101
2024-10-30 17:23:22: [2024-10-30 17:23:22] iter = 19990, loss = 3.0695
2024-10-30 17:23:26: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
2024-10-30 17:23:26: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 17:23:26: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 6824}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:26:03: Evaluate 5 random ConvNet, ACCmean = 0.7849 ACCstd = 0.0045
-------------------------
2024-10-30 17:26:03: Evaluate 5 random ConvNet, SENmean = 0.7828 SENstd = 0.0050
-------------------------
2024-10-30 17:26:03: Evaluate 5 random ConvNet, SPEmean = 0.9784 SPEstd = 0.0005
-------------------------
2024-10-30 17:26:03: Evaluate 5 random ConvNet, F!mean = 0.7711 F!std = 0.0042
-------------------------
2024-10-30 17:26:03: Evaluate 5 random ConvNet, mean = 0.7849 std = 0.0045
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:26:03: [2024-10-30 17:26:03] iter = 20000, loss = 2.1459
2024-10-30 17:26:03: 
================== Exp 1 ==================
 
2024-10-30 17:26:03: Hyper-parameters: 
{'dataset': 'OrganAMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': '/data/users/xiongyuxuan/DD/OBJDD/DatasetCondensation-master/data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f5f8402db20>, 'dsa': True, 'logger': <Logger DM_IPC=10_Model_ConvNet_Data_OrganAMNIST (INFO)>}
2024-10-30 17:26:03: Evaluation model pool: ['ConvNet']
2024-10-30 17:26:07: class c = 0: 1956 real images
2024-10-30 17:26:07: class c = 1: 1390 real images
2024-10-30 17:26:07: class c = 2: 1357 real images
2024-10-30 17:26:07: class c = 3: 1474 real images
2024-10-30 17:26:07: class c = 4: 3963 real images
2024-10-30 17:26:07: class c = 5: 3817 real images
2024-10-30 17:26:07: class c = 6: 6164 real images
2024-10-30 17:26:07: class c = 7: 3919 real images
2024-10-30 17:26:07: class c = 8: 3929 real images
2024-10-30 17:26:07: class c = 9: 3031 real images
2024-10-30 17:26:07: class c = 10: 3561 real images
2024-10-30 17:26:07: real images channel 0, mean = 0.4680, std = 0.2974
main_DM.py:120: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-10-30 17:26:07: initialize synthetic data from random real images
2024-10-30 17:26:07: [2024-10-30 17:26:07] training begins
2024-10-30 17:26:07: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-10-30 17:26:07: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 17:26:07: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 63835}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:28:53: Evaluate 5 random ConvNet, ACCmean = 0.6924 ACCstd = 0.0078
-------------------------
2024-10-30 17:28:53: Evaluate 5 random ConvNet, SENmean = 0.6772 SENstd = 0.0093
-------------------------
2024-10-30 17:28:53: Evaluate 5 random ConvNet, SPEmean = 0.9690 SPEstd = 0.0008
-------------------------
2024-10-30 17:28:53: Evaluate 5 random ConvNet, F!mean = 0.6690 F!std = 0.0084
-------------------------
2024-10-30 17:28:53: Evaluate 5 random ConvNet, mean = 0.6924 std = 0.0078
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:28:53: [2024-10-30 17:28:53] iter = 00000, loss = 14.6016
2024-10-30 17:28:59: [2024-10-30 17:28:59] iter = 00010, loss = 4.7913
2024-10-30 17:29:05: [2024-10-30 17:29:05] iter = 00020, loss = 3.4472
2024-10-30 17:29:09: [2024-10-30 17:29:09] iter = 00030, loss = 4.5949
2024-10-30 17:29:14: [2024-10-30 17:29:14] iter = 00040, loss = 2.8277
2024-10-30 17:29:18: [2024-10-30 17:29:18] iter = 00050, loss = 3.3370
2024-10-30 17:29:23: [2024-10-30 17:29:23] iter = 00060, loss = 3.1462
2024-10-30 17:29:27: [2024-10-30 17:29:27] iter = 00070, loss = 2.8053
2024-10-30 17:29:33: [2024-10-30 17:29:33] iter = 00080, loss = 2.7390
2024-10-30 17:29:36: [2024-10-30 17:29:36] iter = 00090, loss = 3.6212
2024-10-30 17:29:40: [2024-10-30 17:29:40] iter = 00100, loss = 3.7653
2024-10-30 17:29:45: [2024-10-30 17:29:45] iter = 00110, loss = 2.5570
2024-10-30 17:29:49: [2024-10-30 17:29:49] iter = 00120, loss = 2.4832
2024-10-30 17:29:54: [2024-10-30 17:29:54] iter = 00130, loss = 2.7425
2024-10-30 17:29:58: [2024-10-30 17:29:58] iter = 00140, loss = 3.7493
2024-10-30 17:30:02: [2024-10-30 17:30:02] iter = 00150, loss = 2.7499
2024-10-30 17:30:07: [2024-10-30 17:30:07] iter = 00160, loss = 2.5691
2024-10-30 17:30:11: [2024-10-30 17:30:11] iter = 00170, loss = 2.5430
2024-10-30 17:30:14: [2024-10-30 17:30:14] iter = 00180, loss = 3.1588
2024-10-30 17:30:17: [2024-10-30 17:30:17] iter = 00190, loss = 2.5934
2024-10-30 17:30:20: [2024-10-30 17:30:20] iter = 00200, loss = 2.8149
2024-10-30 17:30:24: [2024-10-30 17:30:24] iter = 00210, loss = 2.5329
2024-10-30 17:30:27: [2024-10-30 17:30:27] iter = 00220, loss = 2.0305
2024-10-30 17:30:33: [2024-10-30 17:30:33] iter = 00230, loss = 2.4426
2024-10-30 17:30:36: [2024-10-30 17:30:36] iter = 00240, loss = 2.3144
2024-10-30 17:30:40: [2024-10-30 17:30:40] iter = 00250, loss = 2.0028
2024-10-30 17:30:44: [2024-10-30 17:30:43] iter = 00260, loss = 4.5936
2024-10-30 17:30:47: [2024-10-30 17:30:47] iter = 00270, loss = 2.2046
2024-10-30 17:30:51: [2024-10-30 17:30:51] iter = 00280, loss = 3.0763
2024-10-30 17:30:55: [2024-10-30 17:30:55] iter = 00290, loss = 2.2164
2024-10-30 17:31:00: [2024-10-30 17:31:00] iter = 00300, loss = 1.7810
2024-10-30 17:31:03: [2024-10-30 17:31:03] iter = 00310, loss = 5.5137
2024-10-30 17:31:08: [2024-10-30 17:31:08] iter = 00320, loss = 2.7463
2024-10-30 17:31:12: [2024-10-30 17:31:12] iter = 00330, loss = 2.0639
2024-10-30 17:31:17: [2024-10-30 17:31:17] iter = 00340, loss = 2.1782
2024-10-30 17:31:21: [2024-10-30 17:31:21] iter = 00350, loss = 2.4230
2024-10-30 17:31:27: [2024-10-30 17:31:27] iter = 00360, loss = 2.1142
2024-10-30 17:31:32: [2024-10-30 17:31:32] iter = 00370, loss = 2.5908
2024-10-30 17:31:35: [2024-10-30 17:31:35] iter = 00380, loss = 2.0838
2024-10-30 17:31:40: [2024-10-30 17:31:40] iter = 00390, loss = 3.0720
2024-10-30 17:31:44: [2024-10-30 17:31:44] iter = 00400, loss = 2.3668
2024-10-30 17:31:47: [2024-10-30 17:31:47] iter = 00410, loss = 2.5825
2024-10-30 17:31:51: [2024-10-30 17:31:51] iter = 00420, loss = 2.6116
2024-10-30 17:31:55: [2024-10-30 17:31:55] iter = 00430, loss = 3.0172
2024-10-30 17:31:59: [2024-10-30 17:31:59] iter = 00440, loss = 2.4338
2024-10-30 17:32:04: [2024-10-30 17:32:04] iter = 00450, loss = 3.7626
2024-10-30 17:32:08: [2024-10-30 17:32:08] iter = 00460, loss = 1.9799
2024-10-30 17:32:12: [2024-10-30 17:32:12] iter = 00470, loss = 2.9987
2024-10-30 17:32:16: [2024-10-30 17:32:16] iter = 00480, loss = 3.4584
2024-10-30 17:32:21: [2024-10-30 17:32:21] iter = 00490, loss = 2.0303
2024-10-30 17:32:25: [2024-10-30 17:32:25] iter = 00500, loss = 2.0502
2024-10-30 17:32:30: [2024-10-30 17:32:30] iter = 00510, loss = 2.5444
2024-10-30 17:32:34: [2024-10-30 17:32:34] iter = 00520, loss = 3.1955
2024-10-30 17:32:37: [2024-10-30 17:32:37] iter = 00530, loss = 2.0185
2024-10-30 17:32:41: [2024-10-30 17:32:41] iter = 00540, loss = 3.0785
2024-10-30 17:32:45: [2024-10-30 17:32:45] iter = 00550, loss = 2.5966
2024-10-30 17:32:50: [2024-10-30 17:32:50] iter = 00560, loss = 3.7041
2024-10-30 17:32:55: [2024-10-30 17:32:55] iter = 00570, loss = 2.3412
2024-10-30 17:32:58: [2024-10-30 17:32:58] iter = 00580, loss = 2.2396
2024-10-30 17:33:03: [2024-10-30 17:33:03] iter = 00590, loss = 3.0368
2024-10-30 17:33:07: [2024-10-30 17:33:07] iter = 00600, loss = 8.0559
2024-10-30 17:33:12: [2024-10-30 17:33:12] iter = 00610, loss = 2.7390
2024-10-30 17:33:16: [2024-10-30 17:33:15] iter = 00620, loss = 2.2971
2024-10-30 17:33:18: [2024-10-30 17:33:18] iter = 00630, loss = 2.6931
2024-10-30 17:33:23: [2024-10-30 17:33:23] iter = 00640, loss = 1.9867
2024-10-30 17:33:27: [2024-10-30 17:33:27] iter = 00650, loss = 2.1068
2024-10-30 17:33:31: [2024-10-30 17:33:31] iter = 00660, loss = 3.0636
2024-10-30 17:33:36: [2024-10-30 17:33:36] iter = 00670, loss = 3.9342
2024-10-30 17:33:40: [2024-10-30 17:33:40] iter = 00680, loss = 2.0930
2024-10-30 17:33:44: [2024-10-30 17:33:44] iter = 00690, loss = 2.1597
2024-10-30 17:33:47: [2024-10-30 17:33:47] iter = 00700, loss = 2.4705
2024-10-30 17:33:51: [2024-10-30 17:33:51] iter = 00710, loss = 1.9906
2024-10-30 17:33:56: [2024-10-30 17:33:56] iter = 00720, loss = 2.5235
2024-10-30 17:34:00: [2024-10-30 17:34:00] iter = 00730, loss = 2.7851
2024-10-30 17:34:05: [2024-10-30 17:34:05] iter = 00740, loss = 2.5103
2024-10-30 17:34:10: [2024-10-30 17:34:10] iter = 00750, loss = 2.1534
2024-10-30 17:34:14: [2024-10-30 17:34:14] iter = 00760, loss = 2.5960
2024-10-30 17:34:18: [2024-10-30 17:34:18] iter = 00770, loss = 2.2792
2024-10-30 17:34:22: [2024-10-30 17:34:22] iter = 00780, loss = 1.9953
2024-10-30 17:34:27: [2024-10-30 17:34:27] iter = 00790, loss = 2.7200
2024-10-30 17:34:30: [2024-10-30 17:34:30] iter = 00800, loss = 3.2285
2024-10-30 17:34:34: [2024-10-30 17:34:34] iter = 00810, loss = 7.1084
2024-10-30 17:34:39: [2024-10-30 17:34:39] iter = 00820, loss = 2.0857
2024-10-30 17:34:43: [2024-10-30 17:34:43] iter = 00830, loss = 4.1099
2024-10-30 17:34:47: [2024-10-30 17:34:47] iter = 00840, loss = 2.2453
2024-10-30 17:34:52: [2024-10-30 17:34:52] iter = 00850, loss = 2.3801
2024-10-30 17:34:56: [2024-10-30 17:34:56] iter = 00860, loss = 2.5131
2024-10-30 17:34:59: [2024-10-30 17:34:59] iter = 00870, loss = 2.6722
2024-10-30 17:35:03: [2024-10-30 17:35:03] iter = 00880, loss = 2.0986
2024-10-30 17:35:07: [2024-10-30 17:35:07] iter = 00890, loss = 2.4827
2024-10-30 17:35:11: [2024-10-30 17:35:11] iter = 00900, loss = 2.4445
2024-10-30 17:35:15: [2024-10-30 17:35:15] iter = 00910, loss = 2.1997
2024-10-30 17:35:19: [2024-10-30 17:35:19] iter = 00920, loss = 3.0295
2024-10-30 17:35:23: [2024-10-30 17:35:23] iter = 00930, loss = 1.9992
2024-10-30 17:35:29: [2024-10-30 17:35:29] iter = 00940, loss = 3.5366
2024-10-30 17:35:32: [2024-10-30 17:35:32] iter = 00950, loss = 2.6592
2024-10-30 17:35:37: [2024-10-30 17:35:37] iter = 00960, loss = 3.1036
2024-10-30 17:35:41: [2024-10-30 17:35:41] iter = 00970, loss = 3.0232
2024-10-30 17:35:45: [2024-10-30 17:35:45] iter = 00980, loss = 2.8590
2024-10-30 17:35:49: [2024-10-30 17:35:49] iter = 00990, loss = 3.1923
2024-10-30 17:35:52: [2024-10-30 17:35:52] iter = 01000, loss = 2.0872
2024-10-30 17:35:56: [2024-10-30 17:35:56] iter = 01010, loss = 2.1915
2024-10-30 17:36:01: [2024-10-30 17:36:01] iter = 01020, loss = 2.5644
2024-10-30 17:36:04: [2024-10-30 17:36:04] iter = 01030, loss = 2.9632
2024-10-30 17:36:09: [2024-10-30 17:36:09] iter = 01040, loss = 3.1131
2024-10-30 17:36:13: [2024-10-30 17:36:13] iter = 01050, loss = 2.4277
2024-10-30 17:36:17: [2024-10-30 17:36:17] iter = 01060, loss = 2.4019
2024-10-30 17:36:21: [2024-10-30 17:36:21] iter = 01070, loss = 2.3117
2024-10-30 17:36:27: [2024-10-30 17:36:27] iter = 01080, loss = 2.1239
2024-10-30 17:36:32: [2024-10-30 17:36:32] iter = 01090, loss = 2.8992
2024-10-30 17:36:36: [2024-10-30 17:36:36] iter = 01100, loss = 1.9929
2024-10-30 17:36:41: [2024-10-30 17:36:41] iter = 01110, loss = 2.9395
2024-10-30 17:36:44: [2024-10-30 17:36:44] iter = 01120, loss = 2.9893
2024-10-30 17:36:47: [2024-10-30 17:36:47] iter = 01130, loss = 3.0265
2024-10-30 17:36:51: [2024-10-30 17:36:51] iter = 01140, loss = 2.6876
2024-10-30 17:36:55: [2024-10-30 17:36:55] iter = 01150, loss = 2.0948
2024-10-30 17:36:59: [2024-10-30 17:36:59] iter = 01160, loss = 2.1225
2024-10-30 17:37:02: [2024-10-30 17:37:02] iter = 01170, loss = 1.9702
2024-10-30 17:37:05: [2024-10-30 17:37:05] iter = 01180, loss = 1.9272
2024-10-30 17:37:08: [2024-10-30 17:37:08] iter = 01190, loss = 2.2664
2024-10-30 17:37:12: [2024-10-30 17:37:12] iter = 01200, loss = 2.5079
2024-10-30 17:37:15: [2024-10-30 17:37:15] iter = 01210, loss = 2.0954
2024-10-30 17:37:18: [2024-10-30 17:37:18] iter = 01220, loss = 1.9487
2024-10-30 17:37:22: [2024-10-30 17:37:22] iter = 01230, loss = 2.8924
2024-10-30 17:37:26: [2024-10-30 17:37:26] iter = 01240, loss = 2.6287
2024-10-30 17:37:30: [2024-10-30 17:37:30] iter = 01250, loss = 2.3245
2024-10-30 17:37:33: [2024-10-30 17:37:33] iter = 01260, loss = 2.0456
2024-10-30 17:37:37: [2024-10-30 17:37:37] iter = 01270, loss = 2.4702
2024-10-30 17:37:41: [2024-10-30 17:37:41] iter = 01280, loss = 2.8200
2024-10-30 17:37:45: [2024-10-30 17:37:45] iter = 01290, loss = 3.0314
2024-10-30 17:37:49: [2024-10-30 17:37:49] iter = 01300, loss = 2.4373
2024-10-30 17:37:51: [2024-10-30 17:37:51] iter = 01310, loss = 2.7814
2024-10-30 17:37:54: [2024-10-30 17:37:54] iter = 01320, loss = 2.5748
2024-10-30 17:37:57: [2024-10-30 17:37:57] iter = 01330, loss = 2.2006
2024-10-30 17:38:01: [2024-10-30 17:38:01] iter = 01340, loss = 2.1167
2024-10-30 17:38:05: [2024-10-30 17:38:05] iter = 01350, loss = 3.0102
2024-10-30 17:38:08: [2024-10-30 17:38:08] iter = 01360, loss = 2.5411
2024-10-30 17:38:12: [2024-10-30 17:38:12] iter = 01370, loss = 2.5889
2024-10-30 17:38:16: [2024-10-30 17:38:16] iter = 01380, loss = 2.2609
2024-10-30 17:38:19: [2024-10-30 17:38:19] iter = 01390, loss = 2.4371
2024-10-30 17:38:23: [2024-10-30 17:38:23] iter = 01400, loss = 2.4381
2024-10-30 17:38:27: [2024-10-30 17:38:27] iter = 01410, loss = 2.0392
2024-10-30 17:38:30: [2024-10-30 17:38:30] iter = 01420, loss = 2.9227
2024-10-30 17:38:35: [2024-10-30 17:38:35] iter = 01430, loss = 2.3515
2024-10-30 17:38:39: [2024-10-30 17:38:39] iter = 01440, loss = 2.1863
2024-10-30 17:38:42: [2024-10-30 17:38:42] iter = 01450, loss = 2.7616
2024-10-30 17:38:47: [2024-10-30 17:38:47] iter = 01460, loss = 2.5083
2024-10-30 17:38:52: [2024-10-30 17:38:52] iter = 01470, loss = 1.9054
2024-10-30 17:38:57: [2024-10-30 17:38:57] iter = 01480, loss = 2.3942
2024-10-30 17:39:01: [2024-10-30 17:39:01] iter = 01490, loss = 2.5409
2024-10-30 17:39:05: [2024-10-30 17:39:05] iter = 01500, loss = 2.7683
2024-10-30 17:39:09: [2024-10-30 17:39:09] iter = 01510, loss = 2.4055
2024-10-30 17:39:12: [2024-10-30 17:39:12] iter = 01520, loss = 3.0467
2024-10-30 17:39:16: [2024-10-30 17:39:16] iter = 01530, loss = 2.2553
2024-10-30 17:39:20: [2024-10-30 17:39:20] iter = 01540, loss = 3.0967
2024-10-30 17:39:25: [2024-10-30 17:39:25] iter = 01550, loss = 2.3288
2024-10-30 17:39:30: [2024-10-30 17:39:30] iter = 01560, loss = 3.0085
2024-10-30 17:39:34: [2024-10-30 17:39:34] iter = 01570, loss = 1.9025
2024-10-30 17:39:39: [2024-10-30 17:39:39] iter = 01580, loss = 1.6932
2024-10-30 17:39:43: [2024-10-30 17:39:43] iter = 01590, loss = 3.8014
2024-10-30 17:39:45: [2024-10-30 17:39:45] iter = 01600, loss = 3.2186
2024-10-30 17:39:49: [2024-10-30 17:39:49] iter = 01610, loss = 2.5309
2024-10-30 17:39:52: [2024-10-30 17:39:52] iter = 01620, loss = 2.5615
2024-10-30 17:39:55: [2024-10-30 17:39:55] iter = 01630, loss = 2.1216
2024-10-30 17:39:58: [2024-10-30 17:39:58] iter = 01640, loss = 2.0230
2024-10-30 17:40:00: [2024-10-30 17:40:00] iter = 01650, loss = 3.4133
2024-10-30 17:40:04: [2024-10-30 17:40:04] iter = 01660, loss = 2.4212
2024-10-30 17:40:06: [2024-10-30 17:40:06] iter = 01670, loss = 2.4042
2024-10-30 17:40:09: [2024-10-30 17:40:09] iter = 01680, loss = 2.5193
2024-10-30 17:40:13: [2024-10-30 17:40:13] iter = 01690, loss = 2.0781
2024-10-30 17:40:15: [2024-10-30 17:40:15] iter = 01700, loss = 1.9666
2024-10-30 17:40:19: [2024-10-30 17:40:19] iter = 01710, loss = 2.0393
2024-10-30 17:40:23: [2024-10-30 17:40:23] iter = 01720, loss = 4.3367
2024-10-30 17:40:27: [2024-10-30 17:40:27] iter = 01730, loss = 2.7691
2024-10-30 17:40:30: [2024-10-30 17:40:30] iter = 01740, loss = 1.7385
2024-10-30 17:40:34: [2024-10-30 17:40:34] iter = 01750, loss = 2.1719
2024-10-30 17:40:39: [2024-10-30 17:40:39] iter = 01760, loss = 2.3557
2024-10-30 17:40:44: [2024-10-30 17:40:44] iter = 01770, loss = 2.3506
2024-10-30 17:40:48: [2024-10-30 17:40:48] iter = 01780, loss = 2.1340
2024-10-30 17:40:52: [2024-10-30 17:40:52] iter = 01790, loss = 2.0610
2024-10-30 17:40:56: [2024-10-30 17:40:56] iter = 01800, loss = 2.7471
2024-10-30 17:41:00: [2024-10-30 17:41:00] iter = 01810, loss = 2.0554
2024-10-30 17:41:04: [2024-10-30 17:41:04] iter = 01820, loss = 2.9478
2024-10-30 17:41:07: [2024-10-30 17:41:07] iter = 01830, loss = 1.9513
2024-10-30 17:41:10: [2024-10-30 17:41:10] iter = 01840, loss = 2.3017
2024-10-30 17:41:13: [2024-10-30 17:41:13] iter = 01850, loss = 3.1490
2024-10-30 17:41:18: [2024-10-30 17:41:18] iter = 01860, loss = 2.0097
2024-10-30 17:41:21: [2024-10-30 17:41:21] iter = 01870, loss = 3.7657
2024-10-30 17:41:25: [2024-10-30 17:41:25] iter = 01880, loss = 2.6907
2024-10-30 17:41:28: [2024-10-30 17:41:28] iter = 01890, loss = 2.2313
2024-10-30 17:41:32: [2024-10-30 17:41:32] iter = 01900, loss = 1.8891
2024-10-30 17:41:36: [2024-10-30 17:41:36] iter = 01910, loss = 1.6705
2024-10-30 17:41:39: [2024-10-30 17:41:39] iter = 01920, loss = 2.3764
2024-10-30 17:41:43: [2024-10-30 17:41:43] iter = 01930, loss = 2.2627
2024-10-30 17:41:45: [2024-10-30 17:41:45] iter = 01940, loss = 1.8640
2024-10-30 17:41:49: [2024-10-30 17:41:49] iter = 01950, loss = 2.4007
2024-10-30 17:41:52: [2024-10-30 17:41:52] iter = 01960, loss = 2.0574
2024-10-30 17:41:55: [2024-10-30 17:41:55] iter = 01970, loss = 2.0278
2024-10-30 17:41:58: [2024-10-30 17:41:58] iter = 01980, loss = 2.0930
2024-10-30 17:42:02: [2024-10-30 17:42:02] iter = 01990, loss = 2.2558
2024-10-30 17:42:05: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
2024-10-30 17:42:05: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 17:42:05: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 25491}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:44:37: Evaluate 5 random ConvNet, ACCmean = 0.7822 ACCstd = 0.0044
-------------------------
2024-10-30 17:44:37: Evaluate 5 random ConvNet, SENmean = 0.7768 SENstd = 0.0035
-------------------------
2024-10-30 17:44:37: Evaluate 5 random ConvNet, SPEmean = 0.9781 SPEstd = 0.0004
-------------------------
2024-10-30 17:44:37: Evaluate 5 random ConvNet, F!mean = 0.7643 F!std = 0.0042
-------------------------
2024-10-30 17:44:37: Evaluate 5 random ConvNet, mean = 0.7822 std = 0.0044
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:44:37: [2024-10-30 17:44:37] iter = 02000, loss = 2.6012
2024-10-30 17:44:41: [2024-10-30 17:44:41] iter = 02010, loss = 2.2367
2024-10-30 17:44:45: [2024-10-30 17:44:45] iter = 02020, loss = 3.5023
2024-10-30 17:44:49: [2024-10-30 17:44:49] iter = 02030, loss = 2.5500
2024-10-30 17:44:53: [2024-10-30 17:44:53] iter = 02040, loss = 2.2187
2024-10-30 17:44:56: [2024-10-30 17:44:56] iter = 02050, loss = 3.5513
2024-10-30 17:44:59: [2024-10-30 17:44:59] iter = 02060, loss = 1.9555
2024-10-30 17:45:04: [2024-10-30 17:45:04] iter = 02070, loss = 1.8817
2024-10-30 17:45:08: [2024-10-30 17:45:08] iter = 02080, loss = 2.1170
2024-10-30 17:45:13: [2024-10-30 17:45:13] iter = 02090, loss = 2.2872
2024-10-30 17:45:18: [2024-10-30 17:45:18] iter = 02100, loss = 1.7605
2024-10-30 17:45:22: [2024-10-30 17:45:22] iter = 02110, loss = 2.2550
2024-10-30 17:45:25: [2024-10-30 17:45:25] iter = 02120, loss = 2.5415
2024-10-30 17:45:29: [2024-10-30 17:45:29] iter = 02130, loss = 1.8170
2024-10-30 17:45:33: [2024-10-30 17:45:33] iter = 02140, loss = 3.1886
2024-10-30 17:45:36: [2024-10-30 17:45:36] iter = 02150, loss = 2.0887
2024-10-30 17:45:40: [2024-10-30 17:45:40] iter = 02160, loss = 1.6455
2024-10-30 17:45:44: [2024-10-30 17:45:44] iter = 02170, loss = 2.2440
2024-10-30 17:45:48: [2024-10-30 17:45:48] iter = 02180, loss = 2.7227
2024-10-30 17:45:52: [2024-10-30 17:45:51] iter = 02190, loss = 2.7446
2024-10-30 17:45:55: [2024-10-30 17:45:55] iter = 02200, loss = 4.9260
2024-10-30 17:45:59: [2024-10-30 17:45:59] iter = 02210, loss = 3.6018
2024-10-30 17:46:03: [2024-10-30 17:46:03] iter = 02220, loss = 2.3393
2024-10-30 17:46:06: [2024-10-30 17:46:06] iter = 02230, loss = 4.4201
2024-10-30 17:46:10: [2024-10-30 17:46:10] iter = 02240, loss = 1.9171
2024-10-30 17:46:14: [2024-10-30 17:46:14] iter = 02250, loss = 2.0588
2024-10-30 17:46:18: [2024-10-30 17:46:18] iter = 02260, loss = 2.7944
2024-10-30 17:46:21: [2024-10-30 17:46:21] iter = 02270, loss = 2.8302
2024-10-30 17:46:26: [2024-10-30 17:46:26] iter = 02280, loss = 1.9698
2024-10-30 17:46:30: [2024-10-30 17:46:30] iter = 02290, loss = 1.9365
2024-10-30 17:46:34: [2024-10-30 17:46:34] iter = 02300, loss = 1.9826
2024-10-30 17:46:38: [2024-10-30 17:46:38] iter = 02310, loss = 1.9913
2024-10-30 17:46:42: [2024-10-30 17:46:42] iter = 02320, loss = 3.3642
2024-10-30 17:46:45: [2024-10-30 17:46:45] iter = 02330, loss = 4.1822
2024-10-30 17:46:49: [2024-10-30 17:46:49] iter = 02340, loss = 5.1369
2024-10-30 17:46:54: [2024-10-30 17:46:54] iter = 02350, loss = 1.9653
2024-10-30 17:46:58: [2024-10-30 17:46:58] iter = 02360, loss = 2.2928
2024-10-30 17:47:02: [2024-10-30 17:47:02] iter = 02370, loss = 3.8420
2024-10-30 17:47:06: [2024-10-30 17:47:06] iter = 02380, loss = 1.8529
2024-10-30 17:47:09: [2024-10-30 17:47:09] iter = 02390, loss = 3.1625
2024-10-30 17:47:13: [2024-10-30 17:47:13] iter = 02400, loss = 3.4544
2024-10-30 17:47:16: [2024-10-30 17:47:16] iter = 02410, loss = 2.1572
2024-10-30 17:47:20: [2024-10-30 17:47:20] iter = 02420, loss = 1.8324
2024-10-30 17:47:24: [2024-10-30 17:47:24] iter = 02430, loss = 2.7418
2024-10-30 17:47:28: [2024-10-30 17:47:28] iter = 02440, loss = 3.8462
2024-10-30 17:47:32: [2024-10-30 17:47:32] iter = 02450, loss = 2.3252
2024-10-30 17:47:37: [2024-10-30 17:47:37] iter = 02460, loss = 2.3048
2024-10-30 17:47:40: [2024-10-30 17:47:40] iter = 02470, loss = 2.2759
2024-10-30 17:47:45: [2024-10-30 17:47:45] iter = 02480, loss = 2.1776
2024-10-30 17:47:49: [2024-10-30 17:47:49] iter = 02490, loss = 1.9458
2024-10-30 17:47:52: [2024-10-30 17:47:52] iter = 02500, loss = 2.1434
2024-10-30 17:47:56: [2024-10-30 17:47:56] iter = 02510, loss = 2.0232
2024-10-30 17:47:59: [2024-10-30 17:47:59] iter = 02520, loss = 2.2000
2024-10-30 17:48:03: [2024-10-30 17:48:03] iter = 02530, loss = 2.3331
2024-10-30 17:48:07: [2024-10-30 17:48:07] iter = 02540, loss = 4.0557
2024-10-30 17:48:11: [2024-10-30 17:48:11] iter = 02550, loss = 1.9935
2024-10-30 17:48:15: [2024-10-30 17:48:15] iter = 02560, loss = 1.9048
2024-10-30 17:48:18: [2024-10-30 17:48:18] iter = 02570, loss = 2.7004
2024-10-30 17:48:22: [2024-10-30 17:48:22] iter = 02580, loss = 2.0026
2024-10-30 17:48:26: [2024-10-30 17:48:26] iter = 02590, loss = 1.9294
2024-10-30 17:48:31: [2024-10-30 17:48:31] iter = 02600, loss = 7.3609
2024-10-30 17:48:35: [2024-10-30 17:48:35] iter = 02610, loss = 2.4409
2024-10-30 17:48:39: [2024-10-30 17:48:39] iter = 02620, loss = 2.2332
2024-10-30 17:48:42: [2024-10-30 17:48:42] iter = 02630, loss = 2.3108
2024-10-30 17:48:46: [2024-10-30 17:48:46] iter = 02640, loss = 1.7044
2024-10-30 17:48:50: [2024-10-30 17:48:50] iter = 02650, loss = 1.8390
2024-10-30 17:48:54: [2024-10-30 17:48:54] iter = 02660, loss = 2.8138
2024-10-30 17:48:57: [2024-10-30 17:48:57] iter = 02670, loss = 2.2836
2024-10-30 17:49:01: [2024-10-30 17:49:01] iter = 02680, loss = 3.2620
2024-10-30 17:49:04: [2024-10-30 17:49:04] iter = 02690, loss = 2.8726
2024-10-30 17:49:08: [2024-10-30 17:49:08] iter = 02700, loss = 2.2130
2024-10-30 17:49:12: [2024-10-30 17:49:12] iter = 02710, loss = 2.7177
2024-10-30 17:49:15: [2024-10-30 17:49:15] iter = 02720, loss = 2.0721
2024-10-30 17:49:19: [2024-10-30 17:49:19] iter = 02730, loss = 2.4119
2024-10-30 17:49:22: [2024-10-30 17:49:22] iter = 02740, loss = 2.4848
2024-10-30 17:49:25: [2024-10-30 17:49:25] iter = 02750, loss = 2.1295
2024-10-30 17:49:28: [2024-10-30 17:49:28] iter = 02760, loss = 2.5518
2024-10-30 17:49:32: [2024-10-30 17:49:32] iter = 02770, loss = 3.7338
2024-10-30 17:49:36: [2024-10-30 17:49:36] iter = 02780, loss = 2.6739
2024-10-30 17:49:40: [2024-10-30 17:49:40] iter = 02790, loss = 2.3496
2024-10-30 17:49:43: [2024-10-30 17:49:43] iter = 02800, loss = 2.2585
2024-10-30 17:49:47: [2024-10-30 17:49:47] iter = 02810, loss = 3.1232
2024-10-30 17:49:50: [2024-10-30 17:49:50] iter = 02820, loss = 2.6813
2024-10-30 17:49:53: [2024-10-30 17:49:53] iter = 02830, loss = 3.3337
2024-10-30 17:49:58: [2024-10-30 17:49:58] iter = 02840, loss = 2.6542
2024-10-30 17:50:02: [2024-10-30 17:50:02] iter = 02850, loss = 2.4793
2024-10-30 17:50:06: [2024-10-30 17:50:06] iter = 02860, loss = 1.8087
2024-10-30 17:50:11: [2024-10-30 17:50:11] iter = 02870, loss = 2.0472
2024-10-30 17:50:15: [2024-10-30 17:50:15] iter = 02880, loss = 2.4160
2024-10-30 17:50:19: [2024-10-30 17:50:19] iter = 02890, loss = 2.7899
2024-10-30 17:50:23: [2024-10-30 17:50:23] iter = 02900, loss = 2.5692
2024-10-30 17:50:26: [2024-10-30 17:50:26] iter = 02910, loss = 2.5343
2024-10-30 17:50:30: [2024-10-30 17:50:30] iter = 02920, loss = 2.0918
2024-10-30 17:50:34: [2024-10-30 17:50:34] iter = 02930, loss = 2.2263
2024-10-30 17:50:38: [2024-10-30 17:50:38] iter = 02940, loss = 1.6743
2024-10-30 17:50:41: [2024-10-30 17:50:41] iter = 02950, loss = 2.2966
2024-10-30 17:50:46: [2024-10-30 17:50:46] iter = 02960, loss = 2.0884
2024-10-30 17:50:49: [2024-10-30 17:50:49] iter = 02970, loss = 2.0690
2024-10-30 17:50:53: [2024-10-30 17:50:53] iter = 02980, loss = 1.8917
2024-10-30 17:50:56: [2024-10-30 17:50:56] iter = 02990, loss = 2.4093
2024-10-30 17:50:59: [2024-10-30 17:50:59] iter = 03000, loss = 1.9363
2024-10-30 17:51:03: [2024-10-30 17:51:03] iter = 03010, loss = 3.4743
2024-10-30 17:51:06: [2024-10-30 17:51:06] iter = 03020, loss = 2.5041
2024-10-30 17:51:10: [2024-10-30 17:51:10] iter = 03030, loss = 1.7577
2024-10-30 17:51:14: [2024-10-30 17:51:14] iter = 03040, loss = 2.7339
2024-10-30 17:51:18: [2024-10-30 17:51:18] iter = 03050, loss = 1.8710
2024-10-30 17:51:22: [2024-10-30 17:51:22] iter = 03060, loss = 2.0928
2024-10-30 17:51:26: [2024-10-30 17:51:26] iter = 03070, loss = 3.3536
2024-10-30 17:51:31: [2024-10-30 17:51:31] iter = 03080, loss = 1.8037
2024-10-30 17:51:34: [2024-10-30 17:51:34] iter = 03090, loss = 2.5097
2024-10-30 17:51:38: [2024-10-30 17:51:38] iter = 03100, loss = 2.2740
2024-10-30 17:51:42: [2024-10-30 17:51:42] iter = 03110, loss = 2.5364
2024-10-30 17:51:46: [2024-10-30 17:51:46] iter = 03120, loss = 3.2128
2024-10-30 17:51:50: [2024-10-30 17:51:50] iter = 03130, loss = 2.1299
2024-10-30 17:51:54: [2024-10-30 17:51:54] iter = 03140, loss = 2.0408
2024-10-30 17:51:58: [2024-10-30 17:51:58] iter = 03150, loss = 3.2924
2024-10-30 17:52:02: [2024-10-30 17:52:02] iter = 03160, loss = 2.0493
2024-10-30 17:52:05: [2024-10-30 17:52:05] iter = 03170, loss = 2.4488
2024-10-30 17:52:09: [2024-10-30 17:52:09] iter = 03180, loss = 3.0740
2024-10-30 17:52:13: [2024-10-30 17:52:13] iter = 03190, loss = 2.1372
2024-10-30 17:52:18: [2024-10-30 17:52:18] iter = 03200, loss = 2.3144
2024-10-30 17:52:22: [2024-10-30 17:52:22] iter = 03210, loss = 2.2089
2024-10-30 17:52:25: [2024-10-30 17:52:25] iter = 03220, loss = 2.1789
2024-10-30 17:52:28: [2024-10-30 17:52:28] iter = 03230, loss = 1.8888
2024-10-30 17:52:31: [2024-10-30 17:52:31] iter = 03240, loss = 2.4388
2024-10-30 17:52:35: [2024-10-30 17:52:35] iter = 03250, loss = 2.0233
2024-10-30 17:52:39: [2024-10-30 17:52:39] iter = 03260, loss = 2.8146
2024-10-30 17:52:42: [2024-10-30 17:52:42] iter = 03270, loss = 1.9763
2024-10-30 17:52:47: [2024-10-30 17:52:47] iter = 03280, loss = 2.8917
2024-10-30 17:52:50: [2024-10-30 17:52:50] iter = 03290, loss = 1.9093
2024-10-30 17:52:53: [2024-10-30 17:52:53] iter = 03300, loss = 2.4682
2024-10-30 17:52:57: [2024-10-30 17:52:57] iter = 03310, loss = 2.7981
2024-10-30 17:53:00: [2024-10-30 17:53:00] iter = 03320, loss = 3.9607
2024-10-30 17:53:04: [2024-10-30 17:53:04] iter = 03330, loss = 1.9444
2024-10-30 17:53:07: [2024-10-30 17:53:07] iter = 03340, loss = 1.8760
2024-10-30 17:53:10: [2024-10-30 17:53:10] iter = 03350, loss = 2.0480
2024-10-30 17:53:14: [2024-10-30 17:53:14] iter = 03360, loss = 2.5628
2024-10-30 17:53:18: [2024-10-30 17:53:18] iter = 03370, loss = 2.0024
2024-10-30 17:53:21: [2024-10-30 17:53:21] iter = 03380, loss = 2.5388
2024-10-30 17:53:24: [2024-10-30 17:53:24] iter = 03390, loss = 2.6480
2024-10-30 17:53:28: [2024-10-30 17:53:28] iter = 03400, loss = 2.4110
2024-10-30 17:53:32: [2024-10-30 17:53:32] iter = 03410, loss = 2.2217
2024-10-30 17:53:36: [2024-10-30 17:53:36] iter = 03420, loss = 2.1654
2024-10-30 17:53:39: [2024-10-30 17:53:39] iter = 03430, loss = 2.0496
2024-10-30 17:53:43: [2024-10-30 17:53:43] iter = 03440, loss = 2.0418
2024-10-30 17:53:48: [2024-10-30 17:53:48] iter = 03450, loss = 1.8971
2024-10-30 17:53:51: [2024-10-30 17:53:51] iter = 03460, loss = 2.1861
2024-10-30 17:53:55: [2024-10-30 17:53:55] iter = 03470, loss = 2.8060
2024-10-30 17:53:59: [2024-10-30 17:53:59] iter = 03480, loss = 1.9315
2024-10-30 17:54:02: [2024-10-30 17:54:02] iter = 03490, loss = 2.2650
2024-10-30 17:54:07: [2024-10-30 17:54:07] iter = 03500, loss = 2.0241
2024-10-30 17:54:11: [2024-10-30 17:54:11] iter = 03510, loss = 3.2783
2024-10-30 17:54:15: [2024-10-30 17:54:15] iter = 03520, loss = 3.4230
2024-10-30 17:54:18: [2024-10-30 17:54:18] iter = 03530, loss = 2.4364
2024-10-30 17:54:22: [2024-10-30 17:54:22] iter = 03540, loss = 3.3444
2024-10-30 17:54:25: [2024-10-30 17:54:25] iter = 03550, loss = 2.1919
2024-10-30 17:54:28: [2024-10-30 17:54:28] iter = 03560, loss = 2.2616
2024-10-30 17:54:31: [2024-10-30 17:54:31] iter = 03570, loss = 3.1662
2024-10-30 17:54:35: [2024-10-30 17:54:35] iter = 03580, loss = 2.1125
2024-10-30 17:54:39: [2024-10-30 17:54:39] iter = 03590, loss = 1.8455
2024-10-30 17:54:42: [2024-10-30 17:54:42] iter = 03600, loss = 1.5203
2024-10-30 17:54:45: [2024-10-30 17:54:45] iter = 03610, loss = 3.5325
2024-10-30 17:54:49: [2024-10-30 17:54:49] iter = 03620, loss = 2.6910
2024-10-30 17:54:53: [2024-10-30 17:54:53] iter = 03630, loss = 3.9857
2024-10-30 17:54:57: [2024-10-30 17:54:57] iter = 03640, loss = 2.0874
2024-10-30 17:55:00: [2024-10-30 17:55:00] iter = 03650, loss = 2.2065
2024-10-30 17:55:04: [2024-10-30 17:55:04] iter = 03660, loss = 1.8653
2024-10-30 17:55:08: [2024-10-30 17:55:08] iter = 03670, loss = 2.4182
2024-10-30 17:55:11: [2024-10-30 17:55:11] iter = 03680, loss = 2.0741
2024-10-30 17:55:15: [2024-10-30 17:55:15] iter = 03690, loss = 2.6461
2024-10-30 17:55:19: [2024-10-30 17:55:19] iter = 03700, loss = 2.5914
2024-10-30 17:55:23: [2024-10-30 17:55:23] iter = 03710, loss = 1.9605
2024-10-30 17:55:26: [2024-10-30 17:55:26] iter = 03720, loss = 2.0994
2024-10-30 17:55:29: [2024-10-30 17:55:29] iter = 03730, loss = 2.4279
2024-10-30 17:55:33: [2024-10-30 17:55:33] iter = 03740, loss = 2.2409
2024-10-30 17:55:37: [2024-10-30 17:55:37] iter = 03750, loss = 2.5571
2024-10-30 17:55:41: [2024-10-30 17:55:41] iter = 03760, loss = 2.5798
2024-10-30 17:55:45: [2024-10-30 17:55:45] iter = 03770, loss = 5.7027
2024-10-30 17:55:50: [2024-10-30 17:55:50] iter = 03780, loss = 1.7565
2024-10-30 17:55:53: [2024-10-30 17:55:53] iter = 03790, loss = 2.4165
2024-10-30 17:55:56: [2024-10-30 17:55:56] iter = 03800, loss = 2.3336
2024-10-30 17:55:59: [2024-10-30 17:55:59] iter = 03810, loss = 2.4275
2024-10-30 17:56:03: [2024-10-30 17:56:03] iter = 03820, loss = 2.4680
2024-10-30 17:56:06: [2024-10-30 17:56:06] iter = 03830, loss = 2.0088
2024-10-30 17:56:10: [2024-10-30 17:56:10] iter = 03840, loss = 2.0108
2024-10-30 17:56:14: [2024-10-30 17:56:14] iter = 03850, loss = 2.4270
2024-10-30 17:56:17: [2024-10-30 17:56:17] iter = 03860, loss = 2.1279
2024-10-30 17:56:22: [2024-10-30 17:56:22] iter = 03870, loss = 2.1088
2024-10-30 17:56:26: [2024-10-30 17:56:26] iter = 03880, loss = 1.8426
2024-10-30 17:56:29: [2024-10-30 17:56:29] iter = 03890, loss = 2.1790
2024-10-30 17:56:34: [2024-10-30 17:56:34] iter = 03900, loss = 2.5341
2024-10-30 17:56:37: [2024-10-30 17:56:37] iter = 03910, loss = 2.1246
2024-10-30 17:56:39: [2024-10-30 17:56:39] iter = 03920, loss = 4.6364
2024-10-30 17:56:42: [2024-10-30 17:56:42] iter = 03930, loss = 2.3989
2024-10-30 17:56:46: [2024-10-30 17:56:46] iter = 03940, loss = 2.6639
2024-10-30 17:56:50: [2024-10-30 17:56:50] iter = 03950, loss = 2.6140
2024-10-30 17:56:54: [2024-10-30 17:56:54] iter = 03960, loss = 2.6821
2024-10-30 17:56:57: [2024-10-30 17:56:57] iter = 03970, loss = 2.2181
2024-10-30 17:57:00: [2024-10-30 17:57:00] iter = 03980, loss = 1.8449
2024-10-30 17:57:04: [2024-10-30 17:57:04] iter = 03990, loss = 1.8698
2024-10-30 17:57:07: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
2024-10-30 17:57:07: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 17:57:07: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 27559}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:59:38: Evaluate 5 random ConvNet, ACCmean = 0.7789 ACCstd = 0.0031
-------------------------
2024-10-30 17:59:38: Evaluate 5 random ConvNet, SENmean = 0.7804 SENstd = 0.0024
-------------------------
2024-10-30 17:59:38: Evaluate 5 random ConvNet, SPEmean = 0.9779 SPEstd = 0.0003
-------------------------
2024-10-30 17:59:38: Evaluate 5 random ConvNet, F!mean = 0.7654 F!std = 0.0026
-------------------------
2024-10-30 17:59:38: Evaluate 5 random ConvNet, mean = 0.7789 std = 0.0031
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 17:59:39: [2024-10-30 17:59:39] iter = 04000, loss = 2.2802
2024-10-30 17:59:43: [2024-10-30 17:59:43] iter = 04010, loss = 2.0594
2024-10-30 17:59:47: [2024-10-30 17:59:47] iter = 04020, loss = 2.2571
2024-10-30 17:59:51: [2024-10-30 17:59:51] iter = 04030, loss = 2.2942
2024-10-30 17:59:54: [2024-10-30 17:59:54] iter = 04040, loss = 3.4126
2024-10-30 17:59:58: [2024-10-30 17:59:58] iter = 04050, loss = 3.3582
2024-10-30 18:00:02: [2024-10-30 18:00:02] iter = 04060, loss = 3.0018
2024-10-30 18:00:06: [2024-10-30 18:00:06] iter = 04070, loss = 1.9027
2024-10-30 18:00:10: [2024-10-30 18:00:10] iter = 04080, loss = 2.2785
2024-10-30 18:00:12: [2024-10-30 18:00:12] iter = 04090, loss = 2.3357
2024-10-30 18:00:17: [2024-10-30 18:00:17] iter = 04100, loss = 4.7764
2024-10-30 18:00:20: [2024-10-30 18:00:20] iter = 04110, loss = 2.2917
2024-10-30 18:00:23: [2024-10-30 18:00:23] iter = 04120, loss = 3.2994
2024-10-30 18:00:26: [2024-10-30 18:00:26] iter = 04130, loss = 2.8247
2024-10-30 18:00:30: [2024-10-30 18:00:30] iter = 04140, loss = 3.1419
2024-10-30 18:00:34: [2024-10-30 18:00:34] iter = 04150, loss = 2.0044
2024-10-30 18:00:38: [2024-10-30 18:00:38] iter = 04160, loss = 3.1762
2024-10-30 18:00:42: [2024-10-30 18:00:42] iter = 04170, loss = 2.3976
2024-10-30 18:00:46: [2024-10-30 18:00:46] iter = 04180, loss = 1.8560
2024-10-30 18:00:50: [2024-10-30 18:00:50] iter = 04190, loss = 3.1872
2024-10-30 18:00:53: [2024-10-30 18:00:53] iter = 04200, loss = 8.1739
2024-10-30 18:00:57: [2024-10-30 18:00:57] iter = 04210, loss = 2.6386
2024-10-30 18:01:01: [2024-10-30 18:01:01] iter = 04220, loss = 2.0765
2024-10-30 18:01:05: [2024-10-30 18:01:05] iter = 04230, loss = 2.3745
2024-10-30 18:01:09: [2024-10-30 18:01:09] iter = 04240, loss = 1.9404
2024-10-30 18:01:12: [2024-10-30 18:01:12] iter = 04250, loss = 1.9919
2024-10-30 18:01:16: [2024-10-30 18:01:16] iter = 04260, loss = 1.8496
2024-10-30 18:01:20: [2024-10-30 18:01:20] iter = 04270, loss = 1.8461
2024-10-30 18:01:23: [2024-10-30 18:01:23] iter = 04280, loss = 1.9920
2024-10-30 18:01:25: [2024-10-30 18:01:25] iter = 04290, loss = 2.0166
2024-10-30 18:01:28: [2024-10-30 18:01:28] iter = 04300, loss = 2.2756
2024-10-30 18:01:32: [2024-10-30 18:01:32] iter = 04310, loss = 3.6110
2024-10-30 18:01:35: [2024-10-30 18:01:35] iter = 04320, loss = 2.6965
2024-10-30 18:01:39: [2024-10-30 18:01:39] iter = 04330, loss = 2.8689
2024-10-30 18:01:43: [2024-10-30 18:01:43] iter = 04340, loss = 2.2403
2024-10-30 18:01:47: [2024-10-30 18:01:47] iter = 04350, loss = 1.9411
2024-10-30 18:01:50: [2024-10-30 18:01:50] iter = 04360, loss = 2.1251
2024-10-30 18:01:54: [2024-10-30 18:01:54] iter = 04370, loss = 2.6134
2024-10-30 18:01:57: [2024-10-30 18:01:57] iter = 04380, loss = 2.6764
2024-10-30 18:02:01: [2024-10-30 18:02:01] iter = 04390, loss = 4.1286
2024-10-30 18:02:04: [2024-10-30 18:02:04] iter = 04400, loss = 2.2895
2024-10-30 18:02:08: [2024-10-30 18:02:08] iter = 04410, loss = 2.0726
2024-10-30 18:02:11: [2024-10-30 18:02:11] iter = 04420, loss = 1.9881
2024-10-30 18:02:15: [2024-10-30 18:02:15] iter = 04430, loss = 2.4137
2024-10-30 18:02:19: [2024-10-30 18:02:19] iter = 04440, loss = 1.8425
2024-10-30 18:02:22: [2024-10-30 18:02:22] iter = 04450, loss = 2.9921
2024-10-30 18:02:26: [2024-10-30 18:02:26] iter = 04460, loss = 2.6630
2024-10-30 18:02:30: [2024-10-30 18:02:30] iter = 04470, loss = 1.6738
2024-10-30 18:02:34: [2024-10-30 18:02:34] iter = 04480, loss = 1.9066
2024-10-30 18:02:39: [2024-10-30 18:02:39] iter = 04490, loss = 2.4282
2024-10-30 18:02:41: [2024-10-30 18:02:41] iter = 04500, loss = 2.0920
2024-10-30 18:02:43: [2024-10-30 18:02:43] iter = 04510, loss = 2.0179
2024-10-30 18:02:47: [2024-10-30 18:02:47] iter = 04520, loss = 1.9564
2024-10-30 18:02:49: [2024-10-30 18:02:49] iter = 04530, loss = 2.6404
2024-10-30 18:02:53: [2024-10-30 18:02:53] iter = 04540, loss = 1.8313
2024-10-30 18:02:56: [2024-10-30 18:02:56] iter = 04550, loss = 1.9181
2024-10-30 18:02:59: [2024-10-30 18:02:59] iter = 04560, loss = 2.8560
2024-10-30 18:03:03: [2024-10-30 18:03:03] iter = 04570, loss = 3.5047
2024-10-30 18:03:07: [2024-10-30 18:03:07] iter = 04580, loss = 1.9738
2024-10-30 18:03:11: [2024-10-30 18:03:11] iter = 04590, loss = 1.8845
2024-10-30 18:03:15: [2024-10-30 18:03:15] iter = 04600, loss = 1.8700
2024-10-30 18:03:19: [2024-10-30 18:03:19] iter = 04610, loss = 2.1169
2024-10-30 18:03:23: [2024-10-30 18:03:23] iter = 04620, loss = 2.2768
2024-10-30 18:03:26: [2024-10-30 18:03:26] iter = 04630, loss = 2.6218
2024-10-30 18:03:30: [2024-10-30 18:03:30] iter = 04640, loss = 2.6402
2024-10-30 18:03:34: [2024-10-30 18:03:34] iter = 04650, loss = 2.8127
2024-10-30 18:03:37: [2024-10-30 18:03:37] iter = 04660, loss = 9.7520
2024-10-30 18:03:41: [2024-10-30 18:03:41] iter = 04670, loss = 2.7038
2024-10-30 18:03:44: [2024-10-30 18:03:44] iter = 04680, loss = 2.1327
2024-10-30 18:03:48: [2024-10-30 18:03:48] iter = 04690, loss = 1.9443
2024-10-30 18:03:52: [2024-10-30 18:03:52] iter = 04700, loss = 1.9999
2024-10-30 18:03:55: [2024-10-30 18:03:55] iter = 04710, loss = 2.3228
2024-10-30 18:03:59: [2024-10-30 18:03:59] iter = 04720, loss = 1.9842
2024-10-30 18:04:04: [2024-10-30 18:04:04] iter = 04730, loss = 2.3537
2024-10-30 18:04:09: [2024-10-30 18:04:09] iter = 04740, loss = 3.0962
2024-10-30 18:04:12: [2024-10-30 18:04:12] iter = 04750, loss = 3.0847
2024-10-30 18:04:16: [2024-10-30 18:04:16] iter = 04760, loss = 2.3672
2024-10-30 18:04:19: [2024-10-30 18:04:19] iter = 04770, loss = 2.1375
2024-10-30 18:04:23: [2024-10-30 18:04:23] iter = 04780, loss = 2.9307
2024-10-30 18:04:26: [2024-10-30 18:04:26] iter = 04790, loss = 2.2847
2024-10-30 18:04:30: [2024-10-30 18:04:30] iter = 04800, loss = 2.0758
2024-10-30 18:04:33: [2024-10-30 18:04:33] iter = 04810, loss = 1.8598
2024-10-30 18:04:37: [2024-10-30 18:04:37] iter = 04820, loss = 2.0765
2024-10-30 18:04:40: [2024-10-30 18:04:40] iter = 04830, loss = 2.0201
2024-10-30 18:04:44: [2024-10-30 18:04:44] iter = 04840, loss = 2.0727
2024-10-30 18:04:48: [2024-10-30 18:04:48] iter = 04850, loss = 2.4553
2024-10-30 18:04:51: [2024-10-30 18:04:51] iter = 04860, loss = 2.4181
2024-10-30 18:04:56: [2024-10-30 18:04:56] iter = 04870, loss = 2.6412
2024-10-30 18:04:59: [2024-10-30 18:04:59] iter = 04880, loss = 3.3408
2024-10-30 18:05:03: [2024-10-30 18:05:03] iter = 04890, loss = 2.1510
2024-10-30 18:05:07: [2024-10-30 18:05:07] iter = 04900, loss = 2.0907
2024-10-30 18:05:10: [2024-10-30 18:05:10] iter = 04910, loss = 2.3025
2024-10-30 18:05:13: [2024-10-30 18:05:13] iter = 04920, loss = 3.0054
2024-10-30 18:05:18: [2024-10-30 18:05:18] iter = 04930, loss = 3.3934
2024-10-30 18:05:21: [2024-10-30 18:05:21] iter = 04940, loss = 2.4554
2024-10-30 18:05:25: [2024-10-30 18:05:25] iter = 04950, loss = 1.7820
2024-10-30 18:05:29: [2024-10-30 18:05:29] iter = 04960, loss = 2.6098
2024-10-30 18:05:33: [2024-10-30 18:05:33] iter = 04970, loss = 1.9115
2024-10-30 18:05:37: [2024-10-30 18:05:37] iter = 04980, loss = 1.7624
2024-10-30 18:05:41: [2024-10-30 18:05:41] iter = 04990, loss = 5.6341
2024-10-30 18:05:45: [2024-10-30 18:05:45] iter = 05000, loss = 2.3758
2024-10-30 18:05:49: [2024-10-30 18:05:49] iter = 05010, loss = 1.6115
2024-10-30 18:05:54: [2024-10-30 18:05:54] iter = 05020, loss = 2.5755
2024-10-30 18:05:58: [2024-10-30 18:05:58] iter = 05030, loss = 2.3397
2024-10-30 18:06:01: [2024-10-30 18:06:01] iter = 05040, loss = 2.1220
2024-10-30 18:06:04: [2024-10-30 18:06:04] iter = 05050, loss = 2.4167
2024-10-30 18:06:07: [2024-10-30 18:06:07] iter = 05060, loss = 1.9147
2024-10-30 18:06:11: [2024-10-30 18:06:11] iter = 05070, loss = 2.3551
2024-10-30 18:06:14: [2024-10-30 18:06:14] iter = 05080, loss = 2.0019
2024-10-30 18:06:18: [2024-10-30 18:06:18] iter = 05090, loss = 2.7058
2024-10-30 18:06:22: [2024-10-30 18:06:22] iter = 05100, loss = 1.9191
2024-10-30 18:06:27: [2024-10-30 18:06:27] iter = 05110, loss = 2.0870
2024-10-30 18:06:30: [2024-10-30 18:06:30] iter = 05120, loss = 2.3096
2024-10-30 18:06:35: [2024-10-30 18:06:35] iter = 05130, loss = 2.3172
2024-10-30 18:06:38: [2024-10-30 18:06:38] iter = 05140, loss = 4.5695
2024-10-30 18:06:41: [2024-10-30 18:06:41] iter = 05150, loss = 2.9988
2024-10-30 18:06:44: [2024-10-30 18:06:44] iter = 05160, loss = 2.1831
2024-10-30 18:06:47: [2024-10-30 18:06:47] iter = 05170, loss = 1.8569
2024-10-30 18:06:51: [2024-10-30 18:06:51] iter = 05180, loss = 2.4420
2024-10-30 18:06:55: [2024-10-30 18:06:55] iter = 05190, loss = 2.6064
2024-10-30 18:06:59: [2024-10-30 18:06:59] iter = 05200, loss = 2.6474
2024-10-30 18:07:03: [2024-10-30 18:07:03] iter = 05210, loss = 2.4022
2024-10-30 18:07:07: [2024-10-30 18:07:07] iter = 05220, loss = 1.9519
2024-10-30 18:07:12: [2024-10-30 18:07:12] iter = 05230, loss = 1.9328
2024-10-30 18:07:16: [2024-10-30 18:07:16] iter = 05240, loss = 2.4096
2024-10-30 18:07:19: [2024-10-30 18:07:19] iter = 05250, loss = 2.5851
2024-10-30 18:07:24: [2024-10-30 18:07:24] iter = 05260, loss = 2.2527
2024-10-30 18:07:28: [2024-10-30 18:07:28] iter = 05270, loss = 2.4413
2024-10-30 18:07:32: [2024-10-30 18:07:32] iter = 05280, loss = 2.2324
2024-10-30 18:07:36: [2024-10-30 18:07:36] iter = 05290, loss = 3.4075
2024-10-30 18:07:40: [2024-10-30 18:07:40] iter = 05300, loss = 2.1418
2024-10-30 18:07:44: [2024-10-30 18:07:44] iter = 05310, loss = 2.8830
2024-10-30 18:07:47: [2024-10-30 18:07:47] iter = 05320, loss = 2.5219
2024-10-30 18:07:51: [2024-10-30 18:07:51] iter = 05330, loss = 2.1033
2024-10-30 18:07:54: [2024-10-30 18:07:54] iter = 05340, loss = 2.3535
2024-10-30 18:07:58: [2024-10-30 18:07:58] iter = 05350, loss = 1.8607
2024-10-30 18:08:02: [2024-10-30 18:08:02] iter = 05360, loss = 2.1258
2024-10-30 18:08:06: [2024-10-30 18:08:06] iter = 05370, loss = 2.1267
2024-10-30 18:08:09: [2024-10-30 18:08:09] iter = 05380, loss = 1.9217
2024-10-30 18:08:13: [2024-10-30 18:08:13] iter = 05390, loss = 2.1859
2024-10-30 18:08:18: [2024-10-30 18:08:18] iter = 05400, loss = 1.9365
2024-10-30 18:08:21: [2024-10-30 18:08:21] iter = 05410, loss = 2.0336
2024-10-30 18:08:25: [2024-10-30 18:08:25] iter = 05420, loss = 1.9388
2024-10-30 18:08:28: [2024-10-30 18:08:28] iter = 05430, loss = 1.9673
2024-10-30 18:08:32: [2024-10-30 18:08:32] iter = 05440, loss = 2.1519
2024-10-30 18:08:35: [2024-10-30 18:08:35] iter = 05450, loss = 2.0658
2024-10-30 18:08:39: [2024-10-30 18:08:39] iter = 05460, loss = 2.2325
2024-10-30 18:08:42: [2024-10-30 18:08:42] iter = 05470, loss = 2.0421
2024-10-30 18:08:45: [2024-10-30 18:08:45] iter = 05480, loss = 2.0820
2024-10-30 18:08:49: [2024-10-30 18:08:49] iter = 05490, loss = 2.7929
2024-10-30 18:08:53: [2024-10-30 18:08:53] iter = 05500, loss = 2.0944
2024-10-30 18:08:56: [2024-10-30 18:08:56] iter = 05510, loss = 2.3906
2024-10-30 18:09:00: [2024-10-30 18:09:00] iter = 05520, loss = 2.4662
2024-10-30 18:09:04: [2024-10-30 18:09:04] iter = 05530, loss = 2.1691
2024-10-30 18:09:08: [2024-10-30 18:09:08] iter = 05540, loss = 2.1462
2024-10-30 18:09:12: [2024-10-30 18:09:12] iter = 05550, loss = 2.2126
2024-10-30 18:09:15: [2024-10-30 18:09:15] iter = 05560, loss = 1.9645
2024-10-30 18:09:19: [2024-10-30 18:09:19] iter = 05570, loss = 1.9103
2024-10-30 18:09:23: [2024-10-30 18:09:23] iter = 05580, loss = 2.4632
2024-10-30 18:09:28: [2024-10-30 18:09:28] iter = 05590, loss = 2.4619
2024-10-30 18:09:32: [2024-10-30 18:09:32] iter = 05600, loss = 1.9094
2024-10-30 18:09:35: [2024-10-30 18:09:35] iter = 05610, loss = 1.9257
2024-10-30 18:09:40: [2024-10-30 18:09:40] iter = 05620, loss = 2.1373
2024-10-30 18:09:43: [2024-10-30 18:09:43] iter = 05630, loss = 2.2591
2024-10-30 18:09:47: [2024-10-30 18:09:46] iter = 05640, loss = 2.9345
2024-10-30 18:09:49: [2024-10-30 18:09:49] iter = 05650, loss = 1.6528
2024-10-30 18:09:53: [2024-10-30 18:09:53] iter = 05660, loss = 2.0991
2024-10-30 18:09:57: [2024-10-30 18:09:57] iter = 05670, loss = 2.3407
2024-10-30 18:10:01: [2024-10-30 18:10:01] iter = 05680, loss = 2.8201
2024-10-30 18:10:04: [2024-10-30 18:10:04] iter = 05690, loss = 3.1692
2024-10-30 18:10:07: [2024-10-30 18:10:07] iter = 05700, loss = 2.6623
2024-10-30 18:10:11: [2024-10-30 18:10:11] iter = 05710, loss = 1.8650
2024-10-30 18:10:14: [2024-10-30 18:10:14] iter = 05720, loss = 2.1619
2024-10-30 18:10:18: [2024-10-30 18:10:18] iter = 05730, loss = 2.5760
2024-10-30 18:10:22: [2024-10-30 18:10:22] iter = 05740, loss = 2.1712
2024-10-30 18:10:26: [2024-10-30 18:10:26] iter = 05750, loss = 1.8516
2024-10-30 18:10:29: [2024-10-30 18:10:29] iter = 05760, loss = 2.2555
2024-10-30 18:10:34: [2024-10-30 18:10:34] iter = 05770, loss = 1.9710
2024-10-30 18:10:37: [2024-10-30 18:10:37] iter = 05780, loss = 1.9336
2024-10-30 18:10:41: [2024-10-30 18:10:41] iter = 05790, loss = 2.0586
2024-10-30 18:10:45: [2024-10-30 18:10:45] iter = 05800, loss = 1.9680
2024-10-30 18:10:50: [2024-10-30 18:10:50] iter = 05810, loss = 2.1539
2024-10-30 18:10:54: [2024-10-30 18:10:54] iter = 05820, loss = 2.6750
2024-10-30 18:10:58: [2024-10-30 18:10:58] iter = 05830, loss = 1.9962
2024-10-30 18:11:02: [2024-10-30 18:11:02] iter = 05840, loss = 4.6986
2024-10-30 18:11:06: [2024-10-30 18:11:06] iter = 05850, loss = 2.3507
2024-10-30 18:11:11: [2024-10-30 18:11:11] iter = 05860, loss = 2.9549
2024-10-30 18:11:15: [2024-10-30 18:11:15] iter = 05870, loss = 2.4868
2024-10-30 18:11:19: [2024-10-30 18:11:19] iter = 05880, loss = 2.5033
2024-10-30 18:11:23: [2024-10-30 18:11:23] iter = 05890, loss = 2.6905
2024-10-30 18:11:27: [2024-10-30 18:11:27] iter = 05900, loss = 2.6181
2024-10-30 18:11:31: [2024-10-30 18:11:31] iter = 05910, loss = 1.8057
2024-10-30 18:11:35: [2024-10-30 18:11:35] iter = 05920, loss = 2.2516
2024-10-30 18:11:39: [2024-10-30 18:11:39] iter = 05930, loss = 2.6509
2024-10-30 18:11:43: [2024-10-30 18:11:43] iter = 05940, loss = 2.2080
2024-10-30 18:11:46: [2024-10-30 18:11:46] iter = 05950, loss = 2.0896
2024-10-30 18:11:50: [2024-10-30 18:11:50] iter = 05960, loss = 1.9729
2024-10-30 18:11:54: [2024-10-30 18:11:54] iter = 05970, loss = 3.0088
2024-10-30 18:11:58: [2024-10-30 18:11:58] iter = 05980, loss = 1.9279
2024-10-30 18:12:01: [2024-10-30 18:12:01] iter = 05990, loss = 2.2079
2024-10-30 18:12:05: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
2024-10-30 18:12:05: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 18:12:05: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 25161}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 18:14:42: Evaluate 5 random ConvNet, ACCmean = 0.7899 ACCstd = 0.0039
-------------------------
2024-10-30 18:14:42: Evaluate 5 random ConvNet, SENmean = 0.7867 SENstd = 0.0031
-------------------------
2024-10-30 18:14:42: Evaluate 5 random ConvNet, SPEmean = 0.9790 SPEstd = 0.0004
-------------------------
2024-10-30 18:14:42: Evaluate 5 random ConvNet, F!mean = 0.7744 F!std = 0.0036
-------------------------
2024-10-30 18:14:42: Evaluate 5 random ConvNet, mean = 0.7899 std = 0.0039
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 18:14:43: [2024-10-30 18:14:43] iter = 06000, loss = 1.9543
2024-10-30 18:14:47: [2024-10-30 18:14:47] iter = 06010, loss = 2.6486
2024-10-30 18:14:53: [2024-10-30 18:14:53] iter = 06020, loss = 2.1390
2024-10-30 18:14:56: [2024-10-30 18:14:56] iter = 06030, loss = 2.3401
2024-10-30 18:14:59: [2024-10-30 18:14:59] iter = 06040, loss = 2.8031
2024-10-30 18:15:03: [2024-10-30 18:15:03] iter = 06050, loss = 2.4108
2024-10-30 18:15:06: [2024-10-30 18:15:06] iter = 06060, loss = 2.6526
2024-10-30 18:15:11: [2024-10-30 18:15:11] iter = 06070, loss = 2.1006
2024-10-30 18:15:15: [2024-10-30 18:15:15] iter = 06080, loss = 4.4738
2024-10-30 18:15:19: [2024-10-30 18:15:19] iter = 06090, loss = 9.5925
2024-10-30 18:15:23: [2024-10-30 18:15:23] iter = 06100, loss = 2.0493
2024-10-30 18:15:27: [2024-10-30 18:15:27] iter = 06110, loss = 3.4305
2024-10-30 18:15:32: [2024-10-30 18:15:32] iter = 06120, loss = 1.9784
2024-10-30 18:15:37: [2024-10-30 18:15:37] iter = 06130, loss = 1.9350
2024-10-30 18:15:40: [2024-10-30 18:15:40] iter = 06140, loss = 2.5548
2024-10-30 18:15:44: [2024-10-30 18:15:44] iter = 06150, loss = 3.2422
2024-10-30 18:15:48: [2024-10-30 18:15:48] iter = 06160, loss = 2.4231
2024-10-30 18:15:52: [2024-10-30 18:15:52] iter = 06170, loss = 2.1504
2024-10-30 18:15:57: [2024-10-30 18:15:57] iter = 06180, loss = 2.1484
2024-10-30 18:16:01: [2024-10-30 18:16:01] iter = 06190, loss = 1.9549
2024-10-30 18:16:05: [2024-10-30 18:16:05] iter = 06200, loss = 1.8697
2024-10-30 18:16:10: [2024-10-30 18:16:10] iter = 06210, loss = 1.7765
2024-10-30 18:16:14: [2024-10-30 18:16:14] iter = 06220, loss = 2.2430
2024-10-30 18:16:18: [2024-10-30 18:16:18] iter = 06230, loss = 2.4677
2024-10-30 18:16:23: [2024-10-30 18:16:23] iter = 06240, loss = 2.3112
2024-10-30 18:16:29: [2024-10-30 18:16:29] iter = 06250, loss = 2.0091
2024-10-30 18:16:33: [2024-10-30 18:16:33] iter = 06260, loss = 1.9470
2024-10-30 18:16:37: [2024-10-30 18:16:37] iter = 06270, loss = 2.7909
2024-10-30 18:16:41: [2024-10-30 18:16:41] iter = 06280, loss = 3.2729
2024-10-30 18:16:45: [2024-10-30 18:16:45] iter = 06290, loss = 2.5137
2024-10-30 18:16:48: [2024-10-30 18:16:48] iter = 06300, loss = 2.0247
2024-10-30 18:16:53: [2024-10-30 18:16:53] iter = 06310, loss = 2.1718
2024-10-30 18:16:57: [2024-10-30 18:16:57] iter = 06320, loss = 2.2151
2024-10-30 18:17:02: [2024-10-30 18:17:02] iter = 06330, loss = 2.4066
2024-10-30 18:17:07: [2024-10-30 18:17:07] iter = 06340, loss = 2.1068
2024-10-30 18:17:12: [2024-10-30 18:17:12] iter = 06350, loss = 1.6933
2024-10-30 18:17:16: [2024-10-30 18:17:16] iter = 06360, loss = 3.4301
2024-10-30 18:17:20: [2024-10-30 18:17:20] iter = 06370, loss = 2.4823
2024-10-30 18:17:24: [2024-10-30 18:17:24] iter = 06380, loss = 3.2761
2024-10-30 18:17:29: [2024-10-30 18:17:29] iter = 06390, loss = 2.1590
2024-10-30 18:17:33: [2024-10-30 18:17:33] iter = 06400, loss = 5.8861
2024-10-30 18:17:37: [2024-10-30 18:17:37] iter = 06410, loss = 2.2654
2024-10-30 18:17:42: [2024-10-30 18:17:42] iter = 06420, loss = 2.3305
2024-10-30 18:17:46: [2024-10-30 18:17:46] iter = 06430, loss = 2.0666
2024-10-30 18:17:50: [2024-10-30 18:17:50] iter = 06440, loss = 1.7960
2024-10-30 18:17:55: [2024-10-30 18:17:55] iter = 06450, loss = 2.5951
2024-10-30 18:17:59: [2024-10-30 18:17:59] iter = 06460, loss = 2.3769
2024-10-30 18:18:03: [2024-10-30 18:18:03] iter = 06470, loss = 2.2954
2024-10-30 18:18:07: [2024-10-30 18:18:07] iter = 06480, loss = 2.5451
2024-10-30 18:18:11: [2024-10-30 18:18:11] iter = 06490, loss = 2.8928
2024-10-30 18:18:16: [2024-10-30 18:18:16] iter = 06500, loss = 2.3321
2024-10-30 18:18:20: [2024-10-30 18:18:20] iter = 06510, loss = 1.8992
2024-10-30 18:18:24: [2024-10-30 18:18:24] iter = 06520, loss = 1.9788
2024-10-30 18:18:27: [2024-10-30 18:18:27] iter = 06530, loss = 3.7031
2024-10-30 18:18:31: [2024-10-30 18:18:31] iter = 06540, loss = 1.9192
2024-10-30 18:18:35: [2024-10-30 18:18:35] iter = 06550, loss = 2.0749
2024-10-30 18:18:39: [2024-10-30 18:18:39] iter = 06560, loss = 1.9511
2024-10-30 18:18:43: [2024-10-30 18:18:43] iter = 06570, loss = 2.6625
2024-10-30 18:18:47: [2024-10-30 18:18:47] iter = 06580, loss = 1.8740
2024-10-30 18:18:51: [2024-10-30 18:18:51] iter = 06590, loss = 1.8200
2024-10-30 18:18:56: [2024-10-30 18:18:56] iter = 06600, loss = 2.0085
2024-10-30 18:19:00: [2024-10-30 18:19:00] iter = 06610, loss = 2.1604
2024-10-30 18:19:03: [2024-10-30 18:19:03] iter = 06620, loss = 2.0599
2024-10-30 18:19:07: [2024-10-30 18:19:07] iter = 06630, loss = 2.2550
2024-10-30 18:19:10: [2024-10-30 18:19:10] iter = 06640, loss = 1.9527
2024-10-30 18:19:13: [2024-10-30 18:19:13] iter = 06650, loss = 1.7747
2024-10-30 18:19:18: [2024-10-30 18:19:18] iter = 06660, loss = 2.1802
2024-10-30 18:19:22: [2024-10-30 18:19:22] iter = 06670, loss = 2.5537
2024-10-30 18:19:26: [2024-10-30 18:19:26] iter = 06680, loss = 2.1238
2024-10-30 18:19:30: [2024-10-30 18:19:30] iter = 06690, loss = 3.9672
2024-10-30 18:19:33: [2024-10-30 18:19:33] iter = 06700, loss = 2.2211
2024-10-30 18:19:36: [2024-10-30 18:19:36] iter = 06710, loss = 1.8283
2024-10-30 18:19:39: [2024-10-30 18:19:39] iter = 06720, loss = 1.9812
2024-10-30 18:19:43: [2024-10-30 18:19:43] iter = 06730, loss = 2.8703
2024-10-30 18:19:47: [2024-10-30 18:19:47] iter = 06740, loss = 2.3279
2024-10-30 18:19:51: [2024-10-30 18:19:51] iter = 06750, loss = 3.2662
2024-10-30 18:19:54: [2024-10-30 18:19:54] iter = 06760, loss = 2.1987
2024-10-30 18:19:58: [2024-10-30 18:19:58] iter = 06770, loss = 2.4938
2024-10-30 18:20:01: [2024-10-30 18:20:01] iter = 06780, loss = 1.8893
2024-10-30 18:20:05: [2024-10-30 18:20:05] iter = 06790, loss = 4.5475
2024-10-30 18:20:09: [2024-10-30 18:20:09] iter = 06800, loss = 2.4080
2024-10-30 18:20:13: [2024-10-30 18:20:13] iter = 06810, loss = 2.9277
2024-10-30 18:20:16: [2024-10-30 18:20:16] iter = 06820, loss = 2.0485
2024-10-30 18:20:20: [2024-10-30 18:20:20] iter = 06830, loss = 1.7063
2024-10-30 18:20:24: [2024-10-30 18:20:24] iter = 06840, loss = 4.2274
2024-10-30 18:20:27: [2024-10-30 18:20:27] iter = 06850, loss = 2.1064
2024-10-30 18:20:31: [2024-10-30 18:20:31] iter = 06860, loss = 1.8984
2024-10-30 18:20:34: [2024-10-30 18:20:34] iter = 06870, loss = 3.2725
2024-10-30 18:20:37: [2024-10-30 18:20:37] iter = 06880, loss = 1.8625
2024-10-30 18:20:40: [2024-10-30 18:20:40] iter = 06890, loss = 2.0755
2024-10-30 18:20:44: [2024-10-30 18:20:44] iter = 06900, loss = 2.4147
2024-10-30 18:20:48: [2024-10-30 18:20:48] iter = 06910, loss = 1.8948
2024-10-30 18:20:52: [2024-10-30 18:20:52] iter = 06920, loss = 2.0625
2024-10-30 18:20:55: [2024-10-30 18:20:55] iter = 06930, loss = 2.5528
2024-10-30 18:20:58: [2024-10-30 18:20:58] iter = 06940, loss = 2.3875
2024-10-30 18:21:02: [2024-10-30 18:21:02] iter = 06950, loss = 2.5120
2024-10-30 18:21:06: [2024-10-30 18:21:06] iter = 06960, loss = 2.4554
2024-10-30 18:21:10: [2024-10-30 18:21:10] iter = 06970, loss = 2.3991
2024-10-30 18:21:14: [2024-10-30 18:21:14] iter = 06980, loss = 2.0951
2024-10-30 18:21:17: [2024-10-30 18:21:17] iter = 06990, loss = 2.3740
2024-10-30 18:21:22: [2024-10-30 18:21:22] iter = 07000, loss = 1.6858
2024-10-30 18:21:26: [2024-10-30 18:21:26] iter = 07010, loss = 2.2071
2024-10-30 18:21:29: [2024-10-30 18:21:29] iter = 07020, loss = 2.0779
2024-10-30 18:21:33: [2024-10-30 18:21:33] iter = 07030, loss = 2.0520
2024-10-30 18:21:37: [2024-10-30 18:21:37] iter = 07040, loss = 2.8782
2024-10-30 18:21:41: [2024-10-30 18:21:41] iter = 07050, loss = 3.1136
2024-10-30 18:21:44: [2024-10-30 18:21:44] iter = 07060, loss = 2.8680
2024-10-30 18:21:47: [2024-10-30 18:21:47] iter = 07070, loss = 2.7791
2024-10-30 18:21:51: [2024-10-30 18:21:51] iter = 07080, loss = 2.3453
2024-10-30 18:21:55: [2024-10-30 18:21:55] iter = 07090, loss = 2.4167
2024-10-30 18:21:58: [2024-10-30 18:21:58] iter = 07100, loss = 2.1535
2024-10-30 18:22:02: [2024-10-30 18:22:02] iter = 07110, loss = 2.2252
2024-10-30 18:22:06: [2024-10-30 18:22:06] iter = 07120, loss = 1.8659
2024-10-30 18:22:10: [2024-10-30 18:22:10] iter = 07130, loss = 2.0370
2024-10-30 18:22:15: [2024-10-30 18:22:15] iter = 07140, loss = 2.4780
2024-10-30 18:22:19: [2024-10-30 18:22:19] iter = 07150, loss = 1.6281
2024-10-30 18:22:21: [2024-10-30 18:22:21] iter = 07160, loss = 1.9505
2024-10-30 18:22:25: [2024-10-30 18:22:25] iter = 07170, loss = 2.3398
2024-10-30 18:22:28: [2024-10-30 18:22:28] iter = 07180, loss = 3.2269
2024-10-30 18:22:30: [2024-10-30 18:22:30] iter = 07190, loss = 1.7961
2024-10-30 18:22:33: [2024-10-30 18:22:33] iter = 07200, loss = 1.8509
2024-10-30 18:22:36: [2024-10-30 18:22:36] iter = 07210, loss = 2.8787
2024-10-30 18:22:39: [2024-10-30 18:22:39] iter = 07220, loss = 1.8853
2024-10-30 18:22:43: [2024-10-30 18:22:43] iter = 07230, loss = 2.2176
2024-10-30 18:22:47: [2024-10-30 18:22:47] iter = 07240, loss = 2.1341
2024-10-30 18:22:50: [2024-10-30 18:22:50] iter = 07250, loss = 1.9378
2024-10-30 18:22:55: [2024-10-30 18:22:55] iter = 07260, loss = 2.4120
2024-10-30 18:22:58: [2024-10-30 18:22:58] iter = 07270, loss = 2.2481
2024-10-30 18:23:02: [2024-10-30 18:23:02] iter = 07280, loss = 2.7690
2024-10-30 18:23:06: [2024-10-30 18:23:06] iter = 07290, loss = 2.9813
2024-10-30 18:23:09: [2024-10-30 18:23:09] iter = 07300, loss = 3.4820
2024-10-30 18:23:13: [2024-10-30 18:23:13] iter = 07310, loss = 1.8746
2024-10-30 18:23:17: [2024-10-30 18:23:17] iter = 07320, loss = 2.0411
2024-10-30 18:23:21: [2024-10-30 18:23:21] iter = 07330, loss = 2.5714
2024-10-30 18:23:25: [2024-10-30 18:23:25] iter = 07340, loss = 2.2037
2024-10-30 18:23:29: [2024-10-30 18:23:29] iter = 07350, loss = 2.0297
2024-10-30 18:23:32: [2024-10-30 18:23:32] iter = 07360, loss = 1.7110
2024-10-30 18:23:36: [2024-10-30 18:23:36] iter = 07370, loss = 2.1367
2024-10-30 18:23:40: [2024-10-30 18:23:40] iter = 07380, loss = 1.8208
2024-10-30 18:23:44: [2024-10-30 18:23:44] iter = 07390, loss = 1.9234
2024-10-30 18:23:48: [2024-10-30 18:23:48] iter = 07400, loss = 4.2218
2024-10-30 18:23:52: [2024-10-30 18:23:52] iter = 07410, loss = 2.2707
2024-10-30 18:23:56: [2024-10-30 18:23:56] iter = 07420, loss = 2.2978
2024-10-30 18:23:59: [2024-10-30 18:23:59] iter = 07430, loss = 2.8680
2024-10-30 18:24:02: [2024-10-30 18:24:02] iter = 07440, loss = 2.3287
2024-10-30 18:24:05: [2024-10-30 18:24:05] iter = 07450, loss = 2.1043
2024-10-30 18:24:09: [2024-10-30 18:24:09] iter = 07460, loss = 1.9741
2024-10-30 18:24:12: [2024-10-30 18:24:12] iter = 07470, loss = 3.7571
2024-10-30 18:24:17: [2024-10-30 18:24:17] iter = 07480, loss = 2.3822
2024-10-30 18:24:20: [2024-10-30 18:24:20] iter = 07490, loss = 2.6876
2024-10-30 18:24:24: [2024-10-30 18:24:24] iter = 07500, loss = 2.1887
2024-10-30 18:24:28: [2024-10-30 18:24:28] iter = 07510, loss = 1.9564
2024-10-30 18:24:30: [2024-10-30 18:24:30] iter = 07520, loss = 1.7371
2024-10-30 18:24:34: [2024-10-30 18:24:34] iter = 07530, loss = 2.0170
2024-10-30 18:24:38: [2024-10-30 18:24:38] iter = 07540, loss = 1.7773
2024-10-30 18:24:42: [2024-10-30 18:24:42] iter = 07550, loss = 4.9673
2024-10-30 18:24:46: [2024-10-30 18:24:46] iter = 07560, loss = 2.5885
2024-10-30 18:24:48: [2024-10-30 18:24:48] iter = 07570, loss = 2.0373
2024-10-30 18:24:52: [2024-10-30 18:24:52] iter = 07580, loss = 4.4831
2024-10-30 18:24:55: [2024-10-30 18:24:55] iter = 07590, loss = 3.5112
2024-10-30 18:25:00: [2024-10-30 18:25:00] iter = 07600, loss = 2.1633
2024-10-30 18:25:03: [2024-10-30 18:25:03] iter = 07610, loss = 2.7762
2024-10-30 18:25:08: [2024-10-30 18:25:08] iter = 07620, loss = 6.3531
2024-10-30 18:25:11: [2024-10-30 18:25:11] iter = 07630, loss = 2.0619
2024-10-30 18:25:15: [2024-10-30 18:25:15] iter = 07640, loss = 2.5014
2024-10-30 18:25:19: [2024-10-30 18:25:19] iter = 07650, loss = 2.1710
2024-10-30 18:25:23: [2024-10-30 18:25:23] iter = 07660, loss = 3.0616
2024-10-30 18:25:27: [2024-10-30 18:25:27] iter = 07670, loss = 2.1638
2024-10-30 18:25:32: [2024-10-30 18:25:32] iter = 07680, loss = 2.0706
2024-10-30 18:25:36: [2024-10-30 18:25:36] iter = 07690, loss = 2.0934
2024-10-30 18:25:40: [2024-10-30 18:25:40] iter = 07700, loss = 1.9072
2024-10-30 18:25:44: [2024-10-30 18:25:44] iter = 07710, loss = 1.9891
2024-10-30 18:25:48: [2024-10-30 18:25:48] iter = 07720, loss = 2.4471
2024-10-30 18:25:53: [2024-10-30 18:25:53] iter = 07730, loss = 2.1654
2024-10-30 18:25:57: [2024-10-30 18:25:57] iter = 07740, loss = 3.3829
2024-10-30 18:26:01: [2024-10-30 18:26:01] iter = 07750, loss = 1.6821
2024-10-30 18:26:05: [2024-10-30 18:26:05] iter = 07760, loss = 2.0503
2024-10-30 18:26:08: [2024-10-30 18:26:08] iter = 07770, loss = 2.2497
2024-10-30 18:26:12: [2024-10-30 18:26:12] iter = 07780, loss = 2.1768
2024-10-30 18:26:17: [2024-10-30 18:26:17] iter = 07790, loss = 2.6170
2024-10-30 18:26:20: [2024-10-30 18:26:20] iter = 07800, loss = 2.6906
2024-10-30 18:26:24: [2024-10-30 18:26:24] iter = 07810, loss = 2.4463
2024-10-30 18:26:29: [2024-10-30 18:26:29] iter = 07820, loss = 2.5362
2024-10-30 18:26:33: [2024-10-30 18:26:33] iter = 07830, loss = 2.1059
2024-10-30 18:26:37: [2024-10-30 18:26:37] iter = 07840, loss = 2.0798
2024-10-30 18:26:42: [2024-10-30 18:26:42] iter = 07850, loss = 2.5813
2024-10-30 18:26:47: [2024-10-30 18:26:47] iter = 07860, loss = 3.2593
2024-10-30 18:26:51: [2024-10-30 18:26:51] iter = 07870, loss = 2.5883
2024-10-30 18:26:54: [2024-10-30 18:26:54] iter = 07880, loss = 2.4191
2024-10-30 18:26:58: [2024-10-30 18:26:58] iter = 07890, loss = 3.0650
2024-10-30 18:27:02: [2024-10-30 18:27:02] iter = 07900, loss = 2.1738
2024-10-30 18:27:06: [2024-10-30 18:27:06] iter = 07910, loss = 2.2303
2024-10-30 18:27:10: [2024-10-30 18:27:10] iter = 07920, loss = 1.7105
2024-10-30 18:27:13: [2024-10-30 18:27:13] iter = 07930, loss = 2.5282
2024-10-30 18:27:16: [2024-10-30 18:27:16] iter = 07940, loss = 2.4928
2024-10-30 18:27:20: [2024-10-30 18:27:20] iter = 07950, loss = 1.8689
2024-10-30 18:27:24: [2024-10-30 18:27:24] iter = 07960, loss = 2.4675
2024-10-30 18:27:27: [2024-10-30 18:27:27] iter = 07970, loss = 1.9069
2024-10-30 18:27:31: [2024-10-30 18:27:31] iter = 07980, loss = 2.3784
2024-10-30 18:27:34: [2024-10-30 18:27:34] iter = 07990, loss = 2.1609
2024-10-30 18:27:37: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
2024-10-30 18:27:37: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 18:27:37: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 57377}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 18:30:01: Evaluate 5 random ConvNet, ACCmean = 0.7771 ACCstd = 0.0018
-------------------------
2024-10-30 18:30:01: Evaluate 5 random ConvNet, SENmean = 0.7760 SENstd = 0.0027
-------------------------
2024-10-30 18:30:01: Evaluate 5 random ConvNet, SPEmean = 0.9776 SPEstd = 0.0002
-------------------------
2024-10-30 18:30:01: Evaluate 5 random ConvNet, F!mean = 0.7635 F!std = 0.0012
-------------------------
2024-10-30 18:30:01: Evaluate 5 random ConvNet, mean = 0.7771 std = 0.0018
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 18:30:02: [2024-10-30 18:30:02] iter = 08000, loss = 2.1663
2024-10-30 18:30:06: [2024-10-30 18:30:06] iter = 08010, loss = 2.2856
2024-10-30 18:30:09: [2024-10-30 18:30:09] iter = 08020, loss = 1.8904
2024-10-30 18:30:13: [2024-10-30 18:30:13] iter = 08030, loss = 2.3568
2024-10-30 18:30:17: [2024-10-30 18:30:17] iter = 08040, loss = 2.1268
2024-10-30 18:30:21: [2024-10-30 18:30:21] iter = 08050, loss = 1.9368
2024-10-30 18:30:24: [2024-10-30 18:30:24] iter = 08060, loss = 2.7737
2024-10-30 18:30:28: [2024-10-30 18:30:28] iter = 08070, loss = 1.8202
2024-10-30 18:30:32: [2024-10-30 18:30:32] iter = 08080, loss = 3.1348
2024-10-30 18:30:37: [2024-10-30 18:30:37] iter = 08090, loss = 2.0485
2024-10-30 18:30:40: [2024-10-30 18:30:40] iter = 08100, loss = 2.3802
2024-10-30 18:30:44: [2024-10-30 18:30:44] iter = 08110, loss = 2.1909
2024-10-30 18:30:49: [2024-10-30 18:30:49] iter = 08120, loss = 2.2873
2024-10-30 18:30:52: [2024-10-30 18:30:52] iter = 08130, loss = 2.5552
2024-10-30 18:30:56: [2024-10-30 18:30:56] iter = 08140, loss = 2.4536
2024-10-30 18:31:00: [2024-10-30 18:31:00] iter = 08150, loss = 2.2306
2024-10-30 18:31:04: [2024-10-30 18:31:04] iter = 08160, loss = 2.0084
2024-10-30 18:31:08: [2024-10-30 18:31:08] iter = 08170, loss = 2.5992
2024-10-30 18:31:12: [2024-10-30 18:31:12] iter = 08180, loss = 2.2503
2024-10-30 18:31:17: [2024-10-30 18:31:17] iter = 08190, loss = 1.9927
2024-10-30 18:31:21: [2024-10-30 18:31:21] iter = 08200, loss = 2.6946
2024-10-30 18:31:25: [2024-10-30 18:31:25] iter = 08210, loss = 2.8381
2024-10-30 18:31:28: [2024-10-30 18:31:28] iter = 08220, loss = 2.7612
2024-10-30 18:31:31: [2024-10-30 18:31:31] iter = 08230, loss = 3.2400
2024-10-30 18:31:36: [2024-10-30 18:31:36] iter = 08240, loss = 2.9303
2024-10-30 18:31:41: [2024-10-30 18:31:41] iter = 08250, loss = 2.1293
2024-10-30 18:31:45: [2024-10-30 18:31:45] iter = 08260, loss = 2.5342
2024-10-30 18:31:49: [2024-10-30 18:31:49] iter = 08270, loss = 2.9161
2024-10-30 18:31:52: [2024-10-30 18:31:52] iter = 08280, loss = 3.0050
2024-10-30 18:31:55: [2024-10-30 18:31:55] iter = 08290, loss = 2.2711
2024-10-30 18:31:59: [2024-10-30 18:31:59] iter = 08300, loss = 3.0581
2024-10-30 18:32:02: [2024-10-30 18:32:02] iter = 08310, loss = 2.2451
2024-10-30 18:32:05: [2024-10-30 18:32:05] iter = 08320, loss = 1.9440
2024-10-30 18:32:09: [2024-10-30 18:32:09] iter = 08330, loss = 2.5487
2024-10-30 18:32:13: [2024-10-30 18:32:13] iter = 08340, loss = 2.5801
2024-10-30 18:32:17: [2024-10-30 18:32:17] iter = 08350, loss = 2.0822
2024-10-30 18:32:20: [2024-10-30 18:32:20] iter = 08360, loss = 4.6834
2024-10-30 18:32:23: [2024-10-30 18:32:23] iter = 08370, loss = 1.9251
2024-10-30 18:32:27: [2024-10-30 18:32:27] iter = 08380, loss = 2.0991
2024-10-30 18:32:30: [2024-10-30 18:32:30] iter = 08390, loss = 2.1677
2024-10-30 18:32:34: [2024-10-30 18:32:34] iter = 08400, loss = 2.1520
2024-10-30 18:32:37: [2024-10-30 18:32:37] iter = 08410, loss = 2.4096
2024-10-30 18:32:41: [2024-10-30 18:32:41] iter = 08420, loss = 5.2191
2024-10-30 18:32:45: [2024-10-30 18:32:45] iter = 08430, loss = 4.8849
2024-10-30 18:32:49: [2024-10-30 18:32:49] iter = 08440, loss = 2.0233
2024-10-30 18:32:52: [2024-10-30 18:32:52] iter = 08450, loss = 2.3975
2024-10-30 18:32:56: [2024-10-30 18:32:56] iter = 08460, loss = 1.9008
2024-10-30 18:32:59: [2024-10-30 18:32:59] iter = 08470, loss = 2.5693
2024-10-30 18:33:03: [2024-10-30 18:33:03] iter = 08480, loss = 2.4086
2024-10-30 18:33:06: [2024-10-30 18:33:06] iter = 08490, loss = 1.9768
2024-10-30 18:33:10: [2024-10-30 18:33:10] iter = 08500, loss = 1.9578
2024-10-30 18:33:13: [2024-10-30 18:33:13] iter = 08510, loss = 1.9066
2024-10-30 18:33:17: [2024-10-30 18:33:17] iter = 08520, loss = 2.3073
2024-10-30 18:33:21: [2024-10-30 18:33:21] iter = 08530, loss = 3.0480
2024-10-30 18:33:24: [2024-10-30 18:33:24] iter = 08540, loss = 2.0086
2024-10-30 18:33:28: [2024-10-30 18:33:28] iter = 08550, loss = 2.1849
2024-10-30 18:33:33: [2024-10-30 18:33:33] iter = 08560, loss = 2.0977
2024-10-30 18:33:37: [2024-10-30 18:33:37] iter = 08570, loss = 2.3982
2024-10-30 18:33:41: [2024-10-30 18:33:41] iter = 08580, loss = 4.0556
2024-10-30 18:33:44: [2024-10-30 18:33:44] iter = 08590, loss = 2.0620
2024-10-30 18:33:48: [2024-10-30 18:33:48] iter = 08600, loss = 2.5852
2024-10-30 18:33:52: [2024-10-30 18:33:52] iter = 08610, loss = 2.4163
2024-10-30 18:33:55: [2024-10-30 18:33:55] iter = 08620, loss = 2.2831
2024-10-30 18:33:58: [2024-10-30 18:33:58] iter = 08630, loss = 2.0668
2024-10-30 18:34:02: [2024-10-30 18:34:02] iter = 08640, loss = 2.5241
2024-10-30 18:34:07: [2024-10-30 18:34:07] iter = 08650, loss = 1.8979
2024-10-30 18:34:10: [2024-10-30 18:34:10] iter = 08660, loss = 2.2010
2024-10-30 18:34:14: [2024-10-30 18:34:14] iter = 08670, loss = 1.9239
2024-10-30 18:34:19: [2024-10-30 18:34:19] iter = 08680, loss = 2.1185
2024-10-30 18:34:22: [2024-10-30 18:34:22] iter = 08690, loss = 3.3160
2024-10-30 18:34:26: [2024-10-30 18:34:26] iter = 08700, loss = 1.9737
2024-10-30 18:34:29: [2024-10-30 18:34:29] iter = 08710, loss = 1.9316
2024-10-30 18:34:33: [2024-10-30 18:34:33] iter = 08720, loss = 1.8846
2024-10-30 18:34:37: [2024-10-30 18:34:37] iter = 08730, loss = 2.4054
2024-10-30 18:34:41: [2024-10-30 18:34:41] iter = 08740, loss = 2.6754
2024-10-30 18:34:45: [2024-10-30 18:34:45] iter = 08750, loss = 1.7665
2024-10-30 18:34:49: [2024-10-30 18:34:49] iter = 08760, loss = 1.9396
2024-10-30 18:34:52: [2024-10-30 18:34:52] iter = 08770, loss = 3.3056
2024-10-30 18:34:56: [2024-10-30 18:34:56] iter = 08780, loss = 2.0430
2024-10-30 18:34:59: [2024-10-30 18:34:59] iter = 08790, loss = 1.8980
2024-10-30 18:35:02: [2024-10-30 18:35:02] iter = 08800, loss = 2.0051
2024-10-30 18:35:06: [2024-10-30 18:35:06] iter = 08810, loss = 1.8246
2024-10-30 18:35:10: [2024-10-30 18:35:10] iter = 08820, loss = 2.6064
2024-10-30 18:35:13: [2024-10-30 18:35:13] iter = 08830, loss = 2.1742
2024-10-30 18:35:17: [2024-10-30 18:35:17] iter = 08840, loss = 3.0271
2024-10-30 18:35:20: [2024-10-30 18:35:20] iter = 08850, loss = 2.6271
2024-10-30 18:35:23: [2024-10-30 18:35:23] iter = 08860, loss = 2.2433
2024-10-30 18:35:26: [2024-10-30 18:35:26] iter = 08870, loss = 2.0361
2024-10-30 18:35:29: [2024-10-30 18:35:29] iter = 08880, loss = 2.1795
2024-10-30 18:35:32: [2024-10-30 18:35:32] iter = 08890, loss = 4.2325
2024-10-30 18:35:36: [2024-10-30 18:35:36] iter = 08900, loss = 1.8093
2024-10-30 18:35:40: [2024-10-30 18:35:40] iter = 08910, loss = 3.4531
2024-10-30 18:35:44: [2024-10-30 18:35:44] iter = 08920, loss = 2.7426
2024-10-30 18:35:48: [2024-10-30 18:35:48] iter = 08930, loss = 2.1332
2024-10-30 18:35:51: [2024-10-30 18:35:51] iter = 08940, loss = 2.4381
2024-10-30 18:35:55: [2024-10-30 18:35:55] iter = 08950, loss = 2.1215
2024-10-30 18:35:57: [2024-10-30 18:35:57] iter = 08960, loss = 2.0322
2024-10-30 18:36:01: [2024-10-30 18:36:01] iter = 08970, loss = 2.2937
2024-10-30 18:36:04: [2024-10-30 18:36:04] iter = 08980, loss = 2.3928
2024-10-30 18:36:06: [2024-10-30 18:36:06] iter = 08990, loss = 2.3057
2024-10-30 18:36:10: [2024-10-30 18:36:10] iter = 09000, loss = 1.8640
2024-10-30 18:36:14: [2024-10-30 18:36:14] iter = 09010, loss = 1.7881
2024-10-30 18:36:18: [2024-10-30 18:36:18] iter = 09020, loss = 2.1721
2024-10-30 18:36:22: [2024-10-30 18:36:22] iter = 09030, loss = 2.7567
2024-10-30 18:36:26: [2024-10-30 18:36:26] iter = 09040, loss = 3.0619
2024-10-30 18:36:30: [2024-10-30 18:36:30] iter = 09050, loss = 2.0749
2024-10-30 18:36:34: [2024-10-30 18:36:34] iter = 09060, loss = 2.2230
2024-10-30 18:36:37: [2024-10-30 18:36:37] iter = 09070, loss = 1.8050
2024-10-30 18:36:40: [2024-10-30 18:36:40] iter = 09080, loss = 1.7590
2024-10-30 18:36:44: [2024-10-30 18:36:44] iter = 09090, loss = 2.4117
2024-10-30 18:36:48: [2024-10-30 18:36:48] iter = 09100, loss = 2.0553
2024-10-30 18:36:52: [2024-10-30 18:36:52] iter = 09110, loss = 5.5652
2024-10-30 18:36:56: [2024-10-30 18:36:56] iter = 09120, loss = 2.1322
2024-10-30 18:36:59: [2024-10-30 18:36:59] iter = 09130, loss = 1.8508
2024-10-30 18:37:01: [2024-10-30 18:37:01] iter = 09140, loss = 3.3948
2024-10-30 18:37:05: [2024-10-30 18:37:05] iter = 09150, loss = 1.9084
2024-10-30 18:37:08: [2024-10-30 18:37:08] iter = 09160, loss = 1.9766
2024-10-30 18:37:12: [2024-10-30 18:37:12] iter = 09170, loss = 2.2221
2024-10-30 18:37:15: [2024-10-30 18:37:15] iter = 09180, loss = 2.3759
2024-10-30 18:37:19: [2024-10-30 18:37:19] iter = 09190, loss = 2.2977
2024-10-30 18:37:21: [2024-10-30 18:37:21] iter = 09200, loss = 2.1168
2024-10-30 18:37:25: [2024-10-30 18:37:25] iter = 09210, loss = 2.3065
2024-10-30 18:37:29: [2024-10-30 18:37:29] iter = 09220, loss = 1.6976
2024-10-30 18:37:33: [2024-10-30 18:37:33] iter = 09230, loss = 2.4171
2024-10-30 18:37:37: [2024-10-30 18:37:37] iter = 09240, loss = 2.6288
2024-10-30 18:37:40: [2024-10-30 18:37:40] iter = 09250, loss = 1.8782
2024-10-30 18:37:43: [2024-10-30 18:37:43] iter = 09260, loss = 2.0677
2024-10-30 18:37:47: [2024-10-30 18:37:47] iter = 09270, loss = 8.8589
2024-10-30 18:37:50: [2024-10-30 18:37:50] iter = 09280, loss = 2.0174
2024-10-30 18:37:53: [2024-10-30 18:37:53] iter = 09290, loss = 3.1667
2024-10-30 18:37:57: [2024-10-30 18:37:57] iter = 09300, loss = 2.6406
2024-10-30 18:38:02: [2024-10-30 18:38:02] iter = 09310, loss = 1.9802
2024-10-30 18:38:05: [2024-10-30 18:38:05] iter = 09320, loss = 2.0198
2024-10-30 18:38:08: [2024-10-30 18:38:08] iter = 09330, loss = 3.1819
2024-10-30 18:38:11: [2024-10-30 18:38:11] iter = 09340, loss = 2.0316
2024-10-30 18:38:15: [2024-10-30 18:38:15] iter = 09350, loss = 2.0922
2024-10-30 18:38:19: [2024-10-30 18:38:19] iter = 09360, loss = 3.1321
2024-10-30 18:38:24: [2024-10-30 18:38:24] iter = 09370, loss = 1.7462
2024-10-30 18:38:27: [2024-10-30 18:38:27] iter = 09380, loss = 2.3062
2024-10-30 18:38:31: [2024-10-30 18:38:31] iter = 09390, loss = 2.2482
2024-10-30 18:38:35: [2024-10-30 18:38:35] iter = 09400, loss = 2.5784
2024-10-30 18:38:40: [2024-10-30 18:38:40] iter = 09410, loss = 2.1962
2024-10-30 18:38:43: [2024-10-30 18:38:43] iter = 09420, loss = 2.3547
2024-10-30 18:38:47: [2024-10-30 18:38:47] iter = 09430, loss = 3.0233
2024-10-30 18:38:52: [2024-10-30 18:38:52] iter = 09440, loss = 2.0392
2024-10-30 18:38:57: [2024-10-30 18:38:57] iter = 09450, loss = 2.1052
2024-10-30 18:38:59: [2024-10-30 18:38:59] iter = 09460, loss = 2.3591
2024-10-30 18:39:03: [2024-10-30 18:39:03] iter = 09470, loss = 2.0696
2024-10-30 18:39:07: [2024-10-30 18:39:07] iter = 09480, loss = 1.9817
2024-10-30 18:39:11: [2024-10-30 18:39:11] iter = 09490, loss = 2.1579
2024-10-30 18:39:14: [2024-10-30 18:39:14] iter = 09500, loss = 1.9004
2024-10-30 18:39:17: [2024-10-30 18:39:17] iter = 09510, loss = 7.7404
2024-10-30 18:39:20: [2024-10-30 18:39:20] iter = 09520, loss = 1.7128
2024-10-30 18:39:22: [2024-10-30 18:39:22] iter = 09530, loss = 2.0168
2024-10-30 18:39:26: [2024-10-30 18:39:26] iter = 09540, loss = 2.0579
2024-10-30 18:39:30: [2024-10-30 18:39:30] iter = 09550, loss = 1.9831
2024-10-30 18:39:34: [2024-10-30 18:39:34] iter = 09560, loss = 2.5962
2024-10-30 18:39:37: [2024-10-30 18:39:37] iter = 09570, loss = 2.0038
2024-10-30 18:39:41: [2024-10-30 18:39:41] iter = 09580, loss = 2.2301
2024-10-30 18:39:45: [2024-10-30 18:39:45] iter = 09590, loss = 2.0715
2024-10-30 18:39:49: [2024-10-30 18:39:49] iter = 09600, loss = 2.2375
2024-10-30 18:39:52: [2024-10-30 18:39:52] iter = 09610, loss = 1.8692
2024-10-30 18:39:56: [2024-10-30 18:39:56] iter = 09620, loss = 2.1662
2024-10-30 18:40:00: [2024-10-30 18:40:00] iter = 09630, loss = 2.4577
2024-10-30 18:40:04: [2024-10-30 18:40:04] iter = 09640, loss = 1.8982
2024-10-30 18:40:08: [2024-10-30 18:40:08] iter = 09650, loss = 2.8544
2024-10-30 18:40:13: [2024-10-30 18:40:13] iter = 09660, loss = 2.3665
2024-10-30 18:40:17: [2024-10-30 18:40:17] iter = 09670, loss = 2.9039
2024-10-30 18:40:21: [2024-10-30 18:40:21] iter = 09680, loss = 2.4658
2024-10-30 18:40:24: [2024-10-30 18:40:24] iter = 09690, loss = 1.6966
2024-10-30 18:40:28: [2024-10-30 18:40:28] iter = 09700, loss = 2.2279
2024-10-30 18:40:32: [2024-10-30 18:40:32] iter = 09710, loss = 2.0584
2024-10-30 18:40:35: [2024-10-30 18:40:35] iter = 09720, loss = 2.7371
2024-10-30 18:40:39: [2024-10-30 18:40:39] iter = 09730, loss = 1.9366
2024-10-30 18:40:42: [2024-10-30 18:40:42] iter = 09740, loss = 2.3385
2024-10-30 18:40:45: [2024-10-30 18:40:45] iter = 09750, loss = 2.0903
2024-10-30 18:40:49: [2024-10-30 18:40:49] iter = 09760, loss = 2.1287
2024-10-30 18:40:53: [2024-10-30 18:40:53] iter = 09770, loss = 2.0214
2024-10-30 18:40:57: [2024-10-30 18:40:57] iter = 09780, loss = 2.5160
2024-10-30 18:41:00: [2024-10-30 18:41:00] iter = 09790, loss = 2.0294
2024-10-30 18:41:03: [2024-10-30 18:41:03] iter = 09800, loss = 2.1087
2024-10-30 18:41:08: [2024-10-30 18:41:08] iter = 09810, loss = 1.8735
2024-10-30 18:41:11: [2024-10-30 18:41:11] iter = 09820, loss = 2.0541
2024-10-30 18:41:15: [2024-10-30 18:41:15] iter = 09830, loss = 1.8605
2024-10-30 18:41:19: [2024-10-30 18:41:19] iter = 09840, loss = 1.7092
2024-10-30 18:41:23: [2024-10-30 18:41:23] iter = 09850, loss = 2.4641
2024-10-30 18:41:26: [2024-10-30 18:41:26] iter = 09860, loss = 1.8353
2024-10-30 18:41:29: [2024-10-30 18:41:29] iter = 09870, loss = 2.2197
2024-10-30 18:41:32: [2024-10-30 18:41:32] iter = 09880, loss = 2.4825
2024-10-30 18:41:37: [2024-10-30 18:41:37] iter = 09890, loss = 2.0321
2024-10-30 18:41:40: [2024-10-30 18:41:40] iter = 09900, loss = 2.3484
2024-10-30 18:41:44: [2024-10-30 18:41:44] iter = 09910, loss = 3.3294
2024-10-30 18:41:49: [2024-10-30 18:41:49] iter = 09920, loss = 2.9577
2024-10-30 18:41:53: [2024-10-30 18:41:53] iter = 09930, loss = 2.7241
2024-10-30 18:41:56: [2024-10-30 18:41:56] iter = 09940, loss = 4.7397
2024-10-30 18:41:58: [2024-10-30 18:41:58] iter = 09950, loss = 2.2250
2024-10-30 18:42:02: [2024-10-30 18:42:02] iter = 09960, loss = 3.1227
2024-10-30 18:42:05: [2024-10-30 18:42:05] iter = 09970, loss = 2.3719
2024-10-30 18:42:09: [2024-10-30 18:42:09] iter = 09980, loss = 2.1081
2024-10-30 18:42:13: [2024-10-30 18:42:13] iter = 09990, loss = 2.1929
2024-10-30 18:42:16: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
2024-10-30 18:42:16: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 18:42:16: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 36530}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 18:44:45: Evaluate 5 random ConvNet, ACCmean = 0.7633 ACCstd = 0.0053
-------------------------
2024-10-30 18:44:45: Evaluate 5 random ConvNet, SENmean = 0.7652 SENstd = 0.0053
-------------------------
2024-10-30 18:44:45: Evaluate 5 random ConvNet, SPEmean = 0.9763 SPEstd = 0.0005
-------------------------
2024-10-30 18:44:45: Evaluate 5 random ConvNet, F!mean = 0.7495 F!std = 0.0048
-------------------------
2024-10-30 18:44:45: Evaluate 5 random ConvNet, mean = 0.7633 std = 0.0053
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 18:44:45: [2024-10-30 18:44:45] iter = 10000, loss = 2.2047
2024-10-30 18:44:50: [2024-10-30 18:44:50] iter = 10010, loss = 2.2223
2024-10-30 18:44:53: [2024-10-30 18:44:53] iter = 10020, loss = 2.9373
2024-10-30 18:44:56: [2024-10-30 18:44:56] iter = 10030, loss = 2.2060
2024-10-30 18:44:59: [2024-10-30 18:44:59] iter = 10040, loss = 3.0927
2024-10-30 18:45:03: [2024-10-30 18:45:03] iter = 10050, loss = 2.1719
2024-10-30 18:45:07: [2024-10-30 18:45:07] iter = 10060, loss = 2.4362
2024-10-30 18:45:11: [2024-10-30 18:45:11] iter = 10070, loss = 2.5029
2024-10-30 18:45:15: [2024-10-30 18:45:15] iter = 10080, loss = 2.1482
2024-10-30 18:45:19: [2024-10-30 18:45:19] iter = 10090, loss = 2.0148
2024-10-30 18:45:22: [2024-10-30 18:45:22] iter = 10100, loss = 1.8465
2024-10-30 18:45:25: [2024-10-30 18:45:25] iter = 10110, loss = 2.6214
2024-10-30 18:45:28: [2024-10-30 18:45:28] iter = 10120, loss = 2.7528
2024-10-30 18:45:32: [2024-10-30 18:45:32] iter = 10130, loss = 1.8448
2024-10-30 18:45:35: [2024-10-30 18:45:35] iter = 10140, loss = 2.4777
2024-10-30 18:45:40: [2024-10-30 18:45:40] iter = 10150, loss = 2.5341
2024-10-30 18:45:43: [2024-10-30 18:45:43] iter = 10160, loss = 1.8452
2024-10-30 18:45:48: [2024-10-30 18:45:48] iter = 10170, loss = 3.8243
2024-10-30 18:45:50: [2024-10-30 18:45:50] iter = 10180, loss = 3.9749
2024-10-30 18:45:55: [2024-10-30 18:45:55] iter = 10190, loss = 2.1089
2024-10-30 18:45:58: [2024-10-30 18:45:58] iter = 10200, loss = 2.4311
2024-10-30 18:46:02: [2024-10-30 18:46:02] iter = 10210, loss = 1.7895
2024-10-30 18:46:05: [2024-10-30 18:46:05] iter = 10220, loss = 2.7749
2024-10-30 18:46:09: [2024-10-30 18:46:09] iter = 10230, loss = 2.2973
2024-10-30 18:46:13: [2024-10-30 18:46:13] iter = 10240, loss = 1.9522
2024-10-30 18:46:16: [2024-10-30 18:46:16] iter = 10250, loss = 1.9384
2024-10-30 18:46:20: [2024-10-30 18:46:20] iter = 10260, loss = 2.3065
2024-10-30 18:46:23: [2024-10-30 18:46:23] iter = 10270, loss = 1.7186
2024-10-30 18:46:27: [2024-10-30 18:46:27] iter = 10280, loss = 2.7109
2024-10-30 18:46:31: [2024-10-30 18:46:31] iter = 10290, loss = 3.1674
2024-10-30 18:46:35: [2024-10-30 18:46:35] iter = 10300, loss = 2.2955
2024-10-30 18:46:38: [2024-10-30 18:46:38] iter = 10310, loss = 2.2384
2024-10-30 18:46:42: [2024-10-30 18:46:42] iter = 10320, loss = 2.6934
2024-10-30 18:46:46: [2024-10-30 18:46:45] iter = 10330, loss = 1.8907
2024-10-30 18:46:49: [2024-10-30 18:46:49] iter = 10340, loss = 1.9640
2024-10-30 18:46:53: [2024-10-30 18:46:53] iter = 10350, loss = 1.9766
2024-10-30 18:46:58: [2024-10-30 18:46:58] iter = 10360, loss = 2.0849
2024-10-30 18:47:01: [2024-10-30 18:47:01] iter = 10370, loss = 7.3494
2024-10-30 18:47:04: [2024-10-30 18:47:04] iter = 10380, loss = 2.1307
2024-10-30 18:47:08: [2024-10-30 18:47:08] iter = 10390, loss = 1.8481
2024-10-30 18:47:12: [2024-10-30 18:47:12] iter = 10400, loss = 2.2022
2024-10-30 18:47:16: [2024-10-30 18:47:16] iter = 10410, loss = 2.2299
2024-10-30 18:47:19: [2024-10-30 18:47:19] iter = 10420, loss = 2.6133
2024-10-30 18:47:22: [2024-10-30 18:47:22] iter = 10430, loss = 2.7373
2024-10-30 18:47:25: [2024-10-30 18:47:25] iter = 10440, loss = 2.1717
2024-10-30 18:47:29: [2024-10-30 18:47:29] iter = 10450, loss = 2.0008
2024-10-30 18:47:33: [2024-10-30 18:47:33] iter = 10460, loss = 2.1953
2024-10-30 18:47:37: [2024-10-30 18:47:37] iter = 10470, loss = 3.3105
2024-10-30 18:47:40: [2024-10-30 18:47:40] iter = 10480, loss = 2.0158
2024-10-30 18:47:43: [2024-10-30 18:47:43] iter = 10490, loss = 2.3261
2024-10-30 18:47:47: [2024-10-30 18:47:47] iter = 10500, loss = 2.3125
2024-10-30 18:47:51: [2024-10-30 18:47:51] iter = 10510, loss = 1.6903
2024-10-30 18:47:55: [2024-10-30 18:47:55] iter = 10520, loss = 2.0597
2024-10-30 18:47:59: [2024-10-30 18:47:59] iter = 10530, loss = 1.7221
2024-10-30 18:48:03: [2024-10-30 18:48:03] iter = 10540, loss = 2.4593
2024-10-30 18:48:07: [2024-10-30 18:48:07] iter = 10550, loss = 2.1731
2024-10-30 18:48:11: [2024-10-30 18:48:11] iter = 10560, loss = 1.8853
2024-10-30 18:48:14: [2024-10-30 18:48:14] iter = 10570, loss = 2.3651
2024-10-30 18:48:17: [2024-10-30 18:48:17] iter = 10580, loss = 1.9929
2024-10-30 18:48:21: [2024-10-30 18:48:21] iter = 10590, loss = 3.2130
2024-10-30 18:48:25: [2024-10-30 18:48:25] iter = 10600, loss = 1.8721
2024-10-30 18:48:28: [2024-10-30 18:48:28] iter = 10610, loss = 2.2573
2024-10-30 18:48:32: [2024-10-30 18:48:32] iter = 10620, loss = 1.7941
2024-10-30 18:48:36: [2024-10-30 18:48:36] iter = 10630, loss = 2.1897
2024-10-30 18:48:40: [2024-10-30 18:48:40] iter = 10640, loss = 1.9065
2024-10-30 18:48:44: [2024-10-30 18:48:44] iter = 10650, loss = 2.2032
2024-10-30 18:48:48: [2024-10-30 18:48:48] iter = 10660, loss = 3.1476
2024-10-30 18:48:51: [2024-10-30 18:48:51] iter = 10670, loss = 2.3264
2024-10-30 18:48:55: [2024-10-30 18:48:55] iter = 10680, loss = 1.9849
2024-10-30 18:48:59: [2024-10-30 18:48:59] iter = 10690, loss = 1.9106
2024-10-30 18:49:03: [2024-10-30 18:49:03] iter = 10700, loss = 2.4360
2024-10-30 18:49:06: [2024-10-30 18:49:06] iter = 10710, loss = 2.2578
2024-10-30 18:49:10: [2024-10-30 18:49:10] iter = 10720, loss = 2.8087
2024-10-30 18:49:13: [2024-10-30 18:49:13] iter = 10730, loss = 1.9887
2024-10-30 18:49:17: [2024-10-30 18:49:17] iter = 10740, loss = 2.1135
2024-10-30 18:49:21: [2024-10-30 18:49:21] iter = 10750, loss = 1.7632
2024-10-30 18:49:25: [2024-10-30 18:49:25] iter = 10760, loss = 2.4189
2024-10-30 18:49:29: [2024-10-30 18:49:29] iter = 10770, loss = 1.6987
2024-10-30 18:49:33: [2024-10-30 18:49:33] iter = 10780, loss = 2.2948
2024-10-30 18:49:36: [2024-10-30 18:49:36] iter = 10790, loss = 2.0116
2024-10-30 18:49:41: [2024-10-30 18:49:41] iter = 10800, loss = 5.2375
2024-10-30 18:49:45: [2024-10-30 18:49:45] iter = 10810, loss = 2.1574
2024-10-30 18:49:49: [2024-10-30 18:49:49] iter = 10820, loss = 1.9026
2024-10-30 18:49:53: [2024-10-30 18:49:53] iter = 10830, loss = 3.3821
2024-10-30 18:49:57: [2024-10-30 18:49:57] iter = 10840, loss = 1.8776
2024-10-30 18:50:01: [2024-10-30 18:50:01] iter = 10850, loss = 2.3993
2024-10-30 18:50:06: [2024-10-30 18:50:06] iter = 10860, loss = 1.8822
2024-10-30 18:50:10: [2024-10-30 18:50:10] iter = 10870, loss = 2.0800
2024-10-30 18:50:15: [2024-10-30 18:50:15] iter = 10880, loss = 2.1071
2024-10-30 18:50:18: [2024-10-30 18:50:18] iter = 10890, loss = 2.2565
2024-10-30 18:50:22: [2024-10-30 18:50:22] iter = 10900, loss = 2.0345
2024-10-30 18:50:26: [2024-10-30 18:50:26] iter = 10910, loss = 2.1171
2024-10-30 18:50:29: [2024-10-30 18:50:29] iter = 10920, loss = 2.0998
2024-10-30 18:50:33: [2024-10-30 18:50:33] iter = 10930, loss = 1.8032
2024-10-30 18:50:37: [2024-10-30 18:50:37] iter = 10940, loss = 2.1393
2024-10-30 18:50:41: [2024-10-30 18:50:41] iter = 10950, loss = 2.4391
2024-10-30 18:50:44: [2024-10-30 18:50:44] iter = 10960, loss = 2.3545
2024-10-30 18:50:48: [2024-10-30 18:50:48] iter = 10970, loss = 5.9969
2024-10-30 18:50:53: [2024-10-30 18:50:53] iter = 10980, loss = 2.0114
2024-10-30 18:50:56: [2024-10-30 18:50:56] iter = 10990, loss = 1.8376
2024-10-30 18:51:00: [2024-10-30 18:51:00] iter = 11000, loss = 2.6218
2024-10-30 18:51:04: [2024-10-30 18:51:04] iter = 11010, loss = 2.0935
2024-10-30 18:51:08: [2024-10-30 18:51:08] iter = 11020, loss = 2.3179
2024-10-30 18:51:12: [2024-10-30 18:51:12] iter = 11030, loss = 2.1867
2024-10-30 18:51:16: [2024-10-30 18:51:16] iter = 11040, loss = 2.5710
2024-10-30 18:51:19: [2024-10-30 18:51:19] iter = 11050, loss = 1.5664
2024-10-30 18:51:22: [2024-10-30 18:51:22] iter = 11060, loss = 2.2668
2024-10-30 18:51:26: [2024-10-30 18:51:26] iter = 11070, loss = 3.2943
2024-10-30 18:51:29: [2024-10-30 18:51:29] iter = 11080, loss = 2.4064
2024-10-30 18:51:33: [2024-10-30 18:51:33] iter = 11090, loss = 4.3321
2024-10-30 18:51:36: [2024-10-30 18:51:36] iter = 11100, loss = 2.6634
2024-10-30 18:51:41: [2024-10-30 18:51:41] iter = 11110, loss = 2.5945
2024-10-30 18:51:44: [2024-10-30 18:51:44] iter = 11120, loss = 2.4137
2024-10-30 18:51:47: [2024-10-30 18:51:47] iter = 11130, loss = 1.9104
2024-10-30 18:51:51: [2024-10-30 18:51:51] iter = 11140, loss = 2.0251
2024-10-30 18:51:55: [2024-10-30 18:51:55] iter = 11150, loss = 1.9516
2024-10-30 18:51:59: [2024-10-30 18:51:59] iter = 11160, loss = 2.1659
2024-10-30 18:52:02: [2024-10-30 18:52:02] iter = 11170, loss = 2.0020
2024-10-30 18:52:05: [2024-10-30 18:52:05] iter = 11180, loss = 1.8561
2024-10-30 18:52:09: [2024-10-30 18:52:09] iter = 11190, loss = 1.5885
2024-10-30 18:52:14: [2024-10-30 18:52:14] iter = 11200, loss = 2.9370
2024-10-30 18:52:18: [2024-10-30 18:52:18] iter = 11210, loss = 2.4670
2024-10-30 18:52:22: [2024-10-30 18:52:22] iter = 11220, loss = 1.8105
2024-10-30 18:52:27: [2024-10-30 18:52:27] iter = 11230, loss = 2.1152
2024-10-30 18:52:30: [2024-10-30 18:52:30] iter = 11240, loss = 2.7095
2024-10-30 18:52:34: [2024-10-30 18:52:34] iter = 11250, loss = 1.9017
2024-10-30 18:52:39: [2024-10-30 18:52:39] iter = 11260, loss = 2.1451
2024-10-30 18:52:42: [2024-10-30 18:52:42] iter = 11270, loss = 1.7210
2024-10-30 18:52:46: [2024-10-30 18:52:46] iter = 11280, loss = 3.3914
2024-10-30 18:52:50: [2024-10-30 18:52:50] iter = 11290, loss = 2.1470
2024-10-30 18:52:54: [2024-10-30 18:52:54] iter = 11300, loss = 2.1939
2024-10-30 18:52:58: [2024-10-30 18:52:58] iter = 11310, loss = 2.0806
2024-10-30 18:53:01: [2024-10-30 18:53:01] iter = 11320, loss = 2.5105
2024-10-30 18:53:06: [2024-10-30 18:53:06] iter = 11330, loss = 1.9066
2024-10-30 18:53:09: [2024-10-30 18:53:09] iter = 11340, loss = 4.8461
2024-10-30 18:53:13: [2024-10-30 18:53:13] iter = 11350, loss = 2.2155
2024-10-30 18:53:16: [2024-10-30 18:53:16] iter = 11360, loss = 2.3214
2024-10-30 18:53:20: [2024-10-30 18:53:20] iter = 11370, loss = 2.1370
2024-10-30 18:53:23: [2024-10-30 18:53:23] iter = 11380, loss = 1.9478
2024-10-30 18:53:27: [2024-10-30 18:53:27] iter = 11390, loss = 2.0934
2024-10-30 18:53:31: [2024-10-30 18:53:31] iter = 11400, loss = 2.8863
2024-10-30 18:53:35: [2024-10-30 18:53:35] iter = 11410, loss = 2.3577
2024-10-30 18:53:39: [2024-10-30 18:53:39] iter = 11420, loss = 1.9633
2024-10-30 18:53:42: [2024-10-30 18:53:42] iter = 11430, loss = 2.5443
2024-10-30 18:53:46: [2024-10-30 18:53:46] iter = 11440, loss = 8.1593
2024-10-30 18:53:50: [2024-10-30 18:53:50] iter = 11450, loss = 2.4061
2024-10-30 18:53:54: [2024-10-30 18:53:54] iter = 11460, loss = 1.9501
2024-10-30 18:53:59: [2024-10-30 18:53:59] iter = 11470, loss = 3.6384
2024-10-30 18:54:04: [2024-10-30 18:54:04] iter = 11480, loss = 2.0561
2024-10-30 18:54:08: [2024-10-30 18:54:08] iter = 11490, loss = 2.2893
2024-10-30 18:54:11: [2024-10-30 18:54:11] iter = 11500, loss = 2.0363
2024-10-30 18:54:15: [2024-10-30 18:54:15] iter = 11510, loss = 1.8906
2024-10-30 18:54:19: [2024-10-30 18:54:19] iter = 11520, loss = 2.2839
2024-10-30 18:54:23: [2024-10-30 18:54:23] iter = 11530, loss = 2.1997
2024-10-30 18:54:26: [2024-10-30 18:54:26] iter = 11540, loss = 1.9863
2024-10-30 18:54:31: [2024-10-30 18:54:31] iter = 11550, loss = 1.9174
2024-10-30 18:54:33: [2024-10-30 18:54:33] iter = 11560, loss = 2.1218
2024-10-30 18:54:36: [2024-10-30 18:54:36] iter = 11570, loss = 2.4840
2024-10-30 18:54:41: [2024-10-30 18:54:41] iter = 11580, loss = 2.9826
2024-10-30 18:54:45: [2024-10-30 18:54:45] iter = 11590, loss = 2.0847
2024-10-30 18:54:49: [2024-10-30 18:54:49] iter = 11600, loss = 2.0753
2024-10-30 18:54:54: [2024-10-30 18:54:54] iter = 11610, loss = 2.2498
2024-10-30 18:54:57: [2024-10-30 18:54:57] iter = 11620, loss = 2.3446
2024-10-30 18:55:01: [2024-10-30 18:55:01] iter = 11630, loss = 2.1448
2024-10-30 18:55:05: [2024-10-30 18:55:05] iter = 11640, loss = 2.3163
2024-10-30 18:55:08: [2024-10-30 18:55:08] iter = 11650, loss = 2.6577
2024-10-30 18:55:12: [2024-10-30 18:55:12] iter = 11660, loss = 2.1761
2024-10-30 18:55:16: [2024-10-30 18:55:16] iter = 11670, loss = 1.9789
2024-10-30 18:55:21: [2024-10-30 18:55:20] iter = 11680, loss = 1.8228
2024-10-30 18:55:26: [2024-10-30 18:55:26] iter = 11690, loss = 1.8794
2024-10-30 18:55:30: [2024-10-30 18:55:30] iter = 11700, loss = 2.9398
2024-10-30 18:55:33: [2024-10-30 18:55:33] iter = 11710, loss = 2.0080
2024-10-30 18:55:37: [2024-10-30 18:55:37] iter = 11720, loss = 2.2533
2024-10-30 18:55:41: [2024-10-30 18:55:41] iter = 11730, loss = 2.1111
2024-10-30 18:55:45: [2024-10-30 18:55:45] iter = 11740, loss = 2.3127
2024-10-30 18:55:49: [2024-10-30 18:55:49] iter = 11750, loss = 2.0511
2024-10-30 18:55:53: [2024-10-30 18:55:53] iter = 11760, loss = 1.8777
2024-10-30 18:55:58: [2024-10-30 18:55:57] iter = 11770, loss = 2.0180
2024-10-30 18:56:01: [2024-10-30 18:56:01] iter = 11780, loss = 1.8831
2024-10-30 18:56:05: [2024-10-30 18:56:05] iter = 11790, loss = 3.2235
2024-10-30 18:56:08: [2024-10-30 18:56:08] iter = 11800, loss = 1.9679
2024-10-30 18:56:12: [2024-10-30 18:56:12] iter = 11810, loss = 2.6608
2024-10-30 18:56:16: [2024-10-30 18:56:16] iter = 11820, loss = 3.9058
2024-10-30 18:56:20: [2024-10-30 18:56:20] iter = 11830, loss = 2.1102
2024-10-30 18:56:25: [2024-10-30 18:56:25] iter = 11840, loss = 2.5275
2024-10-30 18:56:30: [2024-10-30 18:56:30] iter = 11850, loss = 1.9657
2024-10-30 18:56:34: [2024-10-30 18:56:34] iter = 11860, loss = 2.2477
2024-10-30 18:56:38: [2024-10-30 18:56:38] iter = 11870, loss = 2.2010
2024-10-30 18:56:43: [2024-10-30 18:56:43] iter = 11880, loss = 3.6316
2024-10-30 18:56:47: [2024-10-30 18:56:47] iter = 11890, loss = 1.9423
2024-10-30 18:56:51: [2024-10-30 18:56:51] iter = 11900, loss = 2.6583
2024-10-30 18:56:55: [2024-10-30 18:56:55] iter = 11910, loss = 2.0892
2024-10-30 18:57:00: [2024-10-30 18:57:00] iter = 11920, loss = 1.9725
2024-10-30 18:57:03: [2024-10-30 18:57:03] iter = 11930, loss = 3.1751
2024-10-30 18:57:06: [2024-10-30 18:57:06] iter = 11940, loss = 2.0535
2024-10-30 18:57:09: [2024-10-30 18:57:09] iter = 11950, loss = 2.4174
2024-10-30 18:57:13: [2024-10-30 18:57:13] iter = 11960, loss = 2.1565
2024-10-30 18:57:17: [2024-10-30 18:57:17] iter = 11970, loss = 2.2758
2024-10-30 18:57:21: [2024-10-30 18:57:21] iter = 11980, loss = 2.2535
2024-10-30 18:57:25: [2024-10-30 18:57:25] iter = 11990, loss = 1.9160
2024-10-30 18:57:29: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
2024-10-30 18:57:29: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 18:57:29: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 49031}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 19:00:01: Evaluate 5 random ConvNet, ACCmean = 0.7715 ACCstd = 0.0056
-------------------------
2024-10-30 19:00:01: Evaluate 5 random ConvNet, SENmean = 0.7704 SENstd = 0.0053
-------------------------
2024-10-30 19:00:01: Evaluate 5 random ConvNet, SPEmean = 0.9771 SPEstd = 0.0006
-------------------------
2024-10-30 19:00:01: Evaluate 5 random ConvNet, F!mean = 0.7589 F!std = 0.0051
-------------------------
2024-10-30 19:00:01: Evaluate 5 random ConvNet, mean = 0.7715 std = 0.0056
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 19:00:01: [2024-10-30 19:00:01] iter = 12000, loss = 1.9822
2024-10-30 19:00:06: [2024-10-30 19:00:06] iter = 12010, loss = 2.0399
2024-10-30 19:00:11: [2024-10-30 19:00:11] iter = 12020, loss = 3.1544
2024-10-30 19:00:14: [2024-10-30 19:00:14] iter = 12030, loss = 2.4744
2024-10-30 19:00:17: [2024-10-30 19:00:17] iter = 12040, loss = 2.1538
2024-10-30 19:00:20: [2024-10-30 19:00:20] iter = 12050, loss = 2.3630
2024-10-30 19:00:24: [2024-10-30 19:00:24] iter = 12060, loss = 2.1912
2024-10-30 19:00:28: [2024-10-30 19:00:28] iter = 12070, loss = 2.1577
2024-10-30 19:00:32: [2024-10-30 19:00:32] iter = 12080, loss = 2.0628
2024-10-30 19:00:35: [2024-10-30 19:00:35] iter = 12090, loss = 2.0104
2024-10-30 19:00:40: [2024-10-30 19:00:40] iter = 12100, loss = 1.8038
2024-10-30 19:00:43: [2024-10-30 19:00:43] iter = 12110, loss = 1.8091
2024-10-30 19:00:46: [2024-10-30 19:00:46] iter = 12120, loss = 3.0351
2024-10-30 19:00:50: [2024-10-30 19:00:50] iter = 12130, loss = 2.0325
2024-10-30 19:00:53: [2024-10-30 19:00:53] iter = 12140, loss = 2.8884
2024-10-30 19:00:57: [2024-10-30 19:00:57] iter = 12150, loss = 2.1836
2024-10-30 19:01:01: [2024-10-30 19:01:01] iter = 12160, loss = 1.9679
2024-10-30 19:01:03: [2024-10-30 19:01:03] iter = 12170, loss = 1.7614
2024-10-30 19:01:08: [2024-10-30 19:01:08] iter = 12180, loss = 2.5806
2024-10-30 19:01:12: [2024-10-30 19:01:12] iter = 12190, loss = 2.1041
2024-10-30 19:01:16: [2024-10-30 19:01:16] iter = 12200, loss = 1.9454
2024-10-30 19:01:20: [2024-10-30 19:01:20] iter = 12210, loss = 2.0441
2024-10-30 19:01:24: [2024-10-30 19:01:24] iter = 12220, loss = 1.9239
2024-10-30 19:01:28: [2024-10-30 19:01:28] iter = 12230, loss = 2.3458
2024-10-30 19:01:32: [2024-10-30 19:01:32] iter = 12240, loss = 2.7388
2024-10-30 19:01:34: [2024-10-30 19:01:34] iter = 12250, loss = 2.8637
2024-10-30 19:01:38: [2024-10-30 19:01:38] iter = 12260, loss = 2.1499
2024-10-30 19:01:42: [2024-10-30 19:01:42] iter = 12270, loss = 2.0772
2024-10-30 19:01:46: [2024-10-30 19:01:46] iter = 12280, loss = 1.9283
2024-10-30 19:01:50: [2024-10-30 19:01:50] iter = 12290, loss = 2.2336
2024-10-30 19:01:54: [2024-10-30 19:01:54] iter = 12300, loss = 1.8392
2024-10-30 19:01:58: [2024-10-30 19:01:58] iter = 12310, loss = 2.2630
2024-10-30 19:02:01: [2024-10-30 19:02:01] iter = 12320, loss = 2.2040
2024-10-30 19:02:05: [2024-10-30 19:02:05] iter = 12330, loss = 2.1656
2024-10-30 19:02:08: [2024-10-30 19:02:08] iter = 12340, loss = 2.4976
2024-10-30 19:02:12: [2024-10-30 19:02:12] iter = 12350, loss = 2.1100
2024-10-30 19:02:15: [2024-10-30 19:02:15] iter = 12360, loss = 1.8079
2024-10-30 19:02:19: [2024-10-30 19:02:19] iter = 12370, loss = 3.5377
2024-10-30 19:02:23: [2024-10-30 19:02:23] iter = 12380, loss = 2.1592
2024-10-30 19:02:27: [2024-10-30 19:02:27] iter = 12390, loss = 2.4428
2024-10-30 19:02:31: [2024-10-30 19:02:31] iter = 12400, loss = 2.1330
2024-10-30 19:02:35: [2024-10-30 19:02:35] iter = 12410, loss = 2.1975
2024-10-30 19:02:39: [2024-10-30 19:02:39] iter = 12420, loss = 4.5476
2024-10-30 19:02:43: [2024-10-30 19:02:43] iter = 12430, loss = 2.3376
2024-10-30 19:02:46: [2024-10-30 19:02:46] iter = 12440, loss = 2.3244
2024-10-30 19:02:50: [2024-10-30 19:02:50] iter = 12450, loss = 2.2237
2024-10-30 19:02:54: [2024-10-30 19:02:54] iter = 12460, loss = 2.8354
2024-10-30 19:02:58: [2024-10-30 19:02:58] iter = 12470, loss = 2.2700
2024-10-30 19:03:03: [2024-10-30 19:03:03] iter = 12480, loss = 1.8294
2024-10-30 19:03:07: [2024-10-30 19:03:07] iter = 12490, loss = 3.0400
2024-10-30 19:03:11: [2024-10-30 19:03:11] iter = 12500, loss = 1.8680
2024-10-30 19:03:15: [2024-10-30 19:03:15] iter = 12510, loss = 2.4025
2024-10-30 19:03:17: [2024-10-30 19:03:17] iter = 12520, loss = 2.2536
2024-10-30 19:03:20: [2024-10-30 19:03:20] iter = 12530, loss = 2.8027
2024-10-30 19:03:24: [2024-10-30 19:03:24] iter = 12540, loss = 2.6793
2024-10-30 19:03:28: [2024-10-30 19:03:28] iter = 12550, loss = 2.8116
2024-10-30 19:03:32: [2024-10-30 19:03:32] iter = 12560, loss = 2.0146
2024-10-30 19:03:36: [2024-10-30 19:03:36] iter = 12570, loss = 2.9470
2024-10-30 19:03:40: [2024-10-30 19:03:40] iter = 12580, loss = 1.9784
2024-10-30 19:03:43: [2024-10-30 19:03:43] iter = 12590, loss = 2.0590
2024-10-30 19:03:46: [2024-10-30 19:03:46] iter = 12600, loss = 2.5560
2024-10-30 19:03:50: [2024-10-30 19:03:50] iter = 12610, loss = 6.8090
2024-10-30 19:03:53: [2024-10-30 19:03:53] iter = 12620, loss = 2.1415
2024-10-30 19:03:56: [2024-10-30 19:03:56] iter = 12630, loss = 2.6395
2024-10-30 19:04:00: [2024-10-30 19:04:00] iter = 12640, loss = 2.2882
2024-10-30 19:04:04: [2024-10-30 19:04:04] iter = 12650, loss = 2.4056
2024-10-30 19:04:07: [2024-10-30 19:04:07] iter = 12660, loss = 2.5621
2024-10-30 19:04:10: [2024-10-30 19:04:10] iter = 12670, loss = 3.2351
2024-10-30 19:04:14: [2024-10-30 19:04:14] iter = 12680, loss = 1.8447
2024-10-30 19:04:18: [2024-10-30 19:04:18] iter = 12690, loss = 2.0910
2024-10-30 19:04:22: [2024-10-30 19:04:22] iter = 12700, loss = 2.1049
2024-10-30 19:04:26: [2024-10-30 19:04:26] iter = 12710, loss = 2.1616
2024-10-30 19:04:29: [2024-10-30 19:04:29] iter = 12720, loss = 2.5225
2024-10-30 19:04:33: [2024-10-30 19:04:33] iter = 12730, loss = 1.9634
2024-10-30 19:04:38: [2024-10-30 19:04:38] iter = 12740, loss = 3.2512
2024-10-30 19:04:42: [2024-10-30 19:04:42] iter = 12750, loss = 2.0502
2024-10-30 19:04:45: [2024-10-30 19:04:45] iter = 12760, loss = 3.1209
2024-10-30 19:04:48: [2024-10-30 19:04:48] iter = 12770, loss = 2.1054
2024-10-30 19:04:51: [2024-10-30 19:04:51] iter = 12780, loss = 1.8246
2024-10-30 19:04:55: [2024-10-30 19:04:55] iter = 12790, loss = 2.1206
2024-10-30 19:04:58: [2024-10-30 19:04:58] iter = 12800, loss = 2.1591
2024-10-30 19:05:02: [2024-10-30 19:05:02] iter = 12810, loss = 1.8503
2024-10-30 19:05:07: [2024-10-30 19:05:07] iter = 12820, loss = 2.1205
2024-10-30 19:05:11: [2024-10-30 19:05:11] iter = 12830, loss = 1.9097
2024-10-30 19:05:15: [2024-10-30 19:05:15] iter = 12840, loss = 2.1611
2024-10-30 19:05:19: [2024-10-30 19:05:19] iter = 12850, loss = 1.9168
2024-10-30 19:05:24: [2024-10-30 19:05:24] iter = 12860, loss = 1.7767
2024-10-30 19:05:29: [2024-10-30 19:05:28] iter = 12870, loss = 3.8578
2024-10-30 19:05:34: [2024-10-30 19:05:34] iter = 12880, loss = 2.9315
2024-10-30 19:05:37: [2024-10-30 19:05:37] iter = 12890, loss = 2.3426
2024-10-30 19:05:41: [2024-10-30 19:05:41] iter = 12900, loss = 3.0904
2024-10-30 19:05:44: [2024-10-30 19:05:44] iter = 12910, loss = 2.2674
2024-10-30 19:05:47: [2024-10-30 19:05:47] iter = 12920, loss = 2.9136
2024-10-30 19:05:51: [2024-10-30 19:05:51] iter = 12930, loss = 4.2037
2024-10-30 19:05:54: [2024-10-30 19:05:54] iter = 12940, loss = 2.0788
2024-10-30 19:05:58: [2024-10-30 19:05:58] iter = 12950, loss = 3.6001
2024-10-30 19:06:01: [2024-10-30 19:06:01] iter = 12960, loss = 2.8744
2024-10-30 19:06:05: [2024-10-30 19:06:05] iter = 12970, loss = 2.1343
2024-10-30 19:06:09: [2024-10-30 19:06:09] iter = 12980, loss = 1.9917
2024-10-30 19:06:13: [2024-10-30 19:06:13] iter = 12990, loss = 2.0434
2024-10-30 19:06:17: [2024-10-30 19:06:17] iter = 13000, loss = 4.5407
2024-10-30 19:06:21: [2024-10-30 19:06:21] iter = 13010, loss = 2.8545
2024-10-30 19:06:25: [2024-10-30 19:06:25] iter = 13020, loss = 1.8442
2024-10-30 19:06:29: [2024-10-30 19:06:29] iter = 13030, loss = 1.9706
2024-10-30 19:06:33: [2024-10-30 19:06:33] iter = 13040, loss = 2.2745
2024-10-30 19:06:37: [2024-10-30 19:06:37] iter = 13050, loss = 2.9731
2024-10-30 19:06:41: [2024-10-30 19:06:41] iter = 13060, loss = 2.3793
2024-10-30 19:06:46: [2024-10-30 19:06:46] iter = 13070, loss = 1.8127
2024-10-30 19:06:49: [2024-10-30 19:06:49] iter = 13080, loss = 2.5274
2024-10-30 19:06:53: [2024-10-30 19:06:53] iter = 13090, loss = 3.0563
2024-10-30 19:06:57: [2024-10-30 19:06:57] iter = 13100, loss = 3.3362
2024-10-30 19:07:02: [2024-10-30 19:07:02] iter = 13110, loss = 2.0638
2024-10-30 19:07:05: [2024-10-30 19:07:05] iter = 13120, loss = 2.1775
2024-10-30 19:07:10: [2024-10-30 19:07:10] iter = 13130, loss = 1.8267
2024-10-30 19:07:13: [2024-10-30 19:07:13] iter = 13140, loss = 2.3528
2024-10-30 19:07:18: [2024-10-30 19:07:18] iter = 13150, loss = 2.8139
2024-10-30 19:07:21: [2024-10-30 19:07:21] iter = 13160, loss = 1.9630
2024-10-30 19:07:23: [2024-10-30 19:07:23] iter = 13170, loss = 2.3780
2024-10-30 19:07:26: [2024-10-30 19:07:26] iter = 13180, loss = 2.0531
2024-10-30 19:07:30: [2024-10-30 19:07:30] iter = 13190, loss = 3.2713
2024-10-30 19:07:34: [2024-10-30 19:07:34] iter = 13200, loss = 1.8066
2024-10-30 19:07:38: [2024-10-30 19:07:38] iter = 13210, loss = 3.0097
2024-10-30 19:07:42: [2024-10-30 19:07:42] iter = 13220, loss = 3.3106
2024-10-30 19:07:46: [2024-10-30 19:07:46] iter = 13230, loss = 4.6310
2024-10-30 19:07:49: [2024-10-30 19:07:49] iter = 13240, loss = 2.3594
2024-10-30 19:07:52: [2024-10-30 19:07:52] iter = 13250, loss = 2.9387
2024-10-30 19:07:56: [2024-10-30 19:07:56] iter = 13260, loss = 3.8634
2024-10-30 19:08:00: [2024-10-30 19:08:00] iter = 13270, loss = 4.1043
2024-10-30 19:08:04: [2024-10-30 19:08:04] iter = 13280, loss = 4.3104
2024-10-30 19:08:08: [2024-10-30 19:08:08] iter = 13290, loss = 2.6489
2024-10-30 19:08:12: [2024-10-30 19:08:12] iter = 13300, loss = 3.8289
2024-10-30 19:08:15: [2024-10-30 19:08:15] iter = 13310, loss = 2.3460
2024-10-30 19:08:19: [2024-10-30 19:08:19] iter = 13320, loss = 2.0302
2024-10-30 19:08:23: [2024-10-30 19:08:23] iter = 13330, loss = 2.7187
2024-10-30 19:08:27: [2024-10-30 19:08:27] iter = 13340, loss = 2.4243
2024-10-30 19:08:31: [2024-10-30 19:08:31] iter = 13350, loss = 3.0380
2024-10-30 19:08:35: [2024-10-30 19:08:35] iter = 13360, loss = 2.0483
2024-10-30 19:08:39: [2024-10-30 19:08:39] iter = 13370, loss = 2.0601
2024-10-30 19:08:42: [2024-10-30 19:08:42] iter = 13380, loss = 2.6287
2024-10-30 19:08:45: [2024-10-30 19:08:45] iter = 13390, loss = 2.3131
2024-10-30 19:08:49: [2024-10-30 19:08:49] iter = 13400, loss = 2.0178
2024-10-30 19:08:53: [2024-10-30 19:08:53] iter = 13410, loss = 1.9886
2024-10-30 19:08:57: [2024-10-30 19:08:57] iter = 13420, loss = 2.0976
2024-10-30 19:09:00: [2024-10-30 19:09:00] iter = 13430, loss = 2.4428
2024-10-30 19:09:03: [2024-10-30 19:09:03] iter = 13440, loss = 2.7811
2024-10-30 19:09:08: [2024-10-30 19:09:08] iter = 13450, loss = 3.1775
2024-10-30 19:09:12: [2024-10-30 19:09:12] iter = 13460, loss = 4.4566
2024-10-30 19:09:15: [2024-10-30 19:09:15] iter = 13470, loss = 2.1233
2024-10-30 19:09:18: [2024-10-30 19:09:18] iter = 13480, loss = 2.2123
2024-10-30 19:09:22: [2024-10-30 19:09:22] iter = 13490, loss = 2.0349
2024-10-30 19:09:26: [2024-10-30 19:09:26] iter = 13500, loss = 3.7525
2024-10-30 19:09:30: [2024-10-30 19:09:30] iter = 13510, loss = 2.3103
2024-10-30 19:09:34: [2024-10-30 19:09:34] iter = 13520, loss = 1.8701
2024-10-30 19:09:37: [2024-10-30 19:09:37] iter = 13530, loss = 1.9904
2024-10-30 19:09:41: [2024-10-30 19:09:41] iter = 13540, loss = 2.2836
2024-10-30 19:09:44: [2024-10-30 19:09:44] iter = 13550, loss = 1.8950
2024-10-30 19:09:47: [2024-10-30 19:09:47] iter = 13560, loss = 1.8816
2024-10-30 19:09:50: [2024-10-30 19:09:50] iter = 13570, loss = 2.2374
2024-10-30 19:09:54: [2024-10-30 19:09:54] iter = 13580, loss = 2.9497
2024-10-30 19:09:58: [2024-10-30 19:09:58] iter = 13590, loss = 2.4121
2024-10-30 19:10:02: [2024-10-30 19:10:02] iter = 13600, loss = 1.8880
2024-10-30 19:10:06: [2024-10-30 19:10:06] iter = 13610, loss = 1.8348
2024-10-30 19:10:10: [2024-10-30 19:10:09] iter = 13620, loss = 2.5708
2024-10-30 19:10:13: [2024-10-30 19:10:13] iter = 13630, loss = 1.9443
2024-10-30 19:10:17: [2024-10-30 19:10:17] iter = 13640, loss = 2.0949
2024-10-30 19:10:21: [2024-10-30 19:10:21] iter = 13650, loss = 1.8364
2024-10-30 19:10:25: [2024-10-30 19:10:25] iter = 13660, loss = 2.8122
2024-10-30 19:10:29: [2024-10-30 19:10:29] iter = 13670, loss = 3.0404
2024-10-30 19:10:33: [2024-10-30 19:10:33] iter = 13680, loss = 2.1691
2024-10-30 19:10:36: [2024-10-30 19:10:36] iter = 13690, loss = 2.2383
2024-10-30 19:10:40: [2024-10-30 19:10:40] iter = 13700, loss = 2.9447
2024-10-30 19:10:43: [2024-10-30 19:10:43] iter = 13710, loss = 1.8564
2024-10-30 19:10:47: [2024-10-30 19:10:47] iter = 13720, loss = 2.3666
2024-10-30 19:10:51: [2024-10-30 19:10:51] iter = 13730, loss = 2.1720
2024-10-30 19:10:54: [2024-10-30 19:10:54] iter = 13740, loss = 2.3660
2024-10-30 19:10:59: [2024-10-30 19:10:59] iter = 13750, loss = 1.9357
2024-10-30 19:11:03: [2024-10-30 19:11:03] iter = 13760, loss = 2.4842
2024-10-30 19:11:06: [2024-10-30 19:11:06] iter = 13770, loss = 2.6443
2024-10-30 19:11:10: [2024-10-30 19:11:10] iter = 13780, loss = 2.3364
2024-10-30 19:11:14: [2024-10-30 19:11:14] iter = 13790, loss = 1.8414
2024-10-30 19:11:17: [2024-10-30 19:11:17] iter = 13800, loss = 2.1947
2024-10-30 19:11:22: [2024-10-30 19:11:22] iter = 13810, loss = 3.0548
2024-10-30 19:11:25: [2024-10-30 19:11:25] iter = 13820, loss = 2.2537
2024-10-30 19:11:29: [2024-10-30 19:11:29] iter = 13830, loss = 1.9645
2024-10-30 19:11:32: [2024-10-30 19:11:32] iter = 13840, loss = 2.2135
2024-10-30 19:11:34: [2024-10-30 19:11:34] iter = 13850, loss = 3.6153
2024-10-30 19:11:38: [2024-10-30 19:11:38] iter = 13860, loss = 1.9704
2024-10-30 19:11:41: [2024-10-30 19:11:41] iter = 13870, loss = 3.7149
2024-10-30 19:11:45: [2024-10-30 19:11:45] iter = 13880, loss = 2.6028
2024-10-30 19:11:50: [2024-10-30 19:11:50] iter = 13890, loss = 2.3595
2024-10-30 19:11:54: [2024-10-30 19:11:54] iter = 13900, loss = 2.5527
2024-10-30 19:11:57: [2024-10-30 19:11:57] iter = 13910, loss = 2.2062
2024-10-30 19:12:00: [2024-10-30 19:12:00] iter = 13920, loss = 3.6951
2024-10-30 19:12:04: [2024-10-30 19:12:04] iter = 13930, loss = 2.5954
2024-10-30 19:12:07: [2024-10-30 19:12:07] iter = 13940, loss = 2.8833
2024-10-30 19:12:11: [2024-10-30 19:12:11] iter = 13950, loss = 1.9519
2024-10-30 19:12:15: [2024-10-30 19:12:15] iter = 13960, loss = 1.9722
2024-10-30 19:12:19: [2024-10-30 19:12:19] iter = 13970, loss = 2.3453
2024-10-30 19:12:23: [2024-10-30 19:12:23] iter = 13980, loss = 1.9136
2024-10-30 19:12:25: [2024-10-30 19:12:25] iter = 13990, loss = 2.6164
2024-10-30 19:12:28: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
2024-10-30 19:12:28: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 19:12:28: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 48765}

[2024-10-30 17:07:33] Evaluate_01: epoch = 1000 train time = 32 s train loss = 0.005222 train acc = 1.0000, test acc = 0.7714, test_sen =0.7733, test_spe =0.9771, test_f1 =0.7588
[2024-10-30 17:08:11] Evaluate_02: epoch = 1000 train time = 35 s train loss = 0.003727 train acc = 1.0000, test acc = 0.7763, test_sen =0.7757, test_spe =0.9775, test_f1 =0.7624
[2024-10-30 17:08:43] Evaluate_03: epoch = 1000 train time = 29 s train loss = 0.022974 train acc = 1.0000, test acc = 0.7793, test_sen =0.7842, test_spe =0.9779, test_f1 =0.7682
[2024-10-30 17:09:16] Evaluate_04: epoch = 1000 train time = 30 s train loss = 0.002901 train acc = 1.0000, test acc = 0.7717, test_sen =0.7738, test_spe =0.9771, test_f1 =0.7585
[2024-10-30 17:24:00] Evaluate_00: epoch = 1000 train time = 31 s train loss = 0.002543 train acc = 1.0000, test acc = 0.7891, test_sen =0.7887, test_spe =0.9789, test_f1 =0.7754
[2024-10-30 17:24:31] Evaluate_01: epoch = 1000 train time = 29 s train loss = 0.006123 train acc = 1.0000, test acc = 0.7877, test_sen =0.7850, test_spe =0.9786, test_f1 =0.7730
[2024-10-30 17:25:02] Evaluate_02: epoch = 1000 train time = 27 s train loss = 0.027483 train acc = 1.0000, test acc = 0.7786, test_sen =0.7751, test_spe =0.9777, test_f1 =0.7645
[2024-10-30 17:25:32] Evaluate_03: epoch = 1000 train time = 28 s train loss = 0.035870 train acc = 1.0000, test acc = 0.7888, test_sen =0.7862, test_spe =0.9788, test_f1 =0.7747
[2024-10-30 17:26:03] Evaluate_04: epoch = 1000 train time = 28 s train loss = 0.021989 train acc = 1.0000, test acc = 0.7804, test_sen =0.7792, test_spe =0.9779, test_f1 =0.7680
[2024-10-30 17:26:40] Evaluate_00: epoch = 1000 train time = 30 s train loss = 0.013802 train acc = 0.9909, test acc = 0.6978, test_sen =0.6814, test_spe =0.9695, test_f1 =0.6742
[2024-10-30 17:27:12] Evaluate_01: epoch = 1000 train time = 30 s train loss = 0.006976 train acc = 1.0000, test acc = 0.6886, test_sen =0.6730, test_spe =0.9687, test_f1 =0.6636
[2024-10-30 17:27:46] Evaluate_02: epoch = 1000 train time = 31 s train loss = 0.002355 train acc = 1.0000, test acc = 0.6905, test_sen =0.6745, test_spe =0.9688, test_f1 =0.6669
[2024-10-30 17:28:16] Evaluate_03: epoch = 1000 train time = 27 s train loss = 0.002325 train acc = 1.0000, test acc = 0.6812, test_sen =0.6646, test_spe =0.9679, test_f1 =0.6582
[2024-10-30 17:28:53] Evaluate_04: epoch = 1000 train time = 34 s train loss = 0.037694 train acc = 0.9909, test acc = 0.7040, test_sen =0.6923, test_spe =0.9702, test_f1 =0.6823
[2024-10-30 17:42:36] Evaluate_00: epoch = 1000 train time = 29 s train loss = 0.005517 train acc = 1.0000, test acc = 0.7791, test_sen =0.7742, test_spe =0.9778, test_f1 =0.7614
[2024-10-30 17:43:11] Evaluate_01: epoch = 1000 train time = 32 s train loss = 0.002608 train acc = 1.0000, test acc = 0.7821, test_sen =0.7770, test_spe =0.9782, test_f1 =0.7614
[2024-10-30 17:43:40] Evaluate_02: epoch = 1000 train time = 26 s train loss = 0.002845 train acc = 1.0000, test acc = 0.7864, test_sen =0.7803, test_spe =0.9786, test_f1 =0.7686
[2024-10-30 17:44:09] Evaluate_03: epoch = 1000 train time = 27 s train loss = 0.008876 train acc = 1.0000, test acc = 0.7757, test_sen =0.7716, test_spe =0.9775, test_f1 =0.7599
[2024-10-30 17:44:37] Evaluate_04: epoch = 1000 train time = 25 s train loss = 0.003608 train acc = 1.0000, test acc = 0.7875, test_sen =0.7810, test_spe =0.9786, test_f1 =0.7702
[2024-10-30 17:57:38] Evaluate_00: epoch = 1000 train time = 29 s train loss = 0.026155 train acc = 1.0000, test acc = 0.7819, test_sen =0.7834, test_spe =0.9782, test_f1 =0.7675
[2024-10-30 17:58:07] Evaluate_01: epoch = 1000 train time = 27 s train loss = 0.003363 train acc = 1.0000, test acc = 0.7733, test_sen =0.7771, test_spe =0.9773, test_f1 =0.7605
[2024-10-30 17:58:36] Evaluate_02: epoch = 1000 train time = 27 s train loss = 0.005985 train acc = 1.0000, test acc = 0.7796, test_sen =0.7806, test_spe =0.9780, test_f1 =0.7658
[2024-10-30 17:59:06] Evaluate_03: epoch = 1000 train time = 28 s train loss = 0.035908 train acc = 1.0000, test acc = 0.7784, test_sen =0.7784, test_spe =0.9778, test_f1 =0.7655
[2024-10-30 17:59:38] Evaluate_04: epoch = 1000 train time = 29 s train loss = 0.022997 train acc = 1.0000, test acc = 0.7815, test_sen =0.7827, test_spe =0.9781, test_f1 =0.7678
[2024-10-30 18:12:39] Evaluate_00: epoch = 1000 train time = 32 s train loss = 0.006795 train acc = 1.0000, test acc = 0.7886, test_sen =0.7858, test_spe =0.9788, test_f1 =0.7747
[2024-10-30 18:13:13] Evaluate_01: epoch = 1000 train time = 31 s train loss = 0.004266 train acc = 1.0000, test acc = 0.7831, test_sen =0.7821, test_spe =0.9783, test_f1 =0.7685
[2024-10-30 18:13:42] Evaluate_02: epoch = 1000 train time = 27 s train loss = 0.005099 train acc = 1.0000, test acc = 0.7942, test_sen =0.7890, test_spe =0.9794, test_f1 =0.7768
[2024-10-30 18:14:11] Evaluate_03: epoch = 1000 train time = 27 s train loss = 0.003161 train acc = 1.0000, test acc = 0.7927, test_sen =0.7910, test_spe =0.9793, test_f1 =0.7792
[2024-10-30 18:14:42] Evaluate_04: epoch = 1000 train time = 28 s train loss = 0.007662 train acc = 1.0000, test acc = 0.7909, test_sen =0.7857, test_spe =0.9791, test_f1 =0.7729
[2024-10-30 18:28:06] Evaluate_00: epoch = 1000 train time = 27 s train loss = 0.007834 train acc = 1.0000, test acc = 0.7767, test_sen =0.7789, test_spe =0.9776, test_f1 =0.7635
[2024-10-30 18:28:35] Evaluate_01: epoch = 1000 train time = 27 s train loss = 0.029938 train acc = 1.0000, test acc = 0.7790, test_sen =0.7758, test_spe =0.9778, test_f1 =0.7636
[2024-10-30 18:29:05] Evaluate_02: epoch = 1000 train time = 27 s train loss = 0.023164 train acc = 1.0000, test acc = 0.7738, test_sen =0.7711, test_spe =0.9773, test_f1 =0.7615
[2024-10-30 18:29:33] Evaluate_03: epoch = 1000 train time = 25 s train loss = 0.004553 train acc = 1.0000, test acc = 0.7780, test_sen =0.7774, test_spe =0.9777, test_f1 =0.7654
[2024-10-30 18:30:01] Evaluate_04: epoch = 1000 train time = 25 s train loss = 0.022393 train acc = 1.0000, test acc = 0.7778, test_sen =0.7769, test_spe =0.9777, test_f1 =0.7637
[2024-10-30 18:42:45] Evaluate_00: epoch = 1000 train time = 26 s train loss = 0.014206 train acc = 1.0000, test acc = 0.7641, test_sen =0.7647, test_spe =0.9763, test_f1 =0.7497
[2024-10-30 18:43:14] Evaluate_01: epoch = 1000 train time = 27 s train loss = 0.002777 train acc = 1.0000, test acc = 0.7602, test_sen =0.7598, test_spe =0.9760, test_f1 =0.7449
[2024-10-30 18:43:44] Evaluate_02: epoch = 1000 train time = 28 s train loss = 0.035074 train acc = 0.9909, test acc = 0.7728, test_sen =0.7748, test_spe =0.9773, test_f1 =0.7580
[2024-10-30 18:44:15] Evaluate_03: epoch = 1000 train time = 28 s train loss = 0.009226 train acc = 1.0000, test acc = 0.7573, test_sen =0.7608, test_spe =0.9757, test_f1 =0.7449
[2024-10-30 18:44:45] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.003394 train acc = 1.0000, test acc = 0.7621, test_sen =0.7660, test_spe =0.9762, test_f1 =0.7501
[2024-10-30 18:57:59] Evaluate_00: epoch = 1000 train time = 28 s train loss = 0.004819 train acc = 1.0000, test acc = 0.7656, test_sen =0.7663, test_spe =0.9765, test_f1 =0.7534
[2024-10-30 18:58:27] Evaluate_01: epoch = 1000 train time = 25 s train loss = 0.012913 train acc = 1.0000, test acc = 0.7699, test_sen =0.7706, test_spe =0.9769, test_f1 =0.7588
[2024-10-30 18:58:59] Evaluate_02: epoch = 1000 train time = 29 s train loss = 0.004632 train acc = 1.0000, test acc = 0.7731, test_sen =0.7720, test_spe =0.9772, test_f1 =0.7612
[2024-10-30 18:59:32] Evaluate_03: epoch = 1000 train time = 31 s train loss = 0.007735 train acc = 1.0000, test acc = 0.7815, test_sen =0.7793, test_spe =0.9781, test_f1 =0.7672
[2024-10-30 19:00:01] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.002948 train acc = 1.0000, test acc = 0.7673, test_sen =0.7640, test_spe =0.9766, test_f1 =0.7540
[2024-10-30 19:12:59] Evaluate_00: epoch = 1000 train time = 28 s train loss = 0.004998 train acc = 1.0000, test acc = 0.7834, test_sen =0.7868, test_spe =0.9783, test_f1 =0.7710
[2024-10-30 19:13:33] Evaluate_01: epoch = 1000 train time = 32 s train loss = 0.041787 train acc = 0.9909, test acc = 0.7850, test_sen =0.7869, test_spe =0.9785, test_f1 =0.7716/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 19:15:05: Evaluate 5 random ConvNet, ACCmean = 0.7819 ACCstd = 0.0060
-------------------------
2024-10-30 19:15:05: Evaluate 5 random ConvNet, SENmean = 0.7835 SENstd = 0.0059
-------------------------
2024-10-30 19:15:05: Evaluate 5 random ConvNet, SPEmean = 0.9782 SPEstd = 0.0006
-------------------------
2024-10-30 19:15:05: Evaluate 5 random ConvNet, F!mean = 0.7685 F!std = 0.0059
-------------------------
2024-10-30 19:15:05: Evaluate 5 random ConvNet, mean = 0.7819 std = 0.0060
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 19:15:06: [2024-10-30 19:15:06] iter = 14000, loss = 1.8897
2024-10-30 19:15:09: [2024-10-30 19:15:09] iter = 14010, loss = 2.3417
2024-10-30 19:15:13: [2024-10-30 19:15:13] iter = 14020, loss = 2.4512
2024-10-30 19:15:17: [2024-10-30 19:15:17] iter = 14030, loss = 2.9719
2024-10-30 19:15:19: [2024-10-30 19:15:19] iter = 14040, loss = 2.0989
2024-10-30 19:15:23: [2024-10-30 19:15:23] iter = 14050, loss = 2.0729
2024-10-30 19:15:27: [2024-10-30 19:15:27] iter = 14060, loss = 2.2961
2024-10-30 19:15:31: [2024-10-30 19:15:31] iter = 14070, loss = 1.9627
2024-10-30 19:15:34: [2024-10-30 19:15:34] iter = 14080, loss = 3.2111
2024-10-30 19:15:38: [2024-10-30 19:15:38] iter = 14090, loss = 2.0231
2024-10-30 19:15:42: [2024-10-30 19:15:42] iter = 14100, loss = 1.9343
2024-10-30 19:15:46: [2024-10-30 19:15:46] iter = 14110, loss = 1.9360
2024-10-30 19:15:51: [2024-10-30 19:15:51] iter = 14120, loss = 3.9597
2024-10-30 19:15:55: [2024-10-30 19:15:55] iter = 14130, loss = 1.8999
2024-10-30 19:15:58: [2024-10-30 19:15:58] iter = 14140, loss = 3.6544
2024-10-30 19:16:02: [2024-10-30 19:16:02] iter = 14150, loss = 1.9244
2024-10-30 19:16:06: [2024-10-30 19:16:06] iter = 14160, loss = 3.6356
2024-10-30 19:16:09: [2024-10-30 19:16:09] iter = 14170, loss = 2.2282
2024-10-30 19:16:13: [2024-10-30 19:16:13] iter = 14180, loss = 2.2468
2024-10-30 19:16:17: [2024-10-30 19:16:17] iter = 14190, loss = 2.3284
2024-10-30 19:16:20: [2024-10-30 19:16:20] iter = 14200, loss = 2.9821
2024-10-30 19:16:23: [2024-10-30 19:16:23] iter = 14210, loss = 2.1884
2024-10-30 19:16:25: [2024-10-30 19:16:25] iter = 14220, loss = 2.3858
2024-10-30 19:16:29: [2024-10-30 19:16:29] iter = 14230, loss = 2.0041
2024-10-30 19:16:32: [2024-10-30 19:16:32] iter = 14240, loss = 2.9178
2024-10-30 19:16:36: [2024-10-30 19:16:36] iter = 14250, loss = 1.8498
2024-10-30 19:16:39: [2024-10-30 19:16:39] iter = 14260, loss = 2.3337
2024-10-30 19:16:43: [2024-10-30 19:16:43] iter = 14270, loss = 2.8541
2024-10-30 19:16:47: [2024-10-30 19:16:47] iter = 14280, loss = 2.2205
2024-10-30 19:16:52: [2024-10-30 19:16:52] iter = 14290, loss = 1.8539
2024-10-30 19:16:55: [2024-10-30 19:16:55] iter = 14300, loss = 2.2912
2024-10-30 19:17:00: [2024-10-30 19:17:00] iter = 14310, loss = 2.1083
2024-10-30 19:17:04: [2024-10-30 19:17:04] iter = 14320, loss = 2.4792
2024-10-30 19:17:08: [2024-10-30 19:17:08] iter = 14330, loss = 2.4111
2024-10-30 19:17:12: [2024-10-30 19:17:12] iter = 14340, loss = 1.9522
2024-10-30 19:17:16: [2024-10-30 19:17:16] iter = 14350, loss = 2.1148
2024-10-30 19:17:20: [2024-10-30 19:17:20] iter = 14360, loss = 2.0247
2024-10-30 19:17:22: [2024-10-30 19:17:22] iter = 14370, loss = 2.2925
2024-10-30 19:17:25: [2024-10-30 19:17:25] iter = 14380, loss = 2.6385
2024-10-30 19:17:28: [2024-10-30 19:17:28] iter = 14390, loss = 3.1381
2024-10-30 19:17:31: [2024-10-30 19:17:31] iter = 14400, loss = 1.7444
2024-10-30 19:17:34: [2024-10-30 19:17:34] iter = 14410, loss = 1.6706
2024-10-30 19:17:37: [2024-10-30 19:17:37] iter = 14420, loss = 2.3753
2024-10-30 19:17:41: [2024-10-30 19:17:41] iter = 14430, loss = 1.8574
2024-10-30 19:17:43: [2024-10-30 19:17:43] iter = 14440, loss = 3.0232
2024-10-30 19:17:47: [2024-10-30 19:17:47] iter = 14450, loss = 2.4502
2024-10-30 19:17:50: [2024-10-30 19:17:50] iter = 14460, loss = 3.0992
2024-10-30 19:17:53: [2024-10-30 19:17:53] iter = 14470, loss = 1.8165
2024-10-30 19:17:56: [2024-10-30 19:17:56] iter = 14480, loss = 3.0598
2024-10-30 19:17:58: [2024-10-30 19:17:58] iter = 14490, loss = 2.2532
2024-10-30 19:18:01: [2024-10-30 19:18:01] iter = 14500, loss = 1.7688
2024-10-30 19:18:04: [2024-10-30 19:18:04] iter = 14510, loss = 4.7081
2024-10-30 19:18:07: [2024-10-30 19:18:07] iter = 14520, loss = 1.8898
2024-10-30 19:18:10: [2024-10-30 19:18:10] iter = 14530, loss = 2.2701
2024-10-30 19:18:14: [2024-10-30 19:18:14] iter = 14540, loss = 1.8160
2024-10-30 19:18:17: [2024-10-30 19:18:17] iter = 14550, loss = 1.7703
2024-10-30 19:18:19: [2024-10-30 19:18:19] iter = 14560, loss = 2.9575
2024-10-30 19:18:22: [2024-10-30 19:18:22] iter = 14570, loss = 2.1836
2024-10-30 19:18:26: [2024-10-30 19:18:26] iter = 14580, loss = 1.9300
2024-10-30 19:18:30: [2024-10-30 19:18:30] iter = 14590, loss = 2.2664
2024-10-30 19:18:34: [2024-10-30 19:18:34] iter = 14600, loss = 2.1499
2024-10-30 19:18:39: [2024-10-30 19:18:39] iter = 14610, loss = 2.3473
2024-10-30 19:18:42: [2024-10-30 19:18:42] iter = 14620, loss = 2.2067
2024-10-30 19:18:47: [2024-10-30 19:18:47] iter = 14630, loss = 2.0436
2024-10-30 19:18:50: [2024-10-30 19:18:50] iter = 14640, loss = 1.8779
2024-10-30 19:18:54: [2024-10-30 19:18:54] iter = 14650, loss = 2.4993
2024-10-30 19:18:57: [2024-10-30 19:18:57] iter = 14660, loss = 1.6900
2024-10-30 19:19:01: [2024-10-30 19:19:01] iter = 14670, loss = 2.6979
2024-10-30 19:19:05: [2024-10-30 19:19:05] iter = 14680, loss = 2.2479
2024-10-30 19:19:08: [2024-10-30 19:19:08] iter = 14690, loss = 2.4139
2024-10-30 19:19:11: [2024-10-30 19:19:11] iter = 14700, loss = 2.2780
2024-10-30 19:19:15: [2024-10-30 19:19:15] iter = 14710, loss = 1.9854
2024-10-30 19:19:19: [2024-10-30 19:19:19] iter = 14720, loss = 5.7313
2024-10-30 19:19:22: [2024-10-30 19:19:22] iter = 14730, loss = 1.9343
2024-10-30 19:19:25: [2024-10-30 19:19:25] iter = 14740, loss = 2.6534
2024-10-30 19:19:28: [2024-10-30 19:19:28] iter = 14750, loss = 1.8650
2024-10-30 19:19:31: [2024-10-30 19:19:31] iter = 14760, loss = 1.9496
2024-10-30 19:19:35: [2024-10-30 19:19:35] iter = 14770, loss = 2.0053
2024-10-30 19:19:39: [2024-10-30 19:19:39] iter = 14780, loss = 2.7163
2024-10-30 19:19:43: [2024-10-30 19:19:43] iter = 14790, loss = 2.1990
2024-10-30 19:19:46: [2024-10-30 19:19:46] iter = 14800, loss = 1.9768
2024-10-30 19:19:51: [2024-10-30 19:19:51] iter = 14810, loss = 1.8608
2024-10-30 19:19:55: [2024-10-30 19:19:55] iter = 14820, loss = 2.0385
2024-10-30 19:19:59: [2024-10-30 19:19:59] iter = 14830, loss = 3.2133
2024-10-30 19:20:04: [2024-10-30 19:20:04] iter = 14840, loss = 1.9983
2024-10-30 19:20:08: [2024-10-30 19:20:08] iter = 14850, loss = 1.8488
2024-10-30 19:20:13: [2024-10-30 19:20:13] iter = 14860, loss = 3.9640
2024-10-30 19:20:16: [2024-10-30 19:20:16] iter = 14870, loss = 2.0869
2024-10-30 19:20:20: [2024-10-30 19:20:20] iter = 14880, loss = 2.0844
2024-10-30 19:20:23: [2024-10-30 19:20:23] iter = 14890, loss = 1.6830
2024-10-30 19:20:27: [2024-10-30 19:20:27] iter = 14900, loss = 2.4142
2024-10-30 19:20:31: [2024-10-30 19:20:31] iter = 14910, loss = 2.6003
2024-10-30 19:20:35: [2024-10-30 19:20:35] iter = 14920, loss = 2.1732
2024-10-30 19:20:40: [2024-10-30 19:20:40] iter = 14930, loss = 3.2104
2024-10-30 19:20:43: [2024-10-30 19:20:43] iter = 14940, loss = 1.9970
2024-10-30 19:20:47: [2024-10-30 19:20:47] iter = 14950, loss = 3.4283
2024-10-30 19:20:50: [2024-10-30 19:20:50] iter = 14960, loss = 2.5156
2024-10-30 19:20:54: [2024-10-30 19:20:54] iter = 14970, loss = 2.6871
2024-10-30 19:20:58: [2024-10-30 19:20:58] iter = 14980, loss = 1.9772
2024-10-30 19:21:02: [2024-10-30 19:21:02] iter = 14990, loss = 1.8011
2024-10-30 19:21:05: [2024-10-30 19:21:05] iter = 15000, loss = 2.0006
2024-10-30 19:21:09: [2024-10-30 19:21:09] iter = 15010, loss = 3.7216
2024-10-30 19:21:12: [2024-10-30 19:21:12] iter = 15020, loss = 2.1632
2024-10-30 19:21:16: [2024-10-30 19:21:16] iter = 15030, loss = 3.1778
2024-10-30 19:21:19: [2024-10-30 19:21:19] iter = 15040, loss = 2.3549
2024-10-30 19:21:24: [2024-10-30 19:21:24] iter = 15050, loss = 2.1322
2024-10-30 19:21:28: [2024-10-30 19:21:28] iter = 15060, loss = 2.7245
2024-10-30 19:21:32: [2024-10-30 19:21:32] iter = 15070, loss = 1.8489
2024-10-30 19:21:36: [2024-10-30 19:21:36] iter = 15080, loss = 2.6038
2024-10-30 19:21:39: [2024-10-30 19:21:39] iter = 15090, loss = 1.9316
2024-10-30 19:21:41: [2024-10-30 19:21:41] iter = 15100, loss = 2.1977
2024-10-30 19:21:45: [2024-10-30 19:21:45] iter = 15110, loss = 3.0315
2024-10-30 19:21:48: [2024-10-30 19:21:48] iter = 15120, loss = 2.2502
2024-10-30 19:21:52: [2024-10-30 19:21:52] iter = 15130, loss = 1.8137
2024-10-30 19:21:55: [2024-10-30 19:21:55] iter = 15140, loss = 6.0881
2024-10-30 19:21:59: [2024-10-30 19:21:59] iter = 15150, loss = 3.7485
2024-10-30 19:22:02: [2024-10-30 19:22:02] iter = 15160, loss = 2.1620
2024-10-30 19:22:05: [2024-10-30 19:22:05] iter = 15170, loss = 1.8451
2024-10-30 19:22:08: [2024-10-30 19:22:08] iter = 15180, loss = 2.4528
2024-10-30 19:22:12: [2024-10-30 19:22:12] iter = 15190, loss = 2.4066
2024-10-30 19:22:16: [2024-10-30 19:22:16] iter = 15200, loss = 2.4386
2024-10-30 19:22:19: [2024-10-30 19:22:19] iter = 15210, loss = 1.9505
2024-10-30 19:22:23: [2024-10-30 19:22:23] iter = 15220, loss = 2.0848
2024-10-30 19:22:27: [2024-10-30 19:22:27] iter = 15230, loss = 2.4753
2024-10-30 19:22:31: [2024-10-30 19:22:31] iter = 15240, loss = 4.6853
2024-10-30 19:22:35: [2024-10-30 19:22:35] iter = 15250, loss = 2.4622
2024-10-30 19:22:37: [2024-10-30 19:22:37] iter = 15260, loss = 1.9406
2024-10-30 19:22:40: [2024-10-30 19:22:40] iter = 15270, loss = 3.2624
2024-10-30 19:22:44: [2024-10-30 19:22:44] iter = 15280, loss = 2.1061
2024-10-30 19:22:48: [2024-10-30 19:22:48] iter = 15290, loss = 1.9468
2024-10-30 19:22:51: [2024-10-30 19:22:50] iter = 15300, loss = 1.9270
2024-10-30 19:22:54: [2024-10-30 19:22:54] iter = 15310, loss = 2.6055
2024-10-30 19:22:57: [2024-10-30 19:22:57] iter = 15320, loss = 1.6833
2024-10-30 19:23:01: [2024-10-30 19:23:01] iter = 15330, loss = 2.4426
2024-10-30 19:23:04: [2024-10-30 19:23:04] iter = 15340, loss = 2.4754
2024-10-30 19:23:08: [2024-10-30 19:23:08] iter = 15350, loss = 2.0853
2024-10-30 19:23:12: [2024-10-30 19:23:12] iter = 15360, loss = 3.0157
2024-10-30 19:23:16: [2024-10-30 19:23:16] iter = 15370, loss = 2.6214
2024-10-30 19:23:19: [2024-10-30 19:23:19] iter = 15380, loss = 2.1230
2024-10-30 19:23:23: [2024-10-30 19:23:23] iter = 15390, loss = 2.5660
2024-10-30 19:23:26: [2024-10-30 19:23:26] iter = 15400, loss = 1.8861
2024-10-30 19:23:30: [2024-10-30 19:23:30] iter = 15410, loss = 5.1099
2024-10-30 19:23:33: [2024-10-30 19:23:33] iter = 15420, loss = 1.8627
2024-10-30 19:23:37: [2024-10-30 19:23:37] iter = 15430, loss = 2.0798
2024-10-30 19:23:41: [2024-10-30 19:23:41] iter = 15440, loss = 2.7344
2024-10-30 19:23:45: [2024-10-30 19:23:45] iter = 15450, loss = 2.2354
2024-10-30 19:23:49: [2024-10-30 19:23:49] iter = 15460, loss = 2.6984
2024-10-30 19:23:53: [2024-10-30 19:23:53] iter = 15470, loss = 1.8531
2024-10-30 19:23:57: [2024-10-30 19:23:57] iter = 15480, loss = 2.3965
2024-10-30 19:24:00: [2024-10-30 19:24:00] iter = 15490, loss = 2.4424
2024-10-30 19:24:03: [2024-10-30 19:24:03] iter = 15500, loss = 2.5555
2024-10-30 19:24:06: [2024-10-30 19:24:06] iter = 15510, loss = 1.9816
2024-10-30 19:24:10: [2024-10-30 19:24:10] iter = 15520, loss = 2.8826
2024-10-30 19:24:13: [2024-10-30 19:24:13] iter = 15530, loss = 2.7933
2024-10-30 19:24:17: [2024-10-30 19:24:17] iter = 15540, loss = 2.5045
2024-10-30 19:24:21: [2024-10-30 19:24:21] iter = 15550, loss = 2.2837
2024-10-30 19:24:24: [2024-10-30 19:24:24] iter = 15560, loss = 2.3754
2024-10-30 19:24:28: [2024-10-30 19:24:28] iter = 15570, loss = 3.0076
2024-10-30 19:24:32: [2024-10-30 19:24:32] iter = 15580, loss = 3.3653
2024-10-30 19:24:35: [2024-10-30 19:24:35] iter = 15590, loss = 1.8840
2024-10-30 19:24:39: [2024-10-30 19:24:39] iter = 15600, loss = 2.0240
2024-10-30 19:24:43: [2024-10-30 19:24:43] iter = 15610, loss = 1.9968
2024-10-30 19:24:47: [2024-10-30 19:24:47] iter = 15620, loss = 2.1928
2024-10-30 19:24:51: [2024-10-30 19:24:51] iter = 15630, loss = 2.2186
2024-10-30 19:24:55: [2024-10-30 19:24:55] iter = 15640, loss = 2.1067
2024-10-30 19:24:59: [2024-10-30 19:24:59] iter = 15650, loss = 2.5040
2024-10-30 19:25:02: [2024-10-30 19:25:02] iter = 15660, loss = 2.0948
2024-10-30 19:25:06: [2024-10-30 19:25:06] iter = 15670, loss = 2.4923
2024-10-30 19:25:11: [2024-10-30 19:25:11] iter = 15680, loss = 2.3332
2024-10-30 19:25:14: [2024-10-30 19:25:14] iter = 15690, loss = 1.9545
2024-10-30 19:25:17: [2024-10-30 19:25:17] iter = 15700, loss = 3.7243
2024-10-30 19:25:21: [2024-10-30 19:25:21] iter = 15710, loss = 2.5203
2024-10-30 19:25:24: [2024-10-30 19:25:24] iter = 15720, loss = 2.2326
2024-10-30 19:25:28: [2024-10-30 19:25:28] iter = 15730, loss = 2.2499
2024-10-30 19:25:32: [2024-10-30 19:25:32] iter = 15740, loss = 1.9685
2024-10-30 19:25:35: [2024-10-30 19:25:35] iter = 15750, loss = 2.8686
2024-10-30 19:25:40: [2024-10-30 19:25:40] iter = 15760, loss = 2.4466
2024-10-30 19:25:44: [2024-10-30 19:25:44] iter = 15770, loss = 2.4160
2024-10-30 19:25:48: [2024-10-30 19:25:48] iter = 15780, loss = 2.8814
2024-10-30 19:25:53: [2024-10-30 19:25:53] iter = 15790, loss = 2.3469
2024-10-30 19:25:57: [2024-10-30 19:25:57] iter = 15800, loss = 2.8115
2024-10-30 19:26:02: [2024-10-30 19:26:02] iter = 15810, loss = 2.2771
2024-10-30 19:26:05: [2024-10-30 19:26:05] iter = 15820, loss = 2.0411
2024-10-30 19:26:09: [2024-10-30 19:26:09] iter = 15830, loss = 2.6039
2024-10-30 19:26:13: [2024-10-30 19:26:13] iter = 15840, loss = 2.5427
2024-10-30 19:26:16: [2024-10-30 19:26:16] iter = 15850, loss = 2.6559
2024-10-30 19:26:19: [2024-10-30 19:26:19] iter = 15860, loss = 2.0397
2024-10-30 19:26:22: [2024-10-30 19:26:22] iter = 15870, loss = 3.4000
2024-10-30 19:26:26: [2024-10-30 19:26:26] iter = 15880, loss = 2.9206
2024-10-30 19:26:31: [2024-10-30 19:26:31] iter = 15890, loss = 2.2356
2024-10-30 19:26:35: [2024-10-30 19:26:35] iter = 15900, loss = 2.2071
2024-10-30 19:26:39: [2024-10-30 19:26:39] iter = 15910, loss = 2.0243
2024-10-30 19:26:44: [2024-10-30 19:26:44] iter = 15920, loss = 1.9067
2024-10-30 19:26:48: [2024-10-30 19:26:48] iter = 15930, loss = 1.9957
2024-10-30 19:26:52: [2024-10-30 19:26:52] iter = 15940, loss = 2.2167
2024-10-30 19:26:57: [2024-10-30 19:26:56] iter = 15950, loss = 2.4335
2024-10-30 19:27:00: [2024-10-30 19:27:00] iter = 15960, loss = 2.0468
2024-10-30 19:27:03: [2024-10-30 19:27:03] iter = 15970, loss = 2.5750
2024-10-30 19:27:07: [2024-10-30 19:27:07] iter = 15980, loss = 1.9320
2024-10-30 19:27:11: [2024-10-30 19:27:11] iter = 15990, loss = 2.1615
2024-10-30 19:27:13: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
2024-10-30 19:27:13: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 19:27:13: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 33893}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 19:29:38: Evaluate 5 random ConvNet, ACCmean = 0.7731 ACCstd = 0.0037
-------------------------
2024-10-30 19:29:38: Evaluate 5 random ConvNet, SENmean = 0.7735 SENstd = 0.0040
-------------------------
2024-10-30 19:29:38: Evaluate 5 random ConvNet, SPEmean = 0.9772 SPEstd = 0.0004
-------------------------
2024-10-30 19:29:38: Evaluate 5 random ConvNet, F!mean = 0.7592 F!std = 0.0042
-------------------------
2024-10-30 19:29:38: Evaluate 5 random ConvNet, mean = 0.7731 std = 0.0037
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 19:29:38: [2024-10-30 19:29:38] iter = 16000, loss = 2.3105
2024-10-30 19:29:42: [2024-10-30 19:29:42] iter = 16010, loss = 2.5469
2024-10-30 19:29:45: [2024-10-30 19:29:45] iter = 16020, loss = 1.8396
2024-10-30 19:29:48: [2024-10-30 19:29:48] iter = 16030, loss = 2.5003
2024-10-30 19:29:51: [2024-10-30 19:29:51] iter = 16040, loss = 2.6264
2024-10-30 19:29:55: [2024-10-30 19:29:55] iter = 16050, loss = 2.0746
2024-10-30 19:29:59: [2024-10-30 19:29:59] iter = 16060, loss = 2.4718
2024-10-30 19:30:02: [2024-10-30 19:30:02] iter = 16070, loss = 3.0042
2024-10-30 19:30:06: [2024-10-30 19:30:06] iter = 16080, loss = 1.8712
2024-10-30 19:30:09: [2024-10-30 19:30:09] iter = 16090, loss = 2.4158
2024-10-30 19:30:13: [2024-10-30 19:30:13] iter = 16100, loss = 2.1512
2024-10-30 19:30:17: [2024-10-30 19:30:17] iter = 16110, loss = 1.9726
2024-10-30 19:30:20: [2024-10-30 19:30:20] iter = 16120, loss = 2.1692
2024-10-30 19:30:24: [2024-10-30 19:30:24] iter = 16130, loss = 2.0348
2024-10-30 19:30:28: [2024-10-30 19:30:28] iter = 16140, loss = 1.9410
2024-10-30 19:30:32: [2024-10-30 19:30:32] iter = 16150, loss = 2.1861
2024-10-30 19:30:35: [2024-10-30 19:30:35] iter = 16160, loss = 2.9970
2024-10-30 19:30:39: [2024-10-30 19:30:39] iter = 16170, loss = 1.6605
2024-10-30 19:30:43: [2024-10-30 19:30:43] iter = 16180, loss = 2.8964
2024-10-30 19:30:46: [2024-10-30 19:30:46] iter = 16190, loss = 1.7905
2024-10-30 19:30:50: [2024-10-30 19:30:50] iter = 16200, loss = 5.6827
2024-10-30 19:30:54: [2024-10-30 19:30:54] iter = 16210, loss = 2.5228
2024-10-30 19:30:57: [2024-10-30 19:30:57] iter = 16220, loss = 3.0565
2024-10-30 19:31:00: [2024-10-30 19:31:00] iter = 16230, loss = 3.2510
2024-10-30 19:31:04: [2024-10-30 19:31:04] iter = 16240, loss = 1.9157
2024-10-30 19:31:09: [2024-10-30 19:31:09] iter = 16250, loss = 2.4033
2024-10-30 19:31:12: [2024-10-30 19:31:12] iter = 16260, loss = 2.5606
2024-10-30 19:31:16: [2024-10-30 19:31:16] iter = 16270, loss = 2.1446
2024-10-30 19:31:19: [2024-10-30 19:31:19] iter = 16280, loss = 2.7910
2024-10-30 19:31:24: [2024-10-30 19:31:24] iter = 16290, loss = 2.1385
2024-10-30 19:31:28: [2024-10-30 19:31:28] iter = 16300, loss = 2.3024
2024-10-30 19:31:31: [2024-10-30 19:31:31] iter = 16310, loss = 1.8284
2024-10-30 19:31:34: [2024-10-30 19:31:34] iter = 16320, loss = 2.5465
2024-10-30 19:31:38: [2024-10-30 19:31:38] iter = 16330, loss = 2.0704
2024-10-30 19:31:41: [2024-10-30 19:31:41] iter = 16340, loss = 2.2030
2024-10-30 19:31:45: [2024-10-30 19:31:45] iter = 16350, loss = 2.9928
2024-10-30 19:31:48: [2024-10-30 19:31:48] iter = 16360, loss = 4.1418
2024-10-30 19:31:52: [2024-10-30 19:31:52] iter = 16370, loss = 2.9994
2024-10-30 19:31:56: [2024-10-30 19:31:56] iter = 16380, loss = 2.1878
2024-10-30 19:31:59: [2024-10-30 19:31:59] iter = 16390, loss = 1.7330
2024-10-30 19:32:02: [2024-10-30 19:32:02] iter = 16400, loss = 1.9947
2024-10-30 19:32:05: [2024-10-30 19:32:05] iter = 16410, loss = 2.0948
2024-10-30 19:32:09: [2024-10-30 19:32:09] iter = 16420, loss = 5.0007
2024-10-30 19:32:13: [2024-10-30 19:32:13] iter = 16430, loss = 2.2372
2024-10-30 19:32:16: [2024-10-30 19:32:16] iter = 16440, loss = 2.5106
2024-10-30 19:32:20: [2024-10-30 19:32:20] iter = 16450, loss = 3.7965
2024-10-30 19:32:23: [2024-10-30 19:32:23] iter = 16460, loss = 2.3837
2024-10-30 19:32:28: [2024-10-30 19:32:28] iter = 16470, loss = 2.6351
2024-10-30 19:32:31: [2024-10-30 19:32:31] iter = 16480, loss = 2.7593
2024-10-30 19:32:33: [2024-10-30 19:32:33] iter = 16490, loss = 2.2219
2024-10-30 19:32:37: [2024-10-30 19:32:37] iter = 16500, loss = 1.9341
2024-10-30 19:32:40: [2024-10-30 19:32:40] iter = 16510, loss = 2.5446
2024-10-30 19:32:44: [2024-10-30 19:32:44] iter = 16520, loss = 1.6884
2024-10-30 19:32:48: [2024-10-30 19:32:48] iter = 16530, loss = 1.8088
2024-10-30 19:32:52: [2024-10-30 19:32:52] iter = 16540, loss = 1.8430
2024-10-30 19:32:56: [2024-10-30 19:32:56] iter = 16550, loss = 5.1677
2024-10-30 19:33:00: [2024-10-30 19:33:00] iter = 16560, loss = 1.8356
2024-10-30 19:33:04: [2024-10-30 19:33:04] iter = 16570, loss = 1.9570
2024-10-30 19:33:09: [2024-10-30 19:33:09] iter = 16580, loss = 4.3646
2024-10-30 19:33:12: [2024-10-30 19:33:12] iter = 16590, loss = 2.4522
2024-10-30 19:33:16: [2024-10-30 19:33:16] iter = 16600, loss = 3.1270
2024-10-30 19:33:20: [2024-10-30 19:33:20] iter = 16610, loss = 3.9172
2024-10-30 19:33:25: [2024-10-30 19:33:25] iter = 16620, loss = 2.2130
2024-10-30 19:33:29: [2024-10-30 19:33:29] iter = 16630, loss = 1.7383
2024-10-30 19:33:32: [2024-10-30 19:33:32] iter = 16640, loss = 3.7891
2024-10-30 19:33:36: [2024-10-30 19:33:36] iter = 16650, loss = 3.1765
2024-10-30 19:33:40: [2024-10-30 19:33:40] iter = 16660, loss = 1.8475
2024-10-30 19:33:44: [2024-10-30 19:33:44] iter = 16670, loss = 1.9828
2024-10-30 19:33:47: [2024-10-30 19:33:47] iter = 16680, loss = 4.0346
2024-10-30 19:33:52: [2024-10-30 19:33:52] iter = 16690, loss = 1.9269
2024-10-30 19:33:57: [2024-10-30 19:33:57] iter = 16700, loss = 2.6951
2024-10-30 19:34:00: [2024-10-30 19:34:00] iter = 16710, loss = 1.8933
2024-10-30 19:34:03: [2024-10-30 19:34:03] iter = 16720, loss = 3.3743
2024-10-30 19:34:06: [2024-10-30 19:34:06] iter = 16730, loss = 2.6533
2024-10-30 19:34:09: [2024-10-30 19:34:09] iter = 16740, loss = 1.8602
2024-10-30 19:34:14: [2024-10-30 19:34:14] iter = 16750, loss = 2.5059
2024-10-30 19:34:19: [2024-10-30 19:34:19] iter = 16760, loss = 2.9744
2024-10-30 19:34:23: [2024-10-30 19:34:23] iter = 16770, loss = 2.0642
2024-10-30 19:34:27: [2024-10-30 19:34:27] iter = 16780, loss = 1.9762
2024-10-30 19:34:31: [2024-10-30 19:34:31] iter = 16790, loss = 2.6346
2024-10-30 19:34:35: [2024-10-30 19:34:35] iter = 16800, loss = 2.0883
2024-10-30 19:34:40: [2024-10-30 19:34:40] iter = 16810, loss = 2.2313
2024-10-30 19:34:44: [2024-10-30 19:34:44] iter = 16820, loss = 1.8789
2024-10-30 19:34:49: [2024-10-30 19:34:49] iter = 16830, loss = 1.9115
2024-10-30 19:34:53: [2024-10-30 19:34:53] iter = 16840, loss = 3.0098
2024-10-30 19:34:56: [2024-10-30 19:34:56] iter = 16850, loss = 2.4769
2024-10-30 19:35:00: [2024-10-30 19:35:00] iter = 16860, loss = 2.0459
2024-10-30 19:35:05: [2024-10-30 19:35:05] iter = 16870, loss = 2.1201
2024-10-30 19:35:08: [2024-10-30 19:35:08] iter = 16880, loss = 3.0133
2024-10-30 19:35:11: [2024-10-30 19:35:11] iter = 16890, loss = 2.2681
2024-10-30 19:35:15: [2024-10-30 19:35:15] iter = 16900, loss = 2.2011
2024-10-30 19:35:18: [2024-10-30 19:35:18] iter = 16910, loss = 1.9849
2024-10-30 19:35:22: [2024-10-30 19:35:22] iter = 16920, loss = 1.9913
2024-10-30 19:35:25: [2024-10-30 19:35:25] iter = 16930, loss = 3.2947
2024-10-30 19:35:27: [2024-10-30 19:35:27] iter = 16940, loss = 1.9672
2024-10-30 19:35:30: [2024-10-30 19:35:30] iter = 16950, loss = 2.3430
2024-10-30 19:35:33: [2024-10-30 19:35:33] iter = 16960, loss = 3.0200
2024-10-30 19:35:36: [2024-10-30 19:35:36] iter = 16970, loss = 1.9055
2024-10-30 19:35:40: [2024-10-30 19:35:40] iter = 16980, loss = 2.5382
2024-10-30 19:35:43: [2024-10-30 19:35:43] iter = 16990, loss = 2.1277
2024-10-30 19:35:46: [2024-10-30 19:35:46] iter = 17000, loss = 3.0514
2024-10-30 19:35:49: [2024-10-30 19:35:49] iter = 17010, loss = 3.9694
2024-10-30 19:35:53: [2024-10-30 19:35:53] iter = 17020, loss = 2.1490
2024-10-30 19:35:57: [2024-10-30 19:35:57] iter = 17030, loss = 2.2604
2024-10-30 19:36:01: [2024-10-30 19:36:01] iter = 17040, loss = 2.0062
2024-10-30 19:36:03: [2024-10-30 19:36:03] iter = 17050, loss = 2.2374
2024-10-30 19:36:07: [2024-10-30 19:36:07] iter = 17060, loss = 2.4160
2024-10-30 19:36:10: [2024-10-30 19:36:10] iter = 17070, loss = 1.8303
2024-10-30 19:36:14: [2024-10-30 19:36:14] iter = 17080, loss = 2.2505
2024-10-30 19:36:17: [2024-10-30 19:36:17] iter = 17090, loss = 3.8424
2024-10-30 19:36:20: [2024-10-30 19:36:20] iter = 17100, loss = 1.6862
2024-10-30 19:36:23: [2024-10-30 19:36:23] iter = 17110, loss = 1.8640
2024-10-30 19:36:27: [2024-10-30 19:36:27] iter = 17120, loss = 1.9836
2024-10-30 19:36:31: [2024-10-30 19:36:31] iter = 17130, loss = 2.0205
2024-10-30 19:36:34: [2024-10-30 19:36:34] iter = 17140, loss = 2.6167
2024-10-30 19:36:37: [2024-10-30 19:36:37] iter = 17150, loss = 2.4394
2024-10-30 19:36:41: [2024-10-30 19:36:41] iter = 17160, loss = 2.2273
2024-10-30 19:36:44: [2024-10-30 19:36:44] iter = 17170, loss = 2.4672
2024-10-30 19:36:48: [2024-10-30 19:36:48] iter = 17180, loss = 2.2835
2024-10-30 19:36:52: [2024-10-30 19:36:52] iter = 17190, loss = 2.6307
2024-10-30 19:36:56: [2024-10-30 19:36:56] iter = 17200, loss = 2.9896
2024-10-30 19:37:00: [2024-10-30 19:37:00] iter = 17210, loss = 2.3522
2024-10-30 19:37:03: [2024-10-30 19:37:03] iter = 17220, loss = 3.7474
2024-10-30 19:37:06: [2024-10-30 19:37:06] iter = 17230, loss = 1.7821
2024-10-30 19:37:09: [2024-10-30 19:37:09] iter = 17240, loss = 2.1235
2024-10-30 19:37:14: [2024-10-30 19:37:14] iter = 17250, loss = 2.7538
2024-10-30 19:37:18: [2024-10-30 19:37:18] iter = 17260, loss = 2.8525
2024-10-30 19:37:22: [2024-10-30 19:37:22] iter = 17270, loss = 1.9742
2024-10-30 19:37:26: [2024-10-30 19:37:26] iter = 17280, loss = 1.8576
2024-10-30 19:37:30: [2024-10-30 19:37:30] iter = 17290, loss = 1.9597
2024-10-30 19:37:33: [2024-10-30 19:37:33] iter = 17300, loss = 2.1652
2024-10-30 19:37:37: [2024-10-30 19:37:37] iter = 17310, loss = 2.4153
2024-10-30 19:37:41: [2024-10-30 19:37:41] iter = 17320, loss = 2.4344
2024-10-30 19:37:44: [2024-10-30 19:37:44] iter = 17330, loss = 2.6276
2024-10-30 19:37:48: [2024-10-30 19:37:48] iter = 17340, loss = 2.3170
2024-10-30 19:37:52: [2024-10-30 19:37:52] iter = 17350, loss = 1.8810
2024-10-30 19:37:56: [2024-10-30 19:37:56] iter = 17360, loss = 2.4702
2024-10-30 19:38:00: [2024-10-30 19:38:00] iter = 17370, loss = 2.7628
2024-10-30 19:38:03: [2024-10-30 19:38:03] iter = 17380, loss = 2.0568
2024-10-30 19:38:06: [2024-10-30 19:38:06] iter = 17390, loss = 3.0970
2024-10-30 19:38:09: [2024-10-30 19:38:09] iter = 17400, loss = 1.9395
2024-10-30 19:38:13: [2024-10-30 19:38:13] iter = 17410, loss = 2.3007
2024-10-30 19:38:18: [2024-10-30 19:38:18] iter = 17420, loss = 1.9888
2024-10-30 19:38:22: [2024-10-30 19:38:22] iter = 17430, loss = 2.1088
2024-10-30 19:38:25: [2024-10-30 19:38:25] iter = 17440, loss = 2.3085
2024-10-30 19:38:30: [2024-10-30 19:38:30] iter = 17450, loss = 2.1658
2024-10-30 19:38:34: [2024-10-30 19:38:34] iter = 17460, loss = 2.4430
2024-10-30 19:38:38: [2024-10-30 19:38:38] iter = 17470, loss = 2.4211
2024-10-30 19:38:41: [2024-10-30 19:38:41] iter = 17480, loss = 2.4380
2024-10-30 19:38:45: [2024-10-30 19:38:45] iter = 17490, loss = 2.3046
2024-10-30 19:38:47: [2024-10-30 19:38:47] iter = 17500, loss = 3.1359
2024-10-30 19:38:51: [2024-10-30 19:38:51] iter = 17510, loss = 2.2389
2024-10-30 19:38:55: [2024-10-30 19:38:55] iter = 17520, loss = 2.0247
2024-10-30 19:38:59: [2024-10-30 19:38:59] iter = 17530, loss = 3.2338
2024-10-30 19:39:03: [2024-10-30 19:39:03] iter = 17540, loss = 2.0639
2024-10-30 19:39:06: [2024-10-30 19:39:06] iter = 17550, loss = 2.1362
2024-10-30 19:39:09: [2024-10-30 19:39:09] iter = 17560, loss = 2.5742
2024-10-30 19:39:12: [2024-10-30 19:39:12] iter = 17570, loss = 4.5506
2024-10-30 19:39:15: [2024-10-30 19:39:15] iter = 17580, loss = 1.9469
2024-10-30 19:39:19: [2024-10-30 19:39:19] iter = 17590, loss = 1.7068
2024-10-30 19:39:22: [2024-10-30 19:39:22] iter = 17600, loss = 1.6954
2024-10-30 19:39:26: [2024-10-30 19:39:26] iter = 17610, loss = 2.5476
2024-10-30 19:39:30: [2024-10-30 19:39:30] iter = 17620, loss = 2.3108
2024-10-30 19:39:34: [2024-10-30 19:39:34] iter = 17630, loss = 1.9229
2024-10-30 19:39:37: [2024-10-30 19:39:37] iter = 17640, loss = 2.1449
2024-10-30 19:39:40: [2024-10-30 19:39:40] iter = 17650, loss = 2.2903
2024-10-30 19:39:43: [2024-10-30 19:39:43] iter = 17660, loss = 2.0146
2024-10-30 19:39:46: [2024-10-30 19:39:46] iter = 17670, loss = 3.5756
2024-10-30 19:39:49: [2024-10-30 19:39:49] iter = 17680, loss = 2.1914
2024-10-30 19:39:53: [2024-10-30 19:39:53] iter = 17690, loss = 1.8813
2024-10-30 19:39:57: [2024-10-30 19:39:57] iter = 17700, loss = 2.4428
2024-10-30 19:40:01: [2024-10-30 19:40:01] iter = 17710, loss = 2.0552
2024-10-30 19:40:06: [2024-10-30 19:40:06] iter = 17720, loss = 2.8404
2024-10-30 19:40:09: [2024-10-30 19:40:09] iter = 17730, loss = 2.9334
2024-10-30 19:40:13: [2024-10-30 19:40:13] iter = 17740, loss = 2.1423
2024-10-30 19:40:17: [2024-10-30 19:40:17] iter = 17750, loss = 2.0426
2024-10-30 19:40:21: [2024-10-30 19:40:21] iter = 17760, loss = 4.6395
2024-10-30 19:40:25: [2024-10-30 19:40:25] iter = 17770, loss = 2.2869
2024-10-30 19:40:30: [2024-10-30 19:40:30] iter = 17780, loss = 4.4310
2024-10-30 19:40:34: [2024-10-30 19:40:34] iter = 17790, loss = 2.4000
2024-10-30 19:40:38: [2024-10-30 19:40:38] iter = 17800, loss = 2.4221
2024-10-30 19:40:42: [2024-10-30 19:40:42] iter = 17810, loss = 2.3135
2024-10-30 19:40:45: [2024-10-30 19:40:45] iter = 17820, loss = 3.5530
2024-10-30 19:40:49: [2024-10-30 19:40:49] iter = 17830, loss = 2.0875
2024-10-30 19:40:54: [2024-10-30 19:40:54] iter = 17840, loss = 1.8522
2024-10-30 19:40:58: [2024-10-30 19:40:58] iter = 17850, loss = 2.7191
2024-10-30 19:41:02: [2024-10-30 19:41:02] iter = 17860, loss = 2.3934
2024-10-30 19:41:06: [2024-10-30 19:41:06] iter = 17870, loss = 1.9910
2024-10-30 19:41:10: [2024-10-30 19:41:10] iter = 17880, loss = 2.4087
2024-10-30 19:41:14: [2024-10-30 19:41:14] iter = 17890, loss = 3.7717
2024-10-30 19:41:19: [2024-10-30 19:41:19] iter = 17900, loss = 2.0027
2024-10-30 19:41:22: [2024-10-30 19:41:22] iter = 17910, loss = 2.1000
2024-10-30 19:41:27: [2024-10-30 19:41:27] iter = 17920, loss = 1.8098
2024-10-30 19:41:31: [2024-10-30 19:41:31] iter = 17930, loss = 2.0986
2024-10-30 19:41:34: [2024-10-30 19:41:34] iter = 17940, loss = 1.7918
2024-10-30 19:41:38: [2024-10-30 19:41:38] iter = 17950, loss = 3.8347
2024-10-30 19:41:43: [2024-10-30 19:41:43] iter = 17960, loss = 2.0885
2024-10-30 19:41:46: [2024-10-30 19:41:46] iter = 17970, loss = 1.9659
2024-10-30 19:41:49: [2024-10-30 19:41:49] iter = 17980, loss = 2.1628
2024-10-30 19:41:54: [2024-10-30 19:41:54] iter = 17990, loss = 1.9269
2024-10-30 19:41:57: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
2024-10-30 19:41:57: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 19:41:57: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 17655}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 19:44:22: Evaluate 5 random ConvNet, ACCmean = 0.7674 ACCstd = 0.0051
-------------------------
2024-10-30 19:44:22: Evaluate 5 random ConvNet, SENmean = 0.7746 SENstd = 0.0043
-------------------------
2024-10-30 19:44:22: Evaluate 5 random ConvNet, SPEmean = 0.9768 SPEstd = 0.0005
-------------------------
2024-10-30 19:44:22: Evaluate 5 random ConvNet, F!mean = 0.7576 F!std = 0.0045
-------------------------
2024-10-30 19:44:22: Evaluate 5 random ConvNet, mean = 0.7674 std = 0.0051
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 19:44:23: [2024-10-30 19:44:23] iter = 18000, loss = 3.3630
2024-10-30 19:44:26: [2024-10-30 19:44:26] iter = 18010, loss = 1.7124
2024-10-30 19:44:29: [2024-10-30 19:44:29] iter = 18020, loss = 2.6746
2024-10-30 19:44:33: [2024-10-30 19:44:33] iter = 18030, loss = 2.5734
2024-10-30 19:44:37: [2024-10-30 19:44:37] iter = 18040, loss = 1.7437
2024-10-30 19:44:41: [2024-10-30 19:44:41] iter = 18050, loss = 1.8252
2024-10-30 19:44:45: [2024-10-30 19:44:45] iter = 18060, loss = 2.2982
2024-10-30 19:44:50: [2024-10-30 19:44:50] iter = 18070, loss = 2.3079
2024-10-30 19:44:53: [2024-10-30 19:44:53] iter = 18080, loss = 3.4349
2024-10-30 19:44:57: [2024-10-30 19:44:57] iter = 18090, loss = 2.6119
2024-10-30 19:45:01: [2024-10-30 19:45:01] iter = 18100, loss = 1.8648
2024-10-30 19:45:05: [2024-10-30 19:45:05] iter = 18110, loss = 1.7675
2024-10-30 19:45:09: [2024-10-30 19:45:09] iter = 18120, loss = 3.4367
2024-10-30 19:45:13: [2024-10-30 19:45:13] iter = 18130, loss = 2.4419
2024-10-30 19:45:17: [2024-10-30 19:45:17] iter = 18140, loss = 2.6941
2024-10-30 19:45:20: [2024-10-30 19:45:20] iter = 18150, loss = 2.1410
2024-10-30 19:45:23: [2024-10-30 19:45:23] iter = 18160, loss = 3.2565
2024-10-30 19:45:26: [2024-10-30 19:45:26] iter = 18170, loss = 1.9001
2024-10-30 19:45:30: [2024-10-30 19:45:30] iter = 18180, loss = 2.3250
2024-10-30 19:45:33: [2024-10-30 19:45:33] iter = 18190, loss = 1.7901
2024-10-30 19:45:36: [2024-10-30 19:45:36] iter = 18200, loss = 2.0298
2024-10-30 19:45:40: [2024-10-30 19:45:40] iter = 18210, loss = 3.1046
2024-10-30 19:45:45: [2024-10-30 19:45:45] iter = 18220, loss = 2.0925
2024-10-30 19:45:49: [2024-10-30 19:45:49] iter = 18230, loss = 6.3611
2024-10-30 19:45:52: [2024-10-30 19:45:52] iter = 18240, loss = 2.4808
2024-10-30 19:45:55: [2024-10-30 19:45:55] iter = 18250, loss = 1.9504
2024-10-30 19:45:59: [2024-10-30 19:45:59] iter = 18260, loss = 2.5240
2024-10-30 19:46:02: [2024-10-30 19:46:02] iter = 18270, loss = 2.1289
2024-10-30 19:46:05: [2024-10-30 19:46:05] iter = 18280, loss = 2.1353
2024-10-30 19:46:09: [2024-10-30 19:46:09] iter = 18290, loss = 2.0293
2024-10-30 19:46:13: [2024-10-30 19:46:13] iter = 18300, loss = 5.4068
2024-10-30 19:46:17: [2024-10-30 19:46:17] iter = 18310, loss = 2.0553
2024-10-30 19:46:21: [2024-10-30 19:46:21] iter = 18320, loss = 3.3879
2024-10-30 19:46:26: [2024-10-30 19:46:26] iter = 18330, loss = 1.9613
2024-10-30 19:46:30: [2024-10-30 19:46:30] iter = 18340, loss = 1.7956
2024-10-30 19:46:34: [2024-10-30 19:46:34] iter = 18350, loss = 1.9463
2024-10-30 19:46:38: [2024-10-30 19:46:38] iter = 18360, loss = 2.2249
2024-10-30 19:46:42: [2024-10-30 19:46:42] iter = 18370, loss = 2.0436
2024-10-30 19:46:46: [2024-10-30 19:46:46] iter = 18380, loss = 1.6383
2024-10-30 19:46:50: [2024-10-30 19:46:50] iter = 18390, loss = 3.0895
2024-10-30 19:46:54: [2024-10-30 19:46:54] iter = 18400, loss = 2.3617
2024-10-30 19:46:57: [2024-10-30 19:46:57] iter = 18410, loss = 2.6678
2024-10-30 19:47:01: [2024-10-30 19:47:01] iter = 18420, loss = 2.4632
2024-10-30 19:47:04: [2024-10-30 19:47:04] iter = 18430, loss = 2.5234
2024-10-30 19:47:07: [2024-10-30 19:47:07] iter = 18440, loss = 2.1359
2024-10-30 19:47:12: [2024-10-30 19:47:12] iter = 18450, loss = 2.4290
2024-10-30 19:47:16: [2024-10-30 19:47:16] iter = 18460, loss = 2.0921
2024-10-30 19:47:20: [2024-10-30 19:47:20] iter = 18470, loss = 2.1571
2024-10-30 19:47:24: [2024-10-30 19:47:24] iter = 18480, loss = 6.8374
2024-10-30 19:47:27: [2024-10-30 19:47:27] iter = 18490, loss = 2.4305
2024-10-30 19:47:31: [2024-10-30 19:47:31] iter = 18500, loss = 2.7953
2024-10-30 19:47:36: [2024-10-30 19:47:36] iter = 18510, loss = 2.6808
2024-10-30 19:47:41: [2024-10-30 19:47:41] iter = 18520, loss = 2.4060
2024-10-30 19:47:45: [2024-10-30 19:47:45] iter = 18530, loss = 2.1979
2024-10-30 19:47:49: [2024-10-30 19:47:49] iter = 18540, loss = 2.0022
2024-10-30 19:47:53: [2024-10-30 19:47:53] iter = 18550, loss = 1.8994
2024-10-30 19:47:57: [2024-10-30 19:47:57] iter = 18560, loss = 2.3247
2024-10-30 19:48:02: [2024-10-30 19:48:02] iter = 18570, loss = 2.9068
2024-10-30 19:48:06: [2024-10-30 19:48:06] iter = 18580, loss = 1.9620
2024-10-30 19:48:10: [2024-10-30 19:48:10] iter = 18590, loss = 1.9931
2024-10-30 19:48:14: [2024-10-30 19:48:14] iter = 18600, loss = 2.0940
2024-10-30 19:48:18: [2024-10-30 19:48:18] iter = 18610, loss = 2.1648
2024-10-30 19:48:21: [2024-10-30 19:48:21] iter = 18620, loss = 2.2680
2024-10-30 19:48:25: [2024-10-30 19:48:25] iter = 18630, loss = 2.9160
2024-10-30 19:48:29: [2024-10-30 19:48:29] iter = 18640, loss = 1.7092
2024-10-30 19:48:34: [2024-10-30 19:48:34] iter = 18650, loss = 3.4263
2024-10-30 19:48:38: [2024-10-30 19:48:38] iter = 18660, loss = 2.2413
2024-10-30 19:48:42: [2024-10-30 19:48:42] iter = 18670, loss = 2.2112
2024-10-30 19:48:47: [2024-10-30 19:48:47] iter = 18680, loss = 1.6462
2024-10-30 19:48:51: [2024-10-30 19:48:51] iter = 18690, loss = 1.9243
2024-10-30 19:48:54: [2024-10-30 19:48:54] iter = 18700, loss = 2.5486
2024-10-30 19:48:58: [2024-10-30 19:48:58] iter = 18710, loss = 2.2373
2024-10-30 19:49:01: [2024-10-30 19:49:01] iter = 18720, loss = 2.6205
2024-10-30 19:49:04: [2024-10-30 19:49:04] iter = 18730, loss = 1.9608
2024-10-30 19:49:07: [2024-10-30 19:49:07] iter = 18740, loss = 2.2020
2024-10-30 19:49:11: [2024-10-30 19:49:11] iter = 18750, loss = 2.8933
2024-10-30 19:49:13: [2024-10-30 19:49:13] iter = 18760, loss = 2.2612
2024-10-30 19:49:17: [2024-10-30 19:49:17] iter = 18770, loss = 3.7390
2024-10-30 19:49:21: [2024-10-30 19:49:21] iter = 18780, loss = 1.9944
2024-10-30 19:49:24: [2024-10-30 19:49:24] iter = 18790, loss = 2.0407
2024-10-30 19:49:27: [2024-10-30 19:49:27] iter = 18800, loss = 2.8929
2024-10-30 19:49:31: [2024-10-30 19:49:31] iter = 18810, loss = 2.6382
2024-10-30 19:49:35: [2024-10-30 19:49:35] iter = 18820, loss = 2.4179
2024-10-30 19:49:39: [2024-10-30 19:49:39] iter = 18830, loss = 1.9457
2024-10-30 19:49:42: [2024-10-30 19:49:42] iter = 18840, loss = 1.9731
2024-10-30 19:49:46: [2024-10-30 19:49:46] iter = 18850, loss = 2.1142
2024-10-30 19:49:50: [2024-10-30 19:49:50] iter = 18860, loss = 1.8717
2024-10-30 19:49:55: [2024-10-30 19:49:55] iter = 18870, loss = 3.5808
2024-10-30 19:49:59: [2024-10-30 19:49:59] iter = 18880, loss = 2.4893
2024-10-30 19:50:01: [2024-10-30 19:50:01] iter = 18890, loss = 1.8661
2024-10-30 19:50:05: [2024-10-30 19:50:05] iter = 18900, loss = 2.7598
2024-10-30 19:50:08: [2024-10-30 19:50:08] iter = 18910, loss = 2.4717
2024-10-30 19:50:11: [2024-10-30 19:50:11] iter = 18920, loss = 3.2958
2024-10-30 19:50:14: [2024-10-30 19:50:14] iter = 18930, loss = 3.1060
2024-10-30 19:50:18: [2024-10-30 19:50:18] iter = 18940, loss = 2.7101
2024-10-30 19:50:22: [2024-10-30 19:50:22] iter = 18950, loss = 3.1729
2024-10-30 19:50:26: [2024-10-30 19:50:26] iter = 18960, loss = 2.2058
2024-10-30 19:50:30: [2024-10-30 19:50:30] iter = 18970, loss = 1.8340
2024-10-30 19:50:35: [2024-10-30 19:50:35] iter = 18980, loss = 2.1396
2024-10-30 19:50:39: [2024-10-30 19:50:39] iter = 18990, loss = 2.4223
2024-10-30 19:50:43: [2024-10-30 19:50:43] iter = 19000, loss = 2.3049
2024-10-30 19:50:47: [2024-10-30 19:50:47] iter = 19010, loss = 2.5499
2024-10-30 19:50:51: [2024-10-30 19:50:51] iter = 19020, loss = 1.9985
2024-10-30 19:50:56: [2024-10-30 19:50:56] iter = 19030, loss = 2.0632
2024-10-30 19:51:00: [2024-10-30 19:51:00] iter = 19040, loss = 2.6651
2024-10-30 19:51:04: [2024-10-30 19:51:04] iter = 19050, loss = 2.1169
2024-10-30 19:51:08: [2024-10-30 19:51:08] iter = 19060, loss = 2.3181
2024-10-30 19:51:12: [2024-10-30 19:51:12] iter = 19070, loss = 2.2896
2024-10-30 19:51:16: [2024-10-30 19:51:16] iter = 19080, loss = 1.9460
2024-10-30 19:51:20: [2024-10-30 19:51:20] iter = 19090, loss = 4.0530
2024-10-30 19:51:24: [2024-10-30 19:51:24] iter = 19100, loss = 2.2143
2024-10-30 19:51:27: [2024-10-30 19:51:27] iter = 19110, loss = 2.3568
2024-10-30 19:51:31: [2024-10-30 19:51:31] iter = 19120, loss = 2.3363
2024-10-30 19:51:36: [2024-10-30 19:51:36] iter = 19130, loss = 2.3575
2024-10-30 19:51:40: [2024-10-30 19:51:40] iter = 19140, loss = 2.0578
2024-10-30 19:51:44: [2024-10-30 19:51:44] iter = 19150, loss = 2.1739
2024-10-30 19:51:49: [2024-10-30 19:51:49] iter = 19160, loss = 2.0078
2024-10-30 19:51:52: [2024-10-30 19:51:52] iter = 19170, loss = 2.6577
2024-10-30 19:51:56: [2024-10-30 19:51:56] iter = 19180, loss = 1.8809
2024-10-30 19:51:58: [2024-10-30 19:51:58] iter = 19190, loss = 1.8476
2024-10-30 19:52:01: [2024-10-30 19:52:01] iter = 19200, loss = 2.0549
2024-10-30 19:52:04: [2024-10-30 19:52:04] iter = 19210, loss = 2.1155
2024-10-30 19:52:08: [2024-10-30 19:52:08] iter = 19220, loss = 2.3699
2024-10-30 19:52:12: [2024-10-30 19:52:12] iter = 19230, loss = 3.3176
2024-10-30 19:52:16: [2024-10-30 19:52:16] iter = 19240, loss = 2.3132
2024-10-30 19:52:19: [2024-10-30 19:52:19] iter = 19250, loss = 1.7960
2024-10-30 19:52:23: [2024-10-30 19:52:23] iter = 19260, loss = 2.5286
2024-10-30 19:52:26: [2024-10-30 19:52:26] iter = 19270, loss = 3.4488
2024-10-30 19:52:29: [2024-10-30 19:52:29] iter = 19280, loss = 2.2738
2024-10-30 19:52:32: [2024-10-30 19:52:32] iter = 19290, loss = 1.9558
2024-10-30 19:52:36: [2024-10-30 19:52:36] iter = 19300, loss = 2.0175
2024-10-30 19:52:39: [2024-10-30 19:52:39] iter = 19310, loss = 2.5296
2024-10-30 19:52:43: [2024-10-30 19:52:43] iter = 19320, loss = 2.1701
2024-10-30 19:52:46: [2024-10-30 19:52:46] iter = 19330, loss = 1.8897
2024-10-30 19:52:50: [2024-10-30 19:52:50] iter = 19340, loss = 2.6998
2024-10-30 19:52:53: [2024-10-30 19:52:53] iter = 19350, loss = 2.5026
2024-10-30 19:52:57: [2024-10-30 19:52:57] iter = 19360, loss = 1.9810
2024-10-30 19:53:00: [2024-10-30 19:53:00] iter = 19370, loss = 2.4077
2024-10-30 19:53:04: [2024-10-30 19:53:04] iter = 19380, loss = 2.5775
2024-10-30 19:53:07: [2024-10-30 19:53:07] iter = 19390, loss = 3.0432
2024-10-30 19:53:11: [2024-10-30 19:53:11] iter = 19400, loss = 5.3483
2024-10-30 19:53:14: [2024-10-30 19:53:14] iter = 19410, loss = 1.8191
2024-10-30 19:53:19: [2024-10-30 19:53:19] iter = 19420, loss = 2.2587
2024-10-30 19:53:23: [2024-10-30 19:53:23] iter = 19430, loss = 2.2575
2024-10-30 19:53:27: [2024-10-30 19:53:27] iter = 19440, loss = 2.1511
2024-10-30 19:53:31: [2024-10-30 19:53:31] iter = 19450, loss = 2.2795
2024-10-30 19:53:34: [2024-10-30 19:53:34] iter = 19460, loss = 2.4784
2024-10-30 19:53:38: [2024-10-30 19:53:38] iter = 19470, loss = 2.5568
2024-10-30 19:53:41: [2024-10-30 19:53:41] iter = 19480, loss = 2.2044
2024-10-30 19:53:44: [2024-10-30 19:53:44] iter = 19490, loss = 2.3443
2024-10-30 19:53:48: [2024-10-30 19:53:48] iter = 19500, loss = 1.9087
2024-10-30 19:53:51: [2024-10-30 19:53:51] iter = 19510, loss = 2.0445
2024-10-30 19:53:55: [2024-10-30 19:53:55] iter = 19520, loss = 3.0085
2024-10-30 19:53:59: [2024-10-30 19:53:59] iter = 19530, loss = 2.3025
2024-10-30 19:54:02: [2024-10-30 19:54:02] iter = 19540, loss = 2.6283
2024-10-30 19:54:06: [2024-10-30 19:54:06] iter = 19550, loss = 2.9340
2024-10-30 19:54:09: [2024-10-30 19:54:09] iter = 19560, loss = 1.8122
2024-10-30 19:54:13: [2024-10-30 19:54:13] iter = 19570, loss = 2.0364
2024-10-30 19:54:16: [2024-10-30 19:54:16] iter = 19580, loss = 2.0338
2024-10-30 19:54:19: [2024-10-30 19:54:19] iter = 19590, loss = 5.1451
2024-10-30 19:54:23: [2024-10-30 19:54:23] iter = 19600, loss = 3.4893
2024-10-30 19:54:27: [2024-10-30 19:54:27] iter = 19610, loss = 1.8273
2024-10-30 19:54:31: [2024-10-30 19:54:31] iter = 19620, loss = 2.5096
2024-10-30 19:54:35: [2024-10-30 19:54:35] iter = 19630, loss = 2.3856
2024-10-30 19:54:39: [2024-10-30 19:54:39] iter = 19640, loss = 2.6303
2024-10-30 19:54:43: [2024-10-30 19:54:43] iter = 19650, loss = 2.4853
2024-10-30 19:54:47: [2024-10-30 19:54:47] iter = 19660, loss = 3.9972
2024-10-30 19:54:50: [2024-10-30 19:54:50] iter = 19670, loss = 1.9232
2024-10-30 19:54:54: [2024-10-30 19:54:54] iter = 19680, loss = 2.2353
2024-10-30 19:54:57: [2024-10-30 19:54:57] iter = 19690, loss = 2.5591
2024-10-30 19:55:01: [2024-10-30 19:55:01] iter = 19700, loss = 2.3439
2024-10-30 19:55:04: [2024-10-30 19:55:04] iter = 19710, loss = 2.1859
2024-10-30 19:55:08: [2024-10-30 19:55:08] iter = 19720, loss = 2.1261
2024-10-30 19:55:11: [2024-10-30 19:55:11] iter = 19730, loss = 2.3168
2024-10-30 19:55:15: [2024-10-30 19:55:15] iter = 19740, loss = 3.0336
2024-10-30 19:55:19: [2024-10-30 19:55:19] iter = 19750, loss = 2.1307
2024-10-30 19:55:23: [2024-10-30 19:55:23] iter = 19760, loss = 1.8023
2024-10-30 19:55:26: [2024-10-30 19:55:26] iter = 19770, loss = 2.1399
2024-10-30 19:55:30: [2024-10-30 19:55:30] iter = 19780, loss = 2.0924
2024-10-30 19:55:34: [2024-10-30 19:55:34] iter = 19790, loss = 2.8022
2024-10-30 19:55:38: [2024-10-30 19:55:38] iter = 19800, loss = 2.2904
2024-10-30 19:55:41: [2024-10-30 19:55:41] iter = 19810, loss = 2.6195
2024-10-30 19:55:44: [2024-10-30 19:55:44] iter = 19820, loss = 2.2446
2024-10-30 19:55:47: [2024-10-30 19:55:47] iter = 19830, loss = 1.9714
2024-10-30 19:55:51: [2024-10-30 19:55:51] iter = 19840, loss = 1.9317
2024-10-30 19:55:56: [2024-10-30 19:55:56] iter = 19850, loss = 2.1417
2024-10-30 19:56:00: [2024-10-30 19:56:00] iter = 19860, loss = 2.1138
2024-10-30 19:56:04: [2024-10-30 19:56:04] iter = 19870, loss = 2.1672
2024-10-30 19:56:08: [2024-10-30 19:56:08] iter = 19880, loss = 2.0071
2024-10-30 19:56:11: [2024-10-30 19:56:11] iter = 19890, loss = 1.9056
2024-10-30 19:56:15: [2024-10-30 19:56:15] iter = 19900, loss = 3.6863
2024-10-30 19:56:19: [2024-10-30 19:56:19] iter = 19910, loss = 2.5516
2024-10-30 19:56:23: [2024-10-30 19:56:23] iter = 19920, loss = 2.0707
2024-10-30 19:56:26: [2024-10-30 19:56:26] iter = 19930, loss = 2.2620
2024-10-30 19:56:29: [2024-10-30 19:56:29] iter = 19940, loss = 1.9384
2024-10-30 19:56:33: [2024-10-30 19:56:33] iter = 19950, loss = 2.0176
2024-10-30 19:56:37: [2024-10-30 19:56:37] iter = 19960, loss = 3.1623
2024-10-30 19:56:41: [2024-10-30 19:56:41] iter = 19970, loss = 2.2936
2024-10-30 19:56:45: [2024-10-30 19:56:45] iter = 19980, loss = 3.2611
2024-10-30 19:56:50: [2024-10-30 19:56:50] iter = 19990, loss = 2.3133
2024-10-30 19:56:54: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
2024-10-30 19:56:54: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 19:56:54: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 14431}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 19:59:31: Evaluate 5 random ConvNet, ACCmean = 0.7853 ACCstd = 0.0028
-------------------------
2024-10-30 19:59:31: Evaluate 5 random ConvNet, SENmean = 0.7847 SENstd = 0.0016
-------------------------
2024-10-30 19:59:31: Evaluate 5 random ConvNet, SPEmean = 0.9786 SPEstd = 0.0003
-------------------------
2024-10-30 19:59:31: Evaluate 5 random ConvNet, F!mean = 0.7701 F!std = 0.0024
-------------------------
2024-10-30 19:59:31: Evaluate 5 random ConvNet, mean = 0.7853 std = 0.0028
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 19:59:32: [2024-10-30 19:59:32] iter = 20000, loss = 2.4288
2024-10-30 19:59:32: 
================== Exp 2 ==================
 
2024-10-30 19:59:32: Hyper-parameters: 
{'dataset': 'OrganAMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': '/data/users/xiongyuxuan/DD/OBJDD/DatasetCondensation-master/data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f5f8402db20>, 'dsa': True, 'logger': <Logger DM_IPC=10_Model_ConvNet_Data_OrganAMNIST (INFO)>}
2024-10-30 19:59:32: Evaluation model pool: ['ConvNet']
2024-10-30 19:59:34: class c = 0: 1956 real images
2024-10-30 19:59:34: class c = 1: 1390 real images
2024-10-30 19:59:34: class c = 2: 1357 real images
2024-10-30 19:59:34: class c = 3: 1474 real images
2024-10-30 19:59:34: class c = 4: 3963 real images
2024-10-30 19:59:34: class c = 5: 3817 real images
2024-10-30 19:59:34: class c = 6: 6164 real images
2024-10-30 19:59:34: class c = 7: 3919 real images
2024-10-30 19:59:34: class c = 8: 3929 real images
2024-10-30 19:59:34: class c = 9: 3031 real images
2024-10-30 19:59:34: class c = 10: 3561 real images
2024-10-30 19:59:34: real images channel 0, mean = 0.4680, std = 0.2974
main_DM.py:120: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-10-30 19:59:34: initialize synthetic data from random real images
2024-10-30 19:59:34: [2024-10-30 19:59:34] training begins
2024-10-30 19:59:34: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-10-30 19:59:34: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 19:59:34: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 72189}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 20:02:11: Evaluate 5 random ConvNet, ACCmean = 0.6922 ACCstd = 0.0045
-------------------------
2024-10-30 20:02:11: Evaluate 5 random ConvNet, SENmean = 0.6738 SENstd = 0.0036
-------------------------
2024-10-30 20:02:11: Evaluate 5 random ConvNet, SPEmean = 0.9689 SPEstd = 0.0004
-------------------------
2024-10-30 20:02:11: Evaluate 5 random ConvNet, F!mean = 0.6700 F!std = 0.0040
-------------------------
2024-10-30 20:02:11: Evaluate 5 random ConvNet, mean = 0.6922 std = 0.0045
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 20:02:11: [2024-10-30 20:02:11] iter = 00000, loss = 10.9124
2024-10-30 20:02:15: [2024-10-30 20:02:15] iter = 00010, loss = 5.5911
2024-10-30 20:02:20: [2024-10-30 20:02:20] iter = 00020, loss = 4.0702
2024-10-30 20:02:24: [2024-10-30 20:02:24] iter = 00030, loss = 4.1553
2024-10-30 20:02:28: [2024-10-30 20:02:28] iter = 00040, loss = 3.3295
2024-10-30 20:02:32: [2024-10-30 20:02:32] iter = 00050, loss = 2.6622
2024-10-30 20:02:37: [2024-10-30 20:02:37] iter = 00060, loss = 3.0582
2024-10-30 20:02:41: [2024-10-30 20:02:41] iter = 00070, loss = 3.0548
2024-10-30 20:02:45: [2024-10-30 20:02:45] iter = 00080, loss = 2.4380
2024-10-30 20:02:48: [2024-10-30 20:02:48] iter = 00090, loss = 2.3921
2024-10-30 20:02:51: [2024-10-30 20:02:51] iter = 00100, loss = 3.0397
2024-10-30 20:02:56: [2024-10-30 20:02:56] iter = 00110, loss = 2.9933
2024-10-30 20:03:00: [2024-10-30 20:03:00] iter = 00120, loss = 2.3352
2024-10-30 20:03:03: [2024-10-30 20:03:03] iter = 00130, loss = 3.2099
2024-10-30 20:03:07: [2024-10-30 20:03:07] iter = 00140, loss = 2.8533
2024-10-30 20:03:11: [2024-10-30 20:03:11] iter = 00150, loss = 2.3952
2024-10-30 20:03:15: [2024-10-30 20:03:15] iter = 00160, loss = 2.5148
2024-10-30 20:03:19: [2024-10-30 20:03:19] iter = 00170, loss = 3.6149
2024-10-30 20:03:24: [2024-10-30 20:03:24] iter = 00180, loss = 8.4076
2024-10-30 20:03:28: [2024-10-30 20:03:28] iter = 00190, loss = 3.0576
2024-10-30 20:03:32: [2024-10-30 20:03:32] iter = 00200, loss = 2.7065
2024-10-30 20:03:35: [2024-10-30 20:03:35] iter = 00210, loss = 2.4092
2024-10-30 20:03:39: [2024-10-30 20:03:39] iter = 00220, loss = 2.8289
2024-10-30 20:03:44: [2024-10-30 20:03:44] iter = 00230, loss = 3.1029
2024-10-30 20:03:49: [2024-10-30 20:03:49] iter = 00240, loss = 3.7336
2024-10-30 20:03:52: [2024-10-30 20:03:52] iter = 00250, loss = 2.0676
2024-10-30 20:03:57: [2024-10-30 20:03:57] iter = 00260, loss = 2.5052
2024-10-30 20:04:01: [2024-10-30 20:04:01] iter = 00270, loss = 2.4175
2024-10-30 20:04:05: [2024-10-30 20:04:05] iter = 00280, loss = 2.0135
2024-10-30 20:04:10: [2024-10-30 20:04:10] iter = 00290, loss = 2.4929
2024-10-30 20:04:15: [2024-10-30 20:04:15] iter = 00300, loss = 1.8155
2024-10-30 20:04:19: [2024-10-30 20:04:19] iter = 00310, loss = 2.1826
2024-10-30 20:04:23: [2024-10-30 20:04:23] iter = 00320, loss = 2.4323
2024-10-30 20:04:26: [2024-10-30 20:04:26] iter = 00330, loss = 2.2715
2024-10-30 20:04:30: [2024-10-30 20:04:30] iter = 00340, loss = 2.3890
2024-10-30 20:04:34: [2024-10-30 20:04:34] iter = 00350, loss = 4.1484
2024-10-30 20:04:37: [2024-10-30 20:04:37] iter = 00360, loss = 2.3485
2024-10-30 20:04:41: [2024-10-30 20:04:41] iter = 00370, loss = 2.8823
2024-10-30 20:04:46: [2024-10-30 20:04:46] iter = 00380, loss = 2.1602
2024-10-30 20:04:49: [2024-10-30 20:04:49] iter = 00390, loss = 1.7824
2024-10-30 20:04:53: [2024-10-30 20:04:53] iter = 00400, loss = 2.5617
2024-10-30 20:04:56: [2024-10-30 20:04:56] iter = 00410, loss = 3.8887
2024-10-30 20:05:00: [2024-10-30 20:05:00] iter = 00420, loss = 2.4454
2024-10-30 20:05:03: [2024-10-30 20:05:03] iter = 00430, loss = 3.0907
2024-10-30 20:05:06: [2024-10-30 20:05:06] iter = 00440, loss = 3.9511
2024-10-30 20:05:10: [2024-10-30 20:05:10] iter = 00450, loss = 2.6015
2024-10-30 20:05:14: [2024-10-30 20:05:14] iter = 00460, loss = 2.5520
2024-10-30 20:05:18: [2024-10-30 20:05:18] iter = 00470, loss = 2.3305
2024-10-30 20:05:22: [2024-10-30 20:05:22] iter = 00480, loss = 2.9152
2024-10-30 20:05:25: [2024-10-30 20:05:25] iter = 00490, loss = 2.5446
2024-10-30 20:05:30: [2024-10-30 20:05:30] iter = 00500, loss = 2.2582
2024-10-30 20:05:34: [2024-10-30 20:05:34] iter = 00510, loss = 2.0789
2024-10-30 20:05:39: [2024-10-30 20:05:39] iter = 00520, loss = 2.2165
2024-10-30 20:05:44: [2024-10-30 20:05:44] iter = 00530, loss = 2.0499
2024-10-30 20:05:48: [2024-10-30 20:05:48] iter = 00540, loss = 2.5536
2024-10-30 20:05:54: [2024-10-30 20:05:54] iter = 00550, loss = 3.2778
2024-10-30 20:05:58: [2024-10-30 20:05:58] iter = 00560, loss = 2.7379
2024-10-30 20:06:03: [2024-10-30 20:06:03] iter = 00570, loss = 2.0229
2024-10-30 20:06:07: [2024-10-30 20:06:07] iter = 00580, loss = 3.6504
2024-10-30 20:06:11: [2024-10-30 20:06:11] iter = 00590, loss = 2.6717
2024-10-30 20:06:14: [2024-10-30 20:06:14] iter = 00600, loss = 2.4133
2024-10-30 20:06:17: [2024-10-30 20:06:17] iter = 00610, loss = 2.6728
2024-10-30 20:06:22: [2024-10-30 20:06:22] iter = 00620, loss = 2.1685
2024-10-30 20:06:25: [2024-10-30 20:06:25] iter = 00630, loss = 2.2285
2024-10-30 20:06:30: [2024-10-30 20:06:30] iter = 00640, loss = 4.2861
2024-10-30 20:06:33: [2024-10-30 20:06:33] iter = 00650, loss = 2.2537
2024-10-30 20:06:37: [2024-10-30 20:06:37] iter = 00660, loss = 2.1281
2024-10-30 20:06:41: [2024-10-30 20:06:41] iter = 00670, loss = 4.6139
2024-10-30 20:06:45: [2024-10-30 20:06:45] iter = 00680, loss = 2.3597
2024-10-30 20:06:49: [2024-10-30 20:06:49] iter = 00690, loss = 2.5767
2024-10-30 20:06:53: [2024-10-30 20:06:53] iter = 00700, loss = 2.6575
2024-10-30 20:06:56: [2024-10-30 20:06:56] iter = 00710, loss = 2.5242
2024-10-30 20:07:00: [2024-10-30 20:07:00] iter = 00720, loss = 2.5058
2024-10-30 20:07:03: [2024-10-30 20:07:03] iter = 00730, loss = 2.5807
2024-10-30 20:07:07: [2024-10-30 20:07:07] iter = 00740, loss = 8.4772
2024-10-30 20:07:12: [2024-10-30 20:07:12] iter = 00750, loss = 2.3218
2024-10-30 20:07:15: [2024-10-30 20:07:15] iter = 00760, loss = 3.6653
2024-10-30 20:07:19: [2024-10-30 20:07:19] iter = 00770, loss = 2.0523
2024-10-30 20:07:23: [2024-10-30 20:07:23] iter = 00780, loss = 3.3013
2024-10-30 20:07:27: [2024-10-30 20:07:27] iter = 00790, loss = 2.3227
2024-10-30 20:07:31: [2024-10-30 20:07:31] iter = 00800, loss = 2.4973
2024-10-30 20:07:34: [2024-10-30 20:07:34] iter = 00810, loss = 2.9373
2024-10-30 20:07:38: [2024-10-30 20:07:38] iter = 00820, loss = 2.4158
2024-10-30 20:07:42: [2024-10-30 20:07:42] iter = 00830, loss = 2.5970
2024-10-30 20:07:45: [2024-10-30 20:07:45] iter = 00840, loss = 2.2635
2024-10-30 20:07:49: [2024-10-30 20:07:49] iter = 00850, loss = 3.7314
2024-10-30 20:07:53: [2024-10-30 20:07:53] iter = 00860, loss = 2.4033
2024-10-30 20:07:58: [2024-10-30 20:07:58] iter = 00870, loss = 3.1772
2024-10-30 20:08:02: [2024-10-30 20:08:02] iter = 00880, loss = 2.0057
2024-10-30 20:08:07: [2024-10-30 20:08:07] iter = 00890, loss = 3.8760
2024-10-30 20:08:10: [2024-10-30 20:08:10] iter = 00900, loss = 2.1578
2024-10-30 20:08:14: [2024-10-30 20:08:14] iter = 00910, loss = 3.0260
2024-10-30 20:08:18: [2024-10-30 20:08:18] iter = 00920, loss = 2.7473
2024-10-30 20:08:21: [2024-10-30 20:08:21] iter = 00930, loss = 5.1207
2024-10-30 20:08:25: [2024-10-30 20:08:25] iter = 00940, loss = 1.8467
2024-10-30 20:08:28: [2024-10-30 20:08:28] iter = 00950, loss = 5.0227
2024-10-30 20:08:32: [2024-10-30 20:08:32] iter = 00960, loss = 2.3563
2024-10-30 20:08:35: [2024-10-30 20:08:35] iter = 00970, loss = 2.0744
2024-10-30 20:08:38: [2024-10-30 20:08:38] iter = 00980, loss = 2.4035
2024-10-30 20:08:41: [2024-10-30 20:08:41] iter = 00990, loss = 2.3477
2024-10-30 20:08:45: [2024-10-30 20:08:45] iter = 01000, loss = 2.8318
2024-10-30 20:08:49: [2024-10-30 20:08:49] iter = 01010, loss = 2.0387
2024-10-30 20:08:53: [2024-10-30 20:08:53] iter = 01020, loss = 1.8209
2024-10-30 20:08:56: [2024-10-30 20:08:56] iter = 01030, loss = 2.3898
2024-10-30 20:09:00: [2024-10-30 20:09:00] iter = 01040, loss = 1.9253
2024-10-30 20:09:04: [2024-10-30 20:09:04] iter = 01050, loss = 2.3035
2024-10-30 20:09:08: [2024-10-30 20:09:08] iter = 01060, loss = 2.1058
2024-10-30 20:09:13: [2024-10-30 20:09:13] iter = 01070, loss = 1.9490
2024-10-30 20:09:17: [2024-10-30 20:09:17] iter = 01080, loss = 2.1453
2024-10-30 20:09:21: [2024-10-30 20:09:21] iter = 01090, loss = 2.3102
2024-10-30 20:09:24: [2024-10-30 20:09:24] iter = 01100, loss = 2.0485
2024-10-30 20:09:28: [2024-10-30 20:09:28] iter = 01110, loss = 2.7447
2024-10-30 20:09:32: [2024-10-30 20:09:32] iter = 01120, loss = 2.2767
2024-10-30 20:09:36: [2024-10-30 20:09:36] iter = 01130, loss = 1.9705
2024-10-30 20:09:40: [2024-10-30 20:09:40] iter = 01140, loss = 2.1290
2024-10-30 20:09:44: [2024-10-30 20:09:44] iter = 01150, loss = 2.0632
2024-10-30 20:09:48: [2024-10-30 20:09:48] iter = 01160, loss = 2.1319
2024-10-30 20:09:51: [2024-10-30 20:09:51] iter = 01170, loss = 2.6717
2024-10-30 20:09:55: [2024-10-30 20:09:55] iter = 01180, loss = 2.1205
2024-10-30 20:10:00: [2024-10-30 20:10:00] iter = 01190, loss = 6.7632
2024-10-30 20:10:03: [2024-10-30 20:10:03] iter = 01200, loss = 2.1059
2024-10-30 20:10:08: [2024-10-30 20:10:08] iter = 01210, loss = 2.2255
2024-10-30 20:10:12: [2024-10-30 20:10:12] iter = 01220, loss = 2.2569
2024-10-30 20:10:15: [2024-10-30 20:10:15] iter = 01230, loss = 1.8752
2024-10-30 20:10:19: [2024-10-30 20:10:19] iter = 01240, loss = 2.4980
2024-10-30 20:10:23: [2024-10-30 20:10:23] iter = 01250, loss = 1.9055
2024-10-30 20:10:25: [2024-10-30 20:10:25] iter = 01260, loss = 2.1177
2024-10-30 20:10:29: [2024-10-30 20:10:29] iter = 01270, loss = 3.0747
2024-10-30 20:10:33: [2024-10-30 20:10:33] iter = 01280, loss = 2.6837
2024-10-30 20:10:37: [2024-10-30 20:10:37] iter = 01290, loss = 2.7251
2024-10-30 20:10:42: [2024-10-30 20:10:42] iter = 01300, loss = 4.0250
2024-10-30 20:10:46: [2024-10-30 20:10:46] iter = 01310, loss = 3.8173
2024-10-30 20:10:51: [2024-10-30 20:10:51] iter = 01320, loss = 1.9578
2024-10-30 20:10:55: [2024-10-30 20:10:55] iter = 01330, loss = 2.0077
2024-10-30 20:11:00: [2024-10-30 20:11:00] iter = 01340, loss = 1.9276
2024-10-30 20:11:04: [2024-10-30 20:11:04] iter = 01350, loss = 2.4970
2024-10-30 20:11:09: [2024-10-30 20:11:09] iter = 01360, loss = 3.6698
2024-10-30 20:11:12: [2024-10-30 20:11:12] iter = 01370, loss = 2.6302
2024-10-30 20:11:16: [2024-10-30 20:11:16] iter = 01380, loss = 2.2642
2024-10-30 20:11:21: [2024-10-30 20:11:21] iter = 01390, loss = 2.0037
2024-10-30 20:11:25: [2024-10-30 20:11:25] iter = 01400, loss = 2.4980
2024-10-30 20:11:29: [2024-10-30 20:11:29] iter = 01410, loss = 2.3556
2024-10-30 20:11:33: [2024-10-30 20:11:33] iter = 01420, loss = 1.8205
2024-10-30 20:11:36: [2024-10-30 20:11:36] iter = 01430, loss = 2.4393
2024-10-30 20:11:40: [2024-10-30 20:11:40] iter = 01440, loss = 2.3552
2024-10-30 20:11:45: [2024-10-30 20:11:45] iter = 01450, loss = 2.4588
2024-10-30 20:11:49: [2024-10-30 20:11:49] iter = 01460, loss = 3.0164
2024-10-30 20:11:54: [2024-10-30 20:11:54] iter = 01470, loss = 2.0949
2024-10-30 20:11:58: [2024-10-30 20:11:58] iter = 01480, loss = 2.1579
2024-10-30 20:12:02: [2024-10-30 20:12:02] iter = 01490, loss = 2.3813
2024-10-30 20:12:06: [2024-10-30 20:12:06] iter = 01500, loss = 1.8324
2024-10-30 20:12:12: [2024-10-30 20:12:12] iter = 01510, loss = 2.0193
2024-10-30 20:12:16: [2024-10-30 20:12:16] iter = 01520, loss = 2.1929
2024-10-30 20:12:21: [2024-10-30 20:12:21] iter = 01530, loss = 2.4473
2024-10-30 20:12:24: [2024-10-30 20:12:24] iter = 01540, loss = 1.9383
2024-10-30 20:12:29: [2024-10-30 20:12:29] iter = 01550, loss = 2.4897
2024-10-30 20:12:34: [2024-10-30 20:12:34] iter = 01560, loss = 1.9915
2024-10-30 20:12:37: [2024-10-30 20:12:37] iter = 01570, loss = 2.1697
2024-10-30 20:12:41: [2024-10-30 20:12:41] iter = 01580, loss = 2.2812
2024-10-30 20:12:45: [2024-10-30 20:12:45] iter = 01590, loss = 2.9639
2024-10-30 20:12:49: [2024-10-30 20:12:49] iter = 01600, loss = 2.1152
2024-10-30 20:12:54: [2024-10-30 20:12:54] iter = 01610, loss = 2.2520
2024-10-30 20:12:58: [2024-10-30 20:12:58] iter = 01620, loss = 4.2196
2024-10-30 20:13:02: [2024-10-30 20:13:02] iter = 01630, loss = 2.1556
2024-10-30 20:13:04: [2024-10-30 20:13:04] iter = 01640, loss = 2.3927
2024-10-30 20:13:07: [2024-10-30 20:13:07] iter = 01650, loss = 2.3787
2024-10-30 20:13:10: [2024-10-30 20:13:10] iter = 01660, loss = 3.4940
2024-10-30 20:13:14: [2024-10-30 20:13:14] iter = 01670, loss = 2.5321
2024-10-30 20:13:18: [2024-10-30 20:13:18] iter = 01680, loss = 2.5472
2024-10-30 20:13:22: [2024-10-30 20:13:22] iter = 01690, loss = 2.2220
2024-10-30 20:13:25: [2024-10-30 20:13:25] iter = 01700, loss = 2.9996
2024-10-30 20:13:28: [2024-10-30 20:13:28] iter = 01710, loss = 3.3684
2024-10-30 20:13:32: [2024-10-30 20:13:32] iter = 01720, loss = 2.7140
2024-10-30 20:13:35: [2024-10-30 20:13:35] iter = 01730, loss = 2.0968
2024-10-30 20:13:39: [2024-10-30 20:13:39] iter = 01740, loss = 3.0690
2024-10-30 20:13:43: [2024-10-30 20:13:43] iter = 01750, loss = 2.7965
2024-10-30 20:13:47: [2024-10-30 20:13:47] iter = 01760, loss = 1.8844
2024-10-30 20:13:50: [2024-10-30 20:13:50] iter = 01770, loss = 3.0384
2024-10-30 20:13:54: [2024-10-30 20:13:54] iter = 01780, loss = 4.9156
2024-10-30 20:13:58: [2024-10-30 20:13:58] iter = 01790, loss = 2.8593
2024-10-30 20:14:02: [2024-10-30 20:14:02] iter = 01800, loss = 2.3540
2024-10-30 20:14:05: [2024-10-30 20:14:05] iter = 01810, loss = 2.6879
2024-10-30 20:14:08: [2024-10-30 20:14:08] iter = 01820, loss = 2.2086
2024-10-30 20:14:12: [2024-10-30 20:14:12] iter = 01830, loss = 2.3118
2024-10-30 20:14:16: [2024-10-30 20:14:16] iter = 01840, loss = 2.2198
2024-10-30 20:14:19: [2024-10-30 20:14:19] iter = 01850, loss = 2.2343
2024-10-30 20:14:23: [2024-10-30 20:14:23] iter = 01860, loss = 2.2978
2024-10-30 20:14:27: [2024-10-30 20:14:27] iter = 01870, loss = 2.5922
2024-10-30 20:14:31: [2024-10-30 20:14:31] iter = 01880, loss = 2.1603
2024-10-30 20:14:35: [2024-10-30 20:14:35] iter = 01890, loss = 3.0114
2024-10-30 20:14:37: [2024-10-30 20:14:37] iter = 01900, loss = 3.8934
2024-10-30 20:14:40: [2024-10-30 20:14:40] iter = 01910, loss = 2.4712
2024-10-30 20:14:44: [2024-10-30 20:14:44] iter = 01920, loss = 2.6743
2024-10-30 20:14:48: [2024-10-30 20:14:48] iter = 01930, loss = 2.1593
2024-10-30 20:14:52: [2024-10-30 20:14:52] iter = 01940, loss = 2.2193
2024-10-30 20:14:56: [2024-10-30 20:14:56] iter = 01950, loss = 2.4394
2024-10-30 20:14:59: [2024-10-30 20:14:59] iter = 01960, loss = 2.7914
2024-10-30 20:15:03: [2024-10-30 20:15:03] iter = 01970, loss = 2.1293
2024-10-30 20:15:07: [2024-10-30 20:15:07] iter = 01980, loss = 2.3107
2024-10-30 20:15:10: [2024-10-30 20:15:10] iter = 01990, loss = 2.1089
2024-10-30 20:15:14: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
2024-10-30 20:15:14: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 20:15:14: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 14436}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 20:18:06: Evaluate 5 random ConvNet, ACCmean = 0.7880 ACCstd = 0.0030
-------------------------
2024-10-30 20:18:06: Evaluate 5 random ConvNet, SENmean = 0.7847 SENstd = 0.0035
-------------------------
2024-10-30 20:18:06: Evaluate 5 random ConvNet, SPEmean = 0.9787 SPEstd = 0.0003
-------------------------
2024-10-30 20:18:06: Evaluate 5 random ConvNet, F!mean = 0.7746 F!std = 0.0038
-------------------------
2024-10-30 20:18:06: Evaluate 5 random ConvNet, mean = 0.7880 std = 0.0030
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 20:18:06: [2024-10-30 20:18:06] iter = 02000, loss = 2.4769
2024-10-30 20:18:10: [2024-10-30 20:18:10] iter = 02010, loss = 2.8897
2024-10-30 20:18:14: [2024-10-30 20:18:14] iter = 02020, loss = 2.6416
2024-10-30 20:18:20: [2024-10-30 20:18:20] iter = 02030, loss = 2.3045
2024-10-30 20:18:24: [2024-10-30 20:18:24] iter = 02040, loss = 2.3091
2024-10-30 20:18:29: [2024-10-30 20:18:29] iter = 02050, loss = 2.8267
2024-10-30 20:18:32: [2024-10-30 20:18:32] iter = 02060, loss = 1.9821
2024-10-30 20:18:37: [2024-10-30 20:18:37] iter = 02070, loss = 2.0380
2024-10-30 20:18:41: [2024-10-30 20:18:41] iter = 02080, loss = 2.4611
2024-10-30 20:18:44: [2024-10-30 20:18:44] iter = 02090, loss = 6.6829
2024-10-30 20:18:50: [2024-10-30 20:18:50] iter = 02100, loss = 2.2134
2024-10-30 20:18:54: [2024-10-30 20:18:54] iter = 02110, loss = 2.3427
2024-10-30 20:18:59: [2024-10-30 20:18:59] iter = 02120, loss = 2.8638
2024-10-30 20:19:03: [2024-10-30 20:19:03] iter = 02130, loss = 2.5347
2024-10-30 20:19:08: [2024-10-30 20:19:08] iter = 02140, loss = 4.5436
2024-10-30 20:19:12: [2024-10-30 20:19:12] iter = 02150, loss = 2.9391
2024-10-30 20:19:18: [2024-10-30 20:19:18] iter = 02160, loss = 1.6639
2024-10-30 20:19:22: [2024-10-30 20:19:22] iter = 02170, loss = 2.6377
2024-10-30 20:19:26: [2024-10-30 20:19:26] iter = 02180, loss = 2.3792
2024-10-30 20:19:30: [2024-10-30 20:19:30] iter = 02190, loss = 2.0583
2024-10-30 20:19:35: [2024-10-30 20:19:35] iter = 02200, loss = 2.4132
2024-10-30 20:19:39: [2024-10-30 20:19:39] iter = 02210, loss = 2.1134
2024-10-30 20:19:44: [2024-10-30 20:19:44] iter = 02220, loss = 2.0196
2024-10-30 20:19:49: [2024-10-30 20:19:49] iter = 02230, loss = 2.0562
2024-10-30 20:19:53: [2024-10-30 20:19:53] iter = 02240, loss = 2.0999
2024-10-30 20:19:57: [2024-10-30 20:19:57] iter = 02250, loss = 2.7619
2024-10-30 20:20:02: [2024-10-30 20:20:02] iter = 02260, loss = 2.0518
2024-10-30 20:20:06: [2024-10-30 20:20:06] iter = 02270, loss = 2.9464
2024-10-30 20:20:12: [2024-10-30 20:20:12] iter = 02280, loss = 3.5052
2024-10-30 20:20:15: [2024-10-30 20:20:15] iter = 02290, loss = 2.8375
2024-10-30 20:20:21: [2024-10-30 20:20:21] iter = 02300, loss = 1.8946
2024-10-30 20:20:26: [2024-10-30 20:20:26] iter = 02310, loss = 2.7911
2024-10-30 20:20:30: [2024-10-30 20:20:30] iter = 02320, loss = 2.3688
2024-10-30 20:20:34: [2024-10-30 20:20:34] iter = 02330, loss = 2.7530
2024-10-30 20:20:38: [2024-10-30 20:20:38] iter = 02340, loss = 2.9250
2024-10-30 20:20:44: [2024-10-30 20:20:44] iter = 02350, loss = 1.9678
2024-10-30 20:20:48: [2024-10-30 20:20:48] iter = 02360, loss = 2.1086
2024-10-30 20:20:53: [2024-10-30 20:20:53] iter = 02370, loss = 1.9866
2024-10-30 20:20:58: [2024-10-30 20:20:58] iter = 02380, loss = 2.3536
2024-10-30 20:21:02: [2024-10-30 20:21:02] iter = 02390, loss = 1.9882
2024-10-30 20:21:07: [2024-10-30 20:21:07] iter = 02400, loss = 4.4584
2024-10-30 20:21:11: [2024-10-30 20:21:11] iter = 02410, loss = 2.7357
2024-10-30 20:21:15: [2024-10-30 20:21:15] iter = 02420, loss = 2.1715
2024-10-30 20:21:20: [2024-10-30 20:21:20] iter = 02430, loss = 1.9995
2024-10-30 20:21:24: [2024-10-30 20:21:24] iter = 02440, loss = 2.6235
2024-10-30 20:21:29: [2024-10-30 20:21:29] iter = 02450, loss = 2.2388
2024-10-30 20:21:34: [2024-10-30 20:21:34] iter = 02460, loss = 2.5701
2024-10-30 20:21:39: [2024-10-30 20:21:39] iter = 02470, loss = 1.9889
2024-10-30 20:21:43: [2024-10-30 20:21:43] iter = 02480, loss = 2.6941
2024-10-30 20:21:48: [2024-10-30 20:21:48] iter = 02490, loss = 3.1721
2024-10-30 20:21:54: [2024-10-30 20:21:54] iter = 02500, loss = 3.0095
2024-10-30 20:21:59: [2024-10-30 20:21:59] iter = 02510, loss = 2.6928
2024-10-30 20:22:03: [2024-10-30 20:22:03] iter = 02520, loss = 1.8448
2024-10-30 20:22:07: [2024-10-30 20:22:07] iter = 02530, loss = 2.1469
2024-10-30 20:22:12: [2024-10-30 20:22:12] iter = 02540, loss = 3.3179
2024-10-30 20:22:17: [2024-10-30 20:22:17] iter = 02550, loss = 2.1252
2024-10-30 20:22:20: [2024-10-30 20:22:20] iter = 02560, loss = 2.6808
2024-10-30 20:22:25: [2024-10-30 20:22:25] iter = 02570, loss = 2.7376
2024-10-30 20:22:30: [2024-10-30 20:22:30] iter = 02580, loss = 1.8176
2024-10-30 20:22:34: [2024-10-30 20:22:34] iter = 02590, loss = 2.2590
2024-10-30 20:22:38: [2024-10-30 20:22:38] iter = 02600, loss = 2.6422
2024-10-30 20:22:43: [2024-10-30 20:22:43] iter = 02610, loss = 3.2923
2024-10-30 20:22:47: [2024-10-30 20:22:47] iter = 02620, loss = 1.8678
2024-10-30 20:22:52: [2024-10-30 20:22:52] iter = 02630, loss = 3.8716
2024-10-30 20:22:55: [2024-10-30 20:22:55] iter = 02640, loss = 2.0010
2024-10-30 20:23:00: [2024-10-30 20:23:00] iter = 02650, loss = 2.1226
2024-10-30 20:23:04: [2024-10-30 20:23:04] iter = 02660, loss = 2.1309
2024-10-30 20:23:08: [2024-10-30 20:23:08] iter = 02670, loss = 2.9362
2024-10-30 20:23:13: [2024-10-30 20:23:13] iter = 02680, loss = 2.5214
2024-10-30 20:23:17: [2024-10-30 20:23:17] iter = 02690, loss = 2.3854
2024-10-30 20:23:22: [2024-10-30 20:23:22] iter = 02700, loss = 3.4543
2024-10-30 20:23:26: [2024-10-30 20:23:26] iter = 02710, loss = 2.2575
2024-10-30 20:23:31: [2024-10-30 20:23:31] iter = 02720, loss = 2.9613
2024-10-30 20:23:34: [2024-10-30 20:23:34] iter = 02730, loss = 1.8391
2024-10-30 20:23:39: [2024-10-30 20:23:39] iter = 02740, loss = 2.4725
2024-10-30 20:23:45: [2024-10-30 20:23:45] iter = 02750, loss = 2.4402
2024-10-30 20:23:49: [2024-10-30 20:23:49] iter = 02760, loss = 2.7685
2024-10-30 20:23:54: [2024-10-30 20:23:54] iter = 02770, loss = 2.2968
2024-10-30 20:23:58: [2024-10-30 20:23:58] iter = 02780, loss = 2.2598
2024-10-30 20:24:03: [2024-10-30 20:24:03] iter = 02790, loss = 2.0576
2024-10-30 20:24:07: [2024-10-30 20:24:07] iter = 02800, loss = 4.4953
2024-10-30 20:24:13: [2024-10-30 20:24:13] iter = 02810, loss = 2.3259
2024-10-30 20:24:18: [2024-10-30 20:24:18] iter = 02820, loss = 1.9959
2024-10-30 20:24:22: [2024-10-30 20:24:22] iter = 02830, loss = 2.5394
2024-10-30 20:24:25: [2024-10-30 20:24:25] iter = 02840, loss = 3.2769
2024-10-30 20:24:30: [2024-10-30 20:24:30] iter = 02850, loss = 2.2976
2024-10-30 20:24:34: [2024-10-30 20:24:34] iter = 02860, loss = 8.2198
2024-10-30 20:24:39: [2024-10-30 20:24:39] iter = 02870, loss = 2.1137
2024-10-30 20:24:44: [2024-10-30 20:24:44] iter = 02880, loss = 3.2612
2024-10-30 20:24:49: [2024-10-30 20:24:49] iter = 02890, loss = 1.9543
2024-10-30 20:24:53: [2024-10-30 20:24:53] iter = 02900, loss = 1.8552
2024-10-30 20:24:57: [2024-10-30 20:24:57] iter = 02910, loss = 1.9416
2024-10-30 20:25:01: [2024-10-30 20:25:01] iter = 02920, loss = 2.5301
2024-10-30 20:25:05: [2024-10-30 20:25:05] iter = 02930, loss = 2.3285
2024-10-30 20:25:09: [2024-10-30 20:25:09] iter = 02940, loss = 1.9374
2024-10-30 20:25:14: [2024-10-30 20:25:14] iter = 02950, loss = 2.6175
2024-10-30 20:25:18: [2024-10-30 20:25:18] iter = 02960, loss = 1.9798
2024-10-30 20:25:23: [2024-10-30 20:25:23] iter = 02970, loss = 2.6531
2024-10-30 20:25:27: [2024-10-30 20:25:27] iter = 02980, loss = 2.2961
2024-10-30 20:25:31: [2024-10-30 20:25:31] iter = 02990, loss = 2.6166
2024-10-30 20:25:37: [2024-10-30 20:25:37] iter = 03000, loss = 2.0708
2024-10-30 20:25:43: [2024-10-30 20:25:43] iter = 03010, loss = 2.5531
2024-10-30 20:25:48: [2024-10-30 20:25:48] iter = 03020, loss = 2.4629
2024-10-30 20:25:54: [2024-10-30 20:25:54] iter = 03030, loss = 2.2933
2024-10-30 20:26:00: [2024-10-30 20:26:00] iter = 03040, loss = 2.5040
2024-10-30 20:26:05: [2024-10-30 20:26:05] iter = 03050, loss = 2.4453
2024-10-30 20:26:09: [2024-10-30 20:26:09] iter = 03060, loss = 2.3551
2024-10-30 20:26:15: [2024-10-30 20:26:15] iter = 03070, loss = 2.0472
2024-10-30 20:26:20: [2024-10-30 20:26:20] iter = 03080, loss = 1.9332
2024-10-30 20:26:24: [2024-10-30 20:26:24] iter = 03090, loss = 1.9861
2024-10-30 20:26:27: [2024-10-30 20:26:27] iter = 03100, loss = 2.1583
2024-10-30 20:26:31: [2024-10-30 20:26:31] iter = 03110, loss = 2.1114
2024-10-30 20:26:37: [2024-10-30 20:26:37] iter = 03120, loss = 1.8280
2024-10-30 20:26:41: [2024-10-30 20:26:41] iter = 03130, loss = 2.0946
2024-10-30 20:26:46: [2024-10-30 20:26:46] iter = 03140, loss = 1.8264
2024-10-30 20:26:51: [2024-10-30 20:26:51] iter = 03150, loss = 2.4798
2024-10-30 20:26:55: [2024-10-30 20:26:55] iter = 03160, loss = 2.3016
2024-10-30 20:27:00: [2024-10-30 20:27:00] iter = 03170, loss = 2.7119
2024-10-30 20:27:05: [2024-10-30 20:27:05] iter = 03180, loss = 2.0603
2024-10-30 20:27:11: [2024-10-30 20:27:11] iter = 03190, loss = 2.9172
2024-10-30 20:27:17: [2024-10-30 20:27:17] iter = 03200, loss = 8.1671
2024-10-30 20:27:22: [2024-10-30 20:27:22] iter = 03210, loss = 2.3470
2024-10-30 20:27:28: [2024-10-30 20:27:28] iter = 03220, loss = 3.5875
2024-10-30 20:27:32: [2024-10-30 20:27:32] iter = 03230, loss = 2.2600
2024-10-30 20:27:38: [2024-10-30 20:27:38] iter = 03240, loss = 3.7470
2024-10-30 20:27:43: [2024-10-30 20:27:43] iter = 03250, loss = 2.0490
2024-10-30 20:27:48: [2024-10-30 20:27:48] iter = 03260, loss = 2.1338
2024-10-30 20:27:53: [2024-10-30 20:27:53] iter = 03270, loss = 2.1753
2024-10-30 20:27:58: [2024-10-30 20:27:58] iter = 03280, loss = 2.3822
2024-10-30 20:28:02: [2024-10-30 20:28:02] iter = 03290, loss = 2.4299
2024-10-30 20:28:07: [2024-10-30 20:28:07] iter = 03300, loss = 2.3077
2024-10-30 20:28:12: [2024-10-30 20:28:12] iter = 03310, loss = 2.2215
2024-10-30 20:28:16: [2024-10-30 20:28:16] iter = 03320, loss = 2.2773
2024-10-30 20:28:21: [2024-10-30 20:28:21] iter = 03330, loss = 2.9186
2024-10-30 20:28:27: [2024-10-30 20:28:27] iter = 03340, loss = 1.8324
2024-10-30 20:28:30: [2024-10-30 20:28:30] iter = 03350, loss = 2.0924
2024-10-30 20:28:35: [2024-10-30 20:28:35] iter = 03360, loss = 2.0360
2024-10-30 20:28:40: [2024-10-30 20:28:40] iter = 03370, loss = 2.4027
2024-10-30 20:28:44: [2024-10-30 20:28:44] iter = 03380, loss = 2.2693
2024-10-30 20:28:48: [2024-10-30 20:28:48] iter = 03390, loss = 2.7468
2024-10-30 20:28:53: [2024-10-30 20:28:53] iter = 03400, loss = 2.2943
2024-10-30 20:28:57: [2024-10-30 20:28:57] iter = 03410, loss = 2.0563
2024-10-30 20:29:02: [2024-10-30 20:29:02] iter = 03420, loss = 2.1668
2024-10-30 20:29:06: [2024-10-30 20:29:06] iter = 03430, loss = 2.1341
2024-10-30 20:29:11: [2024-10-30 20:29:11] iter = 03440, loss = 2.2828
2024-10-30 20:29:16: [2024-10-30 20:29:16] iter = 03450, loss = 2.0341
2024-10-30 20:29:21: [2024-10-30 20:29:21] iter = 03460, loss = 2.6522
2024-10-30 20:29:25: [2024-10-30 20:29:25] iter = 03470, loss = 1.8520
2024-10-30 20:29:29: [2024-10-30 20:29:29] iter = 03480, loss = 2.0250
2024-10-30 20:29:33: [2024-10-30 20:29:33] iter = 03490, loss = 2.0305
2024-10-30 20:29:38: [2024-10-30 20:29:38] iter = 03500, loss = 2.1689
2024-10-30 20:29:43: [2024-10-30 20:29:43] iter = 03510, loss = 2.7083
2024-10-30 20:29:47: [2024-10-30 20:29:47] iter = 03520, loss = 2.5930
2024-10-30 20:29:51: [2024-10-30 20:29:51] iter = 03530, loss = 2.0185
2024-10-30 20:29:56: [2024-10-30 20:29:56] iter = 03540, loss = 2.0702
2024-10-30 20:30:00: [2024-10-30 20:30:00] iter = 03550, loss = 3.2664
2024-10-30 20:30:04: [2024-10-30 20:30:04] iter = 03560, loss = 2.1673
2024-10-30 20:30:08: [2024-10-30 20:30:08] iter = 03570, loss = 2.0274
2024-10-30 20:30:13: [2024-10-30 20:30:13] iter = 03580, loss = 2.3117
2024-10-30 20:30:17: [2024-10-30 20:30:17] iter = 03590, loss = 5.2387
2024-10-30 20:30:20: [2024-10-30 20:30:20] iter = 03600, loss = 3.3286
2024-10-30 20:30:25: [2024-10-30 20:30:25] iter = 03610, loss = 2.4028
2024-10-30 20:30:28: [2024-10-30 20:30:28] iter = 03620, loss = 2.0927
2024-10-30 20:30:32: [2024-10-30 20:30:32] iter = 03630, loss = 2.1006
2024-10-30 20:30:37: [2024-10-30 20:30:37] iter = 03640, loss = 1.8298
2024-10-30 20:30:42: [2024-10-30 20:30:42] iter = 03650, loss = 2.3932
2024-10-30 20:30:47: [2024-10-30 20:30:47] iter = 03660, loss = 2.3845
2024-10-30 20:30:52: [2024-10-30 20:30:52] iter = 03670, loss = 2.5513
2024-10-30 20:30:56: [2024-10-30 20:30:56] iter = 03680, loss = 2.4672
2024-10-30 20:31:00: [2024-10-30 20:31:00] iter = 03690, loss = 2.3157
2024-10-30 20:31:04: [2024-10-30 20:31:04] iter = 03700, loss = 2.3817
2024-10-30 20:31:08: [2024-10-30 20:31:08] iter = 03710, loss = 2.3569
2024-10-30 20:31:13: [2024-10-30 20:31:13] iter = 03720, loss = 1.9483
2024-10-30 20:31:15: [2024-10-30 20:31:15] iter = 03730, loss = 2.3078
2024-10-30 20:31:19: [2024-10-30 20:31:19] iter = 03740, loss = 2.1009
2024-10-30 20:31:22: [2024-10-30 20:31:22] iter = 03750, loss = 2.2819
2024-10-30 20:31:26: [2024-10-30 20:31:26] iter = 03760, loss = 3.0167
2024-10-30 20:31:31: [2024-10-30 20:31:31] iter = 03770, loss = 1.8165
2024-10-30 20:31:35: [2024-10-30 20:31:35] iter = 03780, loss = 1.9542
2024-10-30 20:31:39: [2024-10-30 20:31:39] iter = 03790, loss = 2.4168
2024-10-30 20:31:42: [2024-10-30 20:31:42] iter = 03800, loss = 3.0723
2024-10-30 20:31:47: [2024-10-30 20:31:47] iter = 03810, loss = 3.0269
2024-10-30 20:31:51: [2024-10-30 20:31:51] iter = 03820, loss = 2.1161
2024-10-30 20:31:55: [2024-10-30 20:31:55] iter = 03830, loss = 1.9666
2024-10-30 20:31:59: [2024-10-30 20:31:59] iter = 03840, loss = 2.6167
2024-10-30 20:32:03: [2024-10-30 20:32:03] iter = 03850, loss = 2.6700
2024-10-30 20:32:08: [2024-10-30 20:32:08] iter = 03860, loss = 2.9416
2024-10-30 20:32:11: [2024-10-30 20:32:11] iter = 03870, loss = 2.3333
2024-10-30 20:32:16: [2024-10-30 20:32:16] iter = 03880, loss = 2.5988
2024-10-30 20:32:20: [2024-10-30 20:32:20] iter = 03890, loss = 2.4175
2024-10-30 20:32:25: [2024-10-30 20:32:25] iter = 03900, loss = 1.8382
2024-10-30 20:32:31: [2024-10-30 20:32:31] iter = 03910, loss = 1.9505
2024-10-30 20:32:35: [2024-10-30 20:32:35] iter = 03920, loss = 2.6554
2024-10-30 20:32:39: [2024-10-30 20:32:39] iter = 03930, loss = 2.1821
2024-10-30 20:32:43: [2024-10-30 20:32:43] iter = 03940, loss = 1.8518
2024-10-30 20:32:46: [2024-10-30 20:32:46] iter = 03950, loss = 2.0858
2024-10-30 20:32:50: [2024-10-30 20:32:50] iter = 03960, loss = 2.0864
2024-10-30 20:32:54: [2024-10-30 20:32:54] iter = 03970, loss = 2.3150
2024-10-30 20:32:58: [2024-10-30 20:32:58] iter = 03980, loss = 2.1925
2024-10-30 20:33:01: [2024-10-30 20:33:01] iter = 03990, loss = 1.9799
2024-10-30 20:33:04: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
2024-10-30 20:33:04: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 20:33:04: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 84572}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 20:35:22: Evaluate 5 random ConvNet, ACCmean = 0.7820 ACCstd = 0.0041
-------------------------
2024-10-30 20:35:22: Evaluate 5 random ConvNet, SENmean = 0.7765 SENstd = 0.0041
-------------------------
2024-10-30 20:35:22: Evaluate 5 random ConvNet, SPEmean = 0.9781 SPEstd = 0.0004
-------------------------
2024-10-30 20:35:22: Evaluate 5 random ConvNet, F!mean = 0.7675 F!std = 0.0040
-------------------------
2024-10-30 20:35:22: Evaluate 5 random ConvNet, mean = 0.7820 std = 0.0041
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 20:35:22: [2024-10-30 20:35:22] iter = 04000, loss = 1.9807
2024-10-30 20:35:26: [2024-10-30 20:35:26] iter = 04010, loss = 2.2993
2024-10-30 20:35:30: [2024-10-30 20:35:30] iter = 04020, loss = 2.8425
2024-10-30 20:35:34: [2024-10-30 20:35:34] iter = 04030, loss = 1.7690
2024-10-30 20:35:37: [2024-10-30 20:35:37] iter = 04040, loss = 1.9119
2024-10-30 20:35:41: [2024-10-30 20:35:41] iter = 04050, loss = 2.8976
2024-10-30 20:35:44: [2024-10-30 20:35:44] iter = 04060, loss = 2.3316
2024-10-30 20:35:48: [2024-10-30 20:35:48] iter = 04070, loss = 1.7864
2024-10-30 20:35:51: [2024-10-30 20:35:51] iter = 04080, loss = 1.6867
2024-10-30 20:35:55: [2024-10-30 20:35:55] iter = 04090, loss = 2.5282
2024-10-30 20:35:59: [2024-10-30 20:35:59] iter = 04100, loss = 2.5328
2024-10-30 20:36:03: [2024-10-30 20:36:03] iter = 04110, loss = 4.0762
2024-10-30 20:36:07: [2024-10-30 20:36:07] iter = 04120, loss = 7.1933
2024-10-30 20:36:11: [2024-10-30 20:36:11] iter = 04130, loss = 2.1620
2024-10-30 20:36:15: [2024-10-30 20:36:15] iter = 04140, loss = 1.8306
2024-10-30 20:36:19: [2024-10-30 20:36:19] iter = 04150, loss = 2.1196
2024-10-30 20:36:23: [2024-10-30 20:36:23] iter = 04160, loss = 2.3514
2024-10-30 20:36:27: [2024-10-30 20:36:27] iter = 04170, loss = 4.9131
2024-10-30 20:36:31: [2024-10-30 20:36:31] iter = 04180, loss = 2.5340
2024-10-30 20:36:34: [2024-10-30 20:36:34] iter = 04190, loss = 3.0619
2024-10-30 20:36:39: [2024-10-30 20:36:39] iter = 04200, loss = 2.3129
2024-10-30 20:36:42: [2024-10-30 20:36:42] iter = 04210, loss = 3.1152
2024-10-30 20:36:44: [2024-10-30 20:36:44] iter = 04220, loss = 2.1237
2024-10-30 20:36:48: [2024-10-30 20:36:48] iter = 04230, loss = 8.3501
2024-10-30 20:36:53: [2024-10-30 20:36:53] iter = 04240, loss = 2.9702
2024-10-30 20:36:58: [2024-10-30 20:36:58] iter = 04250, loss = 2.0483
2024-10-30 20:37:02: [2024-10-30 20:37:02] iter = 04260, loss = 2.0978
2024-10-30 20:37:05: [2024-10-30 20:37:05] iter = 04270, loss = 1.9121
2024-10-30 20:37:09: [2024-10-30 20:37:09] iter = 04280, loss = 1.8088
2024-10-30 20:37:13: [2024-10-30 20:37:13] iter = 04290, loss = 2.1861
2024-10-30 20:37:17: [2024-10-30 20:37:17] iter = 04300, loss = 2.0629
2024-10-30 20:37:21: [2024-10-30 20:37:21] iter = 04310, loss = 2.8061
2024-10-30 20:37:25: [2024-10-30 20:37:25] iter = 04320, loss = 2.2801
2024-10-30 20:37:29: [2024-10-30 20:37:29] iter = 04330, loss = 2.0759
2024-10-30 20:37:33: [2024-10-30 20:37:33] iter = 04340, loss = 2.0049
2024-10-30 20:37:37: [2024-10-30 20:37:37] iter = 04350, loss = 1.9931
2024-10-30 20:37:40: [2024-10-30 20:37:40] iter = 04360, loss = 4.3186
2024-10-30 20:37:43: [2024-10-30 20:37:43] iter = 04370, loss = 2.4234
2024-10-30 20:37:47: [2024-10-30 20:37:47] iter = 04380, loss = 1.8160
2024-10-30 20:37:49: [2024-10-30 20:37:49] iter = 04390, loss = 2.1289
2024-10-30 20:37:54: [2024-10-30 20:37:54] iter = 04400, loss = 1.8504
2024-10-30 20:37:57: [2024-10-30 20:37:57] iter = 04410, loss = 4.7130
2024-10-30 20:38:01: [2024-10-30 20:38:01] iter = 04420, loss = 2.3447
2024-10-30 20:38:05: [2024-10-30 20:38:05] iter = 04430, loss = 2.6645
2024-10-30 20:38:10: [2024-10-30 20:38:10] iter = 04440, loss = 2.0824
2024-10-30 20:38:15: [2024-10-30 20:38:15] iter = 04450, loss = 2.1992
2024-10-30 20:38:20: [2024-10-30 20:38:20] iter = 04460, loss = 1.9904
2024-10-30 20:38:25: [2024-10-30 20:38:25] iter = 04470, loss = 1.9068
2024-10-30 20:38:29: [2024-10-30 20:38:29] iter = 04480, loss = 2.1803
2024-10-30 20:38:33: [2024-10-30 20:38:33] iter = 04490, loss = 2.9355
2024-10-30 20:38:38: [2024-10-30 20:38:38] iter = 04500, loss = 1.9922
2024-10-30 20:38:43: [2024-10-30 20:38:43] iter = 04510, loss = 2.1462
2024-10-30 20:38:48: [2024-10-30 20:38:48] iter = 04520, loss = 2.5724
2024-10-30 20:38:51: [2024-10-30 20:38:51] iter = 04530, loss = 2.4744
2024-10-30 20:38:55: [2024-10-30 20:38:55] iter = 04540, loss = 2.3854
2024-10-30 20:39:00: [2024-10-30 20:39:00] iter = 04550, loss = 2.1373
2024-10-30 20:39:05: [2024-10-30 20:39:05] iter = 04560, loss = 2.1278
2024-10-30 20:39:09: [2024-10-30 20:39:09] iter = 04570, loss = 3.0302
2024-10-30 20:39:14: [2024-10-30 20:39:14] iter = 04580, loss = 2.0444
2024-10-30 20:39:20: [2024-10-30 20:39:20] iter = 04590, loss = 2.2262
2024-10-30 20:39:25: [2024-10-30 20:39:25] iter = 04600, loss = 4.7111
2024-10-30 20:39:29: [2024-10-30 20:39:29] iter = 04610, loss = 2.1767
2024-10-30 20:39:33: [2024-10-30 20:39:33] iter = 04620, loss = 2.2403
2024-10-30 20:39:37: [2024-10-30 20:39:37] iter = 04630, loss = 1.6516
2024-10-30 20:39:42: [2024-10-30 20:39:42] iter = 04640, loss = 1.8951
2024-10-30 20:39:46: [2024-10-30 20:39:46] iter = 04650, loss = 4.1893
2024-10-30 20:39:50: [2024-10-30 20:39:50] iter = 04660, loss = 1.7394
2024-10-30 20:39:55: [2024-10-30 20:39:55] iter = 04670, loss = 1.9508
2024-10-30 20:39:58: [2024-10-30 20:39:58] iter = 04680, loss = 2.7811
2024-10-30 20:40:03: [2024-10-30 20:40:02] iter = 04690, loss = 2.9928
2024-10-30 20:40:07: [2024-10-30 20:40:07] iter = 04700, loss = 2.6326
2024-10-30 20:40:12: [2024-10-30 20:40:12] iter = 04710, loss = 2.0835
2024-10-30 20:40:16: [2024-10-30 20:40:16] iter = 04720, loss = 1.8444
2024-10-30 20:40:20: [2024-10-30 20:40:20] iter = 04730, loss = 1.9707
2024-10-30 20:40:24: [2024-10-30 20:40:24] iter = 04740, loss = 2.8223
2024-10-30 20:40:29: [2024-10-30 20:40:29] iter = 04750, loss = 4.9028
2024-10-30 20:40:33: [2024-10-30 20:40:33] iter = 04760, loss = 2.2867
2024-10-30 20:40:37: [2024-10-30 20:40:37] iter = 04770, loss = 2.3273
2024-10-30 20:40:42: [2024-10-30 20:40:42] iter = 04780, loss = 3.0772
2024-10-30 20:40:47: [2024-10-30 20:40:47] iter = 04790, loss = 1.9555
2024-10-30 20:40:53: [2024-10-30 20:40:53] iter = 04800, loss = 2.0987
2024-10-30 20:40:59: [2024-10-30 20:40:59] iter = 04810, loss = 2.2317
2024-10-30 20:41:03: [2024-10-30 20:41:03] iter = 04820, loss = 2.0790
2024-10-30 20:41:07: [2024-10-30 20:41:07] iter = 04830, loss = 2.0446
2024-10-30 20:41:11: [2024-10-30 20:41:11] iter = 04840, loss = 2.0894
2024-10-30 20:41:16: [2024-10-30 20:41:16] iter = 04850, loss = 2.0330
2024-10-30 20:41:22: [2024-10-30 20:41:22] iter = 04860, loss = 2.6570
2024-10-30 20:41:27: [2024-10-30 20:41:27] iter = 04870, loss = 2.0962
2024-10-30 20:41:32: [2024-10-30 20:41:32] iter = 04880, loss = 2.4245
2024-10-30 20:41:37: [2024-10-30 20:41:37] iter = 04890, loss = 1.9619
2024-10-30 20:41:40: [2024-10-30 20:41:40] iter = 04900, loss = 3.4294
2024-10-30 20:41:45: [2024-10-30 20:41:45] iter = 04910, loss = 2.3971
2024-10-30 20:41:49: [2024-10-30 20:41:49] iter = 04920, loss = 2.4576
2024-10-30 20:41:53: [2024-10-30 20:41:53] iter = 04930, loss = 1.9736
2024-10-30 20:41:58: [2024-10-30 20:41:58] iter = 04940, loss = 2.5932
2024-10-30 20:42:02: [2024-10-30 20:42:02] iter = 04950, loss = 1.9352
2024-10-30 20:42:06: [2024-10-30 20:42:06] iter = 04960, loss = 1.8545
2024-10-30 20:42:12: [2024-10-30 20:42:12] iter = 04970, loss = 3.1719
2024-10-30 20:42:16: [2024-10-30 20:42:16] iter = 04980, loss = 3.4038
2024-10-30 20:42:21: [2024-10-30 20:42:21] iter = 04990, loss = 1.9690
2024-10-30 20:42:24: [2024-10-30 20:42:24] iter = 05000, loss = 1.7021
2024-10-30 20:42:30: [2024-10-30 20:42:30] iter = 05010, loss = 8.9865
2024-10-30 20:42:35: [2024-10-30 20:42:35] iter = 05020, loss = 2.2286
2024-10-30 20:42:40: [2024-10-30 20:42:40] iter = 05030, loss = 2.3457
2024-10-30 20:42:45: [2024-10-30 20:42:45] iter = 05040, loss = 2.7443
2024-10-30 20:42:49: [2024-10-30 20:42:49] iter = 05050, loss = 1.7692
2024-10-30 20:42:52: [2024-10-30 20:42:52] iter = 05060, loss = 2.7457
2024-10-30 20:42:58: [2024-10-30 20:42:58] iter = 05070, loss = 2.0733
2024-10-30 20:43:03: [2024-10-30 20:43:03] iter = 05080, loss = 3.1592
2024-10-30 20:43:07: [2024-10-30 20:43:07] iter = 05090, loss = 2.3038
2024-10-30 20:43:10: [2024-10-30 20:43:10] iter = 05100, loss = 2.6964
2024-10-30 20:43:15: [2024-10-30 20:43:15] iter = 05110, loss = 1.8261
2024-10-30 20:43:19: [2024-10-30 20:43:19] iter = 05120, loss = 2.3609
2024-10-30 20:43:24: [2024-10-30 20:43:24] iter = 05130, loss = 1.7404
2024-10-30 20:43:30: [2024-10-30 20:43:30] iter = 05140, loss = 1.9676
2024-10-30 20:43:36: [2024-10-30 20:43:36] iter = 05150, loss = 2.2116
2024-10-30 20:43:43: [2024-10-30 20:43:43] iter = 05160, loss = 2.5078
2024-10-30 20:43:48: [2024-10-30 20:43:48] iter = 05170, loss = 1.9815
2024-10-30 20:43:54: [2024-10-30 20:43:54] iter = 05180, loss = 2.0054
2024-10-30 20:43:59: [2024-10-30 20:43:59] iter = 05190, loss = 2.5509
2024-10-30 20:44:03: [2024-10-30 20:44:03] iter = 05200, loss = 2.8288
2024-10-30 20:44:08: [2024-10-30 20:44:08] iter = 05210, loss = 2.6339
2024-10-30 20:44:15: [2024-10-30 20:44:15] iter = 05220, loss = 1.9940
2024-10-30 20:44:20: [2024-10-30 20:44:20] iter = 05230, loss = 2.6865
2024-10-30 20:44:26: [2024-10-30 20:44:26] iter = 05240, loss = 1.8526
2024-10-30 20:44:32: [2024-10-30 20:44:32] iter = 05250, loss = 4.3923
2024-10-30 20:44:39: [2024-10-30 20:44:39] iter = 05260, loss = 2.5717
2024-10-30 20:44:44: [2024-10-30 20:44:44] iter = 05270, loss = 4.3713
2024-10-30 20:44:48: [2024-10-30 20:44:48] iter = 05280, loss = 2.3906
2024-10-30 20:44:53: [2024-10-30 20:44:53] iter = 05290, loss = 2.4836
2024-10-30 20:44:57: [2024-10-30 20:44:57] iter = 05300, loss = 2.6453
2024-10-30 20:45:02: [2024-10-30 20:45:02] iter = 05310, loss = 2.0621
2024-10-30 20:45:05: [2024-10-30 20:45:05] iter = 05320, loss = 2.2431
2024-10-30 20:45:10: [2024-10-30 20:45:10] iter = 05330, loss = 1.6868
2024-10-30 20:45:15: [2024-10-30 20:45:15] iter = 05340, loss = 2.4698
2024-10-30 20:45:20: [2024-10-30 20:45:20] iter = 05350, loss = 2.6512
2024-10-30 20:45:24: [2024-10-30 20:45:24] iter = 05360, loss = 2.0426
2024-10-30 20:45:31: [2024-10-30 20:45:31] iter = 05370, loss = 2.9852
2024-10-30 20:45:35: [2024-10-30 20:45:35] iter = 05380, loss = 1.9541
2024-10-30 20:45:40: [2024-10-30 20:45:40] iter = 05390, loss = 7.4043
2024-10-30 20:45:45: [2024-10-30 20:45:45] iter = 05400, loss = 2.0112
2024-10-30 20:45:49: [2024-10-30 20:45:49] iter = 05410, loss = 3.2401
2024-10-30 20:45:52: [2024-10-30 20:45:52] iter = 05420, loss = 1.9299
2024-10-30 20:45:57: [2024-10-30 20:45:57] iter = 05430, loss = 2.3332
2024-10-30 20:46:01: [2024-10-30 20:46:01] iter = 05440, loss = 2.5215
2024-10-30 20:46:04: [2024-10-30 20:46:04] iter = 05450, loss = 2.3166
2024-10-30 20:46:11: [2024-10-30 20:46:11] iter = 05460, loss = 2.4307
2024-10-30 20:46:16: [2024-10-30 20:46:16] iter = 05470, loss = 2.6805
2024-10-30 20:46:19: [2024-10-30 20:46:19] iter = 05480, loss = 4.7555
2024-10-30 20:46:23: [2024-10-30 20:46:23] iter = 05490, loss = 4.2130
2024-10-30 20:46:27: [2024-10-30 20:46:27] iter = 05500, loss = 2.1961
2024-10-30 20:46:31: [2024-10-30 20:46:31] iter = 05510, loss = 2.6463
2024-10-30 20:46:36: [2024-10-30 20:46:36] iter = 05520, loss = 2.1953
2024-10-30 20:46:41: [2024-10-30 20:46:41] iter = 05530, loss = 1.9378
2024-10-30 20:46:46: [2024-10-30 20:46:46] iter = 05540, loss = 2.1293
2024-10-30 20:46:50: [2024-10-30 20:46:50] iter = 05550, loss = 2.1385
2024-10-30 20:46:55: [2024-10-30 20:46:55] iter = 05560, loss = 2.6583
2024-10-30 20:46:59: [2024-10-30 20:46:59] iter = 05570, loss = 1.9855
2024-10-30 20:47:03: [2024-10-30 20:47:03] iter = 05580, loss = 2.0124
2024-10-30 20:47:08: [2024-10-30 20:47:08] iter = 05590, loss = 1.9305
2024-10-30 20:47:13: [2024-10-30 20:47:13] iter = 05600, loss = 2.1732
2024-10-30 20:47:18: [2024-10-30 20:47:18] iter = 05610, loss = 2.2676
2024-10-30 20:47:23: [2024-10-30 20:47:23] iter = 05620, loss = 3.1229
2024-10-30 20:47:27: [2024-10-30 20:47:27] iter = 05630, loss = 2.6547
2024-10-30 20:47:32: [2024-10-30 20:47:32] iter = 05640, loss = 2.2168
2024-10-30 20:47:36: [2024-10-30 20:47:36] iter = 05650, loss = 2.4433
2024-10-30 20:47:40: [2024-10-30 20:47:40] iter = 05660, loss = 2.7002
2024-10-30 20:47:45: [2024-10-30 20:47:45] iter = 05670, loss = 2.2965
2024-10-30 20:47:49: [2024-10-30 20:47:49] iter = 05680, loss = 1.9532
2024-10-30 20:47:54: [2024-10-30 20:47:54] iter = 05690, loss = 2.0108
2024-10-30 20:47:59: [2024-10-30 20:47:59] iter = 05700, loss = 3.6464
2024-10-30 20:48:03: [2024-10-30 20:48:03] iter = 05710, loss = 2.2627
2024-10-30 20:48:08: [2024-10-30 20:48:08] iter = 05720, loss = 2.2685
2024-10-30 20:48:12: [2024-10-30 20:48:12] iter = 05730, loss = 2.2043
2024-10-30 20:48:17: [2024-10-30 20:48:17] iter = 05740, loss = 2.6998
2024-10-30 20:48:22: [2024-10-30 20:48:22] iter = 05750, loss = 2.8546
2024-10-30 20:48:27: [2024-10-30 20:48:27] iter = 05760, loss = 1.9023
2024-10-30 20:48:31: [2024-10-30 20:48:31] iter = 05770, loss = 1.8709
2024-10-30 20:48:36: [2024-10-30 20:48:36] iter = 05780, loss = 2.5546
2024-10-30 20:48:40: [2024-10-30 20:48:40] iter = 05790, loss = 2.1349
2024-10-30 20:48:44: [2024-10-30 20:48:44] iter = 05800, loss = 2.7027
2024-10-30 20:48:49: [2024-10-30 20:48:49] iter = 05810, loss = 2.1671
2024-10-30 20:48:53: [2024-10-30 20:48:53] iter = 05820, loss = 2.2445
2024-10-30 20:48:58: [2024-10-30 20:48:58] iter = 05830, loss = 1.9532
2024-10-30 20:49:01: [2024-10-30 20:49:01] iter = 05840, loss = 3.2226
2024-10-30 20:49:04: [2024-10-30 20:49:04] iter = 05850, loss = 1.9109
2024-10-30 20:49:08: [2024-10-30 20:49:08] iter = 05860, loss = 2.8359
2024-10-30 20:49:11: [2024-10-30 20:49:11] iter = 05870, loss = 2.1280
2024-10-30 20:49:15: [2024-10-30 20:49:15] iter = 05880, loss = 1.9705
2024-10-30 20:49:19: [2024-10-30 20:49:19] iter = 05890, loss = 2.7776
2024-10-30 20:49:23: [2024-10-30 20:49:23] iter = 05900, loss = 1.9482
2024-10-30 20:49:28: [2024-10-30 20:49:28] iter = 05910, loss = 2.4227
2024-10-30 20:49:32: [2024-10-30 20:49:32] iter = 05920, loss = 2.7961
2024-10-30 20:49:35: [2024-10-30 20:49:35] iter = 05930, loss = 1.9350
2024-10-30 20:49:39: [2024-10-30 20:49:39] iter = 05940, loss = 2.0345
2024-10-30 20:49:44: [2024-10-30 20:49:44] iter = 05950, loss = 2.5259
2024-10-30 20:49:48: [2024-10-30 20:49:48] iter = 05960, loss = 2.1567
2024-10-30 20:49:51: [2024-10-30 20:49:51] iter = 05970, loss = 2.0625
2024-10-30 20:49:56: [2024-10-30 20:49:56] iter = 05980, loss = 2.0369
2024-10-30 20:50:00: [2024-10-30 20:50:00] iter = 05990, loss = 2.1030
2024-10-30 20:50:05: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
2024-10-30 20:50:05: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 20:50:05: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 5721}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 20:53:04: Evaluate 5 random ConvNet, ACCmean = 0.7837 ACCstd = 0.0052
-------------------------
2024-10-30 20:53:04: Evaluate 5 random ConvNet, SENmean = 0.7798 SENstd = 0.0050
-------------------------
2024-10-30 20:53:04: Evaluate 5 random ConvNet, SPEmean = 0.9783 SPEstd = 0.0005
-------------------------
2024-10-30 20:53:04: Evaluate 5 random ConvNet, F!mean = 0.7698 F!std = 0.0045
-------------------------
2024-10-30 20:53:04: Evaluate 5 random ConvNet, mean = 0.7837 std = 0.0052
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 20:53:05: [2024-10-30 20:53:05] iter = 06000, loss = 2.4447
2024-10-30 20:53:09: [2024-10-30 20:53:09] iter = 06010, loss = 2.3161
2024-10-30 20:53:13: [2024-10-30 20:53:13] iter = 06020, loss = 2.8579
2024-10-30 20:53:18: [2024-10-30 20:53:18] iter = 06030, loss = 2.6414
2024-10-30 20:53:21: [2024-10-30 20:53:21] iter = 06040, loss = 2.8653
2024-10-30 20:53:26: [2024-10-30 20:53:26] iter = 06050, loss = 2.4816
2024-10-30 20:53:31: [2024-10-30 20:53:31] iter = 06060, loss = 1.8432
2024-10-30 20:53:36: [2024-10-30 20:53:36] iter = 06070, loss = 1.8678
2024-10-30 20:53:41: [2024-10-30 20:53:41] iter = 06080, loss = 2.4378
2024-10-30 20:53:45: [2024-10-30 20:53:45] iter = 06090, loss = 1.6846
2024-10-30 20:53:50: [2024-10-30 20:53:50] iter = 06100, loss = 2.1539
2024-10-30 20:53:55: [2024-10-30 20:53:55] iter = 06110, loss = 1.9595
2024-10-30 20:54:00: [2024-10-30 20:54:00] iter = 06120, loss = 3.1513
2024-10-30 20:54:05: [2024-10-30 20:54:05] iter = 06130, loss = 2.0745
2024-10-30 20:54:10: [2024-10-30 20:54:10] iter = 06140, loss = 2.4829
2024-10-30 20:54:14: [2024-10-30 20:54:14] iter = 06150, loss = 2.0564
2024-10-30 20:54:19: [2024-10-30 20:54:19] iter = 06160, loss = 3.2986
2024-10-30 20:54:24: [2024-10-30 20:54:24] iter = 06170, loss = 2.6042
2024-10-30 20:54:29: [2024-10-30 20:54:29] iter = 06180, loss = 2.4614
2024-10-30 20:54:33: [2024-10-30 20:54:33] iter = 06190, loss = 2.4087
2024-10-30 20:54:38: [2024-10-30 20:54:38] iter = 06200, loss = 2.3902
2024-10-30 20:54:43: [2024-10-30 20:54:43] iter = 06210, loss = 2.7852
2024-10-30 20:54:47: [2024-10-30 20:54:47] iter = 06220, loss = 1.7996
2024-10-30 20:54:52: [2024-10-30 20:54:52] iter = 06230, loss = 6.9146
2024-10-30 20:54:57: [2024-10-30 20:54:57] iter = 06240, loss = 2.3305
2024-10-30 20:55:02: [2024-10-30 20:55:02] iter = 06250, loss = 2.3013
2024-10-30 20:55:08: [2024-10-30 20:55:08] iter = 06260, loss = 2.1615
2024-10-30 20:55:13: [2024-10-30 20:55:13] iter = 06270, loss = 2.2012
2024-10-30 20:55:18: [2024-10-30 20:55:18] iter = 06280, loss = 2.5800
2024-10-30 20:55:23: [2024-10-30 20:55:23] iter = 06290, loss = 2.0079
2024-10-30 20:55:28: [2024-10-30 20:55:28] iter = 06300, loss = 2.1801
2024-10-30 20:55:31: [2024-10-30 20:55:31] iter = 06310, loss = 2.9406
2024-10-30 20:55:36: [2024-10-30 20:55:36] iter = 06320, loss = 4.1280
2024-10-30 20:55:40: [2024-10-30 20:55:40] iter = 06330, loss = 2.5996
2024-10-30 20:55:45: [2024-10-30 20:55:45] iter = 06340, loss = 2.3123
2024-10-30 20:55:49: [2024-10-30 20:55:49] iter = 06350, loss = 1.8214
2024-10-30 20:55:53: [2024-10-30 20:55:53] iter = 06360, loss = 2.2616
2024-10-30 20:55:58: [2024-10-30 20:55:58] iter = 06370, loss = 1.7392
2024-10-30 20:56:03: [2024-10-30 20:56:03] iter = 06380, loss = 2.4129
2024-10-30 20:56:07: [2024-10-30 20:56:07] iter = 06390, loss = 3.8566
2024-10-30 20:56:11: [2024-10-30 20:56:11] iter = 06400, loss = 2.6262
2024-10-30 20:56:15: [2024-10-30 20:56:15] iter = 06410, loss = 2.7470
2024-10-30 20:56:20: [2024-10-30 20:56:20] iter = 06420, loss = 2.3489
2024-10-30 20:56:24: [2024-10-30 20:56:24] iter = 06430, loss = 2.4232
2024-10-30 20:56:29: [2024-10-30 20:56:29] iter = 06440, loss = 1.7998
2024-10-30 20:56:33: [2024-10-30 20:56:33] iter = 06450, loss = 2.0555
2024-10-30 20:56:38: [2024-10-30 20:56:38] iter = 06460, loss = 2.8420
2024-10-30 20:56:43: [2024-10-30 20:56:43] iter = 06470, loss = 1.9545
2024-10-30 20:56:47: [2024-10-30 20:56:47] iter = 06480, loss = 2.0712
2024-10-30 20:56:51: [2024-10-30 20:56:51] iter = 06490, loss = 1.9692
2024-10-30 20:56:55: [2024-10-30 20:56:55] iter = 06500, loss = 1.5476
2024-10-30 20:57:00: [2024-10-30 20:57:00] iter = 06510, loss = 2.0458
2024-10-30 20:57:04: [2024-10-30 20:57:04] iter = 06520, loss = 2.4744
2024-10-30 20:57:06: [2024-10-30 20:57:06] iter = 06530, loss = 2.3879
2024-10-30 20:57:11: [2024-10-30 20:57:11] iter = 06540, loss = 2.7386
2024-10-30 20:57:15: [2024-10-30 20:57:15] iter = 06550, loss = 2.0912
2024-10-30 20:57:20: [2024-10-30 20:57:20] iter = 06560, loss = 1.8638
2024-10-30 20:57:23: [2024-10-30 20:57:23] iter = 06570, loss = 3.5305
2024-10-30 20:57:28: [2024-10-30 20:57:28] iter = 06580, loss = 2.0881
2024-10-30 20:57:31: [2024-10-30 20:57:31] iter = 06590, loss = 2.8498
2024-10-30 20:57:36: [2024-10-30 20:57:36] iter = 06600, loss = 1.7894
2024-10-30 20:57:40: [2024-10-30 20:57:40] iter = 06610, loss = 2.4508
2024-10-30 20:57:45: [2024-10-30 20:57:45] iter = 06620, loss = 2.3346
2024-10-30 20:57:49: [2024-10-30 20:57:49] iter = 06630, loss = 1.8417
2024-10-30 20:57:53: [2024-10-30 20:57:53] iter = 06640, loss = 1.9252
2024-10-30 20:57:57: [2024-10-30 20:57:57] iter = 06650, loss = 3.2584
2024-10-30 20:58:01: [2024-10-30 20:58:01] iter = 06660, loss = 2.1791
2024-10-30 20:58:05: [2024-10-30 20:58:05] iter = 06670, loss = 2.8095
2024-10-30 20:58:09: [2024-10-30 20:58:09] iter = 06680, loss = 2.4468
2024-10-30 20:58:13: [2024-10-30 20:58:13] iter = 06690, loss = 2.2612
2024-10-30 20:58:17: [2024-10-30 20:58:17] iter = 06700, loss = 2.1927
2024-10-30 20:58:22: [2024-10-30 20:58:22] iter = 06710, loss = 2.1407
2024-10-30 20:58:26: [2024-10-30 20:58:26] iter = 06720, loss = 2.0850
2024-10-30 20:58:30: [2024-10-30 20:58:30] iter = 06730, loss = 2.1422
2024-10-30 20:58:34: [2024-10-30 20:58:34] iter = 06740, loss = 2.1532
2024-10-30 20:58:39: [2024-10-30 20:58:39] iter = 06750, loss = 2.2056
2024-10-30 20:58:43: [2024-10-30 20:58:43] iter = 06760, loss = 1.8473
2024-10-30 20:58:46: [2024-10-30 20:58:46] iter = 06770, loss = 2.2521
2024-10-30 20:58:50: [2024-10-30 20:58:50] iter = 06780, loss = 2.0691
2024-10-30 20:58:55: [2024-10-30 20:58:55] iter = 06790, loss = 1.7862
2024-10-30 20:58:59: [2024-10-30 20:58:59] iter = 06800, loss = 3.6800
2024-10-30 20:59:05: [2024-10-30 20:59:05] iter = 06810, loss = 2.0367
2024-10-30 20:59:08: [2024-10-30 20:59:08] iter = 06820, loss = 2.3237
2024-10-30 20:59:13: [2024-10-30 20:59:13] iter = 06830, loss = 2.3842
2024-10-30 20:59:17: [2024-10-30 20:59:17] iter = 06840, loss = 2.6404
2024-10-30 20:59:22: [2024-10-30 20:59:22] iter = 06850, loss = 1.9386
2024-10-30 20:59:27: [2024-10-30 20:59:27] iter = 06860, loss = 2.0660
2024-10-30 20:59:32: [2024-10-30 20:59:32] iter = 06870, loss = 1.8196
2024-10-30 20:59:37: [2024-10-30 20:59:37] iter = 06880, loss = 1.6668
2024-10-30 20:59:42: [2024-10-30 20:59:42] iter = 06890, loss = 1.6906
2024-10-30 20:59:47: [2024-10-30 20:59:47] iter = 06900, loss = 1.8594
2024-10-30 20:59:53: [2024-10-30 20:59:53] iter = 06910, loss = 2.5925
2024-10-30 20:59:57: [2024-10-30 20:59:57] iter = 06920, loss = 2.3592
2024-10-30 21:00:01: [2024-10-30 21:00:01] iter = 06930, loss = 2.0574
2024-10-30 21:00:06: [2024-10-30 21:00:06] iter = 06940, loss = 1.9509
2024-10-30 21:00:11: [2024-10-30 21:00:11] iter = 06950, loss = 2.1632
2024-10-30 21:00:15: [2024-10-30 21:00:15] iter = 06960, loss = 2.4925
2024-10-30 21:00:19: [2024-10-30 21:00:19] iter = 06970, loss = 2.2692
2024-10-30 21:00:22: [2024-10-30 21:00:22] iter = 06980, loss = 2.6948
2024-10-30 21:00:26: [2024-10-30 21:00:26] iter = 06990, loss = 2.4642
2024-10-30 21:00:30: [2024-10-30 21:00:30] iter = 07000, loss = 2.2672
2024-10-30 21:00:34: [2024-10-30 21:00:34] iter = 07010, loss = 1.9760
2024-10-30 21:00:38: [2024-10-30 21:00:38] iter = 07020, loss = 3.4902
2024-10-30 21:00:41: [2024-10-30 21:00:41] iter = 07030, loss = 1.7520
2024-10-30 21:00:45: [2024-10-30 21:00:45] iter = 07040, loss = 1.9777
2024-10-30 21:00:50: [2024-10-30 21:00:50] iter = 07050, loss = 2.9414
2024-10-30 21:00:54: [2024-10-30 21:00:54] iter = 07060, loss = 3.5963
2024-10-30 21:00:58: [2024-10-30 21:00:58] iter = 07070, loss = 2.1444
2024-10-30 21:01:02: [2024-10-30 21:01:02] iter = 07080, loss = 2.0809
2024-10-30 21:01:05: [2024-10-30 21:01:05] iter = 07090, loss = 2.0139
2024-10-30 21:01:10: [2024-10-30 21:01:10] iter = 07100, loss = 1.9943
2024-10-30 21:01:14: [2024-10-30 21:01:14] iter = 07110, loss = 3.6295
2024-10-30 21:01:19: [2024-10-30 21:01:19] iter = 07120, loss = 2.9962
2024-10-30 21:01:23: [2024-10-30 21:01:23] iter = 07130, loss = 2.3612
2024-10-30 21:01:28: [2024-10-30 21:01:28] iter = 07140, loss = 1.6868
2024-10-30 21:01:32: [2024-10-30 21:01:32] iter = 07150, loss = 1.9408
2024-10-30 21:01:36: [2024-10-30 21:01:36] iter = 07160, loss = 2.4548
2024-10-30 21:01:40: [2024-10-30 21:01:40] iter = 07170, loss = 1.8366
2024-10-30 21:01:44: [2024-10-30 21:01:44] iter = 07180, loss = 1.7982
2024-10-30 21:01:49: [2024-10-30 21:01:49] iter = 07190, loss = 2.0843
2024-10-30 21:01:53: [2024-10-30 21:01:53] iter = 07200, loss = 2.6488
2024-10-30 21:01:57: [2024-10-30 21:01:57] iter = 07210, loss = 4.0280
2024-10-30 21:02:03: [2024-10-30 21:02:03] iter = 07220, loss = 2.9208
2024-10-30 21:02:07: [2024-10-30 21:02:07] iter = 07230, loss = 2.0375
2024-10-30 21:02:11: [2024-10-30 21:02:11] iter = 07240, loss = 1.9391
2024-10-30 21:02:16: [2024-10-30 21:02:16] iter = 07250, loss = 2.4487
2024-10-30 21:02:20: [2024-10-30 21:02:20] iter = 07260, loss = 2.3158
2024-10-30 21:02:24: [2024-10-30 21:02:24] iter = 07270, loss = 1.8814
2024-10-30 21:02:27: [2024-10-30 21:02:27] iter = 07280, loss = 2.2363
2024-10-30 21:02:31: [2024-10-30 21:02:31] iter = 07290, loss = 2.0727
2024-10-30 21:02:36: [2024-10-30 21:02:36] iter = 07300, loss = 2.0372
2024-10-30 21:02:39: [2024-10-30 21:02:39] iter = 07310, loss = 2.4467
2024-10-30 21:02:44: [2024-10-30 21:02:44] iter = 07320, loss = 2.1312
2024-10-30 21:02:48: [2024-10-30 21:02:48] iter = 07330, loss = 2.2466
2024-10-30 21:02:50: [2024-10-30 21:02:50] iter = 07340, loss = 1.8948
2024-10-30 21:02:54: [2024-10-30 21:02:54] iter = 07350, loss = 2.0336
2024-10-30 21:02:57: [2024-10-30 21:02:57] iter = 07360, loss = 2.1111
2024-10-30 21:03:01: [2024-10-30 21:03:01] iter = 07370, loss = 1.8671
2024-10-30 21:03:05: [2024-10-30 21:03:05] iter = 07380, loss = 2.5519
2024-10-30 21:03:09: [2024-10-30 21:03:09] iter = 07390, loss = 2.4799
2024-10-30 21:03:14: [2024-10-30 21:03:14] iter = 07400, loss = 2.1824
2024-10-30 21:03:18: [2024-10-30 21:03:18] iter = 07410, loss = 2.0223
2024-10-30 21:03:22: [2024-10-30 21:03:22] iter = 07420, loss = 2.5264
2024-10-30 21:03:25: [2024-10-30 21:03:25] iter = 07430, loss = 2.6503
2024-10-30 21:03:30: [2024-10-30 21:03:30] iter = 07440, loss = 2.0550
2024-10-30 21:03:34: [2024-10-30 21:03:34] iter = 07450, loss = 2.0972
2024-10-30 21:03:38: [2024-10-30 21:03:38] iter = 07460, loss = 2.5488
2024-10-30 21:03:42: [2024-10-30 21:03:42] iter = 07470, loss = 2.0069
2024-10-30 21:03:47: [2024-10-30 21:03:47] iter = 07480, loss = 2.0621
2024-10-30 21:03:50: [2024-10-30 21:03:50] iter = 07490, loss = 1.9615
2024-10-30 21:03:54: [2024-10-30 21:03:54] iter = 07500, loss = 2.3909
2024-10-30 21:03:57: [2024-10-30 21:03:57] iter = 07510, loss = 1.9161
2024-10-30 21:04:02: [2024-10-30 21:04:02] iter = 07520, loss = 2.2071
2024-10-30 21:04:06: [2024-10-30 21:04:06] iter = 07530, loss = 2.0656
2024-10-30 21:04:10: [2024-10-30 21:04:10] iter = 07540, loss = 3.2356
2024-10-30 21:04:14: [2024-10-30 21:04:14] iter = 07550, loss = 2.2276
2024-10-30 21:04:19: [2024-10-30 21:04:19] iter = 07560, loss = 3.4745
2024-10-30 21:04:23: [2024-10-30 21:04:23] iter = 07570, loss = 2.4906
2024-10-30 21:04:26: [2024-10-30 21:04:26] iter = 07580, loss = 2.2262
2024-10-30 21:04:30: [2024-10-30 21:04:30] iter = 07590, loss = 1.9426
2024-10-30 21:04:35: [2024-10-30 21:04:35] iter = 07600, loss = 1.9917
2024-10-30 21:04:39: [2024-10-30 21:04:39] iter = 07610, loss = 4.1150
2024-10-30 21:04:43: [2024-10-30 21:04:43] iter = 07620, loss = 2.3525
2024-10-30 21:04:48: [2024-10-30 21:04:48] iter = 07630, loss = 2.5157
2024-10-30 21:04:53: [2024-10-30 21:04:53] iter = 07640, loss = 2.7286
2024-10-30 21:04:57: [2024-10-30 21:04:57] iter = 07650, loss = 1.9926
2024-10-30 21:05:01: [2024-10-30 21:05:01] iter = 07660, loss = 2.5878
2024-10-30 21:05:05: [2024-10-30 21:05:05] iter = 07670, loss = 1.9484
2024-10-30 21:05:10: [2024-10-30 21:05:10] iter = 07680, loss = 2.2101
2024-10-30 21:05:14: [2024-10-30 21:05:14] iter = 07690, loss = 2.1934
2024-10-30 21:05:19: [2024-10-30 21:05:19] iter = 07700, loss = 2.3605
2024-10-30 21:05:22: [2024-10-30 21:05:22] iter = 07710, loss = 2.2315
2024-10-30 21:05:27: [2024-10-30 21:05:27] iter = 07720, loss = 2.2521
2024-10-30 21:05:32: [2024-10-30 21:05:32] iter = 07730, loss = 1.9576
2024-10-30 21:05:37: [2024-10-30 21:05:37] iter = 07740, loss = 2.7525
2024-10-30 21:05:41: [2024-10-30 21:05:41] iter = 07750, loss = 2.0827
2024-10-30 21:05:46: [2024-10-30 21:05:46] iter = 07760, loss = 1.8240
2024-10-30 21:05:49: [2024-10-30 21:05:49] iter = 07770, loss = 2.0163
2024-10-30 21:05:53: [2024-10-30 21:05:53] iter = 07780, loss = 2.5375
2024-10-30 21:05:57: [2024-10-30 21:05:57] iter = 07790, loss = 2.6764
2024-10-30 21:06:01: [2024-10-30 21:06:01] iter = 07800, loss = 2.1669
2024-10-30 21:06:06: [2024-10-30 21:06:06] iter = 07810, loss = 3.9865
2024-10-30 21:06:11: [2024-10-30 21:06:11] iter = 07820, loss = 4.8560
2024-10-30 21:06:16: [2024-10-30 21:06:16] iter = 07830, loss = 2.0814
2024-10-30 21:06:19: [2024-10-30 21:06:19] iter = 07840, loss = 1.8538
2024-10-30 21:06:23: [2024-10-30 21:06:23] iter = 07850, loss = 2.0468
2024-10-30 21:06:28: [2024-10-30 21:06:28] iter = 07860, loss = 2.2350
2024-10-30 21:06:30: [2024-10-30 21:06:30] iter = 07870, loss = 2.6437
2024-10-30 21:06:34: [2024-10-30 21:06:34] iter = 07880, loss = 2.0294
2024-10-30 21:06:37: [2024-10-30 21:06:37] iter = 07890, loss = 2.0381
2024-10-30 21:06:41: [2024-10-30 21:06:41] iter = 07900, loss = 5.6615
2024-10-30 21:06:44: [2024-10-30 21:06:44] iter = 07910, loss = 2.3902
2024-10-30 21:06:48: [2024-10-30 21:06:48] iter = 07920, loss = 2.2068
2024-10-30 21:06:51: [2024-10-30 21:06:51] iter = 07930, loss = 2.0385
2024-10-30 21:06:54: [2024-10-30 21:06:54] iter = 07940, loss = 3.4113
2024-10-30 21:06:58: [2024-10-30 21:06:58] iter = 07950, loss = 2.2686
2024-10-30 21:07:01: [2024-10-30 21:07:01] iter = 07960, loss = 1.7617
2024-10-30 21:07:05: [2024-10-30 21:07:05] iter = 07970, loss = 2.2803
2024-10-30 21:07:07: [2024-10-30 21:07:07] iter = 07980, loss = 2.5939
2024-10-30 21:07:10: [2024-10-30 21:07:10] iter = 07990, loss = 2.1578
2024-10-30 21:07:12: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
2024-10-30 21:07:12: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 21:07:12: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 32544}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 21:09:42: Evaluate 5 random ConvNet, ACCmean = 0.7816 ACCstd = 0.0032
-------------------------
2024-10-30 21:09:42: Evaluate 5 random ConvNet, SENmean = 0.7752 SENstd = 0.0030
-------------------------
2024-10-30 21:09:42: Evaluate 5 random ConvNet, SPEmean = 0.9781 SPEstd = 0.0003
-------------------------
2024-10-30 21:09:42: Evaluate 5 random ConvNet, F!mean = 0.7642 F!std = 0.0025
-------------------------
2024-10-30 21:09:42: Evaluate 5 random ConvNet, mean = 0.7816 std = 0.0032
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 21:09:43: [2024-10-30 21:09:43] iter = 08000, loss = 2.2362
2024-10-30 21:09:47: [2024-10-30 21:09:47] iter = 08010, loss = 3.1006
2024-10-30 21:09:51: [2024-10-30 21:09:51] iter = 08020, loss = 2.2507
2024-10-30 21:09:55: [2024-10-30 21:09:55] iter = 08030, loss = 1.9376
2024-10-30 21:09:59: [2024-10-30 21:09:59] iter = 08040, loss = 3.7002
2024-10-30 21:10:02: [2024-10-30 21:10:02] iter = 08050, loss = 1.9679
2024-10-30 21:10:07: [2024-10-30 21:10:07] iter = 08060, loss = 2.0996
2024-10-30 21:10:11: [2024-10-30 21:10:11] iter = 08070, loss = 2.5027
2024-10-30 21:10:15: [2024-10-30 21:10:15] iter = 08080, loss = 3.2038
2024-10-30 21:10:20: [2024-10-30 21:10:20] iter = 08090, loss = 2.2812
2024-10-30 21:10:23: [2024-10-30 21:10:23] iter = 08100, loss = 1.8552
2024-10-30 21:10:27: [2024-10-30 21:10:27] iter = 08110, loss = 2.3857
2024-10-30 21:10:31: [2024-10-30 21:10:31] iter = 08120, loss = 2.0671
2024-10-30 21:10:35: [2024-10-30 21:10:35] iter = 08130, loss = 1.9789
2024-10-30 21:10:38: [2024-10-30 21:10:38] iter = 08140, loss = 2.8911
2024-10-30 21:10:42: [2024-10-30 21:10:42] iter = 08150, loss = 2.5531
2024-10-30 21:10:46: [2024-10-30 21:10:46] iter = 08160, loss = 3.7978
2024-10-30 21:10:50: [2024-10-30 21:10:50] iter = 08170, loss = 1.7847
2024-10-30 21:10:55: [2024-10-30 21:10:55] iter = 08180, loss = 5.3222
2024-10-30 21:10:59: [2024-10-30 21:10:59] iter = 08190, loss = 2.4830
2024-10-30 21:11:03: [2024-10-30 21:11:03] iter = 08200, loss = 2.3724
2024-10-30 21:11:07: [2024-10-30 21:11:07] iter = 08210, loss = 3.6604
2024-10-30 21:11:11: [2024-10-30 21:11:11] iter = 08220, loss = 2.3133
2024-10-30 21:11:15: [2024-10-30 21:11:15] iter = 08230, loss = 3.0388
2024-10-30 21:11:19: [2024-10-30 21:11:19] iter = 08240, loss = 2.8282
2024-10-30 21:11:22: [2024-10-30 21:11:22] iter = 08250, loss = 1.9722
2024-10-30 21:11:26: [2024-10-30 21:11:26] iter = 08260, loss = 1.9644
2024-10-30 21:11:29: [2024-10-30 21:11:29] iter = 08270, loss = 2.2631
2024-10-30 21:11:34: [2024-10-30 21:11:34] iter = 08280, loss = 2.2846
2024-10-30 21:11:38: [2024-10-30 21:11:38] iter = 08290, loss = 2.6084
2024-10-30 21:11:42: [2024-10-30 21:11:42] iter = 08300, loss = 1.9132
2024-10-30 21:11:46: [2024-10-30 21:11:46] iter = 08310, loss = 2.0303
2024-10-30 21:11:49: [2024-10-30 21:11:49] iter = 08320, loss = 2.3372
2024-10-30 21:11:52: [2024-10-30 21:11:52] iter = 08330, loss = 1.9714
2024-10-30 21:11:56: [2024-10-30 21:11:56] iter = 08340, loss = 1.9143
2024-10-30 21:11:59: [2024-10-30 21:11:59] iter = 08350, loss = 2.4146
2024-10-30 21:12:02: [2024-10-30 21:12:02] iter = 08360, loss = 2.0135
2024-10-30 21:12:06: [2024-10-30 21:12:06] iter = 08370, loss = 2.1116
2024-10-30 21:12:09: [2024-10-30 21:12:09] iter = 08380, loss = 2.3919
2024-10-30 21:12:13: [2024-10-30 21:12:13] iter = 08390, loss = 2.5924
2024-10-30 21:12:17: [2024-10-30 21:12:17] iter = 08400, loss = 1.6836
2024-10-30 21:12:19: [2024-10-30 21:12:19] iter = 08410, loss = 1.8085
2024-10-30 21:12:22: [2024-10-30 21:12:22] iter = 08420, loss = 2.4462
2024-10-30 21:12:25: [2024-10-30 21:12:25] iter = 08430, loss = 1.9824
2024-10-30 21:12:29: [2024-10-30 21:12:29] iter = 08440, loss = 2.5569
2024-10-30 21:12:34: [2024-10-30 21:12:34] iter = 08450, loss = 2.2347
2024-10-30 21:12:38: [2024-10-30 21:12:38] iter = 08460, loss = 2.4617
2024-10-30 21:12:43: [2024-10-30 21:12:43] iter = 08470, loss = 2.8797
2024-10-30 21:12:47: [2024-10-30 21:12:47] iter = 08480, loss = 2.6824
2024-10-30 21:12:51: [2024-10-30 21:12:51] iter = 08490, loss = 2.6049
2024-10-30 21:12:53: [2024-10-30 21:12:53] iter = 08500, loss = 1.9689
2024-10-30 21:12:57: [2024-10-30 21:12:57] iter = 08510, loss = 2.1441
2024-10-30 21:13:01: [2024-10-30 21:13:01] iter = 08520, loss = 2.0041
2024-10-30 21:13:06: [2024-10-30 21:13:06] iter = 08530, loss = 2.3774
2024-10-30 21:13:11: [2024-10-30 21:13:11] iter = 08540, loss = 2.2061
2024-10-30 21:13:15: [2024-10-30 21:13:15] iter = 08550, loss = 2.0006
2024-10-30 21:13:19: [2024-10-30 21:13:19] iter = 08560, loss = 2.6498
2024-10-30 21:13:22: [2024-10-30 21:13:22] iter = 08570, loss = 2.0334
2024-10-30 21:13:26: [2024-10-30 21:13:26] iter = 08580, loss = 2.2089
2024-10-30 21:13:30: [2024-10-30 21:13:30] iter = 08590, loss = 2.7107
2024-10-30 21:13:34: [2024-10-30 21:13:34] iter = 08600, loss = 2.7584
2024-10-30 21:13:38: [2024-10-30 21:13:38] iter = 08610, loss = 1.6769
2024-10-30 21:13:41: [2024-10-30 21:13:41] iter = 08620, loss = 2.8305
2024-10-30 21:13:44: [2024-10-30 21:13:44] iter = 08630, loss = 2.6005
2024-10-30 21:13:48: [2024-10-30 21:13:48] iter = 08640, loss = 2.6594
2024-10-30 21:13:51: [2024-10-30 21:13:51] iter = 08650, loss = 1.9992
2024-10-30 21:13:55: [2024-10-30 21:13:55] iter = 08660, loss = 1.6634
2024-10-30 21:13:59: [2024-10-30 21:13:59] iter = 08670, loss = 2.1075
2024-10-30 21:14:03: [2024-10-30 21:14:03] iter = 08680, loss = 1.9965
2024-10-30 21:14:07: [2024-10-30 21:14:07] iter = 08690, loss = 2.4904
2024-10-30 21:14:11: [2024-10-30 21:14:11] iter = 08700, loss = 2.0250
2024-10-30 21:14:14: [2024-10-30 21:14:14] iter = 08710, loss = 1.9425
2024-10-30 21:14:18: [2024-10-30 21:14:18] iter = 08720, loss = 2.2515
2024-10-30 21:14:22: [2024-10-30 21:14:22] iter = 08730, loss = 1.7922
2024-10-30 21:14:24: [2024-10-30 21:14:24] iter = 08740, loss = 3.2695
2024-10-30 21:14:28: [2024-10-30 21:14:28] iter = 08750, loss = 2.1049
2024-10-30 21:14:31: [2024-10-30 21:14:31] iter = 08760, loss = 2.1402
2024-10-30 21:14:35: [2024-10-30 21:14:35] iter = 08770, loss = 2.6439
2024-10-30 21:14:39: [2024-10-30 21:14:39] iter = 08780, loss = 5.3734
2024-10-30 21:14:44: [2024-10-30 21:14:44] iter = 08790, loss = 2.3228
2024-10-30 21:14:48: [2024-10-30 21:14:48] iter = 08800, loss = 2.5782
2024-10-30 21:14:53: [2024-10-30 21:14:53] iter = 08810, loss = 1.9816
2024-10-30 21:14:56: [2024-10-30 21:14:56] iter = 08820, loss = 1.9938
2024-10-30 21:14:59: [2024-10-30 21:14:59] iter = 08830, loss = 2.3414
2024-10-30 21:15:03: [2024-10-30 21:15:03] iter = 08840, loss = 2.7593
2024-10-30 21:15:07: [2024-10-30 21:15:07] iter = 08850, loss = 2.0689
2024-10-30 21:15:11: [2024-10-30 21:15:11] iter = 08860, loss = 2.3538
2024-10-30 21:15:13: [2024-10-30 21:15:13] iter = 08870, loss = 1.8984
2024-10-30 21:15:17: [2024-10-30 21:15:17] iter = 08880, loss = 2.2403
2024-10-30 21:15:20: [2024-10-30 21:15:20] iter = 08890, loss = 2.5691
2024-10-30 21:15:24: [2024-10-30 21:15:24] iter = 08900, loss = 8.6414
2024-10-30 21:15:27: [2024-10-30 21:15:27] iter = 08910, loss = 2.3745
2024-10-30 21:15:31: [2024-10-30 21:15:31] iter = 08920, loss = 3.8548
2024-10-30 21:15:35: [2024-10-30 21:15:35] iter = 08930, loss = 1.9671
2024-10-30 21:15:39: [2024-10-30 21:15:39] iter = 08940, loss = 2.5642
2024-10-30 21:15:43: [2024-10-30 21:15:43] iter = 08950, loss = 2.0231
2024-10-30 21:15:47: [2024-10-30 21:15:47] iter = 08960, loss = 2.4199
2024-10-30 21:15:51: [2024-10-30 21:15:51] iter = 08970, loss = 2.4888
2024-10-30 21:15:56: [2024-10-30 21:15:56] iter = 08980, loss = 2.0051
2024-10-30 21:16:00: [2024-10-30 21:16:00] iter = 08990, loss = 2.8233
2024-10-30 21:16:04: [2024-10-30 21:16:04] iter = 09000, loss = 2.3022
2024-10-30 21:16:08: [2024-10-30 21:16:08] iter = 09010, loss = 5.0805
2024-10-30 21:16:11: [2024-10-30 21:16:11] iter = 09020, loss = 2.7903
2024-10-30 21:16:15: [2024-10-30 21:16:15] iter = 09030, loss = 2.2008
2024-10-30 21:16:19: [2024-10-30 21:16:19] iter = 09040, loss = 2.3044
2024-10-30 21:16:23: [2024-10-30 21:16:23] iter = 09050, loss = 3.0316
2024-10-30 21:16:27: [2024-10-30 21:16:27] iter = 09060, loss = 2.1485
2024-10-30 21:16:31: [2024-10-30 21:16:31] iter = 09070, loss = 2.1748
2024-10-30 21:16:35: [2024-10-30 21:16:35] iter = 09080, loss = 2.2240
2024-10-30 21:16:39: [2024-10-30 21:16:39] iter = 09090, loss = 2.1122
2024-10-30 21:16:42: [2024-10-30 21:16:42] iter = 09100, loss = 2.0530
2024-10-30 21:16:45: [2024-10-30 21:16:45] iter = 09110, loss = 2.1624
2024-10-30 21:16:49: [2024-10-30 21:16:49] iter = 09120, loss = 1.9639
2024-10-30 21:16:53: [2024-10-30 21:16:53] iter = 09130, loss = 3.6187
2024-10-30 21:16:57: [2024-10-30 21:16:57] iter = 09140, loss = 1.8849
2024-10-30 21:17:01: [2024-10-30 21:17:01] iter = 09150, loss = 1.7325
2024-10-30 21:17:05: [2024-10-30 21:17:05] iter = 09160, loss = 1.8654
2024-10-30 21:17:08: [2024-10-30 21:17:08] iter = 09170, loss = 2.5660
2024-10-30 21:17:12: [2024-10-30 21:17:12] iter = 09180, loss = 2.0789
2024-10-30 21:17:16: [2024-10-30 21:17:16] iter = 09190, loss = 2.1856
2024-10-30 21:17:19: [2024-10-30 21:17:19] iter = 09200, loss = 3.5630
2024-10-30 21:17:23: [2024-10-30 21:17:23] iter = 09210, loss = 1.8987
2024-10-30 21:17:27: [2024-10-30 21:17:27] iter = 09220, loss = 2.2290
2024-10-30 21:17:30: [2024-10-30 21:17:30] iter = 09230, loss = 2.2731
2024-10-30 21:17:34: [2024-10-30 21:17:34] iter = 09240, loss = 2.3835
2024-10-30 21:17:38: [2024-10-30 21:17:38] iter = 09250, loss = 1.9453
2024-10-30 21:17:43: [2024-10-30 21:17:43] iter = 09260, loss = 2.2519
2024-10-30 21:17:48: [2024-10-30 21:17:48] iter = 09270, loss = 1.9414
2024-10-30 21:17:51: [2024-10-30 21:17:51] iter = 09280, loss = 6.7467
2024-10-30 21:17:55: [2024-10-30 21:17:55] iter = 09290, loss = 2.1778
2024-10-30 21:17:59: [2024-10-30 21:17:59] iter = 09300, loss = 2.0220
2024-10-30 21:18:03: [2024-10-30 21:18:03] iter = 09310, loss = 2.6606
2024-10-30 21:18:07: [2024-10-30 21:18:07] iter = 09320, loss = 8.2703
2024-10-30 21:18:11: [2024-10-30 21:18:11] iter = 09330, loss = 2.1678
2024-10-30 21:18:15: [2024-10-30 21:18:15] iter = 09340, loss = 2.2460
2024-10-30 21:18:20: [2024-10-30 21:18:20] iter = 09350, loss = 3.0388
2024-10-30 21:18:24: [2024-10-30 21:18:24] iter = 09360, loss = 2.0928
2024-10-30 21:18:28: [2024-10-30 21:18:28] iter = 09370, loss = 1.8810
2024-10-30 21:18:31: [2024-10-30 21:18:31] iter = 09380, loss = 2.2756
2024-10-30 21:18:35: [2024-10-30 21:18:35] iter = 09390, loss = 2.1162
2024-10-30 21:18:39: [2024-10-30 21:18:39] iter = 09400, loss = 2.3814
2024-10-30 21:18:43: [2024-10-30 21:18:43] iter = 09410, loss = 1.9735
2024-10-30 21:18:47: [2024-10-30 21:18:47] iter = 09420, loss = 3.7795
2024-10-30 21:18:52: [2024-10-30 21:18:52] iter = 09430, loss = 4.8706
2024-10-30 21:18:55: [2024-10-30 21:18:55] iter = 09440, loss = 2.7537
2024-10-30 21:18:59: [2024-10-30 21:18:59] iter = 09450, loss = 1.8655
2024-10-30 21:19:03: [2024-10-30 21:19:03] iter = 09460, loss = 1.8698
2024-10-30 21:19:07: [2024-10-30 21:19:07] iter = 09470, loss = 2.0013
2024-10-30 21:19:11: [2024-10-30 21:19:11] iter = 09480, loss = 2.1603
2024-10-30 21:19:15: [2024-10-30 21:19:15] iter = 09490, loss = 2.2537
2024-10-30 21:19:18: [2024-10-30 21:19:18] iter = 09500, loss = 2.9829
2024-10-30 21:19:21: [2024-10-30 21:19:21] iter = 09510, loss = 2.2233
2024-10-30 21:19:24: [2024-10-30 21:19:24] iter = 09520, loss = 2.4716
2024-10-30 21:19:28: [2024-10-30 21:19:28] iter = 09530, loss = 2.2082
2024-10-30 21:19:32: [2024-10-30 21:19:32] iter = 09540, loss = 1.8101
2024-10-30 21:19:35: [2024-10-30 21:19:35] iter = 09550, loss = 2.3557
2024-10-30 21:19:39: [2024-10-30 21:19:39] iter = 09560, loss = 1.7769
2024-10-30 21:19:43: [2024-10-30 21:19:43] iter = 09570, loss = 4.3012
2024-10-30 21:19:47: [2024-10-30 21:19:47] iter = 09580, loss = 2.2205
2024-10-30 21:19:51: [2024-10-30 21:19:51] iter = 09590, loss = 2.7138
2024-10-30 21:19:56: [2024-10-30 21:19:56] iter = 09600, loss = 3.1063
2024-10-30 21:19:59: [2024-10-30 21:19:59] iter = 09610, loss = 2.2495
2024-10-30 21:20:03: [2024-10-30 21:20:03] iter = 09620, loss = 2.0521
2024-10-30 21:20:07: [2024-10-30 21:20:07] iter = 09630, loss = 2.0961
2024-10-30 21:20:11: [2024-10-30 21:20:11] iter = 09640, loss = 1.8458
2024-10-30 21:20:15: [2024-10-30 21:20:15] iter = 09650, loss = 2.6930
2024-10-30 21:20:19: [2024-10-30 21:20:19] iter = 09660, loss = 2.1070
2024-10-30 21:20:23: [2024-10-30 21:20:23] iter = 09670, loss = 2.4484
2024-10-30 21:20:27: [2024-10-30 21:20:27] iter = 09680, loss = 3.5367
2024-10-30 21:20:31: [2024-10-30 21:20:31] iter = 09690, loss = 2.8626
2024-10-30 21:20:34: [2024-10-30 21:20:34] iter = 09700, loss = 2.1985
2024-10-30 21:20:37: [2024-10-30 21:20:37] iter = 09710, loss = 2.6291
2024-10-30 21:20:41: [2024-10-30 21:20:41] iter = 09720, loss = 2.4935
2024-10-30 21:20:46: [2024-10-30 21:20:46] iter = 09730, loss = 2.0499
2024-10-30 21:20:50: [2024-10-30 21:20:50] iter = 09740, loss = 2.2156
2024-10-30 21:20:54: [2024-10-30 21:20:54] iter = 09750, loss = 3.6293
2024-10-30 21:20:57: [2024-10-30 21:20:57] iter = 09760, loss = 3.1966
2024-10-30 21:21:00: [2024-10-30 21:21:00] iter = 09770, loss = 1.8883
2024-10-30 21:21:04: [2024-10-30 21:21:04] iter = 09780, loss = 1.9310
2024-10-30 21:21:08: [2024-10-30 21:21:08] iter = 09790, loss = 2.5730
2024-10-30 21:21:11: [2024-10-30 21:21:11] iter = 09800, loss = 2.4023
2024-10-30 21:21:14: [2024-10-30 21:21:14] iter = 09810, loss = 2.3219
2024-10-30 21:21:19: [2024-10-30 21:21:19] iter = 09820, loss = 1.9454
2024-10-30 21:21:24: [2024-10-30 21:21:24] iter = 09830, loss = 1.8309
2024-10-30 21:21:27: [2024-10-30 21:21:27] iter = 09840, loss = 1.8400
2024-10-30 21:21:31: [2024-10-30 21:21:31] iter = 09850, loss = 4.5758
2024-10-30 21:21:35: [2024-10-30 21:21:35] iter = 09860, loss = 1.9157
2024-10-30 21:21:39: [2024-10-30 21:21:39] iter = 09870, loss = 2.0978
2024-10-30 21:21:42: [2024-10-30 21:21:42] iter = 09880, loss = 2.0462
2024-10-30 21:21:47: [2024-10-30 21:21:47] iter = 09890, loss = 2.2009
2024-10-30 21:21:51: [2024-10-30 21:21:51] iter = 09900, loss = 2.1796
2024-10-30 21:21:56: [2024-10-30 21:21:56] iter = 09910, loss = 1.7284
2024-10-30 21:22:00: [2024-10-30 21:22:00] iter = 09920, loss = 2.3014
2024-10-30 21:22:04: [2024-10-30 21:22:04] iter = 09930, loss = 3.1917
2024-10-30 21:22:08: [2024-10-30 21:22:08] iter = 09940, loss = 2.2381
2024-10-30 21:22:12: [2024-10-30 21:22:12] iter = 09950, loss = 2.1756
2024-10-30 21:22:16: [2024-10-30 21:22:16] iter = 09960, loss = 2.3773
2024-10-30 21:22:19: [2024-10-30 21:22:19] iter = 09970, loss = 1.9707
2024-10-30 21:22:24: [2024-10-30 21:22:24] iter = 09980, loss = 2.2871
2024-10-30 21:22:28: [2024-10-30 21:22:28] iter = 09990, loss = 2.3702
2024-10-30 21:22:32: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
2024-10-30 21:22:32: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 21:22:32: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 52684}

[2024-10-30 19:14:03] Evaluate_02: epoch = 1000 train time = 27 s train loss = 0.026767 train acc = 0.9909, test acc = 0.7904, test_sen =0.7907, test_spe =0.9790, test_f1 =0.7763
[2024-10-30 19:14:35] Evaluate_03: epoch = 1000 train time = 30 s train loss = 0.012859 train acc = 1.0000, test acc = 0.7734, test_sen =0.7764, test_spe =0.9774, test_f1 =0.7598
[2024-10-30 19:15:05] Evaluate_04: epoch = 1000 train time = 28 s train loss = 0.036040 train acc = 1.0000, test acc = 0.7773, test_sen =0.7768, test_spe =0.9777, test_f1 =0.7637
[2024-10-30 19:27:41] Evaluate_00: epoch = 1000 train time = 26 s train loss = 0.006166 train acc = 1.0000, test acc = 0.7738, test_sen =0.7733, test_spe =0.9772, test_f1 =0.7598
[2024-10-30 19:28:10] Evaluate_01: epoch = 1000 train time = 25 s train loss = 0.005748 train acc = 1.0000, test acc = 0.7711, test_sen =0.7713, test_spe =0.9770, test_f1 =0.7574
[2024-10-30 19:28:41] Evaluate_02: epoch = 1000 train time = 29 s train loss = 0.005461 train acc = 1.0000, test acc = 0.7761, test_sen =0.7773, test_spe =0.9775, test_f1 =0.7631
[2024-10-30 19:29:10] Evaluate_03: epoch = 1000 train time = 26 s train loss = 0.046864 train acc = 0.9909, test acc = 0.7670, test_sen =0.7674, test_spe =0.9766, test_f1 =0.7521
[2024-10-30 19:29:38] Evaluate_04: epoch = 1000 train time = 25 s train loss = 0.002983 train acc = 1.0000, test acc = 0.7775, test_sen =0.7783, test_spe =0.9776, test_f1 =0.7638
[2024-10-30 19:42:26] Evaluate_00: epoch = 1000 train time = 26 s train loss = 0.010235 train acc = 1.0000, test acc = 0.7617, test_sen =0.7693, test_spe =0.9762, test_f1 =0.7520
[2024-10-30 19:42:55] Evaluate_01: epoch = 1000 train time = 27 s train loss = 0.006963 train acc = 1.0000, test acc = 0.7650, test_sen =0.7739, test_spe =0.9765, test_f1 =0.7568
[2024-10-30 19:43:24] Evaluate_02: epoch = 1000 train time = 27 s train loss = 0.030692 train acc = 1.0000, test acc = 0.7655, test_sen =0.7725, test_spe =0.9766, test_f1 =0.7561
[2024-10-30 19:43:52] Evaluate_03: epoch = 1000 train time = 25 s train loss = 0.014139 train acc = 1.0000, test acc = 0.7681, test_sen =0.7754, test_spe =0.9769, test_f1 =0.7572
[2024-10-30 19:44:22] Evaluate_04: epoch = 1000 train time = 27 s train loss = 0.002539 train acc = 1.0000, test acc = 0.7769, test_sen =0.7821, test_spe =0.9777, test_f1 =0.7657
[2024-10-30 19:57:27] Evaluate_00: epoch = 1000 train time = 30 s train loss = 0.006143 train acc = 1.0000, test acc = 0.7814, test_sen =0.7833, test_spe =0.9782, test_f1 =0.7665
[2024-10-30 19:57:54] Evaluate_01: epoch = 1000 train time = 24 s train loss = 0.003626 train acc = 1.0000, test acc = 0.7838, test_sen =0.7830, test_spe =0.9784, test_f1 =0.7694
[2024-10-30 19:58:27] Evaluate_02: epoch = 1000 train time = 30 s train loss = 0.031000 train acc = 1.0000, test acc = 0.7878, test_sen =0.7868, test_spe =0.9788, test_f1 =0.7723
[2024-10-30 19:58:59] Evaluate_03: epoch = 1000 train time = 30 s train loss = 0.004202 train acc = 1.0000, test acc = 0.7844, test_sen =0.7838, test_spe =0.9784, test_f1 =0.7691
[2024-10-30 19:59:31] Evaluate_04: epoch = 1000 train time = 30 s train loss = 0.004522 train acc = 1.0000, test acc = 0.7893, test_sen =0.7863, test_spe =0.9789, test_f1 =0.7732
[2024-10-30 20:00:07] Evaluate_00: epoch = 1000 train time = 30 s train loss = 0.010196 train acc = 1.0000, test acc = 0.6984, test_sen =0.6799, test_spe =0.9696, test_f1 =0.6768
[2024-10-30 20:00:37] Evaluate_01: epoch = 1000 train time = 28 s train loss = 0.006726 train acc = 1.0000, test acc = 0.6921, test_sen =0.6734, test_spe =0.9689, test_f1 =0.6690
[2024-10-30 20:01:09] Evaluate_02: epoch = 1000 train time = 29 s train loss = 0.001845 train acc = 1.0000, test acc = 0.6902, test_sen =0.6712, test_spe =0.9687, test_f1 =0.6664
[2024-10-30 20:01:40] Evaluate_03: epoch = 1000 train time = 28 s train loss = 0.007707 train acc = 1.0000, test acc = 0.6949, test_sen =0.6751, test_spe =0.9692, test_f1 =0.6717
[2024-10-30 20:02:11] Evaluate_04: epoch = 1000 train time = 28 s train loss = 0.002921 train acc = 1.0000, test acc = 0.6852, test_sen =0.6695, test_spe =0.9682, test_f1 =0.6659
[2024-10-30 20:15:46] Evaluate_00: epoch = 1000 train time = 29 s train loss = 0.005589 train acc = 1.0000, test acc = 0.7906, test_sen =0.7888, test_spe =0.9790, test_f1 =0.7776
[2024-10-30 20:16:20] Evaluate_01: epoch = 1000 train time = 32 s train loss = 0.006533 train acc = 1.0000, test acc = 0.7879, test_sen =0.7853, test_spe =0.9787, test_f1 =0.7730
[2024-10-30 20:16:55] Evaluate_02: epoch = 1000 train time = 32 s train loss = 0.004989 train acc = 1.0000, test acc = 0.7904, test_sen =0.7868, test_spe =0.9789, test_f1 =0.7780
[2024-10-30 20:17:32] Evaluate_03: epoch = 1000 train time = 34 s train loss = 0.009223 train acc = 1.0000, test acc = 0.7885, test_sen =0.7846, test_spe =0.9787, test_f1 =0.7766
[2024-10-30 20:18:06] Evaluate_04: epoch = 1000 train time = 32 s train loss = 0.020613 train acc = 1.0000, test acc = 0.7823, test_sen =0.7783, test_spe =0.9781, test_f1 =0.7680
[2024-10-30 20:33:32] Evaluate_00: epoch = 1000 train time = 26 s train loss = 0.004468 train acc = 1.0000, test acc = 0.7799, test_sen =0.7732, test_spe =0.9779, test_f1 =0.7659
[2024-10-30 20:34:00] Evaluate_01: epoch = 1000 train time = 26 s train loss = 0.006285 train acc = 1.0000, test acc = 0.7818, test_sen =0.7763, test_spe =0.9781, test_f1 =0.7664
[2024-10-30 20:34:28] Evaluate_02: epoch = 1000 train time = 26 s train loss = 0.005812 train acc = 1.0000, test acc = 0.7756, test_sen =0.7717, test_spe =0.9775, test_f1 =0.7616
[2024-10-30 20:34:55] Evaluate_03: epoch = 1000 train time = 24 s train loss = 0.008135 train acc = 1.0000, test acc = 0.7850, test_sen =0.7780, test_spe =0.9784, test_f1 =0.7711
[2024-10-30 20:35:22] Evaluate_04: epoch = 1000 train time = 25 s train loss = 0.007801 train acc = 1.0000, test acc = 0.7875, test_sen =0.7835, test_spe =0.9787, test_f1 =0.7727
[2024-10-30 20:50:41] Evaluate_00: epoch = 1000 train time = 33 s train loss = 0.005084 train acc = 1.0000, test acc = 0.7774, test_sen =0.7747, test_spe =0.9776, test_f1 =0.7636
[2024-10-30 20:51:20] Evaluate_01: epoch = 1000 train time = 36 s train loss = 0.007726 train acc = 1.0000, test acc = 0.7857, test_sen =0.7813, test_spe =0.9785, test_f1 =0.7715
[2024-10-30 20:51:54] Evaluate_02: epoch = 1000 train time = 32 s train loss = 0.006110 train acc = 1.0000, test acc = 0.7786, test_sen =0.7732, test_spe =0.9777, test_f1 =0.7654
[2024-10-30 20:52:30] Evaluate_03: epoch = 1000 train time = 33 s train loss = 0.006871 train acc = 1.0000, test acc = 0.7918, test_sen =0.7851, test_spe =0.9791, test_f1 =0.7752
[2024-10-30 20:53:04] Evaluate_04: epoch = 1000 train time = 31 s train loss = 0.031107 train acc = 1.0000, test acc = 0.7851, test_sen =0.7846, test_spe =0.9784, test_f1 =0.7732
[2024-10-30 21:07:42] Evaluate_00: epoch = 1000 train time = 27 s train loss = 0.003875 train acc = 1.0000, test acc = 0.7820, test_sen =0.7763, test_spe =0.9781, test_f1 =0.7647
[2024-10-30 21:08:10] Evaluate_01: epoch = 1000 train time = 26 s train loss = 0.031678 train acc = 1.0000, test acc = 0.7807, test_sen =0.7740, test_spe =0.9780, test_f1 =0.7627
[2024-10-30 21:08:42] Evaluate_02: epoch = 1000 train time = 29 s train loss = 0.006461 train acc = 1.0000, test acc = 0.7843, test_sen =0.7776, test_spe =0.9783, test_f1 =0.7662
[2024-10-30 21:09:13] Evaluate_03: epoch = 1000 train time = 29 s train loss = 0.009897 train acc = 1.0000, test acc = 0.7759, test_sen =0.7700, test_spe =0.9775, test_f1 =0.7602
[2024-10-30 21:09:42] Evaluate_04: epoch = 1000 train time = 27 s train loss = 0.023488 train acc = 1.0000, test acc = 0.7850, test_sen =0.7781, test_spe =0.9784, test_f1 =0.7671
[2024-10-30 21:22:59] Evaluate_00: epoch = 1000 train time = 24 s train loss = 0.003784 train acc = 1.0000, test acc = 0.7562, test_sen =0.7537, test_spe =0.9755, test_f1 =0.7438
[2024-10-30 21:23:32] Evaluate_01: epoch = 1000 train time = 30 s train loss = 0.005927 train acc = 1.0000, test acc = 0.7740, test_sen =0.7701, test_spe =0.9773, test_f1 =0.7607
[2024-10-30 21:24:04] Evaluate_02: epoch = 1000 train time = 30 s train loss = 0.024027 train acc = 1.0000, test acc = 0.7582, test_sen =0.7592, test_spe =0.9757, test_f1 =0.7495/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 21:25:04: Evaluate 5 random ConvNet, ACCmean = 0.7637 ACCstd = 0.0063
-------------------------
2024-10-30 21:25:04: Evaluate 5 random ConvNet, SENmean = 0.7624 SENstd = 0.0056
-------------------------
2024-10-30 21:25:04: Evaluate 5 random ConvNet, SPEmean = 0.9763 SPEstd = 0.0006
-------------------------
2024-10-30 21:25:04: Evaluate 5 random ConvNet, F!mean = 0.7533 F!std = 0.0060
-------------------------
2024-10-30 21:25:04: Evaluate 5 random ConvNet, mean = 0.7637 std = 0.0063
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 21:25:04: [2024-10-30 21:25:04] iter = 10000, loss = 2.3099
2024-10-30 21:25:08: [2024-10-30 21:25:08] iter = 10010, loss = 2.4425
2024-10-30 21:25:12: [2024-10-30 21:25:12] iter = 10020, loss = 1.8557
2024-10-30 21:25:15: [2024-10-30 21:25:15] iter = 10030, loss = 2.4153
2024-10-30 21:25:20: [2024-10-30 21:25:20] iter = 10040, loss = 2.1006
2024-10-30 21:25:23: [2024-10-30 21:25:23] iter = 10050, loss = 1.9497
2024-10-30 21:25:26: [2024-10-30 21:25:26] iter = 10060, loss = 1.9073
2024-10-30 21:25:29: [2024-10-30 21:25:29] iter = 10070, loss = 2.3057
2024-10-30 21:25:32: [2024-10-30 21:25:32] iter = 10080, loss = 3.1673
2024-10-30 21:25:36: [2024-10-30 21:25:36] iter = 10090, loss = 2.0517
2024-10-30 21:25:41: [2024-10-30 21:25:41] iter = 10100, loss = 3.5171
2024-10-30 21:25:45: [2024-10-30 21:25:45] iter = 10110, loss = 2.7899
2024-10-30 21:25:49: [2024-10-30 21:25:49] iter = 10120, loss = 3.7286
2024-10-30 21:25:53: [2024-10-30 21:25:53] iter = 10130, loss = 2.5740
2024-10-30 21:25:57: [2024-10-30 21:25:57] iter = 10140, loss = 2.3179
2024-10-30 21:26:02: [2024-10-30 21:26:02] iter = 10150, loss = 2.2010
2024-10-30 21:26:04: [2024-10-30 21:26:04] iter = 10160, loss = 2.0001
2024-10-30 21:26:07: [2024-10-30 21:26:07] iter = 10170, loss = 1.9185
2024-10-30 21:26:11: [2024-10-30 21:26:11] iter = 10180, loss = 2.1293
2024-10-30 21:26:15: [2024-10-30 21:26:15] iter = 10190, loss = 1.9545
2024-10-30 21:26:19: [2024-10-30 21:26:19] iter = 10200, loss = 2.4763
2024-10-30 21:26:22: [2024-10-30 21:26:22] iter = 10210, loss = 2.3371
2024-10-30 21:26:26: [2024-10-30 21:26:26] iter = 10220, loss = 1.9707
2024-10-30 21:26:30: [2024-10-30 21:26:30] iter = 10230, loss = 2.8280
2024-10-30 21:26:35: [2024-10-30 21:26:35] iter = 10240, loss = 2.0908
2024-10-30 21:26:39: [2024-10-30 21:26:39] iter = 10250, loss = 2.2165
2024-10-30 21:26:42: [2024-10-30 21:26:42] iter = 10260, loss = 2.4827
2024-10-30 21:26:46: [2024-10-30 21:26:46] iter = 10270, loss = 2.7423
2024-10-30 21:26:50: [2024-10-30 21:26:50] iter = 10280, loss = 4.2450
2024-10-30 21:26:55: [2024-10-30 21:26:55] iter = 10290, loss = 2.5970
2024-10-30 21:26:59: [2024-10-30 21:26:59] iter = 10300, loss = 2.1243
2024-10-30 21:27:02: [2024-10-30 21:27:02] iter = 10310, loss = 2.0568
2024-10-30 21:27:06: [2024-10-30 21:27:06] iter = 10320, loss = 2.2855
2024-10-30 21:27:11: [2024-10-30 21:27:11] iter = 10330, loss = 3.9999
2024-10-30 21:27:14: [2024-10-30 21:27:14] iter = 10340, loss = 2.2136
2024-10-30 21:27:18: [2024-10-30 21:27:18] iter = 10350, loss = 1.7462
2024-10-30 21:27:21: [2024-10-30 21:27:21] iter = 10360, loss = 2.1261
2024-10-30 21:27:25: [2024-10-30 21:27:25] iter = 10370, loss = 2.3191
2024-10-30 21:27:29: [2024-10-30 21:27:29] iter = 10380, loss = 3.6212
2024-10-30 21:27:33: [2024-10-30 21:27:33] iter = 10390, loss = 2.4154
2024-10-30 21:27:37: [2024-10-30 21:27:37] iter = 10400, loss = 1.9732
2024-10-30 21:27:41: [2024-10-30 21:27:41] iter = 10410, loss = 3.8852
2024-10-30 21:27:46: [2024-10-30 21:27:46] iter = 10420, loss = 2.1218
2024-10-30 21:27:50: [2024-10-30 21:27:50] iter = 10430, loss = 1.9603
2024-10-30 21:27:55: [2024-10-30 21:27:55] iter = 10440, loss = 4.1385
2024-10-30 21:27:59: [2024-10-30 21:27:59] iter = 10450, loss = 3.3315
2024-10-30 21:28:03: [2024-10-30 21:28:03] iter = 10460, loss = 1.9875
2024-10-30 21:28:06: [2024-10-30 21:28:06] iter = 10470, loss = 2.6884
2024-10-30 21:28:10: [2024-10-30 21:28:10] iter = 10480, loss = 1.9517
2024-10-30 21:28:14: [2024-10-30 21:28:14] iter = 10490, loss = 5.2214
2024-10-30 21:28:18: [2024-10-30 21:28:18] iter = 10500, loss = 2.9431
2024-10-30 21:28:21: [2024-10-30 21:28:21] iter = 10510, loss = 2.2554
2024-10-30 21:28:24: [2024-10-30 21:28:24] iter = 10520, loss = 2.6397
2024-10-30 21:28:28: [2024-10-30 21:28:28] iter = 10530, loss = 2.0292
2024-10-30 21:28:32: [2024-10-30 21:28:32] iter = 10540, loss = 2.4661
2024-10-30 21:28:36: [2024-10-30 21:28:36] iter = 10550, loss = 2.5686
2024-10-30 21:28:41: [2024-10-30 21:28:41] iter = 10560, loss = 2.6818
2024-10-30 21:28:45: [2024-10-30 21:28:45] iter = 10570, loss = 7.6210
2024-10-30 21:28:49: [2024-10-30 21:28:49] iter = 10580, loss = 2.7078
2024-10-30 21:28:53: [2024-10-30 21:28:53] iter = 10590, loss = 2.2766
2024-10-30 21:28:57: [2024-10-30 21:28:57] iter = 10600, loss = 2.7331
2024-10-30 21:29:00: [2024-10-30 21:29:00] iter = 10610, loss = 2.0334
2024-10-30 21:29:03: [2024-10-30 21:29:03] iter = 10620, loss = 1.7881
2024-10-30 21:29:06: [2024-10-30 21:29:06] iter = 10630, loss = 2.5877
2024-10-30 21:29:10: [2024-10-30 21:29:10] iter = 10640, loss = 2.4789
2024-10-30 21:29:15: [2024-10-30 21:29:15] iter = 10650, loss = 2.0315
2024-10-30 21:29:18: [2024-10-30 21:29:18] iter = 10660, loss = 3.0454
2024-10-30 21:29:22: [2024-10-30 21:29:22] iter = 10670, loss = 2.2057
2024-10-30 21:29:26: [2024-10-30 21:29:26] iter = 10680, loss = 2.1727
2024-10-30 21:29:30: [2024-10-30 21:29:30] iter = 10690, loss = 3.6223
2024-10-30 21:29:33: [2024-10-30 21:29:33] iter = 10700, loss = 3.1938
2024-10-30 21:29:37: [2024-10-30 21:29:37] iter = 10710, loss = 2.1764
2024-10-30 21:29:40: [2024-10-30 21:29:40] iter = 10720, loss = 2.9341
2024-10-30 21:29:44: [2024-10-30 21:29:44] iter = 10730, loss = 2.8520
2024-10-30 21:29:48: [2024-10-30 21:29:48] iter = 10740, loss = 3.1705
2024-10-30 21:29:52: [2024-10-30 21:29:52] iter = 10750, loss = 2.8023
2024-10-30 21:29:55: [2024-10-30 21:29:55] iter = 10760, loss = 2.3749
2024-10-30 21:29:59: [2024-10-30 21:29:59] iter = 10770, loss = 2.2192
2024-10-30 21:30:03: [2024-10-30 21:30:03] iter = 10780, loss = 5.4656
2024-10-30 21:30:06: [2024-10-30 21:30:06] iter = 10790, loss = 2.1675
2024-10-30 21:30:09: [2024-10-30 21:30:09] iter = 10800, loss = 1.7140
2024-10-30 21:30:12: [2024-10-30 21:30:12] iter = 10810, loss = 2.1459
2024-10-30 21:30:16: [2024-10-30 21:30:16] iter = 10820, loss = 1.7345
2024-10-30 21:30:18: [2024-10-30 21:30:18] iter = 10830, loss = 3.3109
2024-10-30 21:30:21: [2024-10-30 21:30:21] iter = 10840, loss = 1.7587
2024-10-30 21:30:25: [2024-10-30 21:30:25] iter = 10850, loss = 2.5536
2024-10-30 21:30:29: [2024-10-30 21:30:29] iter = 10860, loss = 1.8033
2024-10-30 21:30:33: [2024-10-30 21:30:33] iter = 10870, loss = 1.8719
2024-10-30 21:30:37: [2024-10-30 21:30:37] iter = 10880, loss = 2.9301
2024-10-30 21:30:41: [2024-10-30 21:30:41] iter = 10890, loss = 2.4504
2024-10-30 21:30:44: [2024-10-30 21:30:44] iter = 10900, loss = 2.0391
2024-10-30 21:30:49: [2024-10-30 21:30:49] iter = 10910, loss = 2.3053
2024-10-30 21:30:52: [2024-10-30 21:30:52] iter = 10920, loss = 2.1131
2024-10-30 21:30:55: [2024-10-30 21:30:55] iter = 10930, loss = 3.4391
2024-10-30 21:30:59: [2024-10-30 21:30:59] iter = 10940, loss = 2.1320
2024-10-30 21:31:02: [2024-10-30 21:31:02] iter = 10950, loss = 1.9456
2024-10-30 21:31:06: [2024-10-30 21:31:06] iter = 10960, loss = 2.4797
2024-10-30 21:31:10: [2024-10-30 21:31:10] iter = 10970, loss = 2.2635
2024-10-30 21:31:14: [2024-10-30 21:31:14] iter = 10980, loss = 3.4668
2024-10-30 21:31:18: [2024-10-30 21:31:18] iter = 10990, loss = 2.1165
2024-10-30 21:31:23: [2024-10-30 21:31:23] iter = 11000, loss = 3.4676
2024-10-30 21:31:28: [2024-10-30 21:31:28] iter = 11010, loss = 2.3257
2024-10-30 21:31:32: [2024-10-30 21:31:32] iter = 11020, loss = 2.5212
2024-10-30 21:31:36: [2024-10-30 21:31:36] iter = 11030, loss = 2.2404
2024-10-30 21:31:40: [2024-10-30 21:31:40] iter = 11040, loss = 2.0960
2024-10-30 21:31:44: [2024-10-30 21:31:44] iter = 11050, loss = 1.9824
2024-10-30 21:31:47: [2024-10-30 21:31:47] iter = 11060, loss = 2.0052
2024-10-30 21:31:53: [2024-10-30 21:31:53] iter = 11070, loss = 3.0852
2024-10-30 21:31:56: [2024-10-30 21:31:56] iter = 11080, loss = 2.7968
2024-10-30 21:32:00: [2024-10-30 21:32:00] iter = 11090, loss = 3.5008
2024-10-30 21:32:03: [2024-10-30 21:32:03] iter = 11100, loss = 2.9367
2024-10-30 21:32:07: [2024-10-30 21:32:07] iter = 11110, loss = 1.8140
2024-10-30 21:32:11: [2024-10-30 21:32:11] iter = 11120, loss = 2.9405
2024-10-30 21:32:15: [2024-10-30 21:32:15] iter = 11130, loss = 3.7728
2024-10-30 21:32:19: [2024-10-30 21:32:19] iter = 11140, loss = 2.3094
2024-10-30 21:32:23: [2024-10-30 21:32:23] iter = 11150, loss = 2.9561
2024-10-30 21:32:26: [2024-10-30 21:32:26] iter = 11160, loss = 2.4257
2024-10-30 21:32:29: [2024-10-30 21:32:29] iter = 11170, loss = 1.7224
2024-10-30 21:32:32: [2024-10-30 21:32:32] iter = 11180, loss = 2.6972
2024-10-30 21:32:37: [2024-10-30 21:32:37] iter = 11190, loss = 2.3838
2024-10-30 21:32:40: [2024-10-30 21:32:40] iter = 11200, loss = 2.0225
2024-10-30 21:32:44: [2024-10-30 21:32:44] iter = 11210, loss = 2.6112
2024-10-30 21:32:48: [2024-10-30 21:32:48] iter = 11220, loss = 2.1237
2024-10-30 21:32:51: [2024-10-30 21:32:51] iter = 11230, loss = 1.9785
2024-10-30 21:32:55: [2024-10-30 21:32:55] iter = 11240, loss = 2.0860
2024-10-30 21:32:58: [2024-10-30 21:32:58] iter = 11250, loss = 2.2005
2024-10-30 21:33:02: [2024-10-30 21:33:02] iter = 11260, loss = 2.2075
2024-10-30 21:33:05: [2024-10-30 21:33:05] iter = 11270, loss = 1.9377
2024-10-30 21:33:08: [2024-10-30 21:33:08] iter = 11280, loss = 2.0800
2024-10-30 21:33:11: [2024-10-30 21:33:11] iter = 11290, loss = 2.3367
2024-10-30 21:33:14: [2024-10-30 21:33:14] iter = 11300, loss = 1.9943
2024-10-30 21:33:19: [2024-10-30 21:33:19] iter = 11310, loss = 1.9036
2024-10-30 21:33:22: [2024-10-30 21:33:22] iter = 11320, loss = 2.0296
2024-10-30 21:33:26: [2024-10-30 21:33:26] iter = 11330, loss = 1.9864
2024-10-30 21:33:29: [2024-10-30 21:33:29] iter = 11340, loss = 1.6598
2024-10-30 21:33:33: [2024-10-30 21:33:33] iter = 11350, loss = 3.1990
2024-10-30 21:33:36: [2024-10-30 21:33:36] iter = 11360, loss = 2.4796
2024-10-30 21:33:40: [2024-10-30 21:33:40] iter = 11370, loss = 2.3760
2024-10-30 21:33:44: [2024-10-30 21:33:44] iter = 11380, loss = 2.7211
2024-10-30 21:33:48: [2024-10-30 21:33:48] iter = 11390, loss = 2.1062
2024-10-30 21:33:52: [2024-10-30 21:33:52] iter = 11400, loss = 2.8735
2024-10-30 21:33:55: [2024-10-30 21:33:55] iter = 11410, loss = 3.0383
2024-10-30 21:34:00: [2024-10-30 21:34:00] iter = 11420, loss = 2.3430
2024-10-30 21:34:03: [2024-10-30 21:34:03] iter = 11430, loss = 1.9740
2024-10-30 21:34:07: [2024-10-30 21:34:06] iter = 11440, loss = 2.1704
2024-10-30 21:34:11: [2024-10-30 21:34:11] iter = 11450, loss = 2.2877
2024-10-30 21:34:15: [2024-10-30 21:34:15] iter = 11460, loss = 3.3803
2024-10-30 21:34:19: [2024-10-30 21:34:19] iter = 11470, loss = 1.9842
2024-10-30 21:34:22: [2024-10-30 21:34:22] iter = 11480, loss = 2.5023
2024-10-30 21:34:25: [2024-10-30 21:34:25] iter = 11490, loss = 2.0841
2024-10-30 21:34:29: [2024-10-30 21:34:29] iter = 11500, loss = 2.1817
2024-10-30 21:34:33: [2024-10-30 21:34:33] iter = 11510, loss = 2.1683
2024-10-30 21:34:37: [2024-10-30 21:34:37] iter = 11520, loss = 2.0921
2024-10-30 21:34:41: [2024-10-30 21:34:41] iter = 11530, loss = 1.9101
2024-10-30 21:34:45: [2024-10-30 21:34:45] iter = 11540, loss = 3.0657
2024-10-30 21:34:48: [2024-10-30 21:34:48] iter = 11550, loss = 3.5946
2024-10-30 21:34:51: [2024-10-30 21:34:51] iter = 11560, loss = 2.3311
2024-10-30 21:34:54: [2024-10-30 21:34:54] iter = 11570, loss = 1.8812
2024-10-30 21:34:57: [2024-10-30 21:34:57] iter = 11580, loss = 1.8173
2024-10-30 21:35:01: [2024-10-30 21:35:01] iter = 11590, loss = 2.3595
2024-10-30 21:35:04: [2024-10-30 21:35:04] iter = 11600, loss = 1.8595
2024-10-30 21:35:08: [2024-10-30 21:35:08] iter = 11610, loss = 2.0349
2024-10-30 21:35:12: [2024-10-30 21:35:12] iter = 11620, loss = 2.5507
2024-10-30 21:35:15: [2024-10-30 21:35:15] iter = 11630, loss = 2.2975
2024-10-30 21:35:18: [2024-10-30 21:35:18] iter = 11640, loss = 2.2917
2024-10-30 21:35:21: [2024-10-30 21:35:21] iter = 11650, loss = 2.0238
2024-10-30 21:35:25: [2024-10-30 21:35:25] iter = 11660, loss = 2.1523
2024-10-30 21:35:29: [2024-10-30 21:35:29] iter = 11670, loss = 3.5683
2024-10-30 21:35:33: [2024-10-30 21:35:33] iter = 11680, loss = 3.9316
2024-10-30 21:35:36: [2024-10-30 21:35:36] iter = 11690, loss = 1.9257
2024-10-30 21:35:41: [2024-10-30 21:35:41] iter = 11700, loss = 2.1255
2024-10-30 21:35:44: [2024-10-30 21:35:44] iter = 11710, loss = 2.0651
2024-10-30 21:35:49: [2024-10-30 21:35:49] iter = 11720, loss = 1.8630
2024-10-30 21:35:53: [2024-10-30 21:35:53] iter = 11730, loss = 1.7012
2024-10-30 21:35:57: [2024-10-30 21:35:57] iter = 11740, loss = 2.0775
2024-10-30 21:36:01: [2024-10-30 21:36:01] iter = 11750, loss = 2.6693
2024-10-30 21:36:04: [2024-10-30 21:36:04] iter = 11760, loss = 2.0472
2024-10-30 21:36:08: [2024-10-30 21:36:08] iter = 11770, loss = 2.6134
2024-10-30 21:36:11: [2024-10-30 21:36:11] iter = 11780, loss = 2.8932
2024-10-30 21:36:14: [2024-10-30 21:36:14] iter = 11790, loss = 2.2904
2024-10-30 21:36:18: [2024-10-30 21:36:18] iter = 11800, loss = 2.4395
2024-10-30 21:36:21: [2024-10-30 21:36:21] iter = 11810, loss = 3.2217
2024-10-30 21:36:25: [2024-10-30 21:36:25] iter = 11820, loss = 2.0784
2024-10-30 21:36:29: [2024-10-30 21:36:29] iter = 11830, loss = 2.5286
2024-10-30 21:36:32: [2024-10-30 21:36:32] iter = 11840, loss = 1.9743
2024-10-30 21:36:36: [2024-10-30 21:36:36] iter = 11850, loss = 2.3466
2024-10-30 21:36:40: [2024-10-30 21:36:40] iter = 11860, loss = 1.9603
2024-10-30 21:36:43: [2024-10-30 21:36:43] iter = 11870, loss = 1.8940
2024-10-30 21:36:48: [2024-10-30 21:36:48] iter = 11880, loss = 1.9885
2024-10-30 21:36:52: [2024-10-30 21:36:52] iter = 11890, loss = 2.0351
2024-10-30 21:36:57: [2024-10-30 21:36:57] iter = 11900, loss = 2.2778
2024-10-30 21:37:00: [2024-10-30 21:37:00] iter = 11910, loss = 2.0774
2024-10-30 21:37:04: [2024-10-30 21:37:04] iter = 11920, loss = 2.8394
2024-10-30 21:37:09: [2024-10-30 21:37:09] iter = 11930, loss = 3.1698
2024-10-30 21:37:14: [2024-10-30 21:37:14] iter = 11940, loss = 1.8245
2024-10-30 21:37:17: [2024-10-30 21:37:17] iter = 11950, loss = 2.8190
2024-10-30 21:37:20: [2024-10-30 21:37:20] iter = 11960, loss = 2.2857
2024-10-30 21:37:24: [2024-10-30 21:37:24] iter = 11970, loss = 2.5533
2024-10-30 21:37:27: [2024-10-30 21:37:27] iter = 11980, loss = 2.4416
2024-10-30 21:37:31: [2024-10-30 21:37:31] iter = 11990, loss = 2.3105
2024-10-30 21:37:36: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
2024-10-30 21:37:36: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 21:37:36: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 56219}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 21:39:55: Evaluate 5 random ConvNet, ACCmean = 0.7799 ACCstd = 0.0048
-------------------------
2024-10-30 21:39:55: Evaluate 5 random ConvNet, SENmean = 0.7725 SENstd = 0.0042
-------------------------
2024-10-30 21:39:55: Evaluate 5 random ConvNet, SPEmean = 0.9779 SPEstd = 0.0005
-------------------------
2024-10-30 21:39:55: Evaluate 5 random ConvNet, F!mean = 0.7647 F!std = 0.0047
-------------------------
2024-10-30 21:39:55: Evaluate 5 random ConvNet, mean = 0.7799 std = 0.0048
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 21:39:56: [2024-10-30 21:39:56] iter = 12000, loss = 2.3681
2024-10-30 21:39:59: [2024-10-30 21:39:59] iter = 12010, loss = 2.2660
2024-10-30 21:40:02: [2024-10-30 21:40:02] iter = 12020, loss = 1.8299
2024-10-30 21:40:06: [2024-10-30 21:40:06] iter = 12030, loss = 2.3096
2024-10-30 21:40:09: [2024-10-30 21:40:09] iter = 12040, loss = 2.1221
2024-10-30 21:40:12: [2024-10-30 21:40:12] iter = 12050, loss = 2.2700
2024-10-30 21:40:15: [2024-10-30 21:40:15] iter = 12060, loss = 2.3039
2024-10-30 21:40:19: [2024-10-30 21:40:19] iter = 12070, loss = 2.0347
2024-10-30 21:40:22: [2024-10-30 21:40:22] iter = 12080, loss = 1.9959
2024-10-30 21:40:26: [2024-10-30 21:40:26] iter = 12090, loss = 2.6302
2024-10-30 21:40:30: [2024-10-30 21:40:30] iter = 12100, loss = 2.4322
2024-10-30 21:40:33: [2024-10-30 21:40:33] iter = 12110, loss = 4.8711
2024-10-30 21:40:37: [2024-10-30 21:40:37] iter = 12120, loss = 2.2409
2024-10-30 21:40:41: [2024-10-30 21:40:41] iter = 12130, loss = 2.3245
2024-10-30 21:40:44: [2024-10-30 21:40:44] iter = 12140, loss = 1.9055
2024-10-30 21:40:47: [2024-10-30 21:40:47] iter = 12150, loss = 2.0475
2024-10-30 21:40:51: [2024-10-30 21:40:51] iter = 12160, loss = 2.2683
2024-10-30 21:40:54: [2024-10-30 21:40:54] iter = 12170, loss = 2.5130
2024-10-30 21:40:58: [2024-10-30 21:40:58] iter = 12180, loss = 2.3643
2024-10-30 21:41:02: [2024-10-30 21:41:02] iter = 12190, loss = 2.4634
2024-10-30 21:41:07: [2024-10-30 21:41:07] iter = 12200, loss = 2.0861
2024-10-30 21:41:11: [2024-10-30 21:41:11] iter = 12210, loss = 2.0779
2024-10-30 21:41:15: [2024-10-30 21:41:15] iter = 12220, loss = 2.5613
2024-10-30 21:41:19: [2024-10-30 21:41:19] iter = 12230, loss = 2.0251
2024-10-30 21:41:23: [2024-10-30 21:41:23] iter = 12240, loss = 2.2301
2024-10-30 21:41:27: [2024-10-30 21:41:27] iter = 12250, loss = 2.0382
2024-10-30 21:41:31: [2024-10-30 21:41:31] iter = 12260, loss = 2.7140
2024-10-30 21:41:35: [2024-10-30 21:41:35] iter = 12270, loss = 1.8157
2024-10-30 21:41:41: [2024-10-30 21:41:41] iter = 12280, loss = 3.6138
2024-10-30 21:41:44: [2024-10-30 21:41:43] iter = 12290, loss = 2.4187
2024-10-30 21:41:47: [2024-10-30 21:41:47] iter = 12300, loss = 3.4139
2024-10-30 21:41:51: [2024-10-30 21:41:51] iter = 12310, loss = 2.2182
2024-10-30 21:41:55: [2024-10-30 21:41:55] iter = 12320, loss = 2.5094
2024-10-30 21:41:58: [2024-10-30 21:41:58] iter = 12330, loss = 2.8642
2024-10-30 21:42:02: [2024-10-30 21:42:02] iter = 12340, loss = 1.9269
2024-10-30 21:42:06: [2024-10-30 21:42:06] iter = 12350, loss = 2.2672
2024-10-30 21:42:11: [2024-10-30 21:42:11] iter = 12360, loss = 1.8039
2024-10-30 21:42:15: [2024-10-30 21:42:15] iter = 12370, loss = 2.5882
2024-10-30 21:42:19: [2024-10-30 21:42:19] iter = 12380, loss = 2.7672
2024-10-30 21:42:23: [2024-10-30 21:42:23] iter = 12390, loss = 2.1621
2024-10-30 21:42:26: [2024-10-30 21:42:26] iter = 12400, loss = 1.9206
2024-10-30 21:42:30: [2024-10-30 21:42:30] iter = 12410, loss = 2.0904
2024-10-30 21:42:35: [2024-10-30 21:42:35] iter = 12420, loss = 2.4749
2024-10-30 21:42:42: [2024-10-30 21:42:42] iter = 12430, loss = 2.1197
2024-10-30 21:42:48: [2024-10-30 21:42:48] iter = 12440, loss = 2.0407
2024-10-30 21:42:55: [2024-10-30 21:42:55] iter = 12450, loss = 2.6559
2024-10-30 21:43:00: [2024-10-30 21:43:00] iter = 12460, loss = 1.8927
2024-10-30 21:43:05: [2024-10-30 21:43:05] iter = 12470, loss = 1.9266
2024-10-30 21:43:09: [2024-10-30 21:43:09] iter = 12480, loss = 2.2529
2024-10-30 21:43:14: [2024-10-30 21:43:14] iter = 12490, loss = 1.7360
2024-10-30 21:43:19: [2024-10-30 21:43:19] iter = 12500, loss = 1.8943
2024-10-30 21:43:24: [2024-10-30 21:43:24] iter = 12510, loss = 1.9260
2024-10-30 21:43:30: [2024-10-30 21:43:30] iter = 12520, loss = 1.6328
2024-10-30 21:43:34: [2024-10-30 21:43:34] iter = 12530, loss = 5.2788
2024-10-30 21:43:37: [2024-10-30 21:43:37] iter = 12540, loss = 1.6894
2024-10-30 21:43:42: [2024-10-30 21:43:42] iter = 12550, loss = 2.0565
2024-10-30 21:43:45: [2024-10-30 21:43:45] iter = 12560, loss = 2.8608
2024-10-30 21:43:48: [2024-10-30 21:43:48] iter = 12570, loss = 2.2202
2024-10-30 21:43:52: [2024-10-30 21:43:52] iter = 12580, loss = 2.4694
2024-10-30 21:43:56: [2024-10-30 21:43:56] iter = 12590, loss = 2.0614
2024-10-30 21:44:01: [2024-10-30 21:44:01] iter = 12600, loss = 2.1910
2024-10-30 21:44:04: [2024-10-30 21:44:04] iter = 12610, loss = 1.7667
2024-10-30 21:44:07: [2024-10-30 21:44:07] iter = 12620, loss = 2.0706
2024-10-30 21:44:10: [2024-10-30 21:44:10] iter = 12630, loss = 1.9474
2024-10-30 21:44:14: [2024-10-30 21:44:14] iter = 12640, loss = 1.9515
2024-10-30 21:44:17: [2024-10-30 21:44:17] iter = 12650, loss = 2.5391
2024-10-30 21:44:20: [2024-10-30 21:44:20] iter = 12660, loss = 4.1413
2024-10-30 21:44:23: [2024-10-30 21:44:23] iter = 12670, loss = 2.0199
2024-10-30 21:44:27: [2024-10-30 21:44:27] iter = 12680, loss = 1.9085
2024-10-30 21:44:31: [2024-10-30 21:44:31] iter = 12690, loss = 2.2754
2024-10-30 21:44:34: [2024-10-30 21:44:34] iter = 12700, loss = 1.7951
2024-10-30 21:44:39: [2024-10-30 21:44:39] iter = 12710, loss = 2.1810
2024-10-30 21:44:44: [2024-10-30 21:44:44] iter = 12720, loss = 1.9516
2024-10-30 21:44:48: [2024-10-30 21:44:48] iter = 12730, loss = 2.7395
2024-10-30 21:44:53: [2024-10-30 21:44:53] iter = 12740, loss = 2.2362
2024-10-30 21:44:56: [2024-10-30 21:44:56] iter = 12750, loss = 2.0542
2024-10-30 21:45:00: [2024-10-30 21:45:00] iter = 12760, loss = 1.7636
2024-10-30 21:45:04: [2024-10-30 21:45:04] iter = 12770, loss = 2.3900
2024-10-30 21:45:08: [2024-10-30 21:45:08] iter = 12780, loss = 5.4303
2024-10-30 21:45:12: [2024-10-30 21:45:12] iter = 12790, loss = 4.0140
2024-10-30 21:45:16: [2024-10-30 21:45:16] iter = 12800, loss = 1.7443
2024-10-30 21:45:21: [2024-10-30 21:45:21] iter = 12810, loss = 2.9511
2024-10-30 21:45:25: [2024-10-30 21:45:25] iter = 12820, loss = 2.8908
2024-10-30 21:45:28: [2024-10-30 21:45:28] iter = 12830, loss = 2.6127
2024-10-30 21:45:32: [2024-10-30 21:45:32] iter = 12840, loss = 3.5058
2024-10-30 21:45:35: [2024-10-30 21:45:35] iter = 12850, loss = 2.3315
2024-10-30 21:45:39: [2024-10-30 21:45:39] iter = 12860, loss = 1.9243
2024-10-30 21:45:42: [2024-10-30 21:45:42] iter = 12870, loss = 1.9335
2024-10-30 21:45:47: [2024-10-30 21:45:47] iter = 12880, loss = 1.6834
2024-10-30 21:45:50: [2024-10-30 21:45:50] iter = 12890, loss = 2.1825
2024-10-30 21:45:54: [2024-10-30 21:45:54] iter = 12900, loss = 2.1449
2024-10-30 21:45:59: [2024-10-30 21:45:59] iter = 12910, loss = 2.7045
2024-10-30 21:46:02: [2024-10-30 21:46:02] iter = 12920, loss = 2.2030
2024-10-30 21:46:06: [2024-10-30 21:46:06] iter = 12930, loss = 3.2890
2024-10-30 21:46:10: [2024-10-30 21:46:10] iter = 12940, loss = 1.7858
2024-10-30 21:46:14: [2024-10-30 21:46:14] iter = 12950, loss = 2.3320
2024-10-30 21:46:19: [2024-10-30 21:46:19] iter = 12960, loss = 2.1240
2024-10-30 21:46:23: [2024-10-30 21:46:23] iter = 12970, loss = 6.2233
2024-10-30 21:46:27: [2024-10-30 21:46:27] iter = 12980, loss = 2.4183
2024-10-30 21:46:31: [2024-10-30 21:46:31] iter = 12990, loss = 2.1508
2024-10-30 21:46:35: [2024-10-30 21:46:35] iter = 13000, loss = 2.2245
2024-10-30 21:46:39: [2024-10-30 21:46:39] iter = 13010, loss = 2.5513
2024-10-30 21:46:42: [2024-10-30 21:46:42] iter = 13020, loss = 2.4434
2024-10-30 21:46:46: [2024-10-30 21:46:46] iter = 13030, loss = 1.8661
2024-10-30 21:46:49: [2024-10-30 21:46:49] iter = 13040, loss = 3.2378
2024-10-30 21:46:52: [2024-10-30 21:46:52] iter = 13050, loss = 2.1307
2024-10-30 21:46:56: [2024-10-30 21:46:56] iter = 13060, loss = 2.7680
2024-10-30 21:47:00: [2024-10-30 21:47:00] iter = 13070, loss = 1.9207
2024-10-30 21:47:03: [2024-10-30 21:47:03] iter = 13080, loss = 4.6731
2024-10-30 21:47:06: [2024-10-30 21:47:06] iter = 13090, loss = 2.0726
2024-10-30 21:47:10: [2024-10-30 21:47:10] iter = 13100, loss = 2.4669
2024-10-30 21:47:13: [2024-10-30 21:47:13] iter = 13110, loss = 2.0447
2024-10-30 21:47:17: [2024-10-30 21:47:17] iter = 13120, loss = 1.9174
2024-10-30 21:47:21: [2024-10-30 21:47:21] iter = 13130, loss = 2.3461
2024-10-30 21:47:25: [2024-10-30 21:47:25] iter = 13140, loss = 2.3121
2024-10-30 21:47:29: [2024-10-30 21:47:29] iter = 13150, loss = 2.2242
2024-10-30 21:47:33: [2024-10-30 21:47:33] iter = 13160, loss = 1.7823
2024-10-30 21:47:36: [2024-10-30 21:47:36] iter = 13170, loss = 2.0698
2024-10-30 21:47:40: [2024-10-30 21:47:40] iter = 13180, loss = 2.6122
2024-10-30 21:47:43: [2024-10-30 21:47:43] iter = 13190, loss = 4.0459
2024-10-30 21:47:47: [2024-10-30 21:47:47] iter = 13200, loss = 3.4547
2024-10-30 21:47:50: [2024-10-30 21:47:50] iter = 13210, loss = 2.6776
2024-10-30 21:47:54: [2024-10-30 21:47:54] iter = 13220, loss = 1.8769
2024-10-30 21:47:57: [2024-10-30 21:47:57] iter = 13230, loss = 1.9873
2024-10-30 21:48:02: [2024-10-30 21:48:02] iter = 13240, loss = 1.9794
2024-10-30 21:48:06: [2024-10-30 21:48:06] iter = 13250, loss = 2.2502
2024-10-30 21:48:11: [2024-10-30 21:48:11] iter = 13260, loss = 2.7391
2024-10-30 21:48:15: [2024-10-30 21:48:15] iter = 13270, loss = 1.9049
2024-10-30 21:48:18: [2024-10-30 21:48:18] iter = 13280, loss = 3.1769
2024-10-30 21:48:22: [2024-10-30 21:48:22] iter = 13290, loss = 2.1709
2024-10-30 21:48:26: [2024-10-30 21:48:26] iter = 13300, loss = 2.4852
2024-10-30 21:48:30: [2024-10-30 21:48:30] iter = 13310, loss = 3.3234
2024-10-30 21:48:34: [2024-10-30 21:48:34] iter = 13320, loss = 2.7229
2024-10-30 21:48:38: [2024-10-30 21:48:38] iter = 13330, loss = 2.0805
2024-10-30 21:48:42: [2024-10-30 21:48:42] iter = 13340, loss = 1.9573
2024-10-30 21:48:46: [2024-10-30 21:48:46] iter = 13350, loss = 1.6560
2024-10-30 21:48:49: [2024-10-30 21:48:49] iter = 13360, loss = 2.7405
2024-10-30 21:48:53: [2024-10-30 21:48:53] iter = 13370, loss = 2.1616
2024-10-30 21:48:56: [2024-10-30 21:48:56] iter = 13380, loss = 2.1949
2024-10-30 21:49:00: [2024-10-30 21:49:00] iter = 13390, loss = 2.1727
2024-10-30 21:49:03: [2024-10-30 21:49:03] iter = 13400, loss = 2.5607
2024-10-30 21:49:07: [2024-10-30 21:49:07] iter = 13410, loss = 3.2288
2024-10-30 21:49:11: [2024-10-30 21:49:11] iter = 13420, loss = 1.9927
2024-10-30 21:49:13: [2024-10-30 21:49:13] iter = 13430, loss = 1.8358
2024-10-30 21:49:17: [2024-10-30 21:49:17] iter = 13440, loss = 2.2114
2024-10-30 21:49:19: [2024-10-30 21:49:19] iter = 13450, loss = 2.1259
2024-10-30 21:49:22: [2024-10-30 21:49:22] iter = 13460, loss = 2.0194
2024-10-30 21:49:26: [2024-10-30 21:49:26] iter = 13470, loss = 2.4767
2024-10-30 21:49:30: [2024-10-30 21:49:30] iter = 13480, loss = 2.5444
2024-10-30 21:49:33: [2024-10-30 21:49:33] iter = 13490, loss = 2.1939
2024-10-30 21:49:36: [2024-10-30 21:49:36] iter = 13500, loss = 2.5794
2024-10-30 21:49:39: [2024-10-30 21:49:39] iter = 13510, loss = 6.8644
2024-10-30 21:49:42: [2024-10-30 21:49:42] iter = 13520, loss = 1.9256
2024-10-30 21:49:46: [2024-10-30 21:49:46] iter = 13530, loss = 2.3599
2024-10-30 21:49:50: [2024-10-30 21:49:50] iter = 13540, loss = 2.1925
2024-10-30 21:49:54: [2024-10-30 21:49:54] iter = 13550, loss = 2.1777
2024-10-30 21:49:57: [2024-10-30 21:49:57] iter = 13560, loss = 2.9629
2024-10-30 21:50:01: [2024-10-30 21:50:01] iter = 13570, loss = 2.8114
2024-10-30 21:50:04: [2024-10-30 21:50:04] iter = 13580, loss = 2.0190
2024-10-30 21:50:09: [2024-10-30 21:50:09] iter = 13590, loss = 1.9934
2024-10-30 21:50:12: [2024-10-30 21:50:12] iter = 13600, loss = 3.2495
2024-10-30 21:50:16: [2024-10-30 21:50:16] iter = 13610, loss = 2.0541
2024-10-30 21:50:20: [2024-10-30 21:50:20] iter = 13620, loss = 2.5535
2024-10-30 21:50:24: [2024-10-30 21:50:24] iter = 13630, loss = 2.3485
2024-10-30 21:50:27: [2024-10-30 21:50:27] iter = 13640, loss = 2.1618
2024-10-30 21:50:31: [2024-10-30 21:50:31] iter = 13650, loss = 2.4612
2024-10-30 21:50:34: [2024-10-30 21:50:34] iter = 13660, loss = 2.7418
2024-10-30 21:50:37: [2024-10-30 21:50:37] iter = 13670, loss = 2.4848
2024-10-30 21:50:41: [2024-10-30 21:50:41] iter = 13680, loss = 2.1678
2024-10-30 21:50:45: [2024-10-30 21:50:45] iter = 13690, loss = 2.3121
2024-10-30 21:50:48: [2024-10-30 21:50:48] iter = 13700, loss = 3.2825
2024-10-30 21:50:52: [2024-10-30 21:50:52] iter = 13710, loss = 2.3148
2024-10-30 21:50:55: [2024-10-30 21:50:55] iter = 13720, loss = 1.8471
2024-10-30 21:50:59: [2024-10-30 21:50:59] iter = 13730, loss = 2.2513
2024-10-30 21:51:02: [2024-10-30 21:51:02] iter = 13740, loss = 2.5240
2024-10-30 21:51:06: [2024-10-30 21:51:06] iter = 13750, loss = 2.1429
2024-10-30 21:51:09: [2024-10-30 21:51:09] iter = 13760, loss = 2.3386
2024-10-30 21:51:13: [2024-10-30 21:51:13] iter = 13770, loss = 1.8476
2024-10-30 21:51:18: [2024-10-30 21:51:18] iter = 13780, loss = 2.2399
2024-10-30 21:51:22: [2024-10-30 21:51:22] iter = 13790, loss = 3.7705
2024-10-30 21:51:26: [2024-10-30 21:51:26] iter = 13800, loss = 1.8766
2024-10-30 21:51:29: [2024-10-30 21:51:29] iter = 13810, loss = 2.1082
2024-10-30 21:51:33: [2024-10-30 21:51:33] iter = 13820, loss = 2.3453
2024-10-30 21:51:36: [2024-10-30 21:51:36] iter = 13830, loss = 2.9054
2024-10-30 21:51:40: [2024-10-30 21:51:40] iter = 13840, loss = 1.7191
2024-10-30 21:51:44: [2024-10-30 21:51:44] iter = 13850, loss = 1.9279
2024-10-30 21:51:47: [2024-10-30 21:51:47] iter = 13860, loss = 1.9964
2024-10-30 21:51:50: [2024-10-30 21:51:50] iter = 13870, loss = 4.3418
2024-10-30 21:51:53: [2024-10-30 21:51:53] iter = 13880, loss = 1.9600
2024-10-30 21:51:57: [2024-10-30 21:51:57] iter = 13890, loss = 3.2858
2024-10-30 21:52:01: [2024-10-30 21:52:01] iter = 13900, loss = 1.6592
2024-10-30 21:52:03: [2024-10-30 21:52:03] iter = 13910, loss = 2.3174
2024-10-30 21:52:06: [2024-10-30 21:52:06] iter = 13920, loss = 2.8623
2024-10-30 21:52:09: [2024-10-30 21:52:09] iter = 13930, loss = 2.9066
2024-10-30 21:52:12: [2024-10-30 21:52:12] iter = 13940, loss = 2.2937
2024-10-30 21:52:15: [2024-10-30 21:52:15] iter = 13950, loss = 3.6907
2024-10-30 21:52:18: [2024-10-30 21:52:18] iter = 13960, loss = 1.7785
2024-10-30 21:52:23: [2024-10-30 21:52:23] iter = 13970, loss = 2.0697
2024-10-30 21:52:26: [2024-10-30 21:52:26] iter = 13980, loss = 2.7510
2024-10-30 21:52:29: [2024-10-30 21:52:29] iter = 13990, loss = 2.2128
2024-10-30 21:52:32: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
2024-10-30 21:52:32: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 21:52:32: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 52298}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 21:55:00: Evaluate 5 random ConvNet, ACCmean = 0.7818 ACCstd = 0.0027
-------------------------
2024-10-30 21:55:00: Evaluate 5 random ConvNet, SENmean = 0.7747 SENstd = 0.0025
-------------------------
2024-10-30 21:55:00: Evaluate 5 random ConvNet, SPEmean = 0.9780 SPEstd = 0.0003
-------------------------
2024-10-30 21:55:00: Evaluate 5 random ConvNet, F!mean = 0.7672 F!std = 0.0016
-------------------------
2024-10-30 21:55:00: Evaluate 5 random ConvNet, mean = 0.7818 std = 0.0027
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 21:55:00: [2024-10-30 21:55:00] iter = 14000, loss = 2.6814
2024-10-30 21:55:07: [2024-10-30 21:55:07] iter = 14010, loss = 2.1227
2024-10-30 21:55:13: [2024-10-30 21:55:13] iter = 14020, loss = 2.8751
2024-10-30 21:55:17: [2024-10-30 21:55:17] iter = 14030, loss = 2.1588
2024-10-30 21:55:25: [2024-10-30 21:55:25] iter = 14040, loss = 1.9502
2024-10-30 21:55:30: [2024-10-30 21:55:30] iter = 14050, loss = 2.0210
2024-10-30 21:55:35: [2024-10-30 21:55:35] iter = 14060, loss = 2.2991
2024-10-30 21:55:39: [2024-10-30 21:55:39] iter = 14070, loss = 2.8882
2024-10-30 21:55:43: [2024-10-30 21:55:43] iter = 14080, loss = 2.1885
2024-10-30 21:55:47: [2024-10-30 21:55:47] iter = 14090, loss = 2.5460
2024-10-30 21:55:53: [2024-10-30 21:55:53] iter = 14100, loss = 2.8413
2024-10-30 21:55:58: [2024-10-30 21:55:58] iter = 14110, loss = 2.0484
2024-10-30 21:56:04: [2024-10-30 21:56:04] iter = 14120, loss = 2.4281
2024-10-30 21:56:08: [2024-10-30 21:56:08] iter = 14130, loss = 2.1403
2024-10-30 21:56:14: [2024-10-30 21:56:14] iter = 14140, loss = 2.1814
2024-10-30 21:56:19: [2024-10-30 21:56:19] iter = 14150, loss = 2.7393
2024-10-30 21:56:25: [2024-10-30 21:56:25] iter = 14160, loss = 1.8922
2024-10-30 21:56:30: [2024-10-30 21:56:30] iter = 14170, loss = 2.0748
2024-10-30 21:56:35: [2024-10-30 21:56:35] iter = 14180, loss = 1.7004
2024-10-30 21:56:38: [2024-10-30 21:56:38] iter = 14190, loss = 2.3459
2024-10-30 21:56:42: [2024-10-30 21:56:42] iter = 14200, loss = 1.9608
2024-10-30 21:56:46: [2024-10-30 21:56:46] iter = 14210, loss = 2.2254
2024-10-30 21:56:51: [2024-10-30 21:56:51] iter = 14220, loss = 2.6797
2024-10-30 21:56:55: [2024-10-30 21:56:55] iter = 14230, loss = 2.1445
2024-10-30 21:56:59: [2024-10-30 21:56:59] iter = 14240, loss = 1.7444
2024-10-30 21:57:04: [2024-10-30 21:57:04] iter = 14250, loss = 2.6045
2024-10-30 21:57:08: [2024-10-30 21:57:08] iter = 14260, loss = 2.2296
2024-10-30 21:57:12: [2024-10-30 21:57:12] iter = 14270, loss = 2.4180
2024-10-30 21:57:15: [2024-10-30 21:57:15] iter = 14280, loss = 2.6180
2024-10-30 21:57:20: [2024-10-30 21:57:20] iter = 14290, loss = 2.9161
2024-10-30 21:57:24: [2024-10-30 21:57:24] iter = 14300, loss = 1.9013
2024-10-30 21:57:27: [2024-10-30 21:57:27] iter = 14310, loss = 2.2650
2024-10-30 21:57:30: [2024-10-30 21:57:30] iter = 14320, loss = 2.7656
2024-10-30 21:57:34: [2024-10-30 21:57:34] iter = 14330, loss = 1.7994
2024-10-30 21:57:39: [2024-10-30 21:57:39] iter = 14340, loss = 2.6797
2024-10-30 21:57:43: [2024-10-30 21:57:43] iter = 14350, loss = 2.1622
2024-10-30 21:57:48: [2024-10-30 21:57:48] iter = 14360, loss = 2.1539
2024-10-30 21:57:52: [2024-10-30 21:57:52] iter = 14370, loss = 2.1208
2024-10-30 21:57:58: [2024-10-30 21:57:58] iter = 14380, loss = 2.1904
2024-10-30 21:58:02: [2024-10-30 21:58:02] iter = 14390, loss = 2.1213
2024-10-30 21:58:05: [2024-10-30 21:58:05] iter = 14400, loss = 2.1878
2024-10-30 21:58:09: [2024-10-30 21:58:09] iter = 14410, loss = 2.3402
2024-10-30 21:58:14: [2024-10-30 21:58:14] iter = 14420, loss = 2.0088
2024-10-30 21:58:17: [2024-10-30 21:58:17] iter = 14430, loss = 2.0425
2024-10-30 21:58:21: [2024-10-30 21:58:21] iter = 14440, loss = 3.2066
2024-10-30 21:58:25: [2024-10-30 21:58:25] iter = 14450, loss = 2.2086
2024-10-30 21:58:29: [2024-10-30 21:58:29] iter = 14460, loss = 2.2731
2024-10-30 21:58:33: [2024-10-30 21:58:33] iter = 14470, loss = 2.9613
2024-10-30 21:58:36: [2024-10-30 21:58:36] iter = 14480, loss = 1.7305
2024-10-30 21:58:41: [2024-10-30 21:58:41] iter = 14490, loss = 2.5407
2024-10-30 21:58:45: [2024-10-30 21:58:45] iter = 14500, loss = 9.1463
2024-10-30 21:58:48: [2024-10-30 21:58:48] iter = 14510, loss = 2.0761
2024-10-30 21:58:52: [2024-10-30 21:58:52] iter = 14520, loss = 2.2711
2024-10-30 21:58:56: [2024-10-30 21:58:56] iter = 14530, loss = 2.6280
2024-10-30 21:59:00: [2024-10-30 21:59:00] iter = 14540, loss = 2.8402
2024-10-30 21:59:04: [2024-10-30 21:59:04] iter = 14550, loss = 1.9946
2024-10-30 21:59:09: [2024-10-30 21:59:09] iter = 14560, loss = 2.0010
2024-10-30 21:59:14: [2024-10-30 21:59:14] iter = 14570, loss = 2.5391
2024-10-30 21:59:19: [2024-10-30 21:59:19] iter = 14580, loss = 3.3230
2024-10-30 21:59:23: [2024-10-30 21:59:23] iter = 14590, loss = 2.4125
2024-10-30 21:59:27: [2024-10-30 21:59:27] iter = 14600, loss = 2.2749
2024-10-30 21:59:30: [2024-10-30 21:59:30] iter = 14610, loss = 2.8919
2024-10-30 21:59:35: [2024-10-30 21:59:35] iter = 14620, loss = 2.2482
2024-10-30 21:59:40: [2024-10-30 21:59:40] iter = 14630, loss = 2.5639
2024-10-30 21:59:46: [2024-10-30 21:59:46] iter = 14640, loss = 4.7915
2024-10-30 21:59:50: [2024-10-30 21:59:50] iter = 14650, loss = 3.9125
2024-10-30 21:59:56: [2024-10-30 21:59:56] iter = 14660, loss = 2.8512
2024-10-30 22:00:00: [2024-10-30 22:00:00] iter = 14670, loss = 2.0506
2024-10-30 22:00:05: [2024-10-30 22:00:05] iter = 14680, loss = 2.1188
2024-10-30 22:00:09: [2024-10-30 22:00:09] iter = 14690, loss = 2.1216
2024-10-30 22:00:16: [2024-10-30 22:00:16] iter = 14700, loss = 2.9386
2024-10-30 22:00:20: [2024-10-30 22:00:20] iter = 14710, loss = 1.9697
2024-10-30 22:00:25: [2024-10-30 22:00:25] iter = 14720, loss = 2.4378
2024-10-30 22:00:30: [2024-10-30 22:00:30] iter = 14730, loss = 2.4778
2024-10-30 22:00:34: [2024-10-30 22:00:34] iter = 14740, loss = 2.6238
2024-10-30 22:00:39: [2024-10-30 22:00:39] iter = 14750, loss = 2.1222
2024-10-30 22:00:43: [2024-10-30 22:00:43] iter = 14760, loss = 1.8053
2024-10-30 22:00:47: [2024-10-30 22:00:47] iter = 14770, loss = 2.1373
2024-10-30 22:00:52: [2024-10-30 22:00:52] iter = 14780, loss = 2.7699
2024-10-30 22:00:56: [2024-10-30 22:00:56] iter = 14790, loss = 2.1773
2024-10-30 22:01:00: [2024-10-30 22:01:00] iter = 14800, loss = 2.1556
2024-10-30 22:01:05: [2024-10-30 22:01:05] iter = 14810, loss = 1.9819
2024-10-30 22:01:09: [2024-10-30 22:01:09] iter = 14820, loss = 1.7940
2024-10-30 22:01:13: [2024-10-30 22:01:13] iter = 14830, loss = 2.4642
2024-10-30 22:01:17: [2024-10-30 22:01:17] iter = 14840, loss = 2.3368
2024-10-30 22:01:21: [2024-10-30 22:01:21] iter = 14850, loss = 2.5201
2024-10-30 22:01:26: [2024-10-30 22:01:26] iter = 14860, loss = 2.0375
2024-10-30 22:01:30: [2024-10-30 22:01:30] iter = 14870, loss = 2.0590
2024-10-30 22:01:34: [2024-10-30 22:01:34] iter = 14880, loss = 2.5667
2024-10-30 22:01:38: [2024-10-30 22:01:38] iter = 14890, loss = 2.6631
2024-10-30 22:01:42: [2024-10-30 22:01:42] iter = 14900, loss = 2.8077
2024-10-30 22:01:46: [2024-10-30 22:01:46] iter = 14910, loss = 2.4348
2024-10-30 22:01:50: [2024-10-30 22:01:50] iter = 14920, loss = 2.3603
2024-10-30 22:01:55: [2024-10-30 22:01:55] iter = 14930, loss = 2.4416
2024-10-30 22:02:00: [2024-10-30 22:02:00] iter = 14940, loss = 2.5694
2024-10-30 22:02:03: [2024-10-30 22:02:03] iter = 14950, loss = 2.5825
2024-10-30 22:02:07: [2024-10-30 22:02:07] iter = 14960, loss = 2.0643
2024-10-30 22:02:10: [2024-10-30 22:02:10] iter = 14970, loss = 2.8509
2024-10-30 22:02:13: [2024-10-30 22:02:13] iter = 14980, loss = 2.3856
2024-10-30 22:02:17: [2024-10-30 22:02:17] iter = 14990, loss = 2.2083
2024-10-30 22:02:21: [2024-10-30 22:02:21] iter = 15000, loss = 2.5195
2024-10-30 22:02:24: [2024-10-30 22:02:24] iter = 15010, loss = 1.9260
2024-10-30 22:02:27: [2024-10-30 22:02:27] iter = 15020, loss = 2.2905
2024-10-30 22:02:30: [2024-10-30 22:02:30] iter = 15030, loss = 2.9212
2024-10-30 22:02:34: [2024-10-30 22:02:34] iter = 15040, loss = 2.1366
2024-10-30 22:02:37: [2024-10-30 22:02:37] iter = 15050, loss = 2.0651
2024-10-30 22:02:40: [2024-10-30 22:02:40] iter = 15060, loss = 2.7795
2024-10-30 22:02:43: [2024-10-30 22:02:43] iter = 15070, loss = 2.0454
2024-10-30 22:02:47: [2024-10-30 22:02:47] iter = 15080, loss = 2.3308
2024-10-30 22:02:51: [2024-10-30 22:02:51] iter = 15090, loss = 2.2445
2024-10-30 22:02:54: [2024-10-30 22:02:54] iter = 15100, loss = 1.9404
2024-10-30 22:02:57: [2024-10-30 22:02:57] iter = 15110, loss = 4.5725
2024-10-30 22:03:00: [2024-10-30 22:03:00] iter = 15120, loss = 2.0839
2024-10-30 22:03:03: [2024-10-30 22:03:03] iter = 15130, loss = 2.2896
2024-10-30 22:03:07: [2024-10-30 22:03:07] iter = 15140, loss = 2.2909
2024-10-30 22:03:10: [2024-10-30 22:03:10] iter = 15150, loss = 2.1488
2024-10-30 22:03:14: [2024-10-30 22:03:14] iter = 15160, loss = 2.1373
2024-10-30 22:03:18: [2024-10-30 22:03:18] iter = 15170, loss = 1.8870
2024-10-30 22:03:22: [2024-10-30 22:03:22] iter = 15180, loss = 2.5977
2024-10-30 22:03:26: [2024-10-30 22:03:26] iter = 15190, loss = 2.6751
2024-10-30 22:03:30: [2024-10-30 22:03:30] iter = 15200, loss = 2.3694
2024-10-30 22:03:33: [2024-10-30 22:03:33] iter = 15210, loss = 3.0040
2024-10-30 22:03:37: [2024-10-30 22:03:37] iter = 15220, loss = 2.7880
2024-10-30 22:03:40: [2024-10-30 22:03:40] iter = 15230, loss = 1.7867
2024-10-30 22:03:44: [2024-10-30 22:03:44] iter = 15240, loss = 2.2027
2024-10-30 22:03:47: [2024-10-30 22:03:47] iter = 15250, loss = 2.3227
2024-10-30 22:03:52: [2024-10-30 22:03:52] iter = 15260, loss = 2.3100
2024-10-30 22:03:56: [2024-10-30 22:03:56] iter = 15270, loss = 2.2658
2024-10-30 22:04:00: [2024-10-30 22:04:00] iter = 15280, loss = 2.2841
2024-10-30 22:04:04: [2024-10-30 22:04:04] iter = 15290, loss = 2.0628
2024-10-30 22:04:06: [2024-10-30 22:04:06] iter = 15300, loss = 2.1042
2024-10-30 22:04:11: [2024-10-30 22:04:11] iter = 15310, loss = 2.3183
2024-10-30 22:04:14: [2024-10-30 22:04:14] iter = 15320, loss = 1.7538
2024-10-30 22:04:18: [2024-10-30 22:04:18] iter = 15330, loss = 2.2824
2024-10-30 22:04:21: [2024-10-30 22:04:21] iter = 15340, loss = 3.5661
2024-10-30 22:04:24: [2024-10-30 22:04:24] iter = 15350, loss = 2.4279
2024-10-30 22:04:28: [2024-10-30 22:04:28] iter = 15360, loss = 1.9046
2024-10-30 22:04:32: [2024-10-30 22:04:32] iter = 15370, loss = 2.4327
2024-10-30 22:04:36: [2024-10-30 22:04:36] iter = 15380, loss = 2.1442
2024-10-30 22:04:40: [2024-10-30 22:04:40] iter = 15390, loss = 2.4579
2024-10-30 22:04:45: [2024-10-30 22:04:45] iter = 15400, loss = 1.7198
2024-10-30 22:04:49: [2024-10-30 22:04:49] iter = 15410, loss = 2.2513
2024-10-30 22:04:53: [2024-10-30 22:04:53] iter = 15420, loss = 2.1495
2024-10-30 22:04:58: [2024-10-30 22:04:58] iter = 15430, loss = 3.2456
2024-10-30 22:05:02: [2024-10-30 22:05:02] iter = 15440, loss = 2.3689
2024-10-30 22:05:05: [2024-10-30 22:05:05] iter = 15450, loss = 2.2312
2024-10-30 22:05:08: [2024-10-30 22:05:08] iter = 15460, loss = 2.0408
2024-10-30 22:05:13: [2024-10-30 22:05:13] iter = 15470, loss = 2.1957
2024-10-30 22:05:17: [2024-10-30 22:05:17] iter = 15480, loss = 2.1199
2024-10-30 22:05:21: [2024-10-30 22:05:21] iter = 15490, loss = 1.9421
2024-10-30 22:05:25: [2024-10-30 22:05:25] iter = 15500, loss = 3.4515
2024-10-30 22:05:28: [2024-10-30 22:05:28] iter = 15510, loss = 2.4588
2024-10-30 22:05:33: [2024-10-30 22:05:33] iter = 15520, loss = 1.9549
2024-10-30 22:05:35: [2024-10-30 22:05:35] iter = 15530, loss = 2.3987
2024-10-30 22:05:38: [2024-10-30 22:05:38] iter = 15540, loss = 2.1101
2024-10-30 22:05:41: [2024-10-30 22:05:41] iter = 15550, loss = 2.0939
2024-10-30 22:05:44: [2024-10-30 22:05:44] iter = 15560, loss = 2.0576
2024-10-30 22:05:47: [2024-10-30 22:05:47] iter = 15570, loss = 2.6030
2024-10-30 22:05:50: [2024-10-30 22:05:50] iter = 15580, loss = 2.0114
2024-10-30 22:05:54: [2024-10-30 22:05:54] iter = 15590, loss = 2.6343
2024-10-30 22:05:58: [2024-10-30 22:05:58] iter = 15600, loss = 2.1469
2024-10-30 22:06:01: [2024-10-30 22:06:01] iter = 15610, loss = 2.4847
2024-10-30 22:06:05: [2024-10-30 22:06:05] iter = 15620, loss = 1.9885
2024-10-30 22:06:08: [2024-10-30 22:06:08] iter = 15630, loss = 1.9867
2024-10-30 22:06:12: [2024-10-30 22:06:12] iter = 15640, loss = 2.9189
2024-10-30 22:06:16: [2024-10-30 22:06:16] iter = 15650, loss = 2.2223
2024-10-30 22:06:20: [2024-10-30 22:06:20] iter = 15660, loss = 2.7969
2024-10-30 22:06:24: [2024-10-30 22:06:24] iter = 15670, loss = 2.5370
2024-10-30 22:06:29: [2024-10-30 22:06:29] iter = 15680, loss = 1.9606
2024-10-30 22:06:34: [2024-10-30 22:06:34] iter = 15690, loss = 2.2508
2024-10-30 22:06:38: [2024-10-30 22:06:38] iter = 15700, loss = 4.0978
2024-10-30 22:06:41: [2024-10-30 22:06:41] iter = 15710, loss = 2.2747
2024-10-30 22:06:45: [2024-10-30 22:06:45] iter = 15720, loss = 2.0365
2024-10-30 22:06:48: [2024-10-30 22:06:48] iter = 15730, loss = 3.9675
2024-10-30 22:06:50: [2024-10-30 22:06:50] iter = 15740, loss = 2.2371
2024-10-30 22:06:54: [2024-10-30 22:06:54] iter = 15750, loss = 2.1424
2024-10-30 22:06:57: [2024-10-30 22:06:57] iter = 15760, loss = 2.9877
2024-10-30 22:07:02: [2024-10-30 22:07:02] iter = 15770, loss = 2.2374
2024-10-30 22:07:06: [2024-10-30 22:07:06] iter = 15780, loss = 1.8255
2024-10-30 22:07:11: [2024-10-30 22:07:11] iter = 15790, loss = 1.7723
2024-10-30 22:07:15: [2024-10-30 22:07:15] iter = 15800, loss = 2.0885
2024-10-30 22:07:20: [2024-10-30 22:07:20] iter = 15810, loss = 1.9823
2024-10-30 22:07:24: [2024-10-30 22:07:24] iter = 15820, loss = 2.1676
2024-10-30 22:07:27: [2024-10-30 22:07:27] iter = 15830, loss = 3.4957
2024-10-30 22:07:30: [2024-10-30 22:07:30] iter = 15840, loss = 1.8675
2024-10-30 22:07:34: [2024-10-30 22:07:34] iter = 15850, loss = 2.1245
2024-10-30 22:07:37: [2024-10-30 22:07:37] iter = 15860, loss = 1.9867
2024-10-30 22:07:40: [2024-10-30 22:07:40] iter = 15870, loss = 2.1895
2024-10-30 22:07:44: [2024-10-30 22:07:44] iter = 15880, loss = 2.0848
2024-10-30 22:07:47: [2024-10-30 22:07:47] iter = 15890, loss = 1.8927
2024-10-30 22:07:51: [2024-10-30 22:07:51] iter = 15900, loss = 2.2572
2024-10-30 22:07:54: [2024-10-30 22:07:54] iter = 15910, loss = 3.1889
2024-10-30 22:07:57: [2024-10-30 22:07:57] iter = 15920, loss = 2.2857
2024-10-30 22:08:01: [2024-10-30 22:08:01] iter = 15930, loss = 1.9404
2024-10-30 22:08:05: [2024-10-30 22:08:05] iter = 15940, loss = 2.1548
2024-10-30 22:08:09: [2024-10-30 22:08:09] iter = 15950, loss = 1.8438
2024-10-30 22:08:13: [2024-10-30 22:08:13] iter = 15960, loss = 2.1978
2024-10-30 22:08:17: [2024-10-30 22:08:17] iter = 15970, loss = 2.1091
2024-10-30 22:08:21: [2024-10-30 22:08:21] iter = 15980, loss = 2.2332
2024-10-30 22:08:24: [2024-10-30 22:08:24] iter = 15990, loss = 2.3522
2024-10-30 22:08:28: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
2024-10-30 22:08:28: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 22:08:28: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 8148}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 22:10:53: Evaluate 5 random ConvNet, ACCmean = 0.7720 ACCstd = 0.0073
-------------------------
2024-10-30 22:10:53: Evaluate 5 random ConvNet, SENmean = 0.7670 SENstd = 0.0074
-------------------------
2024-10-30 22:10:53: Evaluate 5 random ConvNet, SPEmean = 0.9772 SPEstd = 0.0007
-------------------------
2024-10-30 22:10:53: Evaluate 5 random ConvNet, F!mean = 0.7563 F!std = 0.0072
-------------------------
2024-10-30 22:10:53: Evaluate 5 random ConvNet, mean = 0.7720 std = 0.0073
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 22:10:53: [2024-10-30 22:10:53] iter = 16000, loss = 1.9621
2024-10-30 22:10:57: [2024-10-30 22:10:57] iter = 16010, loss = 2.0868
2024-10-30 22:11:00: [2024-10-30 22:11:00] iter = 16020, loss = 2.3343
2024-10-30 22:11:04: [2024-10-30 22:11:04] iter = 16030, loss = 3.9307
2024-10-30 22:11:08: [2024-10-30 22:11:08] iter = 16040, loss = 1.6667
2024-10-30 22:11:12: [2024-10-30 22:11:12] iter = 16050, loss = 1.9593
2024-10-30 22:11:16: [2024-10-30 22:11:16] iter = 16060, loss = 1.8836
2024-10-30 22:11:21: [2024-10-30 22:11:21] iter = 16070, loss = 1.7908
2024-10-30 22:11:26: [2024-10-30 22:11:26] iter = 16080, loss = 1.7759
2024-10-30 22:11:30: [2024-10-30 22:11:30] iter = 16090, loss = 2.3926
2024-10-30 22:11:34: [2024-10-30 22:11:34] iter = 16100, loss = 2.2389
2024-10-30 22:11:38: [2024-10-30 22:11:38] iter = 16110, loss = 5.8591
2024-10-30 22:11:42: [2024-10-30 22:11:42] iter = 16120, loss = 1.8272
2024-10-30 22:11:45: [2024-10-30 22:11:45] iter = 16130, loss = 2.1741
2024-10-30 22:11:48: [2024-10-30 22:11:48] iter = 16140, loss = 1.8247
2024-10-30 22:11:51: [2024-10-30 22:11:51] iter = 16150, loss = 3.2816
2024-10-30 22:11:55: [2024-10-30 22:11:55] iter = 16160, loss = 2.3219
2024-10-30 22:12:00: [2024-10-30 22:12:00] iter = 16170, loss = 2.9192
2024-10-30 22:12:04: [2024-10-30 22:12:04] iter = 16180, loss = 2.2482
2024-10-30 22:12:08: [2024-10-30 22:12:08] iter = 16190, loss = 3.0791
2024-10-30 22:12:13: [2024-10-30 22:12:13] iter = 16200, loss = 9.6389
2024-10-30 22:12:16: [2024-10-30 22:12:16] iter = 16210, loss = 2.6877
2024-10-30 22:12:20: [2024-10-30 22:12:20] iter = 16220, loss = 2.0064
2024-10-30 22:12:25: [2024-10-30 22:12:25] iter = 16230, loss = 1.9503
2024-10-30 22:12:29: [2024-10-30 22:12:29] iter = 16240, loss = 1.8564
2024-10-30 22:12:33: [2024-10-30 22:12:33] iter = 16250, loss = 2.5523
2024-10-30 22:12:37: [2024-10-30 22:12:37] iter = 16260, loss = 2.2089
2024-10-30 22:12:40: [2024-10-30 22:12:40] iter = 16270, loss = 2.0184
2024-10-30 22:12:43: [2024-10-30 22:12:43] iter = 16280, loss = 2.5234
2024-10-30 22:12:47: [2024-10-30 22:12:47] iter = 16290, loss = 2.0275
2024-10-30 22:12:50: [2024-10-30 22:12:50] iter = 16300, loss = 2.2898
2024-10-30 22:12:53: [2024-10-30 22:12:53] iter = 16310, loss = 2.1506
2024-10-30 22:12:57: [2024-10-30 22:12:57] iter = 16320, loss = 5.9703
2024-10-30 22:13:00: [2024-10-30 22:13:00] iter = 16330, loss = 2.4344
2024-10-30 22:13:05: [2024-10-30 22:13:05] iter = 16340, loss = 2.9568
2024-10-30 22:13:09: [2024-10-30 22:13:09] iter = 16350, loss = 2.7818
2024-10-30 22:13:12: [2024-10-30 22:13:12] iter = 16360, loss = 2.8039
2024-10-30 22:13:15: [2024-10-30 22:13:15] iter = 16370, loss = 2.9733
2024-10-30 22:13:18: [2024-10-30 22:13:18] iter = 16380, loss = 2.5424
2024-10-30 22:13:22: [2024-10-30 22:13:22] iter = 16390, loss = 2.3534
2024-10-30 22:13:26: [2024-10-30 22:13:26] iter = 16400, loss = 2.7658
2024-10-30 22:13:30: [2024-10-30 22:13:30] iter = 16410, loss = 2.5778
2024-10-30 22:13:34: [2024-10-30 22:13:34] iter = 16420, loss = 2.2824
2024-10-30 22:13:37: [2024-10-30 22:13:37] iter = 16430, loss = 1.7840
2024-10-30 22:13:41: [2024-10-30 22:13:41] iter = 16440, loss = 3.2396
2024-10-30 22:13:45: [2024-10-30 22:13:45] iter = 16450, loss = 3.9157
2024-10-30 22:13:48: [2024-10-30 22:13:48] iter = 16460, loss = 2.4138
2024-10-30 22:13:52: [2024-10-30 22:13:52] iter = 16470, loss = 3.6439
2024-10-30 22:13:56: [2024-10-30 22:13:56] iter = 16480, loss = 1.7375
2024-10-30 22:14:00: [2024-10-30 22:14:00] iter = 16490, loss = 2.1294
2024-10-30 22:14:04: [2024-10-30 22:14:04] iter = 16500, loss = 2.4814
2024-10-30 22:14:07: [2024-10-30 22:14:07] iter = 16510, loss = 2.4559
2024-10-30 22:14:10: [2024-10-30 22:14:10] iter = 16520, loss = 2.5761
2024-10-30 22:14:14: [2024-10-30 22:14:14] iter = 16530, loss = 2.8340
2024-10-30 22:14:18: [2024-10-30 22:14:18] iter = 16540, loss = 4.1896
2024-10-30 22:14:22: [2024-10-30 22:14:22] iter = 16550, loss = 2.3993
2024-10-30 22:14:25: [2024-10-30 22:14:25] iter = 16560, loss = 2.2497
2024-10-30 22:14:28: [2024-10-30 22:14:28] iter = 16570, loss = 2.6624
2024-10-30 22:14:33: [2024-10-30 22:14:33] iter = 16580, loss = 2.0465
2024-10-30 22:14:37: [2024-10-30 22:14:37] iter = 16590, loss = 2.5127
2024-10-30 22:14:41: [2024-10-30 22:14:41] iter = 16600, loss = 1.7018
2024-10-30 22:14:44: [2024-10-30 22:14:44] iter = 16610, loss = 1.8717
2024-10-30 22:14:48: [2024-10-30 22:14:48] iter = 16620, loss = 2.4262
2024-10-30 22:14:51: [2024-10-30 22:14:51] iter = 16630, loss = 2.3262
2024-10-30 22:14:55: [2024-10-30 22:14:55] iter = 16640, loss = 1.9611
2024-10-30 22:14:58: [2024-10-30 22:14:58] iter = 16650, loss = 2.5571
2024-10-30 22:15:02: [2024-10-30 22:15:02] iter = 16660, loss = 2.6019
2024-10-30 22:15:05: [2024-10-30 22:15:05] iter = 16670, loss = 3.0699
2024-10-30 22:15:09: [2024-10-30 22:15:09] iter = 16680, loss = 2.4072
2024-10-30 22:15:13: [2024-10-30 22:15:13] iter = 16690, loss = 2.0368
2024-10-30 22:15:16: [2024-10-30 22:15:16] iter = 16700, loss = 2.6775
2024-10-30 22:15:19: [2024-10-30 22:15:19] iter = 16710, loss = 1.6544
2024-10-30 22:15:22: [2024-10-30 22:15:22] iter = 16720, loss = 3.6328
2024-10-30 22:15:26: [2024-10-30 22:15:26] iter = 16730, loss = 1.9397
2024-10-30 22:15:29: [2024-10-30 22:15:29] iter = 16740, loss = 3.8395
2024-10-30 22:15:33: [2024-10-30 22:15:33] iter = 16750, loss = 2.2975
2024-10-30 22:15:37: [2024-10-30 22:15:37] iter = 16760, loss = 2.4948
2024-10-30 22:15:40: [2024-10-30 22:15:40] iter = 16770, loss = 3.1406
2024-10-30 22:15:43: [2024-10-30 22:15:43] iter = 16780, loss = 1.8559
2024-10-30 22:15:47: [2024-10-30 22:15:47] iter = 16790, loss = 2.0783
2024-10-30 22:15:50: [2024-10-30 22:15:50] iter = 16800, loss = 2.1299
2024-10-30 22:15:54: [2024-10-30 22:15:54] iter = 16810, loss = 1.7766
2024-10-30 22:15:57: [2024-10-30 22:15:57] iter = 16820, loss = 2.4796
2024-10-30 22:16:01: [2024-10-30 22:16:01] iter = 16830, loss = 1.7741
2024-10-30 22:16:03: [2024-10-30 22:16:03] iter = 16840, loss = 2.4742
2024-10-30 22:16:07: [2024-10-30 22:16:07] iter = 16850, loss = 4.7566
2024-10-30 22:16:11: [2024-10-30 22:16:11] iter = 16860, loss = 1.9929
2024-10-30 22:16:16: [2024-10-30 22:16:16] iter = 16870, loss = 2.0056
2024-10-30 22:16:19: [2024-10-30 22:16:19] iter = 16880, loss = 2.2822
2024-10-30 22:16:24: [2024-10-30 22:16:24] iter = 16890, loss = 2.0332
2024-10-30 22:16:29: [2024-10-30 22:16:29] iter = 16900, loss = 2.1742
2024-10-30 22:16:33: [2024-10-30 22:16:33] iter = 16910, loss = 3.1066
2024-10-30 22:16:37: [2024-10-30 22:16:37] iter = 16920, loss = 2.8495
2024-10-30 22:16:43: [2024-10-30 22:16:43] iter = 16930, loss = 2.3760
2024-10-30 22:16:47: [2024-10-30 22:16:47] iter = 16940, loss = 2.1664
2024-10-30 22:16:51: [2024-10-30 22:16:51] iter = 16950, loss = 2.2041
2024-10-30 22:16:55: [2024-10-30 22:16:55] iter = 16960, loss = 2.4712
2024-10-30 22:16:56: [2024-10-30 22:16:56] iter = 16970, loss = 2.4063
2024-10-30 22:17:01: [2024-10-30 22:17:01] iter = 16980, loss = 2.7024
2024-10-30 22:17:05: [2024-10-30 22:17:05] iter = 16990, loss = 2.0179
2024-10-30 22:17:08: [2024-10-30 22:17:08] iter = 17000, loss = 2.6052
2024-10-30 22:17:12: [2024-10-30 22:17:12] iter = 17010, loss = 2.4949
2024-10-30 22:17:17: [2024-10-30 22:17:17] iter = 17020, loss = 4.7206
2024-10-30 22:17:20: [2024-10-30 22:17:20] iter = 17030, loss = 2.3494
2024-10-30 22:17:24: [2024-10-30 22:17:24] iter = 17040, loss = 1.9681
2024-10-30 22:17:27: [2024-10-30 22:17:27] iter = 17050, loss = 2.0093
2024-10-30 22:17:31: [2024-10-30 22:17:31] iter = 17060, loss = 2.2381
2024-10-30 22:17:34: [2024-10-30 22:17:34] iter = 17070, loss = 1.7143
2024-10-30 22:17:38: [2024-10-30 22:17:38] iter = 17080, loss = 2.2809
2024-10-30 22:17:41: [2024-10-30 22:17:41] iter = 17090, loss = 2.1748
2024-10-30 22:17:44: [2024-10-30 22:17:44] iter = 17100, loss = 2.2641
2024-10-30 22:17:47: [2024-10-30 22:17:47] iter = 17110, loss = 2.1061
2024-10-30 22:17:50: [2024-10-30 22:17:50] iter = 17120, loss = 2.5290
2024-10-30 22:17:54: [2024-10-30 22:17:54] iter = 17130, loss = 3.0649
2024-10-30 22:17:57: [2024-10-30 22:17:57] iter = 17140, loss = 1.9984
2024-10-30 22:17:58: [2024-10-30 22:17:58] iter = 17150, loss = 2.1954
2024-10-30 22:18:00: [2024-10-30 22:18:00] iter = 17160, loss = 2.2005
2024-10-30 22:18:03: [2024-10-30 22:18:03] iter = 17170, loss = 2.8917
2024-10-30 22:18:05: [2024-10-30 22:18:05] iter = 17180, loss = 1.8763
2024-10-30 22:18:07: [2024-10-30 22:18:07] iter = 17190, loss = 2.0946
2024-10-30 22:18:09: [2024-10-30 22:18:09] iter = 17200, loss = 2.1001
2024-10-30 22:18:12: [2024-10-30 22:18:12] iter = 17210, loss = 2.2282
2024-10-30 22:18:15: [2024-10-30 22:18:15] iter = 17220, loss = 1.8195
2024-10-30 22:18:18: [2024-10-30 22:18:18] iter = 17230, loss = 2.4075
2024-10-30 22:18:22: [2024-10-30 22:18:22] iter = 17240, loss = 2.2577
2024-10-30 22:18:24: [2024-10-30 22:18:24] iter = 17250, loss = 2.3332
2024-10-30 22:18:27: [2024-10-30 22:18:27] iter = 17260, loss = 2.7456
2024-10-30 22:18:29: [2024-10-30 22:18:29] iter = 17270, loss = 2.2237
2024-10-30 22:18:32: [2024-10-30 22:18:32] iter = 17280, loss = 2.0108
2024-10-30 22:18:36: [2024-10-30 22:18:36] iter = 17290, loss = 1.7641
2024-10-30 22:18:38: [2024-10-30 22:18:38] iter = 17300, loss = 2.6097
2024-10-30 22:18:41: [2024-10-30 22:18:41] iter = 17310, loss = 3.1177
2024-10-30 22:18:44: [2024-10-30 22:18:44] iter = 17320, loss = 2.1135
2024-10-30 22:18:46: [2024-10-30 22:18:46] iter = 17330, loss = 1.9194
2024-10-30 22:18:49: [2024-10-30 22:18:49] iter = 17340, loss = 2.7718
2024-10-30 22:18:52: [2024-10-30 22:18:52] iter = 17350, loss = 1.8447
2024-10-30 22:18:54: [2024-10-30 22:18:54] iter = 17360, loss = 2.3628
2024-10-30 22:18:57: [2024-10-30 22:18:57] iter = 17370, loss = 1.8779
2024-10-30 22:19:00: [2024-10-30 22:19:00] iter = 17380, loss = 2.9903
2024-10-30 22:19:02: [2024-10-30 22:19:02] iter = 17390, loss = 2.2529
2024-10-30 22:19:05: [2024-10-30 22:19:05] iter = 17400, loss = 2.2698
2024-10-30 22:19:07: [2024-10-30 22:19:07] iter = 17410, loss = 2.2164
2024-10-30 22:19:10: [2024-10-30 22:19:10] iter = 17420, loss = 2.0251
2024-10-30 22:19:12: [2024-10-30 22:19:12] iter = 17430, loss = 2.2482
2024-10-30 22:19:15: [2024-10-30 22:19:15] iter = 17440, loss = 3.0617
2024-10-30 22:19:18: [2024-10-30 22:19:18] iter = 17450, loss = 2.3955
2024-10-30 22:19:21: [2024-10-30 22:19:21] iter = 17460, loss = 2.6297
2024-10-30 22:19:24: [2024-10-30 22:19:24] iter = 17470, loss = 2.4134
2024-10-30 22:19:26: [2024-10-30 22:19:26] iter = 17480, loss = 2.0122
2024-10-30 22:19:29: [2024-10-30 22:19:29] iter = 17490, loss = 2.5887
2024-10-30 22:19:31: [2024-10-30 22:19:31] iter = 17500, loss = 2.0480
2024-10-30 22:19:34: [2024-10-30 22:19:34] iter = 17510, loss = 1.9636
2024-10-30 22:19:37: [2024-10-30 22:19:37] iter = 17520, loss = 1.9850
2024-10-30 22:19:39: [2024-10-30 22:19:39] iter = 17530, loss = 2.2042
2024-10-30 22:19:43: [2024-10-30 22:19:43] iter = 17540, loss = 1.9252
2024-10-30 22:19:46: [2024-10-30 22:19:46] iter = 17550, loss = 1.7227
2024-10-30 22:19:49: [2024-10-30 22:19:49] iter = 17560, loss = 1.9418
2024-10-30 22:19:54: [2024-10-30 22:19:54] iter = 17570, loss = 2.8374
2024-10-30 22:19:57: [2024-10-30 22:19:57] iter = 17580, loss = 2.1676
2024-10-30 22:20:01: [2024-10-30 22:20:01] iter = 17590, loss = 1.9676
2024-10-30 22:20:05: [2024-10-30 22:20:05] iter = 17600, loss = 2.3982
2024-10-30 22:20:08: [2024-10-30 22:20:08] iter = 17610, loss = 1.9835
2024-10-30 22:20:11: [2024-10-30 22:20:11] iter = 17620, loss = 1.9997
2024-10-30 22:20:15: [2024-10-30 22:20:15] iter = 17630, loss = 2.9442
2024-10-30 22:20:19: [2024-10-30 22:20:19] iter = 17640, loss = 2.2628
2024-10-30 22:20:24: [2024-10-30 22:20:24] iter = 17650, loss = 1.9804
2024-10-30 22:20:28: [2024-10-30 22:20:28] iter = 17660, loss = 2.8134
2024-10-30 22:20:31: [2024-10-30 22:20:31] iter = 17670, loss = 1.9320
2024-10-30 22:20:35: [2024-10-30 22:20:35] iter = 17680, loss = 3.2540
2024-10-30 22:20:38: [2024-10-30 22:20:38] iter = 17690, loss = 1.7976
2024-10-30 22:20:42: [2024-10-30 22:20:42] iter = 17700, loss = 2.3806
2024-10-30 22:20:45: [2024-10-30 22:20:45] iter = 17710, loss = 2.4747
2024-10-30 22:20:48: [2024-10-30 22:20:48] iter = 17720, loss = 2.3327
2024-10-30 22:20:51: [2024-10-30 22:20:51] iter = 17730, loss = 2.0115
2024-10-30 22:20:55: [2024-10-30 22:20:55] iter = 17740, loss = 3.5176
2024-10-30 22:20:59: [2024-10-30 22:20:59] iter = 17750, loss = 2.7748
2024-10-30 22:21:04: [2024-10-30 22:21:04] iter = 17760, loss = 2.5298
2024-10-30 22:21:07: [2024-10-30 22:21:07] iter = 17770, loss = 2.0520
2024-10-30 22:21:10: [2024-10-30 22:21:10] iter = 17780, loss = 2.0679
2024-10-30 22:21:14: [2024-10-30 22:21:14] iter = 17790, loss = 1.9383
2024-10-30 22:21:17: [2024-10-30 22:21:17] iter = 17800, loss = 2.5235
2024-10-30 22:21:20: [2024-10-30 22:21:20] iter = 17810, loss = 2.0651
2024-10-30 22:21:24: [2024-10-30 22:21:24] iter = 17820, loss = 2.1176
2024-10-30 22:21:28: [2024-10-30 22:21:28] iter = 17830, loss = 4.6552
2024-10-30 22:21:31: [2024-10-30 22:21:31] iter = 17840, loss = 2.2027
2024-10-30 22:21:35: [2024-10-30 22:21:35] iter = 17850, loss = 5.1485
2024-10-30 22:21:38: [2024-10-30 22:21:38] iter = 17860, loss = 2.1390
2024-10-30 22:21:41: [2024-10-30 22:21:41] iter = 17870, loss = 2.2311
2024-10-30 22:21:45: [2024-10-30 22:21:45] iter = 17880, loss = 2.7653
2024-10-30 22:21:48: [2024-10-30 22:21:48] iter = 17890, loss = 1.9106
2024-10-30 22:21:51: [2024-10-30 22:21:51] iter = 17900, loss = 2.0647
2024-10-30 22:21:54: [2024-10-30 22:21:54] iter = 17910, loss = 2.0560
2024-10-30 22:21:57: [2024-10-30 22:21:57] iter = 17920, loss = 2.7606
2024-10-30 22:22:01: [2024-10-30 22:22:01] iter = 17930, loss = 2.5985
2024-10-30 22:22:05: [2024-10-30 22:22:05] iter = 17940, loss = 2.1445
2024-10-30 22:22:08: [2024-10-30 22:22:08] iter = 17950, loss = 3.1900
2024-10-30 22:22:13: [2024-10-30 22:22:13] iter = 17960, loss = 2.2809
2024-10-30 22:22:16: [2024-10-30 22:22:16] iter = 17970, loss = 4.0918
2024-10-30 22:22:19: [2024-10-30 22:22:19] iter = 17980, loss = 2.1037
2024-10-30 22:22:22: [2024-10-30 22:22:22] iter = 17990, loss = 2.4393
2024-10-30 22:22:26: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
2024-10-30 22:22:26: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 22:22:26: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 46313}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 22:24:52: Evaluate 5 random ConvNet, ACCmean = 0.7856 ACCstd = 0.0080
-------------------------
2024-10-30 22:24:52: Evaluate 5 random ConvNet, SENmean = 0.7829 SENstd = 0.0074
-------------------------
2024-10-30 22:24:52: Evaluate 5 random ConvNet, SPEmean = 0.9785 SPEstd = 0.0008
-------------------------
2024-10-30 22:24:52: Evaluate 5 random ConvNet, F!mean = 0.7736 F!std = 0.0071
-------------------------
2024-10-30 22:24:52: Evaluate 5 random ConvNet, mean = 0.7856 std = 0.0080
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 22:24:52: [2024-10-30 22:24:52] iter = 18000, loss = 1.9212
2024-10-30 22:24:56: [2024-10-30 22:24:56] iter = 18010, loss = 1.8838
2024-10-30 22:25:00: [2024-10-30 22:25:00] iter = 18020, loss = 3.6810
2024-10-30 22:25:04: [2024-10-30 22:25:04] iter = 18030, loss = 2.2209
2024-10-30 22:25:07: [2024-10-30 22:25:07] iter = 18040, loss = 3.2539
2024-10-30 22:25:11: [2024-10-30 22:25:11] iter = 18050, loss = 4.0825
2024-10-30 22:25:14: [2024-10-30 22:25:14] iter = 18060, loss = 2.1154
2024-10-30 22:25:18: [2024-10-30 22:25:18] iter = 18070, loss = 1.8455
2024-10-30 22:25:22: [2024-10-30 22:25:22] iter = 18080, loss = 2.4454
2024-10-30 22:25:25: [2024-10-30 22:25:25] iter = 18090, loss = 4.3042
2024-10-30 22:25:29: [2024-10-30 22:25:29] iter = 18100, loss = 4.3067
2024-10-30 22:25:33: [2024-10-30 22:25:33] iter = 18110, loss = 2.5544
2024-10-30 22:25:37: [2024-10-30 22:25:37] iter = 18120, loss = 2.0246
2024-10-30 22:25:41: [2024-10-30 22:25:41] iter = 18130, loss = 2.1072
2024-10-30 22:25:45: [2024-10-30 22:25:45] iter = 18140, loss = 1.8742
2024-10-30 22:25:48: [2024-10-30 22:25:48] iter = 18150, loss = 1.7449
2024-10-30 22:25:52: [2024-10-30 22:25:52] iter = 18160, loss = 1.9280
2024-10-30 22:25:56: [2024-10-30 22:25:56] iter = 18170, loss = 1.8307
2024-10-30 22:26:00: [2024-10-30 22:26:00] iter = 18180, loss = 2.0955
2024-10-30 22:26:03: [2024-10-30 22:26:03] iter = 18190, loss = 2.6921
2024-10-30 22:26:08: [2024-10-30 22:26:08] iter = 18200, loss = 2.9633
2024-10-30 22:26:12: [2024-10-30 22:26:12] iter = 18210, loss = 5.6445
2024-10-30 22:26:16: [2024-10-30 22:26:16] iter = 18220, loss = 3.1091
2024-10-30 22:26:20: [2024-10-30 22:26:20] iter = 18230, loss = 2.6784
2024-10-30 22:26:23: [2024-10-30 22:26:23] iter = 18240, loss = 3.6200
2024-10-30 22:26:27: [2024-10-30 22:26:27] iter = 18250, loss = 1.9165
2024-10-30 22:26:31: [2024-10-30 22:26:31] iter = 18260, loss = 3.4111
2024-10-30 22:26:35: [2024-10-30 22:26:35] iter = 18270, loss = 2.5607
2024-10-30 22:26:39: [2024-10-30 22:26:39] iter = 18280, loss = 2.7874
2024-10-30 22:26:43: [2024-10-30 22:26:43] iter = 18290, loss = 1.9426
2024-10-30 22:26:46: [2024-10-30 22:26:46] iter = 18300, loss = 2.0466
2024-10-30 22:26:50: [2024-10-30 22:26:50] iter = 18310, loss = 2.9168
2024-10-30 22:26:53: [2024-10-30 22:26:53] iter = 18320, loss = 1.8905
2024-10-30 22:26:57: [2024-10-30 22:26:57] iter = 18330, loss = 2.0807
2024-10-30 22:27:00: [2024-10-30 22:27:00] iter = 18340, loss = 2.3217
2024-10-30 22:27:03: [2024-10-30 22:27:03] iter = 18350, loss = 2.5316
2024-10-30 22:27:07: [2024-10-30 22:27:07] iter = 18360, loss = 2.0496
2024-10-30 22:27:11: [2024-10-30 22:27:11] iter = 18370, loss = 1.6992
2024-10-30 22:27:14: [2024-10-30 22:27:14] iter = 18380, loss = 1.8807
2024-10-30 22:27:18: [2024-10-30 22:27:18] iter = 18390, loss = 1.9422
2024-10-30 22:27:21: [2024-10-30 22:27:21] iter = 18400, loss = 2.1457
2024-10-30 22:27:25: [2024-10-30 22:27:25] iter = 18410, loss = 2.6600
2024-10-30 22:27:28: [2024-10-30 22:27:28] iter = 18420, loss = 2.2404
2024-10-30 22:27:31: [2024-10-30 22:27:31] iter = 18430, loss = 2.2014
2024-10-30 22:27:34: [2024-10-30 22:27:34] iter = 18440, loss = 2.1915
2024-10-30 22:27:38: [2024-10-30 22:27:38] iter = 18450, loss = 2.2644
2024-10-30 22:27:41: [2024-10-30 22:27:41] iter = 18460, loss = 2.1004
2024-10-30 22:27:44: [2024-10-30 22:27:44] iter = 18470, loss = 3.7476
2024-10-30 22:27:48: [2024-10-30 22:27:48] iter = 18480, loss = 2.0756
2024-10-30 22:27:51: [2024-10-30 22:27:51] iter = 18490, loss = 2.0543
2024-10-30 22:27:55: [2024-10-30 22:27:55] iter = 18500, loss = 3.2057
2024-10-30 22:27:58: [2024-10-30 22:27:58] iter = 18510, loss = 2.4377
2024-10-30 22:28:02: [2024-10-30 22:28:02] iter = 18520, loss = 2.0118
2024-10-30 22:28:06: [2024-10-30 22:28:06] iter = 18530, loss = 2.0141
2024-10-30 22:28:10: [2024-10-30 22:28:10] iter = 18540, loss = 5.0113
2024-10-30 22:28:14: [2024-10-30 22:28:14] iter = 18550, loss = 2.4617
2024-10-30 22:28:18: [2024-10-30 22:28:18] iter = 18560, loss = 2.1630
2024-10-30 22:28:21: [2024-10-30 22:28:21] iter = 18570, loss = 1.8867
2024-10-30 22:28:25: [2024-10-30 22:28:25] iter = 18580, loss = 2.0227
2024-10-30 22:28:29: [2024-10-30 22:28:29] iter = 18590, loss = 2.4357
2024-10-30 22:28:32: [2024-10-30 22:28:32] iter = 18600, loss = 1.8337
2024-10-30 22:28:35: [2024-10-30 22:28:35] iter = 18610, loss = 2.2861
2024-10-30 22:28:39: [2024-10-30 22:28:39] iter = 18620, loss = 2.2182
2024-10-30 22:28:43: [2024-10-30 22:28:43] iter = 18630, loss = 2.3631
2024-10-30 22:28:47: [2024-10-30 22:28:47] iter = 18640, loss = 2.8525
2024-10-30 22:28:51: [2024-10-30 22:28:51] iter = 18650, loss = 2.1298
2024-10-30 22:28:54: [2024-10-30 22:28:54] iter = 18660, loss = 2.3389
2024-10-30 22:28:58: [2024-10-30 22:28:58] iter = 18670, loss = 2.3560
2024-10-30 22:29:02: [2024-10-30 22:29:02] iter = 18680, loss = 1.9895
2024-10-30 22:29:06: [2024-10-30 22:29:06] iter = 18690, loss = 3.0025
2024-10-30 22:29:09: [2024-10-30 22:29:09] iter = 18700, loss = 3.0995
2024-10-30 22:29:13: [2024-10-30 22:29:13] iter = 18710, loss = 9.0446
2024-10-30 22:29:17: [2024-10-30 22:29:17] iter = 18720, loss = 2.6937
2024-10-30 22:29:21: [2024-10-30 22:29:21] iter = 18730, loss = 1.8972
2024-10-30 22:29:25: [2024-10-30 22:29:25] iter = 18740, loss = 2.1565
2024-10-30 22:29:28: [2024-10-30 22:29:28] iter = 18750, loss = 2.2000
2024-10-30 22:29:32: [2024-10-30 22:29:32] iter = 18760, loss = 2.6521
2024-10-30 22:29:36: [2024-10-30 22:29:36] iter = 18770, loss = 2.2371
2024-10-30 22:29:39: [2024-10-30 22:29:39] iter = 18780, loss = 3.2995
2024-10-30 22:29:42: [2024-10-30 22:29:42] iter = 18790, loss = 1.8642
2024-10-30 22:29:46: [2024-10-30 22:29:46] iter = 18800, loss = 2.6682
2024-10-30 22:29:48: [2024-10-30 22:29:48] iter = 18810, loss = 2.0867
2024-10-30 22:29:52: [2024-10-30 22:29:52] iter = 18820, loss = 2.2805
2024-10-30 22:29:55: [2024-10-30 22:29:55] iter = 18830, loss = 2.0926
2024-10-30 22:29:58: [2024-10-30 22:29:58] iter = 18840, loss = 2.0295
2024-10-30 22:30:02: [2024-10-30 22:30:02] iter = 18850, loss = 1.9774
2024-10-30 22:30:04: [2024-10-30 22:30:04] iter = 18860, loss = 7.7916
2024-10-30 22:30:07: [2024-10-30 22:30:07] iter = 18870, loss = 2.1283
2024-10-30 22:30:10: [2024-10-30 22:30:10] iter = 18880, loss = 2.9818
2024-10-30 22:30:14: [2024-10-30 22:30:14] iter = 18890, loss = 2.1987
2024-10-30 22:30:17: [2024-10-30 22:30:17] iter = 18900, loss = 2.1676
2024-10-30 22:30:20: [2024-10-30 22:30:20] iter = 18910, loss = 2.2933
2024-10-30 22:30:24: [2024-10-30 22:30:24] iter = 18920, loss = 3.2560
2024-10-30 22:30:28: [2024-10-30 22:30:28] iter = 18930, loss = 2.1354
2024-10-30 22:30:31: [2024-10-30 22:30:31] iter = 18940, loss = 2.2960
2024-10-30 22:30:35: [2024-10-30 22:30:35] iter = 18950, loss = 2.4224
2024-10-30 22:30:39: [2024-10-30 22:30:39] iter = 18960, loss = 2.1722
2024-10-30 22:30:42: [2024-10-30 22:30:42] iter = 18970, loss = 2.6806
2024-10-30 22:30:46: [2024-10-30 22:30:46] iter = 18980, loss = 2.8393
2024-10-30 22:30:50: [2024-10-30 22:30:50] iter = 18990, loss = 1.8896
2024-10-30 22:30:53: [2024-10-30 22:30:53] iter = 19000, loss = 3.3968
2024-10-30 22:30:56: [2024-10-30 22:30:56] iter = 19010, loss = 2.3489
2024-10-30 22:30:58: [2024-10-30 22:30:58] iter = 19020, loss = 1.8505
2024-10-30 22:31:02: [2024-10-30 22:31:02] iter = 19030, loss = 1.9630
2024-10-30 22:31:04: [2024-10-30 22:31:04] iter = 19040, loss = 1.8951
2024-10-30 22:31:07: [2024-10-30 22:31:07] iter = 19050, loss = 1.9319
2024-10-30 22:31:10: [2024-10-30 22:31:10] iter = 19060, loss = 3.0959
2024-10-30 22:31:14: [2024-10-30 22:31:14] iter = 19070, loss = 2.3563
2024-10-30 22:31:18: [2024-10-30 22:31:18] iter = 19080, loss = 2.4603
2024-10-30 22:31:21: [2024-10-30 22:31:21] iter = 19090, loss = 1.8047
2024-10-30 22:31:25: [2024-10-30 22:31:25] iter = 19100, loss = 2.0553
2024-10-30 22:31:29: [2024-10-30 22:31:29] iter = 19110, loss = 2.0152
2024-10-30 22:31:33: [2024-10-30 22:31:33] iter = 19120, loss = 3.8877
2024-10-30 22:31:37: [2024-10-30 22:31:37] iter = 19130, loss = 2.2087
2024-10-30 22:31:40: [2024-10-30 22:31:40] iter = 19140, loss = 1.9587
2024-10-30 22:31:43: [2024-10-30 22:31:43] iter = 19150, loss = 1.9634
2024-10-30 22:31:47: [2024-10-30 22:31:47] iter = 19160, loss = 2.7667
2024-10-30 22:31:51: [2024-10-30 22:31:51] iter = 19170, loss = 2.2406
2024-10-30 22:31:54: [2024-10-30 22:31:54] iter = 19180, loss = 2.0375
2024-10-30 22:31:57: [2024-10-30 22:31:57] iter = 19190, loss = 1.6731
2024-10-30 22:32:01: [2024-10-30 22:32:01] iter = 19200, loss = 2.3559
2024-10-30 22:32:05: [2024-10-30 22:32:05] iter = 19210, loss = 1.7159
2024-10-30 22:32:08: [2024-10-30 22:32:08] iter = 19220, loss = 1.9657
2024-10-30 22:32:11: [2024-10-30 22:32:11] iter = 19230, loss = 2.2232
2024-10-30 22:32:14: [2024-10-30 22:32:14] iter = 19240, loss = 2.5306
2024-10-30 22:32:18: [2024-10-30 22:32:18] iter = 19250, loss = 3.8459
2024-10-30 22:32:21: [2024-10-30 22:32:21] iter = 19260, loss = 2.3551
2024-10-30 22:32:25: [2024-10-30 22:32:25] iter = 19270, loss = 2.0512
2024-10-30 22:32:29: [2024-10-30 22:32:29] iter = 19280, loss = 1.6403
2024-10-30 22:32:32: [2024-10-30 22:32:32] iter = 19290, loss = 2.2360
2024-10-30 22:32:36: [2024-10-30 22:32:36] iter = 19300, loss = 4.9211
2024-10-30 22:32:39: [2024-10-30 22:32:39] iter = 19310, loss = 3.2751
2024-10-30 22:32:42: [2024-10-30 22:32:42] iter = 19320, loss = 2.4740
2024-10-30 22:32:46: [2024-10-30 22:32:46] iter = 19330, loss = 2.0211
2024-10-30 22:32:49: [2024-10-30 22:32:49] iter = 19340, loss = 2.7397
2024-10-30 22:32:52: [2024-10-30 22:32:52] iter = 19350, loss = 2.3005
2024-10-30 22:32:56: [2024-10-30 22:32:56] iter = 19360, loss = 2.0831
2024-10-30 22:33:01: [2024-10-30 22:33:01] iter = 19370, loss = 2.5761
2024-10-30 22:33:05: [2024-10-30 22:33:05] iter = 19380, loss = 2.6720
2024-10-30 22:33:09: [2024-10-30 22:33:09] iter = 19390, loss = 2.3631
2024-10-30 22:33:12: [2024-10-30 22:33:12] iter = 19400, loss = 3.5249
2024-10-30 22:33:16: [2024-10-30 22:33:16] iter = 19410, loss = 2.4751
2024-10-30 22:33:21: [2024-10-30 22:33:21] iter = 19420, loss = 1.8148
2024-10-30 22:33:25: [2024-10-30 22:33:25] iter = 19430, loss = 1.9478
2024-10-30 22:33:28: [2024-10-30 22:33:28] iter = 19440, loss = 2.2877
2024-10-30 22:33:32: [2024-10-30 22:33:32] iter = 19450, loss = 2.3180
2024-10-30 22:33:35: [2024-10-30 22:33:35] iter = 19460, loss = 1.8739
2024-10-30 22:33:38: [2024-10-30 22:33:38] iter = 19470, loss = 2.2572
2024-10-30 22:33:42: [2024-10-30 22:33:42] iter = 19480, loss = 2.4989
2024-10-30 22:33:46: [2024-10-30 22:33:46] iter = 19490, loss = 2.1780
2024-10-30 22:33:49: [2024-10-30 22:33:49] iter = 19500, loss = 7.3170
2024-10-30 22:33:53: [2024-10-30 22:33:53] iter = 19510, loss = 2.2927
2024-10-30 22:33:56: [2024-10-30 22:33:56] iter = 19520, loss = 4.6268
2024-10-30 22:33:59: [2024-10-30 22:33:59] iter = 19530, loss = 2.3445
2024-10-30 22:34:03: [2024-10-30 22:34:03] iter = 19540, loss = 2.2886
2024-10-30 22:34:07: [2024-10-30 22:34:07] iter = 19550, loss = 2.6213
2024-10-30 22:34:11: [2024-10-30 22:34:11] iter = 19560, loss = 4.9338
2024-10-30 22:34:14: [2024-10-30 22:34:14] iter = 19570, loss = 1.9116
2024-10-30 22:34:17: [2024-10-30 22:34:17] iter = 19580, loss = 1.9008
2024-10-30 22:34:21: [2024-10-30 22:34:21] iter = 19590, loss = 3.5253
2024-10-30 22:34:25: [2024-10-30 22:34:25] iter = 19600, loss = 3.0014
2024-10-30 22:34:28: [2024-10-30 22:34:28] iter = 19610, loss = 2.6291
2024-10-30 22:34:31: [2024-10-30 22:34:31] iter = 19620, loss = 2.2433
2024-10-30 22:34:36: [2024-10-30 22:34:36] iter = 19630, loss = 1.8115
2024-10-30 22:34:40: [2024-10-30 22:34:40] iter = 19640, loss = 2.5945
2024-10-30 22:34:43: [2024-10-30 22:34:43] iter = 19650, loss = 2.5155
2024-10-30 22:34:47: [2024-10-30 22:34:47] iter = 19660, loss = 2.5847
2024-10-30 22:34:50: [2024-10-30 22:34:50] iter = 19670, loss = 2.3659
2024-10-30 22:34:52: [2024-10-30 22:34:52] iter = 19680, loss = 4.2759
2024-10-30 22:34:56: [2024-10-30 22:34:56] iter = 19690, loss = 3.1864
2024-10-30 22:34:59: [2024-10-30 22:34:59] iter = 19700, loss = 2.3420
2024-10-30 22:35:03: [2024-10-30 22:35:03] iter = 19710, loss = 2.9694
2024-10-30 22:35:07: [2024-10-30 22:35:07] iter = 19720, loss = 1.7108
2024-10-30 22:35:10: [2024-10-30 22:35:10] iter = 19730, loss = 2.7930
2024-10-30 22:35:13: [2024-10-30 22:35:13] iter = 19740, loss = 2.1372
2024-10-30 22:35:17: [2024-10-30 22:35:17] iter = 19750, loss = 1.9345
2024-10-30 22:35:20: [2024-10-30 22:35:20] iter = 19760, loss = 1.9033
2024-10-30 22:35:24: [2024-10-30 22:35:24] iter = 19770, loss = 2.1694
2024-10-30 22:35:26: [2024-10-30 22:35:26] iter = 19780, loss = 2.0505
2024-10-30 22:35:30: [2024-10-30 22:35:30] iter = 19790, loss = 2.3105
2024-10-30 22:35:34: [2024-10-30 22:35:34] iter = 19800, loss = 1.8732
2024-10-30 22:35:38: [2024-10-30 22:35:38] iter = 19810, loss = 2.0958
2024-10-30 22:35:43: [2024-10-30 22:35:43] iter = 19820, loss = 2.8602
2024-10-30 22:35:47: [2024-10-30 22:35:47] iter = 19830, loss = 2.3700
2024-10-30 22:35:50: [2024-10-30 22:35:50] iter = 19840, loss = 2.1099
2024-10-30 22:35:53: [2024-10-30 22:35:53] iter = 19850, loss = 2.6337
2024-10-30 22:35:57: [2024-10-30 22:35:57] iter = 19860, loss = 1.9503
2024-10-30 22:36:00: [2024-10-30 22:36:00] iter = 19870, loss = 2.1109
2024-10-30 22:36:02: [2024-10-30 22:36:02] iter = 19880, loss = 3.5999
2024-10-30 22:36:04: [2024-10-30 22:36:04] iter = 19890, loss = 2.1422
2024-10-30 22:36:08: [2024-10-30 22:36:08] iter = 19900, loss = 1.9696
2024-10-30 22:36:12: [2024-10-30 22:36:12] iter = 19910, loss = 2.7548
2024-10-30 22:36:16: [2024-10-30 22:36:16] iter = 19920, loss = 2.0014
2024-10-30 22:36:19: [2024-10-30 22:36:19] iter = 19930, loss = 2.2073
2024-10-30 22:36:22: [2024-10-30 22:36:22] iter = 19940, loss = 3.6215
2024-10-30 22:36:25: [2024-10-30 22:36:25] iter = 19950, loss = 4.0016
2024-10-30 22:36:28: [2024-10-30 22:36:28] iter = 19960, loss = 2.2972
2024-10-30 22:36:32: [2024-10-30 22:36:32] iter = 19970, loss = 2.0420
2024-10-30 22:36:35: [2024-10-30 22:36:35] iter = 19980, loss = 2.2330
2024-10-30 22:36:38: [2024-10-30 22:36:38] iter = 19990, loss = 2.6656
2024-10-30 22:36:42: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
2024-10-30 22:36:42: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 22:36:42: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 2197}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 22:38:59: Evaluate 5 random ConvNet, ACCmean = 0.7758 ACCstd = 0.0051
-------------------------
2024-10-30 22:38:59: Evaluate 5 random ConvNet, SENmean = 0.7712 SENstd = 0.0043
-------------------------
2024-10-30 22:38:59: Evaluate 5 random ConvNet, SPEmean = 0.9775 SPEstd = 0.0005
-------------------------
2024-10-30 22:38:59: Evaluate 5 random ConvNet, F!mean = 0.7605 F!std = 0.0053
-------------------------
2024-10-30 22:38:59: Evaluate 5 random ConvNet, mean = 0.7758 std = 0.0051
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 22:38:59: [2024-10-30 22:38:59] iter = 20000, loss = 1.9227
2024-10-30 22:38:59: 
================== Exp 3 ==================
 
2024-10-30 22:38:59: Hyper-parameters: 
{'dataset': 'OrganAMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': '/data/users/xiongyuxuan/DD/OBJDD/DatasetCondensation-master/data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f5f8402db20>, 'dsa': True, 'logger': <Logger DM_IPC=10_Model_ConvNet_Data_OrganAMNIST (INFO)>}
2024-10-30 22:38:59: Evaluation model pool: ['ConvNet']
2024-10-30 22:39:02: class c = 0: 1956 real images
2024-10-30 22:39:02: class c = 1: 1390 real images
2024-10-30 22:39:02: class c = 2: 1357 real images
2024-10-30 22:39:02: class c = 3: 1474 real images
2024-10-30 22:39:02: class c = 4: 3963 real images
2024-10-30 22:39:02: class c = 5: 3817 real images
2024-10-30 22:39:02: class c = 6: 6164 real images
2024-10-30 22:39:02: class c = 7: 3919 real images
2024-10-30 22:39:02: class c = 8: 3929 real images
2024-10-30 22:39:02: class c = 9: 3031 real images
2024-10-30 22:39:02: class c = 10: 3561 real images
2024-10-30 22:39:02: real images channel 0, mean = 0.4680, std = 0.2974
main_DM.py:120: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-10-30 22:39:02: initialize synthetic data from random real images
2024-10-30 22:39:02: [2024-10-30 22:39:02] training begins
2024-10-30 22:39:02: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-10-30 22:39:02: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 22:39:02: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 39509}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 22:41:18: Evaluate 5 random ConvNet, ACCmean = 0.6752 ACCstd = 0.0042
-------------------------
2024-10-30 22:41:18: Evaluate 5 random ConvNet, SENmean = 0.6696 SENstd = 0.0022
-------------------------
2024-10-30 22:41:18: Evaluate 5 random ConvNet, SPEmean = 0.9673 SPEstd = 0.0004
-------------------------
2024-10-30 22:41:18: Evaluate 5 random ConvNet, F!mean = 0.6628 F!std = 0.0029
-------------------------
2024-10-30 22:41:18: Evaluate 5 random ConvNet, mean = 0.6752 std = 0.0042
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 22:41:18: [2024-10-30 22:41:18] iter = 00000, loss = 11.6671
2024-10-30 22:41:22: [2024-10-30 22:41:22] iter = 00010, loss = 4.3303
2024-10-30 22:41:24: [2024-10-30 22:41:24] iter = 00020, loss = 3.2531
2024-10-30 22:41:28: [2024-10-30 22:41:28] iter = 00030, loss = 3.5649
2024-10-30 22:41:31: [2024-10-30 22:41:31] iter = 00040, loss = 4.5459
2024-10-30 22:41:34: [2024-10-30 22:41:34] iter = 00050, loss = 3.9114
2024-10-30 22:41:37: [2024-10-30 22:41:37] iter = 00060, loss = 2.7552
2024-10-30 22:41:40: [2024-10-30 22:41:40] iter = 00070, loss = 2.6268
2024-10-30 22:41:44: [2024-10-30 22:41:44] iter = 00080, loss = 2.6816
2024-10-30 22:41:48: [2024-10-30 22:41:48] iter = 00090, loss = 2.7099
2024-10-30 22:41:52: [2024-10-30 22:41:52] iter = 00100, loss = 3.1525
2024-10-30 22:41:55: [2024-10-30 22:41:55] iter = 00110, loss = 2.9123
2024-10-30 22:41:57: [2024-10-30 22:41:57] iter = 00120, loss = 2.7739
2024-10-30 22:42:00: [2024-10-30 22:42:00] iter = 00130, loss = 2.5075
2024-10-30 22:42:02: [2024-10-30 22:42:02] iter = 00140, loss = 3.0619
2024-10-30 22:42:06: [2024-10-30 22:42:06] iter = 00150, loss = 2.1458
2024-10-30 22:42:08: [2024-10-30 22:42:08] iter = 00160, loss = 2.3811
2024-10-30 22:42:11: [2024-10-30 22:42:11] iter = 00170, loss = 2.5778
2024-10-30 22:42:13: [2024-10-30 22:42:13] iter = 00180, loss = 2.2188
2024-10-30 22:42:16: [2024-10-30 22:42:16] iter = 00190, loss = 2.7249
2024-10-30 22:42:20: [2024-10-30 22:42:20] iter = 00200, loss = 2.3470
2024-10-30 22:42:24: [2024-10-30 22:42:23] iter = 00210, loss = 2.1170
2024-10-30 22:42:27: [2024-10-30 22:42:27] iter = 00220, loss = 2.2322
2024-10-30 22:42:30: [2024-10-30 22:42:30] iter = 00230, loss = 2.7877
2024-10-30 22:42:33: [2024-10-30 22:42:33] iter = 00240, loss = 2.3058
2024-10-30 22:42:37: [2024-10-30 22:42:37] iter = 00250, loss = 3.2314
2024-10-30 22:42:40: [2024-10-30 22:42:40] iter = 00260, loss = 2.2168
2024-10-30 22:42:44: [2024-10-30 22:42:44] iter = 00270, loss = 2.4720
2024-10-30 22:42:47: [2024-10-30 22:42:47] iter = 00280, loss = 2.7633
2024-10-30 22:42:51: [2024-10-30 22:42:51] iter = 00290, loss = 2.3634
2024-10-30 22:42:55: [2024-10-30 22:42:55] iter = 00300, loss = 2.2886
2024-10-30 22:42:58: [2024-10-30 22:42:58] iter = 00310, loss = 2.7934
2024-10-30 22:43:02: [2024-10-30 22:43:02] iter = 00320, loss = 2.5910
2024-10-30 22:43:05: [2024-10-30 22:43:05] iter = 00330, loss = 2.9532
2024-10-30 22:43:09: [2024-10-30 22:43:09] iter = 00340, loss = 2.6763
2024-10-30 22:43:12: [2024-10-30 22:43:12] iter = 00350, loss = 2.2530
2024-10-30 22:43:14: [2024-10-30 22:43:14] iter = 00360, loss = 2.0278
2024-10-30 22:43:18: [2024-10-30 22:43:18] iter = 00370, loss = 2.4551
2024-10-30 22:43:21: [2024-10-30 22:43:21] iter = 00380, loss = 2.8018
2024-10-30 22:43:25: [2024-10-30 22:43:25] iter = 00390, loss = 8.7458
2024-10-30 22:43:29: [2024-10-30 22:43:29] iter = 00400, loss = 2.4751
2024-10-30 22:43:33: [2024-10-30 22:43:33] iter = 00410, loss = 1.9295
2024-10-30 22:43:36: [2024-10-30 22:43:36] iter = 00420, loss = 2.2913
2024-10-30 22:43:40: [2024-10-30 22:43:40] iter = 00430, loss = 2.3989
2024-10-30 22:43:43: [2024-10-30 22:43:43] iter = 00440, loss = 4.8848
2024-10-30 22:43:45: [2024-10-30 22:43:45] iter = 00450, loss = 2.0899
2024-10-30 22:43:48: [2024-10-30 22:43:48] iter = 00460, loss = 2.1976
2024-10-30 22:43:51: [2024-10-30 22:43:51] iter = 00470, loss = 2.3861
2024-10-30 22:43:55: [2024-10-30 22:43:55] iter = 00480, loss = 3.1995
2024-10-30 22:43:58: [2024-10-30 22:43:58] iter = 00490, loss = 2.2022
2024-10-30 22:44:00: [2024-10-30 22:44:00] iter = 00500, loss = 1.9856
2024-10-30 22:44:04: [2024-10-30 22:44:04] iter = 00510, loss = 2.2244
2024-10-30 22:44:07: [2024-10-30 22:44:07] iter = 00520, loss = 2.8105
2024-10-30 22:44:10: [2024-10-30 22:44:10] iter = 00530, loss = 2.2916
2024-10-30 22:44:13: [2024-10-30 22:44:13] iter = 00540, loss = 2.0814
2024-10-30 22:44:17: [2024-10-30 22:44:17] iter = 00550, loss = 4.1558
2024-10-30 22:44:21: [2024-10-30 22:44:21] iter = 00560, loss = 3.1677
2024-10-30 22:44:24: [2024-10-30 22:44:24] iter = 00570, loss = 2.3862
2024-10-30 22:44:27: [2024-10-30 22:44:27] iter = 00580, loss = 2.0661
2024-10-30 22:44:31: [2024-10-30 22:44:31] iter = 00590, loss = 2.5469
2024-10-30 22:44:35: [2024-10-30 22:44:35] iter = 00600, loss = 2.7135
2024-10-30 22:44:39: [2024-10-30 22:44:39] iter = 00610, loss = 3.7403
2024-10-30 22:44:42: [2024-10-30 22:44:42] iter = 00620, loss = 2.4977
2024-10-30 22:44:46: [2024-10-30 22:44:46] iter = 00630, loss = 12.0734
2024-10-30 22:44:50: [2024-10-30 22:44:50] iter = 00640, loss = 2.3219
2024-10-30 22:44:54: [2024-10-30 22:44:54] iter = 00650, loss = 2.6749
2024-10-30 22:44:58: [2024-10-30 22:44:58] iter = 00660, loss = 2.1607
2024-10-30 22:45:02: [2024-10-30 22:45:02] iter = 00670, loss = 2.4402
2024-10-30 22:45:06: [2024-10-30 22:45:06] iter = 00680, loss = 3.2987
2024-10-30 22:45:10: [2024-10-30 22:45:10] iter = 00690, loss = 2.7928
2024-10-30 22:45:13: [2024-10-30 22:45:13] iter = 00700, loss = 2.2158
2024-10-30 22:45:17: [2024-10-30 22:45:17] iter = 00710, loss = 2.1463
2024-10-30 22:45:21: [2024-10-30 22:45:21] iter = 00720, loss = 2.2017
2024-10-30 22:45:24: [2024-10-30 22:45:24] iter = 00730, loss = 2.2910
2024-10-30 22:45:27: [2024-10-30 22:45:27] iter = 00740, loss = 3.1411
2024-10-30 22:45:31: [2024-10-30 22:45:31] iter = 00750, loss = 1.8533
2024-10-30 22:45:36: [2024-10-30 22:45:36] iter = 00760, loss = 3.4323
2024-10-30 22:45:40: [2024-10-30 22:45:40] iter = 00770, loss = 6.3872
2024-10-30 22:45:43: [2024-10-30 22:45:43] iter = 00780, loss = 3.5663
2024-10-30 22:45:47: [2024-10-30 22:45:47] iter = 00790, loss = 2.0565
2024-10-30 22:45:51: [2024-10-30 22:45:51] iter = 00800, loss = 2.0163
2024-10-30 22:45:54: [2024-10-30 22:45:54] iter = 00810, loss = 2.2410
2024-10-30 22:45:57: [2024-10-30 22:45:57] iter = 00820, loss = 3.3548
2024-10-30 22:46:00: [2024-10-30 22:46:00] iter = 00830, loss = 2.0931
2024-10-30 22:46:03: [2024-10-30 22:46:03] iter = 00840, loss = 2.5935
2024-10-30 22:46:05: [2024-10-30 22:46:05] iter = 00850, loss = 2.6781
2024-10-30 22:46:09: [2024-10-30 22:46:09] iter = 00860, loss = 2.8543
2024-10-30 22:46:12: [2024-10-30 22:46:12] iter = 00870, loss = 1.6882
2024-10-30 22:46:16: [2024-10-30 22:46:16] iter = 00880, loss = 2.3589
2024-10-30 22:46:19: [2024-10-30 22:46:19] iter = 00890, loss = 2.4820
2024-10-30 22:46:22: [2024-10-30 22:46:22] iter = 00900, loss = 2.4970
2024-10-30 22:46:26: [2024-10-30 22:46:26] iter = 00910, loss = 1.9337
2024-10-30 22:46:29: [2024-10-30 22:46:29] iter = 00920, loss = 2.1719
2024-10-30 22:46:33: [2024-10-30 22:46:33] iter = 00930, loss = 2.5668
2024-10-30 22:46:37: [2024-10-30 22:46:37] iter = 00940, loss = 2.1360
2024-10-30 22:46:41: [2024-10-30 22:46:41] iter = 00950, loss = 4.1346
2024-10-30 22:46:46: [2024-10-30 22:46:46] iter = 00960, loss = 2.0352
2024-10-30 22:46:50: [2024-10-30 22:46:50] iter = 00970, loss = 1.8976
2024-10-30 22:46:55: [2024-10-30 22:46:55] iter = 00980, loss = 2.2122
2024-10-30 22:46:59: [2024-10-30 22:46:59] iter = 00990, loss = 2.2852
2024-10-30 22:47:03: [2024-10-30 22:47:03] iter = 01000, loss = 3.1690
2024-10-30 22:47:08: [2024-10-30 22:47:08] iter = 01010, loss = 2.5366
2024-10-30 22:47:12: [2024-10-30 22:47:12] iter = 01020, loss = 2.2977
2024-10-30 22:47:17: [2024-10-30 22:47:17] iter = 01030, loss = 2.0269
2024-10-30 22:47:21: [2024-10-30 22:47:21] iter = 01040, loss = 2.2849
2024-10-30 22:47:25: [2024-10-30 22:47:25] iter = 01050, loss = 2.4407
2024-10-30 22:47:29: [2024-10-30 22:47:29] iter = 01060, loss = 4.1032
2024-10-30 22:47:33: [2024-10-30 22:47:33] iter = 01070, loss = 2.0952
2024-10-30 22:47:36: [2024-10-30 22:47:36] iter = 01080, loss = 2.3939
2024-10-30 22:47:41: [2024-10-30 22:47:41] iter = 01090, loss = 1.9678
2024-10-30 22:47:44: [2024-10-30 22:47:44] iter = 01100, loss = 3.5767
2024-10-30 22:47:47: [2024-10-30 22:47:47] iter = 01110, loss = 2.5502
2024-10-30 22:47:50: [2024-10-30 22:47:50] iter = 01120, loss = 2.1888
2024-10-30 22:47:53: [2024-10-30 22:47:53] iter = 01130, loss = 3.3374
2024-10-30 22:47:57: [2024-10-30 22:47:57] iter = 01140, loss = 2.5402
2024-10-30 22:48:01: [2024-10-30 22:48:01] iter = 01150, loss = 3.2119
2024-10-30 22:48:06: [2024-10-30 22:48:06] iter = 01160, loss = 2.3456
2024-10-30 22:48:10: [2024-10-30 22:48:10] iter = 01170, loss = 2.4686
2024-10-30 22:48:14: [2024-10-30 22:48:14] iter = 01180, loss = 2.2978
2024-10-30 22:48:17: [2024-10-30 22:48:17] iter = 01190, loss = 2.2399
2024-10-30 22:48:21: [2024-10-30 22:48:21] iter = 01200, loss = 2.8170
2024-10-30 22:48:25: [2024-10-30 22:48:25] iter = 01210, loss = 2.3626
2024-10-30 22:48:28: [2024-10-30 22:48:28] iter = 01220, loss = 2.8562
2024-10-30 22:48:32: [2024-10-30 22:48:32] iter = 01230, loss = 1.9579
2024-10-30 22:48:35: [2024-10-30 22:48:35] iter = 01240, loss = 2.8818
2024-10-30 22:48:40: [2024-10-30 22:48:40] iter = 01250, loss = 1.8234
2024-10-30 22:48:44: [2024-10-30 22:48:44] iter = 01260, loss = 2.2799
2024-10-30 22:48:47: [2024-10-30 22:48:47] iter = 01270, loss = 2.1545
2024-10-30 22:48:50: [2024-10-30 22:48:50] iter = 01280, loss = 2.2763
2024-10-30 22:48:53: [2024-10-30 22:48:53] iter = 01290, loss = 3.0724
2024-10-30 22:48:57: [2024-10-30 22:48:57] iter = 01300, loss = 2.3757
2024-10-30 22:49:01: [2024-10-30 22:49:01] iter = 01310, loss = 2.7663
2024-10-30 22:49:05: [2024-10-30 22:49:05] iter = 01320, loss = 2.0726
2024-10-30 22:49:09: [2024-10-30 22:49:09] iter = 01330, loss = 2.7722
2024-10-30 22:49:12: [2024-10-30 22:49:12] iter = 01340, loss = 2.6584
2024-10-30 22:49:15: [2024-10-30 22:49:15] iter = 01350, loss = 2.4397
2024-10-30 22:49:18: [2024-10-30 22:49:18] iter = 01360, loss = 2.0288
2024-10-30 22:49:21: [2024-10-30 22:49:21] iter = 01370, loss = 2.5909
2024-10-30 22:49:23: [2024-10-30 22:49:23] iter = 01380, loss = 2.0107
2024-10-30 22:49:27: [2024-10-30 22:49:27] iter = 01390, loss = 2.1460
2024-10-30 22:49:30: [2024-10-30 22:49:30] iter = 01400, loss = 1.8021
2024-10-30 22:49:34: [2024-10-30 22:49:34] iter = 01410, loss = 2.0360
2024-10-30 22:49:37: [2024-10-30 22:49:37] iter = 01420, loss = 7.7282
2024-10-30 22:49:40: [2024-10-30 22:49:40] iter = 01430, loss = 2.1549
2024-10-30 22:49:44: [2024-10-30 22:49:44] iter = 01440, loss = 2.0566
2024-10-30 22:49:48: [2024-10-30 22:49:48] iter = 01450, loss = 3.1924
2024-10-30 22:49:52: [2024-10-30 22:49:52] iter = 01460, loss = 2.6192
2024-10-30 22:49:55: [2024-10-30 22:49:55] iter = 01470, loss = 2.1941
2024-10-30 22:49:58: [2024-10-30 22:49:58] iter = 01480, loss = 2.9735
2024-10-30 22:50:01: [2024-10-30 22:50:01] iter = 01490, loss = 2.1474
2024-10-30 22:50:06: [2024-10-30 22:50:06] iter = 01500, loss = 1.9899
2024-10-30 22:50:08: [2024-10-30 22:50:08] iter = 01510, loss = 2.2895
2024-10-30 22:50:11: [2024-10-30 22:50:11] iter = 01520, loss = 3.2531
2024-10-30 22:50:16: [2024-10-30 22:50:16] iter = 01530, loss = 2.0662
2024-10-30 22:50:19: [2024-10-30 22:50:19] iter = 01540, loss = 2.2480
2024-10-30 22:50:22: [2024-10-30 22:50:22] iter = 01550, loss = 3.0133
2024-10-30 22:50:26: [2024-10-30 22:50:26] iter = 01560, loss = 2.0740
2024-10-30 22:50:30: [2024-10-30 22:50:30] iter = 01570, loss = 2.1887
2024-10-30 22:50:35: [2024-10-30 22:50:35] iter = 01580, loss = 1.8206
2024-10-30 22:50:37: [2024-10-30 22:50:37] iter = 01590, loss = 2.2419
2024-10-30 22:50:40: [2024-10-30 22:50:40] iter = 01600, loss = 2.7774
2024-10-30 22:50:44: [2024-10-30 22:50:44] iter = 01610, loss = 2.4371
2024-10-30 22:50:47: [2024-10-30 22:50:47] iter = 01620, loss = 3.9381
2024-10-30 22:50:50: [2024-10-30 22:50:50] iter = 01630, loss = 3.7181
2024-10-30 22:50:53: [2024-10-30 22:50:53] iter = 01640, loss = 1.9992
2024-10-30 22:50:57: [2024-10-30 22:50:57] iter = 01650, loss = 3.2343
2024-10-30 22:51:00: [2024-10-30 22:51:00] iter = 01660, loss = 2.2215
2024-10-30 22:51:04: [2024-10-30 22:51:04] iter = 01670, loss = 2.4598
2024-10-30 22:51:08: [2024-10-30 22:51:08] iter = 01680, loss = 2.1177
2024-10-30 22:51:12: [2024-10-30 22:51:12] iter = 01690, loss = 2.7533
2024-10-30 22:51:16: [2024-10-30 22:51:16] iter = 01700, loss = 9.8437
2024-10-30 22:51:20: [2024-10-30 22:51:20] iter = 01710, loss = 2.3448
2024-10-30 22:51:23: [2024-10-30 22:51:23] iter = 01720, loss = 2.1111
2024-10-30 22:51:26: [2024-10-30 22:51:26] iter = 01730, loss = 2.6958
2024-10-30 22:51:28: [2024-10-30 22:51:28] iter = 01740, loss = 2.3470
2024-10-30 22:51:32: [2024-10-30 22:51:32] iter = 01750, loss = 2.2807
2024-10-30 22:51:36: [2024-10-30 22:51:36] iter = 01760, loss = 2.0241
2024-10-30 22:51:40: [2024-10-30 22:51:40] iter = 01770, loss = 3.0202
2024-10-30 22:51:44: [2024-10-30 22:51:44] iter = 01780, loss = 2.0292
2024-10-30 22:51:47: [2024-10-30 22:51:47] iter = 01790, loss = 1.8188
2024-10-30 22:51:50: [2024-10-30 22:51:50] iter = 01800, loss = 2.6980
2024-10-30 22:51:53: [2024-10-30 22:51:53] iter = 01810, loss = 2.7619
2024-10-30 22:51:56: [2024-10-30 22:51:56] iter = 01820, loss = 2.4108
2024-10-30 22:51:58: [2024-10-30 22:51:58] iter = 01830, loss = 2.4148
2024-10-30 22:52:02: [2024-10-30 22:52:02] iter = 01840, loss = 1.7988
2024-10-30 22:52:05: [2024-10-30 22:52:05] iter = 01850, loss = 2.4562
2024-10-30 22:52:09: [2024-10-30 22:52:09] iter = 01860, loss = 1.9219
2024-10-30 22:52:13: [2024-10-30 22:52:13] iter = 01870, loss = 2.2349
2024-10-30 22:52:16: [2024-10-30 22:52:16] iter = 01880, loss = 2.4123
2024-10-30 22:52:20: [2024-10-30 22:52:20] iter = 01890, loss = 1.9120
2024-10-30 22:52:24: [2024-10-30 22:52:24] iter = 01900, loss = 2.2288
2024-10-30 22:52:28: [2024-10-30 22:52:28] iter = 01910, loss = 2.1528
2024-10-30 22:52:32: [2024-10-30 22:52:32] iter = 01920, loss = 2.7537
2024-10-30 22:52:36: [2024-10-30 22:52:36] iter = 01930, loss = 2.3164
2024-10-30 22:52:40: [2024-10-30 22:52:40] iter = 01940, loss = 2.2601
2024-10-30 22:52:43: [2024-10-30 22:52:43] iter = 01950, loss = 3.6195
2024-10-30 22:52:45: [2024-10-30 22:52:45] iter = 01960, loss = 2.2219
2024-10-30 22:52:48: [2024-10-30 22:52:48] iter = 01970, loss = 2.2387
2024-10-30 22:52:51: [2024-10-30 22:52:51] iter = 01980, loss = 2.5470
2024-10-30 22:52:54: [2024-10-30 22:52:54] iter = 01990, loss = 2.5127
2024-10-30 22:52:57: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
2024-10-30 22:52:57: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 22:52:57: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 77694}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 22:55:15: Evaluate 5 random ConvNet, ACCmean = 0.7866 ACCstd = 0.0064
-------------------------
2024-10-30 22:55:15: Evaluate 5 random ConvNet, SENmean = 0.7823 SENstd = 0.0053
-------------------------
2024-10-30 22:55:15: Evaluate 5 random ConvNet, SPEmean = 0.9786 SPEstd = 0.0006
-------------------------
2024-10-30 22:55:15: Evaluate 5 random ConvNet, F!mean = 0.7708 F!std = 0.0062
-------------------------
2024-10-30 22:55:15: Evaluate 5 random ConvNet, mean = 0.7866 std = 0.0064
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 22:55:15: [2024-10-30 22:55:15] iter = 02000, loss = 2.4684
2024-10-30 22:55:19: [2024-10-30 22:55:19] iter = 02010, loss = 2.1151
2024-10-30 22:55:22: [2024-10-30 22:55:22] iter = 02020, loss = 2.3526
2024-10-30 22:55:26: [2024-10-30 22:55:26] iter = 02030, loss = 2.4062
2024-10-30 22:55:29: [2024-10-30 22:55:29] iter = 02040, loss = 2.7369
2024-10-30 22:55:32: [2024-10-30 22:55:32] iter = 02050, loss = 1.8259
2024-10-30 22:55:36: [2024-10-30 22:55:36] iter = 02060, loss = 2.3674
2024-10-30 22:55:39: [2024-10-30 22:55:39] iter = 02070, loss = 1.8756
2024-10-30 22:55:43: [2024-10-30 22:55:43] iter = 02080, loss = 2.3865
2024-10-30 22:55:46: [2024-10-30 22:55:46] iter = 02090, loss = 2.3160
2024-10-30 22:55:50: [2024-10-30 22:55:50] iter = 02100, loss = 3.3531
2024-10-30 22:55:53: [2024-10-30 22:55:53] iter = 02110, loss = 2.1199
2024-10-30 22:55:56: [2024-10-30 22:55:56] iter = 02120, loss = 2.1279
2024-10-30 22:56:00: [2024-10-30 22:56:00] iter = 02130, loss = 2.1950
2024-10-30 22:56:04: [2024-10-30 22:56:04] iter = 02140, loss = 2.5271
2024-10-30 22:56:07: [2024-10-30 22:56:07] iter = 02150, loss = 8.0764
2024-10-30 22:56:11: [2024-10-30 22:56:11] iter = 02160, loss = 2.9686
2024-10-30 22:56:13: [2024-10-30 22:56:13] iter = 02170, loss = 2.4042
2024-10-30 22:56:16: [2024-10-30 22:56:16] iter = 02180, loss = 2.7382
2024-10-30 22:56:19: [2024-10-30 22:56:19] iter = 02190, loss = 1.8524
2024-10-30 22:56:22: [2024-10-30 22:56:22] iter = 02200, loss = 2.1335
2024-10-30 22:56:26: [2024-10-30 22:56:26] iter = 02210, loss = 2.0762
2024-10-30 22:56:30: [2024-10-30 22:56:30] iter = 02220, loss = 2.8583
2024-10-30 22:56:34: [2024-10-30 22:56:34] iter = 02230, loss = 2.0678
2024-10-30 22:56:37: [2024-10-30 22:56:37] iter = 02240, loss = 1.8131
2024-10-30 22:56:40: [2024-10-30 22:56:40] iter = 02250, loss = 2.1218
2024-10-30 22:56:42: [2024-10-30 22:56:42] iter = 02260, loss = 2.1606
2024-10-30 22:56:45: [2024-10-30 22:56:45] iter = 02270, loss = 2.3152
2024-10-30 22:56:47: [2024-10-30 22:56:47] iter = 02280, loss = 6.9185
2024-10-30 22:56:50: [2024-10-30 22:56:50] iter = 02290, loss = 2.0213
2024-10-30 22:56:52: [2024-10-30 22:56:52] iter = 02300, loss = 1.7931
2024-10-30 22:56:55: [2024-10-30 22:56:55] iter = 02310, loss = 2.5607
2024-10-30 22:56:58: [2024-10-30 22:56:58] iter = 02320, loss = 2.2489
2024-10-30 22:57:01: [2024-10-30 22:57:01] iter = 02330, loss = 5.6343
2024-10-30 22:57:06: [2024-10-30 22:57:06] iter = 02340, loss = 1.9392
2024-10-30 22:57:11: [2024-10-30 22:57:11] iter = 02350, loss = 2.2257
2024-10-30 22:57:15: [2024-10-30 22:57:15] iter = 02360, loss = 2.7109
2024-10-30 22:57:20: [2024-10-30 22:57:20] iter = 02370, loss = 3.7601
2024-10-30 22:57:25: [2024-10-30 22:57:25] iter = 02380, loss = 2.1059
2024-10-30 22:57:29: [2024-10-30 22:57:29] iter = 02390, loss = 2.0915
2024-10-30 22:57:33: [2024-10-30 22:57:33] iter = 02400, loss = 2.2252
2024-10-30 22:57:36: [2024-10-30 22:57:36] iter = 02410, loss = 2.5295
2024-10-30 22:57:41: [2024-10-30 22:57:41] iter = 02420, loss = 1.8435
2024-10-30 22:57:45: [2024-10-30 22:57:45] iter = 02430, loss = 2.2028
2024-10-30 22:57:49: [2024-10-30 22:57:49] iter = 02440, loss = 2.0079
2024-10-30 22:57:52: [2024-10-30 22:57:52] iter = 02450, loss = 2.0251
2024-10-30 22:57:55: [2024-10-30 22:57:55] iter = 02460, loss = 1.9619
2024-10-30 22:57:58: [2024-10-30 22:57:58] iter = 02470, loss = 1.9451
2024-10-30 22:58:02: [2024-10-30 22:58:02] iter = 02480, loss = 1.7519
2024-10-30 22:58:06: [2024-10-30 22:58:06] iter = 02490, loss = 2.7201
2024-10-30 22:58:10: [2024-10-30 22:58:10] iter = 02500, loss = 5.4846
2024-10-30 22:58:14: [2024-10-30 22:58:14] iter = 02510, loss = 2.2415
2024-10-30 22:58:19: [2024-10-30 22:58:19] iter = 02520, loss = 2.3102
2024-10-30 22:58:23: [2024-10-30 22:58:23] iter = 02530, loss = 2.9254
2024-10-30 22:58:27: [2024-10-30 22:58:27] iter = 02540, loss = 2.0169
2024-10-30 22:58:31: [2024-10-30 22:58:31] iter = 02550, loss = 3.1549
2024-10-30 22:58:35: [2024-10-30 22:58:35] iter = 02560, loss = 2.4149
2024-10-30 22:58:39: [2024-10-30 22:58:39] iter = 02570, loss = 2.0797
2024-10-30 22:58:42: [2024-10-30 22:58:42] iter = 02580, loss = 1.9763
2024-10-30 22:58:46: [2024-10-30 22:58:46] iter = 02590, loss = 2.3481
2024-10-30 22:58:49: [2024-10-30 22:58:49] iter = 02600, loss = 2.1942
2024-10-30 22:58:53: [2024-10-30 22:58:53] iter = 02610, loss = 2.1057
2024-10-30 22:58:56: [2024-10-30 22:58:56] iter = 02620, loss = 1.9542
2024-10-30 22:59:00: [2024-10-30 22:59:00] iter = 02630, loss = 2.5326
2024-10-30 22:59:03: [2024-10-30 22:59:03] iter = 02640, loss = 1.6701
2024-10-30 22:59:07: [2024-10-30 22:59:07] iter = 02650, loss = 2.4381
2024-10-30 22:59:10: [2024-10-30 22:59:10] iter = 02660, loss = 1.8586
2024-10-30 22:59:14: [2024-10-30 22:59:14] iter = 02670, loss = 2.4011
2024-10-30 22:59:17: [2024-10-30 22:59:17] iter = 02680, loss = 2.0329
2024-10-30 22:59:21: [2024-10-30 22:59:21] iter = 02690, loss = 2.6215
2024-10-30 22:59:25: [2024-10-30 22:59:25] iter = 02700, loss = 1.7401
2024-10-30 22:59:28: [2024-10-30 22:59:28] iter = 02710, loss = 2.2766
2024-10-30 22:59:31: [2024-10-30 22:59:31] iter = 02720, loss = 2.0837
2024-10-30 22:59:35: [2024-10-30 22:59:35] iter = 02730, loss = 1.9439
2024-10-30 22:59:39: [2024-10-30 22:59:39] iter = 02740, loss = 2.2791
2024-10-30 22:59:43: [2024-10-30 22:59:43] iter = 02750, loss = 1.9134
2024-10-30 22:59:46: [2024-10-30 22:59:46] iter = 02760, loss = 2.8011
2024-10-30 22:59:49: [2024-10-30 22:59:49] iter = 02770, loss = 2.0645
2024-10-30 22:59:52: [2024-10-30 22:59:52] iter = 02780, loss = 2.6515
2024-10-30 22:59:55: [2024-10-30 22:59:55] iter = 02790, loss = 2.4384
2024-10-30 22:59:58: [2024-10-30 22:59:58] iter = 02800, loss = 2.2990
2024-10-30 23:00:01: [2024-10-30 23:00:01] iter = 02810, loss = 1.8401
2024-10-30 23:00:05: [2024-10-30 23:00:05] iter = 02820, loss = 1.9508
2024-10-30 23:00:10: [2024-10-30 23:00:10] iter = 02830, loss = 2.8242
2024-10-30 23:00:14: [2024-10-30 23:00:14] iter = 02840, loss = 2.0471
2024-10-30 23:00:18: [2024-10-30 23:00:18] iter = 02850, loss = 2.1530
2024-10-30 23:00:22: [2024-10-30 23:00:22] iter = 02860, loss = 2.5538
2024-10-30 23:00:26: [2024-10-30 23:00:26] iter = 02870, loss = 2.1940
2024-10-30 23:00:30: [2024-10-30 23:00:30] iter = 02880, loss = 2.2968
2024-10-30 23:00:34: [2024-10-30 23:00:34] iter = 02890, loss = 1.9672
2024-10-30 23:00:39: [2024-10-30 23:00:39] iter = 02900, loss = 2.0870
2024-10-30 23:00:42: [2024-10-30 23:00:42] iter = 02910, loss = 1.8744
2024-10-30 23:00:46: [2024-10-30 23:00:46] iter = 02920, loss = 2.2636
2024-10-30 23:00:49: [2024-10-30 23:00:49] iter = 02930, loss = 2.4949
2024-10-30 23:00:53: [2024-10-30 23:00:53] iter = 02940, loss = 2.0670
2024-10-30 23:00:58: [2024-10-30 23:00:58] iter = 02950, loss = 1.9337
2024-10-30 23:01:02: [2024-10-30 23:01:02] iter = 02960, loss = 2.3423
2024-10-30 23:01:06: [2024-10-30 23:01:06] iter = 02970, loss = 1.8132
2024-10-30 23:01:10: [2024-10-30 23:01:10] iter = 02980, loss = 2.3964
2024-10-30 23:01:15: [2024-10-30 23:01:15] iter = 02990, loss = 4.8183
2024-10-30 23:01:18: [2024-10-30 23:01:18] iter = 03000, loss = 2.0601
2024-10-30 23:01:21: [2024-10-30 23:01:21] iter = 03010, loss = 2.4256
2024-10-30 23:01:24: [2024-10-30 23:01:24] iter = 03020, loss = 1.8908
2024-10-30 23:01:28: [2024-10-30 23:01:28] iter = 03030, loss = 2.6802
2024-10-30 23:01:32: [2024-10-30 23:01:32] iter = 03040, loss = 3.4732
2024-10-30 23:01:36: [2024-10-30 23:01:36] iter = 03050, loss = 2.3191
2024-10-30 23:01:40: [2024-10-30 23:01:40] iter = 03060, loss = 2.8221
2024-10-30 23:01:45: [2024-10-30 23:01:45] iter = 03070, loss = 2.7815
2024-10-30 23:01:48: [2024-10-30 23:01:48] iter = 03080, loss = 2.2081
2024-10-30 23:01:52: [2024-10-30 23:01:52] iter = 03090, loss = 3.3673
2024-10-30 23:01:55: [2024-10-30 23:01:55] iter = 03100, loss = 2.7134
2024-10-30 23:01:58: [2024-10-30 23:01:58] iter = 03110, loss = 7.7078
2024-10-30 23:02:02: [2024-10-30 23:02:02] iter = 03120, loss = 2.1923
2024-10-30 23:02:05: [2024-10-30 23:02:05] iter = 03130, loss = 2.4010
2024-10-30 23:02:08: [2024-10-30 23:02:08] iter = 03140, loss = 2.9466
2024-10-30 23:02:12: [2024-10-30 23:02:12] iter = 03150, loss = 3.0814
2024-10-30 23:02:17: [2024-10-30 23:02:17] iter = 03160, loss = 3.5072
2024-10-30 23:02:20: [2024-10-30 23:02:20] iter = 03170, loss = 1.8182
2024-10-30 23:02:25: [2024-10-30 23:02:25] iter = 03180, loss = 2.5322
2024-10-30 23:02:30: [2024-10-30 23:02:30] iter = 03190, loss = 2.1822
2024-10-30 23:02:33: [2024-10-30 23:02:33] iter = 03200, loss = 1.8687
2024-10-30 23:02:37: [2024-10-30 23:02:37] iter = 03210, loss = 5.1605
2024-10-30 23:02:41: [2024-10-30 23:02:41] iter = 03220, loss = 2.1952
2024-10-30 23:02:44: [2024-10-30 23:02:44] iter = 03230, loss = 2.4091
2024-10-30 23:02:49: [2024-10-30 23:02:49] iter = 03240, loss = 1.9123
2024-10-30 23:02:53: [2024-10-30 23:02:53] iter = 03250, loss = 2.1138
2024-10-30 23:02:58: [2024-10-30 23:02:58] iter = 03260, loss = 2.2410
2024-10-30 23:03:01: [2024-10-30 23:03:01] iter = 03270, loss = 2.0260
2024-10-30 23:03:04: [2024-10-30 23:03:04] iter = 03280, loss = 1.9688
2024-10-30 23:03:07: [2024-10-30 23:03:07] iter = 03290, loss = 1.8547
2024-10-30 23:03:12: [2024-10-30 23:03:12] iter = 03300, loss = 2.2138
2024-10-30 23:03:15: [2024-10-30 23:03:15] iter = 03310, loss = 2.2508
2024-10-30 23:03:19: [2024-10-30 23:03:19] iter = 03320, loss = 2.0864
2024-10-30 23:03:24: [2024-10-30 23:03:24] iter = 03330, loss = 2.0470
2024-10-30 23:03:27: [2024-10-30 23:03:27] iter = 03340, loss = 2.7083
2024-10-30 23:03:30: [2024-10-30 23:03:30] iter = 03350, loss = 2.7235
2024-10-30 23:03:34: [2024-10-30 23:03:34] iter = 03360, loss = 2.3217
2024-10-30 23:03:37: [2024-10-30 23:03:37] iter = 03370, loss = 6.7582
2024-10-30 23:03:41: [2024-10-30 23:03:41] iter = 03380, loss = 2.2280
2024-10-30 23:03:45: [2024-10-30 23:03:45] iter = 03390, loss = 4.2786
2024-10-30 23:03:48: [2024-10-30 23:03:48] iter = 03400, loss = 2.1481
2024-10-30 23:03:52: [2024-10-30 23:03:52] iter = 03410, loss = 2.2531
2024-10-30 23:03:55: [2024-10-30 23:03:55] iter = 03420, loss = 1.9824
2024-10-30 23:03:59: [2024-10-30 23:03:59] iter = 03430, loss = 1.9787
2024-10-30 23:04:02: [2024-10-30 23:04:02] iter = 03440, loss = 2.2400
2024-10-30 23:04:05: [2024-10-30 23:04:05] iter = 03450, loss = 2.1403
2024-10-30 23:04:08: [2024-10-30 23:04:08] iter = 03460, loss = 2.5969
2024-10-30 23:04:11: [2024-10-30 23:04:11] iter = 03470, loss = 2.2029
2024-10-30 23:04:14: [2024-10-30 23:04:14] iter = 03480, loss = 2.1211
2024-10-30 23:04:18: [2024-10-30 23:04:18] iter = 03490, loss = 2.5062
2024-10-30 23:04:21: [2024-10-30 23:04:21] iter = 03500, loss = 2.0262
2024-10-30 23:04:25: [2024-10-30 23:04:25] iter = 03510, loss = 6.3107
2024-10-30 23:04:29: [2024-10-30 23:04:29] iter = 03520, loss = 2.0609
2024-10-30 23:04:32: [2024-10-30 23:04:32] iter = 03530, loss = 2.3538
2024-10-30 23:04:36: [2024-10-30 23:04:36] iter = 03540, loss = 2.0750
2024-10-30 23:04:38: [2024-10-30 23:04:38] iter = 03550, loss = 2.2940
2024-10-30 23:04:41: [2024-10-30 23:04:41] iter = 03560, loss = 1.9278
2024-10-30 23:04:44: [2024-10-30 23:04:44] iter = 03570, loss = 2.0172
2024-10-30 23:04:48: [2024-10-30 23:04:48] iter = 03580, loss = 2.2572
2024-10-30 23:04:51: [2024-10-30 23:04:51] iter = 03590, loss = 2.2275
2024-10-30 23:04:56: [2024-10-30 23:04:56] iter = 03600, loss = 2.5786
2024-10-30 23:05:00: [2024-10-30 23:05:00] iter = 03610, loss = 2.9312
2024-10-30 23:05:04: [2024-10-30 23:05:04] iter = 03620, loss = 1.9238
2024-10-30 23:05:07: [2024-10-30 23:05:07] iter = 03630, loss = 2.3564
2024-10-30 23:05:11: [2024-10-30 23:05:11] iter = 03640, loss = 2.2727
2024-10-30 23:05:14: [2024-10-30 23:05:14] iter = 03650, loss = 6.7377
2024-10-30 23:05:18: [2024-10-30 23:05:18] iter = 03660, loss = 4.2718
2024-10-30 23:05:22: [2024-10-30 23:05:22] iter = 03670, loss = 2.1132
2024-10-30 23:05:25: [2024-10-30 23:05:25] iter = 03680, loss = 2.3841
2024-10-30 23:05:29: [2024-10-30 23:05:29] iter = 03690, loss = 3.1364
2024-10-30 23:05:32: [2024-10-30 23:05:32] iter = 03700, loss = 2.2041
2024-10-30 23:05:35: [2024-10-30 23:05:35] iter = 03710, loss = 1.9153
2024-10-30 23:05:39: [2024-10-30 23:05:39] iter = 03720, loss = 2.0278
2024-10-30 23:05:43: [2024-10-30 23:05:43] iter = 03730, loss = 2.3031
2024-10-30 23:05:45: [2024-10-30 23:05:45] iter = 03740, loss = 2.1684
2024-10-30 23:05:49: [2024-10-30 23:05:49] iter = 03750, loss = 2.3522
2024-10-30 23:05:52: [2024-10-30 23:05:52] iter = 03760, loss = 1.9730
2024-10-30 23:05:55: [2024-10-30 23:05:55] iter = 03770, loss = 2.4651
2024-10-30 23:05:59: [2024-10-30 23:05:59] iter = 03780, loss = 2.7571
2024-10-30 23:06:02: [2024-10-30 23:06:02] iter = 03790, loss = 2.6070
2024-10-30 23:06:05: [2024-10-30 23:06:05] iter = 03800, loss = 2.0640
2024-10-30 23:06:08: [2024-10-30 23:06:08] iter = 03810, loss = 2.8302
2024-10-30 23:06:12: [2024-10-30 23:06:12] iter = 03820, loss = 2.5338
2024-10-30 23:06:15: [2024-10-30 23:06:15] iter = 03830, loss = 2.2186
2024-10-30 23:06:18: [2024-10-30 23:06:18] iter = 03840, loss = 2.4129
2024-10-30 23:06:21: [2024-10-30 23:06:21] iter = 03850, loss = 2.0371
2024-10-30 23:06:23: [2024-10-30 23:06:23] iter = 03860, loss = 2.3742
2024-10-30 23:06:26: [2024-10-30 23:06:26] iter = 03870, loss = 1.8649
2024-10-30 23:06:29: [2024-10-30 23:06:29] iter = 03880, loss = 2.4957
2024-10-30 23:06:33: [2024-10-30 23:06:33] iter = 03890, loss = 1.8587
2024-10-30 23:06:36: [2024-10-30 23:06:36] iter = 03900, loss = 2.1338
2024-10-30 23:06:40: [2024-10-30 23:06:40] iter = 03910, loss = 2.2730
2024-10-30 23:06:43: [2024-10-30 23:06:43] iter = 03920, loss = 2.1815
2024-10-30 23:06:46: [2024-10-30 23:06:46] iter = 03930, loss = 1.9147
2024-10-30 23:06:50: [2024-10-30 23:06:50] iter = 03940, loss = 4.9362
2024-10-30 23:06:54: [2024-10-30 23:06:54] iter = 03950, loss = 2.3667
2024-10-30 23:06:57: [2024-10-30 23:06:57] iter = 03960, loss = 1.9659
2024-10-30 23:06:59: [2024-10-30 23:06:59] iter = 03970, loss = 2.4880
2024-10-30 23:07:02: [2024-10-30 23:07:02] iter = 03980, loss = 2.1104
2024-10-30 23:07:05: [2024-10-30 23:07:05] iter = 03990, loss = 2.1771
2024-10-30 23:07:08: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
2024-10-30 23:07:08: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 23:07:08: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 28079}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 23:09:11: Evaluate 5 random ConvNet, ACCmean = 0.7866 ACCstd = 0.0036
-------------------------
2024-10-30 23:09:11: Evaluate 5 random ConvNet, SENmean = 0.7830 SENstd = 0.0039
-------------------------
2024-10-30 23:09:11: Evaluate 5 random ConvNet, SPEmean = 0.9785 SPEstd = 0.0004
-------------------------
2024-10-30 23:09:11: Evaluate 5 random ConvNet, F!mean = 0.7727 F!std = 0.0044
-------------------------
2024-10-30 23:09:11: Evaluate 5 random ConvNet, mean = 0.7866 std = 0.0036
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 23:09:12: [2024-10-30 23:09:12] iter = 04000, loss = 3.0003
2024-10-30 23:09:16: [2024-10-30 23:09:16] iter = 04010, loss = 3.8448
2024-10-30 23:09:19: [2024-10-30 23:09:19] iter = 04020, loss = 3.8460
2024-10-30 23:09:22: [2024-10-30 23:09:22] iter = 04030, loss = 3.8712
2024-10-30 23:09:26: [2024-10-30 23:09:26] iter = 04040, loss = 2.3779
2024-10-30 23:09:29: [2024-10-30 23:09:29] iter = 04050, loss = 2.2524
2024-10-30 23:09:33: [2024-10-30 23:09:33] iter = 04060, loss = 2.2490
2024-10-30 23:09:37: [2024-10-30 23:09:37] iter = 04070, loss = 1.7452
2024-10-30 23:09:41: [2024-10-30 23:09:41] iter = 04080, loss = 3.1534
2024-10-30 23:09:44: [2024-10-30 23:09:44] iter = 04090, loss = 3.5295
2024-10-30 23:09:47: [2024-10-30 23:09:47] iter = 04100, loss = 2.0746
2024-10-30 23:09:49: [2024-10-30 23:09:49] iter = 04110, loss = 2.1354
2024-10-30 23:09:53: [2024-10-30 23:09:53] iter = 04120, loss = 1.9100
2024-10-30 23:09:57: [2024-10-30 23:09:57] iter = 04130, loss = 2.5660
2024-10-30 23:10:00: [2024-10-30 23:10:00] iter = 04140, loss = 3.7844
2024-10-30 23:10:04: [2024-10-30 23:10:04] iter = 04150, loss = 2.4469
2024-10-30 23:10:07: [2024-10-30 23:10:07] iter = 04160, loss = 2.2052
2024-10-30 23:10:11: [2024-10-30 23:10:11] iter = 04170, loss = 1.8343
2024-10-30 23:10:14: [2024-10-30 23:10:14] iter = 04180, loss = 2.2375
2024-10-30 23:10:17: [2024-10-30 23:10:17] iter = 04190, loss = 2.2408
2024-10-30 23:10:21: [2024-10-30 23:10:21] iter = 04200, loss = 2.3586
2024-10-30 23:10:25: [2024-10-30 23:10:25] iter = 04210, loss = 4.0555
2024-10-30 23:10:29: [2024-10-30 23:10:29] iter = 04220, loss = 2.0875
2024-10-30 23:10:32: [2024-10-30 23:10:32] iter = 04230, loss = 2.0742
2024-10-30 23:10:35: [2024-10-30 23:10:35] iter = 04240, loss = 2.1688
2024-10-30 23:10:39: [2024-10-30 23:10:39] iter = 04250, loss = 2.8542
2024-10-30 23:10:42: [2024-10-30 23:10:42] iter = 04260, loss = 1.8899
2024-10-30 23:10:46: [2024-10-30 23:10:46] iter = 04270, loss = 1.7530
2024-10-30 23:10:50: [2024-10-30 23:10:50] iter = 04280, loss = 2.1325
2024-10-30 23:10:54: [2024-10-30 23:10:54] iter = 04290, loss = 2.2088
2024-10-30 23:10:57: [2024-10-30 23:10:57] iter = 04300, loss = 4.4686
2024-10-30 23:11:01: [2024-10-30 23:11:01] iter = 04310, loss = 2.1782
2024-10-30 23:11:04: [2024-10-30 23:11:04] iter = 04320, loss = 3.3734
2024-10-30 23:11:08: [2024-10-30 23:11:08] iter = 04330, loss = 2.9657
2024-10-30 23:11:11: [2024-10-30 23:11:11] iter = 04340, loss = 1.9586
2024-10-30 23:11:15: [2024-10-30 23:11:15] iter = 04350, loss = 2.1371
2024-10-30 23:11:18: [2024-10-30 23:11:18] iter = 04360, loss = 3.1374
2024-10-30 23:11:21: [2024-10-30 23:11:21] iter = 04370, loss = 2.1053
2024-10-30 23:11:24: [2024-10-30 23:11:24] iter = 04380, loss = 2.6538
2024-10-30 23:11:28: [2024-10-30 23:11:28] iter = 04390, loss = 2.0212
2024-10-30 23:11:31: [2024-10-30 23:11:31] iter = 04400, loss = 2.6260
2024-10-30 23:11:34: [2024-10-30 23:11:34] iter = 04410, loss = 2.8739
2024-10-30 23:11:38: [2024-10-30 23:11:38] iter = 04420, loss = 2.1182
2024-10-30 23:11:42: [2024-10-30 23:11:42] iter = 04430, loss = 2.5386
2024-10-30 23:11:45: [2024-10-30 23:11:45] iter = 04440, loss = 3.8556
2024-10-30 23:11:49: [2024-10-30 23:11:49] iter = 04450, loss = 2.3246
2024-10-30 23:11:52: [2024-10-30 23:11:52] iter = 04460, loss = 2.4557
2024-10-30 23:11:54: [2024-10-30 23:11:54] iter = 04470, loss = 1.8112
2024-10-30 23:11:57: [2024-10-30 23:11:57] iter = 04480, loss = 1.9625
2024-10-30 23:12:00: [2024-10-30 23:12:00] iter = 04490, loss = 2.3667
2024-10-30 23:12:03: [2024-10-30 23:12:03] iter = 04500, loss = 2.6266
2024-10-30 23:12:05: [2024-10-30 23:12:05] iter = 04510, loss = 2.1575
2024-10-30 23:12:09: [2024-10-30 23:12:09] iter = 04520, loss = 2.0181
2024-10-30 23:12:11: [2024-10-30 23:12:11] iter = 04530, loss = 2.0703
2024-10-30 23:12:13: [2024-10-30 23:12:13] iter = 04540, loss = 2.3772
2024-10-30 23:12:16: [2024-10-30 23:12:16] iter = 04550, loss = 2.5004
2024-10-30 23:12:19: [2024-10-30 23:12:19] iter = 04560, loss = 2.0023
2024-10-30 23:12:22: [2024-10-30 23:12:22] iter = 04570, loss = 2.3671
2024-10-30 23:12:26: [2024-10-30 23:12:26] iter = 04580, loss = 4.4388
2024-10-30 23:12:29: [2024-10-30 23:12:29] iter = 04590, loss = 2.4062
2024-10-30 23:12:32: [2024-10-30 23:12:32] iter = 04600, loss = 2.4165
2024-10-30 23:12:35: [2024-10-30 23:12:35] iter = 04610, loss = 1.8990
2024-10-30 23:12:38: [2024-10-30 23:12:38] iter = 04620, loss = 2.1498
2024-10-30 23:12:41: [2024-10-30 23:12:41] iter = 04630, loss = 2.2387
2024-10-30 23:12:44: [2024-10-30 23:12:44] iter = 04640, loss = 2.2928
2024-10-30 23:12:47: [2024-10-30 23:12:47] iter = 04650, loss = 2.2968
2024-10-30 23:12:50: [2024-10-30 23:12:50] iter = 04660, loss = 7.3119
2024-10-30 23:12:54: [2024-10-30 23:12:54] iter = 04670, loss = 2.1890
2024-10-30 23:12:57: [2024-10-30 23:12:57] iter = 04680, loss = 2.6626
2024-10-30 23:13:01: [2024-10-30 23:13:01] iter = 04690, loss = 2.1775
2024-10-30 23:13:04: [2024-10-30 23:13:04] iter = 04700, loss = 2.0925
2024-10-30 23:13:07: [2024-10-30 23:13:07] iter = 04710, loss = 2.0810
2024-10-30 23:13:11: [2024-10-30 23:13:11] iter = 04720, loss = 3.0167
2024-10-30 23:13:13: [2024-10-30 23:13:13] iter = 04730, loss = 1.8365
2024-10-30 23:13:16: [2024-10-30 23:13:16] iter = 04740, loss = 1.9985
2024-10-30 23:13:19: [2024-10-30 23:13:19] iter = 04750, loss = 2.1899
2024-10-30 23:13:22: [2024-10-30 23:13:22] iter = 04760, loss = 3.5192
2024-10-30 23:13:25: [2024-10-30 23:13:25] iter = 04770, loss = 2.0298
2024-10-30 23:13:29: [2024-10-30 23:13:29] iter = 04780, loss = 2.5672
2024-10-30 23:13:32: [2024-10-30 23:13:32] iter = 04790, loss = 10.1119
2024-10-30 23:13:35: [2024-10-30 23:13:35] iter = 04800, loss = 2.0507
2024-10-30 23:13:38: [2024-10-30 23:13:38] iter = 04810, loss = 1.9982
2024-10-30 23:13:41: [2024-10-30 23:13:41] iter = 04820, loss = 2.1032
2024-10-30 23:13:44: [2024-10-30 23:13:44] iter = 04830, loss = 2.3793
2024-10-30 23:13:47: [2024-10-30 23:13:47] iter = 04840, loss = 2.0250
2024-10-30 23:13:50: [2024-10-30 23:13:50] iter = 04850, loss = 2.3670
2024-10-30 23:13:54: [2024-10-30 23:13:54] iter = 04860, loss = 3.3790
2024-10-30 23:13:57: [2024-10-30 23:13:57] iter = 04870, loss = 3.2526
2024-10-30 23:14:00: [2024-10-30 23:14:00] iter = 04880, loss = 3.0655
2024-10-30 23:14:03: [2024-10-30 23:14:03] iter = 04890, loss = 3.0610
2024-10-30 23:14:06: [2024-10-30 23:14:06] iter = 04900, loss = 2.1142
2024-10-30 23:14:11: [2024-10-30 23:14:11] iter = 04910, loss = 2.7706
2024-10-30 23:14:14: [2024-10-30 23:14:14] iter = 04920, loss = 2.1547
2024-10-30 23:14:18: [2024-10-30 23:14:18] iter = 04930, loss = 5.0929
2024-10-30 23:14:21: [2024-10-30 23:14:21] iter = 04940, loss = 2.2694
2024-10-30 23:14:25: [2024-10-30 23:14:25] iter = 04950, loss = 2.1665
2024-10-30 23:14:28: [2024-10-30 23:14:28] iter = 04960, loss = 1.8980
2024-10-30 23:14:32: [2024-10-30 23:14:32] iter = 04970, loss = 2.3868
2024-10-30 23:14:36: [2024-10-30 23:14:36] iter = 04980, loss = 4.8416
2024-10-30 23:14:39: [2024-10-30 23:14:39] iter = 04990, loss = 2.2947
2024-10-30 23:14:42: [2024-10-30 23:14:42] iter = 05000, loss = 1.7367
2024-10-30 23:14:45: [2024-10-30 23:14:45] iter = 05010, loss = 2.3933
2024-10-30 23:14:49: [2024-10-30 23:14:49] iter = 05020, loss = 2.5614
2024-10-30 23:14:51: [2024-10-30 23:14:51] iter = 05030, loss = 2.2575
2024-10-30 23:14:54: [2024-10-30 23:14:54] iter = 05040, loss = 2.1028
2024-10-30 23:14:57: [2024-10-30 23:14:57] iter = 05050, loss = 1.9484
2024-10-30 23:15:02: [2024-10-30 23:15:02] iter = 05060, loss = 2.2670
2024-10-30 23:15:06: [2024-10-30 23:15:06] iter = 05070, loss = 2.3349
2024-10-30 23:15:10: [2024-10-30 23:15:10] iter = 05080, loss = 1.9066
2024-10-30 23:15:14: [2024-10-30 23:15:14] iter = 05090, loss = 2.2159
2024-10-30 23:15:18: [2024-10-30 23:15:18] iter = 05100, loss = 1.7863
2024-10-30 23:15:21: [2024-10-30 23:15:21] iter = 05110, loss = 1.9135
2024-10-30 23:15:25: [2024-10-30 23:15:25] iter = 05120, loss = 4.0298
2024-10-30 23:15:28: [2024-10-30 23:15:28] iter = 05130, loss = 2.0884
2024-10-30 23:15:32: [2024-10-30 23:15:32] iter = 05140, loss = 2.2954
2024-10-30 23:15:35: [2024-10-30 23:15:35] iter = 05150, loss = 2.3598
2024-10-30 23:15:38: [2024-10-30 23:15:38] iter = 05160, loss = 2.1620
2024-10-30 23:15:42: [2024-10-30 23:15:42] iter = 05170, loss = 2.1586
2024-10-30 23:15:46: [2024-10-30 23:15:46] iter = 05180, loss = 2.0610
2024-10-30 23:15:50: [2024-10-30 23:15:50] iter = 05190, loss = 1.8926
2024-10-30 23:15:53: [2024-10-30 23:15:53] iter = 05200, loss = 2.1867
2024-10-30 23:15:56: [2024-10-30 23:15:56] iter = 05210, loss = 3.2663
2024-10-30 23:16:00: [2024-10-30 23:16:00] iter = 05220, loss = 1.9977
2024-10-30 23:16:03: [2024-10-30 23:16:03] iter = 05230, loss = 2.3743
2024-10-30 23:16:07: [2024-10-30 23:16:07] iter = 05240, loss = 2.4159
2024-10-30 23:16:10: [2024-10-30 23:16:10] iter = 05250, loss = 2.1413
2024-10-30 23:16:15: [2024-10-30 23:16:15] iter = 05260, loss = 1.9721
2024-10-30 23:16:18: [2024-10-30 23:16:18] iter = 05270, loss = 2.3526
2024-10-30 23:16:21: [2024-10-30 23:16:21] iter = 05280, loss = 1.7946
2024-10-30 23:16:24: [2024-10-30 23:16:24] iter = 05290, loss = 1.8163
2024-10-30 23:16:27: [2024-10-30 23:16:27] iter = 05300, loss = 2.0022
2024-10-30 23:16:30: [2024-10-30 23:16:30] iter = 05310, loss = 1.8406
2024-10-30 23:16:34: [2024-10-30 23:16:34] iter = 05320, loss = 2.3353
2024-10-30 23:16:37: [2024-10-30 23:16:37] iter = 05330, loss = 1.8414
2024-10-30 23:16:41: [2024-10-30 23:16:41] iter = 05340, loss = 1.9459
2024-10-30 23:16:44: [2024-10-30 23:16:44] iter = 05350, loss = 2.3289
2024-10-30 23:16:48: [2024-10-30 23:16:48] iter = 05360, loss = 2.0184
2024-10-30 23:16:50: [2024-10-30 23:16:50] iter = 05370, loss = 2.2551
2024-10-30 23:16:54: [2024-10-30 23:16:54] iter = 05380, loss = 2.3675
2024-10-30 23:16:57: [2024-10-30 23:16:57] iter = 05390, loss = 2.0908
2024-10-30 23:17:01: [2024-10-30 23:17:01] iter = 05400, loss = 2.1237
2024-10-30 23:17:04: [2024-10-30 23:17:04] iter = 05410, loss = 3.9236
2024-10-30 23:17:07: [2024-10-30 23:17:07] iter = 05420, loss = 1.6920
2024-10-30 23:17:11: [2024-10-30 23:17:11] iter = 05430, loss = 2.0949
2024-10-30 23:17:15: [2024-10-30 23:17:15] iter = 05440, loss = 2.6180
2024-10-30 23:17:18: [2024-10-30 23:17:18] iter = 05450, loss = 2.4125
2024-10-30 23:17:21: [2024-10-30 23:17:21] iter = 05460, loss = 2.1454
2024-10-30 23:17:25: [2024-10-30 23:17:25] iter = 05470, loss = 1.9357
2024-10-30 23:17:28: [2024-10-30 23:17:28] iter = 05480, loss = 1.6858
2024-10-30 23:17:32: [2024-10-30 23:17:32] iter = 05490, loss = 2.3901
2024-10-30 23:17:36: [2024-10-30 23:17:36] iter = 05500, loss = 2.1576
2024-10-30 23:17:39: [2024-10-30 23:17:39] iter = 05510, loss = 2.5066
2024-10-30 23:17:43: [2024-10-30 23:17:43] iter = 05520, loss = 2.0535
2024-10-30 23:17:46: [2024-10-30 23:17:46] iter = 05530, loss = 2.3043
2024-10-30 23:17:49: [2024-10-30 23:17:49] iter = 05540, loss = 1.9542
2024-10-30 23:17:53: [2024-10-30 23:17:53] iter = 05550, loss = 1.9171
2024-10-30 23:17:57: [2024-10-30 23:17:57] iter = 05560, loss = 2.5581
2024-10-30 23:18:00: [2024-10-30 23:18:00] iter = 05570, loss = 2.3926
2024-10-30 23:18:03: [2024-10-30 23:18:03] iter = 05580, loss = 2.2539
2024-10-30 23:18:07: [2024-10-30 23:18:07] iter = 05590, loss = 1.9074
2024-10-30 23:18:11: [2024-10-30 23:18:11] iter = 05600, loss = 2.1047
2024-10-30 23:18:14: [2024-10-30 23:18:14] iter = 05610, loss = 2.1002
2024-10-30 23:18:18: [2024-10-30 23:18:18] iter = 05620, loss = 2.1758
2024-10-30 23:18:22: [2024-10-30 23:18:22] iter = 05630, loss = 2.2617
2024-10-30 23:18:25: [2024-10-30 23:18:25] iter = 05640, loss = 2.0896
2024-10-30 23:18:29: [2024-10-30 23:18:29] iter = 05650, loss = 2.6282
2024-10-30 23:18:33: [2024-10-30 23:18:33] iter = 05660, loss = 2.1254
2024-10-30 23:18:36: [2024-10-30 23:18:36] iter = 05670, loss = 2.0224
2024-10-30 23:18:40: [2024-10-30 23:18:40] iter = 05680, loss = 2.4626
2024-10-30 23:18:42: [2024-10-30 23:18:42] iter = 05690, loss = 2.3661
2024-10-30 23:18:45: [2024-10-30 23:18:45] iter = 05700, loss = 1.9794
2024-10-30 23:18:47: [2024-10-30 23:18:47] iter = 05710, loss = 3.2848
2024-10-30 23:18:51: [2024-10-30 23:18:51] iter = 05720, loss = 1.9920
2024-10-30 23:18:54: [2024-10-30 23:18:54] iter = 05730, loss = 6.7192
2024-10-30 23:18:58: [2024-10-30 23:18:58] iter = 05740, loss = 2.3432
2024-10-30 23:19:02: [2024-10-30 23:19:02] iter = 05750, loss = 2.4738
2024-10-30 23:19:05: [2024-10-30 23:19:05] iter = 05760, loss = 2.7589
2024-10-30 23:19:08: [2024-10-30 23:19:08] iter = 05770, loss = 2.1811
2024-10-30 23:19:11: [2024-10-30 23:19:11] iter = 05780, loss = 2.7105
2024-10-30 23:19:15: [2024-10-30 23:19:15] iter = 05790, loss = 1.9954
2024-10-30 23:19:18: [2024-10-30 23:19:18] iter = 05800, loss = 3.1886
2024-10-30 23:19:22: [2024-10-30 23:19:22] iter = 05810, loss = 2.2938
2024-10-30 23:19:25: [2024-10-30 23:19:25] iter = 05820, loss = 3.0675
2024-10-30 23:19:28: [2024-10-30 23:19:28] iter = 05830, loss = 2.1397
2024-10-30 23:19:31: [2024-10-30 23:19:31] iter = 05840, loss = 2.4549
2024-10-30 23:19:34: [2024-10-30 23:19:34] iter = 05850, loss = 2.6702
2024-10-30 23:19:37: [2024-10-30 23:19:37] iter = 05860, loss = 1.9895
2024-10-30 23:19:40: [2024-10-30 23:19:40] iter = 05870, loss = 2.6774
2024-10-30 23:19:43: [2024-10-30 23:19:43] iter = 05880, loss = 1.9963
2024-10-30 23:19:47: [2024-10-30 23:19:47] iter = 05890, loss = 2.5631
2024-10-30 23:19:50: [2024-10-30 23:19:50] iter = 05900, loss = 1.9814
2024-10-30 23:19:53: [2024-10-30 23:19:53] iter = 05910, loss = 2.6760
2024-10-30 23:19:56: [2024-10-30 23:19:56] iter = 05920, loss = 1.8947
2024-10-30 23:19:59: [2024-10-30 23:19:59] iter = 05930, loss = 2.2149
2024-10-30 23:20:02: [2024-10-30 23:20:02] iter = 05940, loss = 1.9958
2024-10-30 23:20:05: [2024-10-30 23:20:05] iter = 05950, loss = 2.6692
2024-10-30 23:20:08: [2024-10-30 23:20:08] iter = 05960, loss = 2.2331
2024-10-30 23:20:11: [2024-10-30 23:20:11] iter = 05970, loss = 2.6033
2024-10-30 23:20:15: [2024-10-30 23:20:15] iter = 05980, loss = 2.2574
2024-10-30 23:20:19: [2024-10-30 23:20:19] iter = 05990, loss = 2.2464
2024-10-30 23:20:22: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
2024-10-30 23:20:22: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 23:20:22: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 22465}

[2024-10-30 21:24:36] Evaluate_03: epoch = 1000 train time = 29 s train loss = 0.005038 train acc = 1.0000, test acc = 0.7654, test_sen =0.7653, test_spe =0.9765, test_f1 =0.7552
[2024-10-30 21:25:04] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.005215 train acc = 1.0000, test acc = 0.7647, test_sen =0.7635, test_spe =0.9763, test_f1 =0.7574
[2024-10-30 21:38:05] Evaluate_00: epoch = 1000 train time = 27 s train loss = 0.047517 train acc = 1.0000, test acc = 0.7805, test_sen =0.7742, test_spe =0.9779, test_f1 =0.7669
[2024-10-30 21:38:31] Evaluate_01: epoch = 1000 train time = 23 s train loss = 0.006376 train acc = 1.0000, test acc = 0.7766, test_sen =0.7711, test_spe =0.9777, test_f1 =0.7604
[2024-10-30 21:38:59] Evaluate_02: epoch = 1000 train time = 27 s train loss = 0.021597 train acc = 1.0000, test acc = 0.7783, test_sen =0.7691, test_spe =0.9777, test_f1 =0.7635
[2024-10-30 21:39:28] Evaluate_03: epoch = 1000 train time = 25 s train loss = 0.004362 train acc = 1.0000, test acc = 0.7888, test_sen =0.7798, test_spe =0.9788, test_f1 =0.7728
[2024-10-30 21:39:55] Evaluate_04: epoch = 1000 train time = 24 s train loss = 0.007243 train acc = 1.0000, test acc = 0.7752, test_sen =0.7684, test_spe =0.9775, test_f1 =0.7602
[2024-10-30 21:52:58] Evaluate_00: epoch = 1000 train time = 24 s train loss = 0.025455 train acc = 1.0000, test acc = 0.7832, test_sen =0.7765, test_spe =0.9781, test_f1 =0.7690
[2024-10-30 21:53:26] Evaluate_01: epoch = 1000 train time = 25 s train loss = 0.005086 train acc = 1.0000, test acc = 0.7815, test_sen =0.7723, test_spe =0.9779, test_f1 =0.7675
[2024-10-30 21:53:57] Evaluate_02: epoch = 1000 train time = 30 s train loss = 0.028509 train acc = 1.0000, test acc = 0.7851, test_sen =0.7782, test_spe =0.9784, test_f1 =0.7680
[2024-10-30 21:54:27] Evaluate_03: epoch = 1000 train time = 27 s train loss = 0.002733 train acc = 1.0000, test acc = 0.7822, test_sen =0.7751, test_spe =0.9781, test_f1 =0.7673
[2024-10-30 21:54:59] Evaluate_04: epoch = 1000 train time = 29 s train loss = 0.007287 train acc = 1.0000, test acc = 0.7771, test_sen =0.7715, test_spe =0.9775, test_f1 =0.7642
[2024-10-30 22:08:56] Evaluate_00: epoch = 1000 train time = 26 s train loss = 0.007177 train acc = 1.0000, test acc = 0.7733, test_sen =0.7682, test_spe =0.9773, test_f1 =0.7567
[2024-10-30 22:09:25] Evaluate_01: epoch = 1000 train time = 26 s train loss = 0.003504 train acc = 1.0000, test acc = 0.7770, test_sen =0.7723, test_spe =0.9777, test_f1 =0.7620
[2024-10-30 22:09:53] Evaluate_02: epoch = 1000 train time = 25 s train loss = 0.005750 train acc = 1.0000, test acc = 0.7819, test_sen =0.7774, test_spe =0.9782, test_f1 =0.7661
[2024-10-30 22:10:23] Evaluate_03: epoch = 1000 train time = 27 s train loss = 0.023488 train acc = 1.0000, test acc = 0.7622, test_sen =0.7580, test_spe =0.9762, test_f1 =0.7478
[2024-10-30 22:10:53] Evaluate_04: epoch = 1000 train time = 27 s train loss = 0.004283 train acc = 1.0000, test acc = 0.7653, test_sen =0.7594, test_spe =0.9765, test_f1 =0.7487
[2024-10-30 22:22:54] Evaluate_00: epoch = 1000 train time = 26 s train loss = 0.008466 train acc = 1.0000, test acc = 0.7985, test_sen =0.7950, test_spe =0.9797, test_f1 =0.7855
[2024-10-30 22:23:23] Evaluate_01: epoch = 1000 train time = 27 s train loss = 0.028855 train acc = 1.0000, test acc = 0.7767, test_sen =0.7757, test_spe =0.9775, test_f1 =0.7675
[2024-10-30 22:23:53] Evaluate_02: epoch = 1000 train time = 27 s train loss = 0.002227 train acc = 1.0000, test acc = 0.7890, test_sen =0.7874, test_spe =0.9788, test_f1 =0.7772
[2024-10-30 22:24:24] Evaluate_03: epoch = 1000 train time = 28 s train loss = 0.032059 train acc = 1.0000, test acc = 0.7857, test_sen =0.7806, test_spe =0.9785, test_f1 =0.7719
[2024-10-30 22:24:52] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.004088 train acc = 1.0000, test acc = 0.7779, test_sen =0.7759, test_spe =0.9777, test_f1 =0.7661
[2024-10-30 22:37:07] Evaluate_00: epoch = 1000 train time = 23 s train loss = 0.026019 train acc = 1.0000, test acc = 0.7820, test_sen =0.7758, test_spe =0.9782, test_f1 =0.7669
[2024-10-30 22:37:33] Evaluate_01: epoch = 1000 train time = 24 s train loss = 0.008093 train acc = 1.0000, test acc = 0.7803, test_sen =0.7762, test_spe =0.9780, test_f1 =0.7650
[2024-10-30 22:38:01] Evaluate_02: epoch = 1000 train time = 25 s train loss = 0.005228 train acc = 1.0000, test acc = 0.7766, test_sen =0.7712, test_spe =0.9776, test_f1 =0.7617
[2024-10-30 22:38:29] Evaluate_03: epoch = 1000 train time = 26 s train loss = 0.033712 train acc = 1.0000, test acc = 0.7690, test_sen =0.7665, test_spe =0.9769, test_f1 =0.7528
[2024-10-30 22:38:59] Evaluate_04: epoch = 1000 train time = 27 s train loss = 0.011960 train acc = 1.0000, test acc = 0.7710, test_sen =0.7662, test_spe =0.9771, test_f1 =0.7560
[2024-10-30 22:39:28] Evaluate_00: epoch = 1000 train time = 24 s train loss = 0.001916 train acc = 1.0000, test acc = 0.6802, test_sen =0.6731, test_spe =0.9678, test_f1 =0.6649
[2024-10-30 22:39:54] Evaluate_01: epoch = 1000 train time = 23 s train loss = 0.002418 train acc = 1.0000, test acc = 0.6741, test_sen =0.6697, test_spe =0.9672, test_f1 =0.6650
[2024-10-30 22:40:20] Evaluate_02: epoch = 1000 train time = 24 s train loss = 0.002017 train acc = 1.0000, test acc = 0.6698, test_sen =0.6663, test_spe =0.9667, test_f1 =0.6585
[2024-10-30 22:40:50] Evaluate_03: epoch = 1000 train time = 28 s train loss = 0.020971 train acc = 1.0000, test acc = 0.6799, test_sen =0.6702, test_spe =0.9678, test_f1 =0.6653
[2024-10-30 22:41:18] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.028537 train acc = 0.9909, test acc = 0.6721, test_sen =0.6687, test_spe =0.9670, test_f1 =0.6601
[2024-10-30 22:53:25] Evaluate_00: epoch = 1000 train time = 25 s train loss = 0.017422 train acc = 1.0000, test acc = 0.7969, test_sen =0.7903, test_spe =0.9796, test_f1 =0.7803
[2024-10-30 22:53:52] Evaluate_01: epoch = 1000 train time = 25 s train loss = 0.052034 train acc = 0.9818, test acc = 0.7779, test_sen =0.7760, test_spe =0.9778, test_f1 =0.7616
[2024-10-30 22:54:21] Evaluate_02: epoch = 1000 train time = 26 s train loss = 0.008215 train acc = 1.0000, test acc = 0.7860, test_sen =0.7814, test_spe =0.9785, test_f1 =0.7704
[2024-10-30 22:54:46] Evaluate_03: epoch = 1000 train time = 23 s train loss = 0.015792 train acc = 1.0000, test acc = 0.7828, test_sen =0.7778, test_spe =0.9783, test_f1 =0.7677
[2024-10-30 22:55:15] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.002872 train acc = 1.0000, test acc = 0.7895, test_sen =0.7861, test_spe =0.9789, test_f1 =0.7740
[2024-10-30 23:07:30] Evaluate_00: epoch = 1000 train time = 21 s train loss = 0.011254 train acc = 1.0000, test acc = 0.7802, test_sen =0.7758, test_spe =0.9779, test_f1 =0.7644
[2024-10-30 23:07:52] Evaluate_01: epoch = 1000 train time = 20 s train loss = 0.009239 train acc = 1.0000, test acc = 0.7870, test_sen =0.7828, test_spe =0.9785, test_f1 =0.7748
[2024-10-30 23:08:18] Evaluate_02: epoch = 1000 train time = 23 s train loss = 0.003918 train acc = 1.0000, test acc = 0.7901, test_sen =0.7860, test_spe =0.9789, test_f1 =0.7761
[2024-10-30 23:08:43] Evaluate_03: epoch = 1000 train time = 23 s train loss = 0.004059 train acc = 1.0000, test acc = 0.7859, test_sen =0.7838, test_spe =0.9785, test_f1 =0.7719
[2024-10-30 23:09:11] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.007392 train acc = 1.0000, test acc = 0.7899, test_sen =0.7867, test_spe =0.9788, test_f1 =0.7762
[2024-10-30 23:20:51] Evaluate_00: epoch = 1000 train time = 26 s train loss = 0.009848 train acc = 1.0000, test acc = 0.7830, test_sen =0.7815, test_spe =0.9783, test_f1 =0.7701
[2024-10-30 23:21:16] Evaluate_01: epoch = 1000 train time = 23 s train loss = 0.004450 train acc = 1.0000, test acc = 0.7757, test_sen =0.7711, test_spe =0.9776, test_f1 =0.7587
[2024-10-30 23:21:43] Evaluate_02: epoch = 1000 train time = 25 s train loss = 0.003083 train acc = 1.0000, test acc = 0.7865, test_sen =0.7860, test_spe =0.9786, test_f1 =0.7736
[2024-10-30 23:22:12] Evaluate_03: epoch = 1000 train time = 26 s train loss = 0.003230 train acc = 1.0000, test acc = 0.7890, test_sen =0.7832, test_spe =0.9789, test_f1 =0.7739/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 23:22:39: Evaluate 5 random ConvNet, ACCmean = 0.7853 ACCstd = 0.0056
-------------------------
2024-10-30 23:22:39: Evaluate 5 random ConvNet, SENmean = 0.7826 SENstd = 0.0066
-------------------------
2024-10-30 23:22:39: Evaluate 5 random ConvNet, SPEmean = 0.9785 SPEstd = 0.0006
-------------------------
2024-10-30 23:22:39: Evaluate 5 random ConvNet, F!mean = 0.7710 F!std = 0.0067
-------------------------
2024-10-30 23:22:39: Evaluate 5 random ConvNet, mean = 0.7853 std = 0.0056
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 23:22:39: [2024-10-30 23:22:39] iter = 06000, loss = 1.9294
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 23:22:43: [2024-10-30 23:22:43] iter = 06010, loss = 1.8872
2024-10-30 23:22:46: [2024-10-30 23:22:46] iter = 06020, loss = 2.2931
2024-10-30 23:22:48: [2024-10-30 23:22:48] iter = 06030, loss = 2.5060
2024-10-30 23:22:52: [2024-10-30 23:22:52] iter = 06040, loss = 2.0638
2024-10-30 23:22:55: [2024-10-30 23:22:55] iter = 06050, loss = 2.4414
2024-10-30 23:22:59: [2024-10-30 23:22:59] iter = 06060, loss = 2.3491
2024-10-30 23:23:02: [2024-10-30 23:23:02] iter = 06070, loss = 1.9523
2024-10-30 23:23:06: [2024-10-30 23:23:06] iter = 06080, loss = 2.2505
2024-10-30 23:23:10: [2024-10-30 23:23:10] iter = 06090, loss = 2.4326
2024-10-30 23:23:15: [2024-10-30 23:23:15] iter = 06100, loss = 2.5173
2024-10-30 23:23:18: [2024-10-30 23:23:18] iter = 06110, loss = 2.9741
2024-10-30 23:23:23: [2024-10-30 23:23:23] iter = 06120, loss = 2.5028
2024-10-30 23:23:26: [2024-10-30 23:23:26] iter = 06130, loss = 2.0275
2024-10-30 23:23:30: [2024-10-30 23:23:30] iter = 06140, loss = 2.1245
2024-10-30 23:23:32: [2024-10-30 23:23:32] iter = 06150, loss = 1.8522
2024-10-30 23:23:34: [2024-10-30 23:23:34] iter = 06160, loss = 3.0383
2024-10-30 23:23:37: [2024-10-30 23:23:37] iter = 06170, loss = 2.3207
2024-10-30 23:23:40: [2024-10-30 23:23:40] iter = 06180, loss = 2.3088
2024-10-30 23:23:43: [2024-10-30 23:23:43] iter = 06190, loss = 3.5986
2024-10-30 23:23:47: [2024-10-30 23:23:47] iter = 06200, loss = 2.1118
2024-10-30 23:23:50: [2024-10-30 23:23:50] iter = 06210, loss = 2.3151
2024-10-30 23:23:54: [2024-10-30 23:23:54] iter = 06220, loss = 1.8669
2024-10-30 23:23:58: [2024-10-30 23:23:58] iter = 06230, loss = 2.3235
2024-10-30 23:24:02: [2024-10-30 23:24:02] iter = 06240, loss = 2.4067
2024-10-30 23:24:05: [2024-10-30 23:24:05] iter = 06250, loss = 2.3064
2024-10-30 23:24:09: [2024-10-30 23:24:09] iter = 06260, loss = 1.9647
2024-10-30 23:24:13: [2024-10-30 23:24:13] iter = 06270, loss = 2.0492
2024-10-30 23:24:17: [2024-10-30 23:24:17] iter = 06280, loss = 1.7944
2024-10-30 23:24:21: [2024-10-30 23:24:21] iter = 06290, loss = 2.1388
2024-10-30 23:24:25: [2024-10-30 23:24:25] iter = 06300, loss = 1.9519
2024-10-30 23:24:28: [2024-10-30 23:24:28] iter = 06310, loss = 2.1048
2024-10-30 23:24:31: [2024-10-30 23:24:31] iter = 06320, loss = 2.1362
2024-10-30 23:24:35: [2024-10-30 23:24:35] iter = 06330, loss = 2.5202
2024-10-30 23:24:38: [2024-10-30 23:24:38] iter = 06340, loss = 2.3271
2024-10-30 23:24:40: [2024-10-30 23:24:40] iter = 06350, loss = 2.7193
2024-10-30 23:24:43: [2024-10-30 23:24:43] iter = 06360, loss = 2.1820
2024-10-30 23:24:47: [2024-10-30 23:24:47] iter = 06370, loss = 2.0765
2024-10-30 23:24:51: [2024-10-30 23:24:51] iter = 06380, loss = 2.3123
2024-10-30 23:24:55: [2024-10-30 23:24:55] iter = 06390, loss = 1.9688
2024-10-30 23:24:58: [2024-10-30 23:24:58] iter = 06400, loss = 2.2302
2024-10-30 23:25:01: [2024-10-30 23:25:01] iter = 06410, loss = 2.2951
2024-10-30 23:25:05: [2024-10-30 23:25:05] iter = 06420, loss = 2.1200
2024-10-30 23:25:09: [2024-10-30 23:25:09] iter = 06430, loss = 2.2132
2024-10-30 23:25:13: [2024-10-30 23:25:13] iter = 06440, loss = 2.8563
2024-10-30 23:25:16: [2024-10-30 23:25:16] iter = 06450, loss = 1.7112
2024-10-30 23:25:20: [2024-10-30 23:25:20] iter = 06460, loss = 1.9093
2024-10-30 23:25:24: [2024-10-30 23:25:24] iter = 06470, loss = 2.3239
2024-10-30 23:25:26: [2024-10-30 23:25:26] iter = 06480, loss = 2.0558
2024-10-30 23:25:30: [2024-10-30 23:25:30] iter = 06490, loss = 1.9889
2024-10-30 23:25:34: [2024-10-30 23:25:34] iter = 06500, loss = 1.7361
2024-10-30 23:25:37: [2024-10-30 23:25:37] iter = 06510, loss = 2.2638
2024-10-30 23:25:41: [2024-10-30 23:25:41] iter = 06520, loss = 2.2836
2024-10-30 23:25:43: [2024-10-30 23:25:43] iter = 06530, loss = 1.9194
2024-10-30 23:25:47: [2024-10-30 23:25:47] iter = 06540, loss = 2.2974
2024-10-30 23:25:51: [2024-10-30 23:25:51] iter = 06550, loss = 2.2230
2024-10-30 23:25:54: [2024-10-30 23:25:54] iter = 06560, loss = 1.9161
2024-10-30 23:25:57: [2024-10-30 23:25:57] iter = 06570, loss = 2.3152
2024-10-30 23:26:01: [2024-10-30 23:26:01] iter = 06580, loss = 4.9827
2024-10-30 23:26:05: [2024-10-30 23:26:05] iter = 06590, loss = 2.3065
2024-10-30 23:26:09: [2024-10-30 23:26:09] iter = 06600, loss = 2.8017
2024-10-30 23:26:13: [2024-10-30 23:26:13] iter = 06610, loss = 2.4103
2024-10-30 23:26:17: [2024-10-30 23:26:17] iter = 06620, loss = 1.8600
2024-10-30 23:26:19: [2024-10-30 23:26:19] iter = 06630, loss = 2.4069
2024-10-30 23:26:22: [2024-10-30 23:26:22] iter = 06640, loss = 3.1068
2024-10-30 23:26:26: [2024-10-30 23:26:26] iter = 06650, loss = 2.3128
2024-10-30 23:26:30: [2024-10-30 23:26:30] iter = 06660, loss = 3.0756
2024-10-30 23:26:34: [2024-10-30 23:26:34] iter = 06670, loss = 3.0316
2024-10-30 23:26:38: [2024-10-30 23:26:38] iter = 06680, loss = 5.3694
2024-10-30 23:26:41: [2024-10-30 23:26:41] iter = 06690, loss = 3.9341
2024-10-30 23:26:45: [2024-10-30 23:26:45] iter = 06700, loss = 2.8511
2024-10-30 23:26:48: [2024-10-30 23:26:48] iter = 06710, loss = 4.6305
2024-10-30 23:26:50: [2024-10-30 23:26:50] iter = 06720, loss = 1.9815
2024-10-30 23:26:53: [2024-10-30 23:26:53] iter = 06730, loss = 2.9928
2024-10-30 23:26:56: [2024-10-30 23:26:56] iter = 06740, loss = 2.4805
2024-10-30 23:26:59: [2024-10-30 23:26:59] iter = 06750, loss = 2.6604
2024-10-30 23:27:03: [2024-10-30 23:27:03] iter = 06760, loss = 1.9283
2024-10-30 23:27:06: [2024-10-30 23:27:06] iter = 06770, loss = 2.1409
2024-10-30 23:27:10: [2024-10-30 23:27:10] iter = 06780, loss = 3.6044
2024-10-30 23:27:14: [2024-10-30 23:27:14] iter = 06790, loss = 2.7278
2024-10-30 23:27:17: [2024-10-30 23:27:17] iter = 06800, loss = 1.9735
2024-10-30 23:27:20: [2024-10-30 23:27:20] iter = 06810, loss = 2.3549
2024-10-30 23:27:24: [2024-10-30 23:27:24] iter = 06820, loss = 1.9971
2024-10-30 23:27:28: [2024-10-30 23:27:28] iter = 06830, loss = 2.3552
2024-10-30 23:27:33: [2024-10-30 23:27:33] iter = 06840, loss = 2.3415
2024-10-30 23:27:36: [2024-10-30 23:27:36] iter = 06850, loss = 1.9650
2024-10-30 23:27:40: [2024-10-30 23:27:40] iter = 06860, loss = 2.0778
2024-10-30 23:27:44: [2024-10-30 23:27:44] iter = 06870, loss = 2.0771
2024-10-30 23:27:47: [2024-10-30 23:27:47] iter = 06880, loss = 2.1186
2024-10-30 23:27:52: [2024-10-30 23:27:52] iter = 06890, loss = 4.2037
2024-10-30 23:27:55: [2024-10-30 23:27:55] iter = 06900, loss = 2.2560
2024-10-30 23:27:59: [2024-10-30 23:27:59] iter = 06910, loss = 2.9651
2024-10-30 23:28:03: [2024-10-30 23:28:03] iter = 06920, loss = 2.6410
2024-10-30 23:28:06: [2024-10-30 23:28:06] iter = 06930, loss = 2.2140
2024-10-30 23:28:10: [2024-10-30 23:28:10] iter = 06940, loss = 2.3451
2024-10-30 23:28:14: [2024-10-30 23:28:14] iter = 06950, loss = 1.7703
2024-10-30 23:28:18: [2024-10-30 23:28:18] iter = 06960, loss = 2.0256
2024-10-30 23:28:22: [2024-10-30 23:28:22] iter = 06970, loss = 3.0504
2024-10-30 23:28:26: [2024-10-30 23:28:26] iter = 06980, loss = 2.1064
2024-10-30 23:28:30: [2024-10-30 23:28:30] iter = 06990, loss = 1.8104
2024-10-30 23:28:35: [2024-10-30 23:28:35] iter = 07000, loss = 3.1414
2024-10-30 23:28:39: [2024-10-30 23:28:39] iter = 07010, loss = 3.2296
2024-10-30 23:28:42: [2024-10-30 23:28:42] iter = 07020, loss = 2.0467
2024-10-30 23:28:46: [2024-10-30 23:28:46] iter = 07030, loss = 2.4089
2024-10-30 23:28:49: [2024-10-30 23:28:49] iter = 07040, loss = 4.3457
2024-10-30 23:28:52: [2024-10-30 23:28:52] iter = 07050, loss = 3.9165
2024-10-30 23:28:55: [2024-10-30 23:28:55] iter = 07060, loss = 2.0325
2024-10-30 23:28:59: [2024-10-30 23:28:59] iter = 07070, loss = 1.8770
2024-10-30 23:29:03: [2024-10-30 23:29:03] iter = 07080, loss = 2.6313
2024-10-30 23:29:07: [2024-10-30 23:29:07] iter = 07090, loss = 2.4072
2024-10-30 23:29:10: [2024-10-30 23:29:10] iter = 07100, loss = 3.0312
2024-10-30 23:29:15: [2024-10-30 23:29:15] iter = 07110, loss = 2.8037
2024-10-30 23:29:18: [2024-10-30 23:29:18] iter = 07120, loss = 2.6683
2024-10-30 23:29:21: [2024-10-30 23:29:21] iter = 07130, loss = 2.2895
2024-10-30 23:29:24: [2024-10-30 23:29:24] iter = 07140, loss = 2.6453
2024-10-30 23:29:28: [2024-10-30 23:29:28] iter = 07150, loss = 2.3712
2024-10-30 23:29:32: [2024-10-30 23:29:32] iter = 07160, loss = 1.8247
2024-10-30 23:29:35: [2024-10-30 23:29:35] iter = 07170, loss = 1.8768
2024-10-30 23:29:38: [2024-10-30 23:29:38] iter = 07180, loss = 2.3938
2024-10-30 23:29:42: [2024-10-30 23:29:42] iter = 07190, loss = 1.7476
2024-10-30 23:29:45: [2024-10-30 23:29:45] iter = 07200, loss = 2.6734
2024-10-30 23:29:48: [2024-10-30 23:29:48] iter = 07210, loss = 2.4288
2024-10-30 23:29:51: [2024-10-30 23:29:51] iter = 07220, loss = 2.4169
2024-10-30 23:29:54: [2024-10-30 23:29:54] iter = 07230, loss = 2.8368
2024-10-30 23:29:58: [2024-10-30 23:29:58] iter = 07240, loss = 2.3211
2024-10-30 23:30:00: [2024-10-30 23:30:00] iter = 07250, loss = 2.1279
2024-10-30 23:30:04: [2024-10-30 23:30:04] iter = 07260, loss = 2.0498
2024-10-30 23:30:08: [2024-10-30 23:30:08] iter = 07270, loss = 2.5128
2024-10-30 23:30:11: [2024-10-30 23:30:11] iter = 07280, loss = 3.2001
2024-10-30 23:30:15: [2024-10-30 23:30:15] iter = 07290, loss = 3.4429
2024-10-30 23:30:18: [2024-10-30 23:30:18] iter = 07300, loss = 1.9325
2024-10-30 23:30:21: [2024-10-30 23:30:21] iter = 07310, loss = 2.1002
2024-10-30 23:30:24: [2024-10-30 23:30:24] iter = 07320, loss = 2.1995
2024-10-30 23:30:27: [2024-10-30 23:30:27] iter = 07330, loss = 1.9118
2024-10-30 23:30:32: [2024-10-30 23:30:32] iter = 07340, loss = 1.9750
2024-10-30 23:30:35: [2024-10-30 23:30:35] iter = 07350, loss = 2.9491
2024-10-30 23:30:41: [2024-10-30 23:30:41] iter = 07360, loss = 2.5929
2024-10-30 23:30:46: [2024-10-30 23:30:46] iter = 07370, loss = 1.9830
2024-10-30 23:30:50: [2024-10-30 23:30:50] iter = 07380, loss = 2.2945
2024-10-30 23:30:54: [2024-10-30 23:30:54] iter = 07390, loss = 3.4304
2024-10-30 23:30:59: [2024-10-30 23:30:59] iter = 07400, loss = 3.3430
2024-10-30 23:31:02: [2024-10-30 23:31:02] iter = 07410, loss = 3.5672
2024-10-30 23:31:05: [2024-10-30 23:31:05] iter = 07420, loss = 3.0019
2024-10-30 23:31:08: [2024-10-30 23:31:08] iter = 07430, loss = 6.0096
2024-10-30 23:31:13: [2024-10-30 23:31:13] iter = 07440, loss = 2.1276
2024-10-30 23:31:17: [2024-10-30 23:31:17] iter = 07450, loss = 2.6286
2024-10-30 23:31:21: [2024-10-30 23:31:21] iter = 07460, loss = 2.7064
2024-10-30 23:31:25: [2024-10-30 23:31:25] iter = 07470, loss = 2.8104
2024-10-30 23:31:30: [2024-10-30 23:31:29] iter = 07480, loss = 2.3836
2024-10-30 23:31:34: [2024-10-30 23:31:34] iter = 07490, loss = 2.3907
2024-10-30 23:31:38: [2024-10-30 23:31:38] iter = 07500, loss = 3.2698
2024-10-30 23:31:42: [2024-10-30 23:31:42] iter = 07510, loss = 2.8288
2024-10-30 23:31:45: [2024-10-30 23:31:45] iter = 07520, loss = 3.8368
2024-10-30 23:31:49: [2024-10-30 23:31:49] iter = 07530, loss = 2.4897
2024-10-30 23:31:52: [2024-10-30 23:31:52] iter = 07540, loss = 2.0956
2024-10-30 23:31:56: [2024-10-30 23:31:56] iter = 07550, loss = 2.3761
2024-10-30 23:32:00: [2024-10-30 23:32:00] iter = 07560, loss = 2.0557
2024-10-30 23:32:04: [2024-10-30 23:32:04] iter = 07570, loss = 2.1138
2024-10-30 23:32:08: [2024-10-30 23:32:08] iter = 07580, loss = 2.2441
2024-10-30 23:32:13: [2024-10-30 23:32:13] iter = 07590, loss = 1.8905
2024-10-30 23:32:16: [2024-10-30 23:32:16] iter = 07600, loss = 2.0188
2024-10-30 23:32:20: [2024-10-30 23:32:20] iter = 07610, loss = 2.4214
2024-10-30 23:32:24: [2024-10-30 23:32:24] iter = 07620, loss = 2.6016
2024-10-30 23:32:28: [2024-10-30 23:32:28] iter = 07630, loss = 2.3651
2024-10-30 23:32:32: [2024-10-30 23:32:32] iter = 07640, loss = 3.1998
2024-10-30 23:32:36: [2024-10-30 23:32:36] iter = 07650, loss = 2.7492
2024-10-30 23:32:40: [2024-10-30 23:32:40] iter = 07660, loss = 2.1204
2024-10-30 23:32:45: [2024-10-30 23:32:45] iter = 07670, loss = 2.3745
2024-10-30 23:32:49: [2024-10-30 23:32:49] iter = 07680, loss = 2.4446
2024-10-30 23:32:52: [2024-10-30 23:32:52] iter = 07690, loss = 2.1178
2024-10-30 23:32:54: [2024-10-30 23:32:54] iter = 07700, loss = 2.0780
2024-10-30 23:32:57: [2024-10-30 23:32:57] iter = 07710, loss = 2.0734
2024-10-30 23:33:00: [2024-10-30 23:33:00] iter = 07720, loss = 2.2636
2024-10-30 23:33:04: [2024-10-30 23:33:04] iter = 07730, loss = 5.7466
2024-10-30 23:33:08: [2024-10-30 23:33:08] iter = 07740, loss = 2.0585
2024-10-30 23:33:11: [2024-10-30 23:33:11] iter = 07750, loss = 2.5904
2024-10-30 23:33:15: [2024-10-30 23:33:15] iter = 07760, loss = 2.0605
2024-10-30 23:33:17: [2024-10-30 23:33:17] iter = 07770, loss = 1.9881
2024-10-30 23:33:21: [2024-10-30 23:33:21] iter = 07780, loss = 2.3061
2024-10-30 23:33:25: [2024-10-30 23:33:25] iter = 07790, loss = 2.2228
2024-10-30 23:33:28: [2024-10-30 23:33:28] iter = 07800, loss = 2.6992
2024-10-30 23:33:31: [2024-10-30 23:33:31] iter = 07810, loss = 1.8845
2024-10-30 23:33:36: [2024-10-30 23:33:36] iter = 07820, loss = 2.1770
2024-10-30 23:33:39: [2024-10-30 23:33:39] iter = 07830, loss = 2.7642
2024-10-30 23:33:41: [2024-10-30 23:33:41] iter = 07840, loss = 1.7857
2024-10-30 23:33:44: [2024-10-30 23:33:44] iter = 07850, loss = 2.6112
2024-10-30 23:33:48: [2024-10-30 23:33:48] iter = 07860, loss = 4.6905
2024-10-30 23:33:52: [2024-10-30 23:33:52] iter = 07870, loss = 1.8965
2024-10-30 23:33:56: [2024-10-30 23:33:56] iter = 07880, loss = 2.0358
2024-10-30 23:34:00: [2024-10-30 23:34:00] iter = 07890, loss = 2.8341
2024-10-30 23:34:04: [2024-10-30 23:34:04] iter = 07900, loss = 1.9777
2024-10-30 23:34:07: [2024-10-30 23:34:07] iter = 07910, loss = 2.2341
2024-10-30 23:34:11: [2024-10-30 23:34:11] iter = 07920, loss = 1.8964
2024-10-30 23:34:14: [2024-10-30 23:34:14] iter = 07930, loss = 2.2765
2024-10-30 23:34:18: [2024-10-30 23:34:18] iter = 07940, loss = 2.1495
2024-10-30 23:34:22: [2024-10-30 23:34:22] iter = 07950, loss = 2.0834
2024-10-30 23:34:25: [2024-10-30 23:34:25] iter = 07960, loss = 1.7574
2024-10-30 23:34:29: [2024-10-30 23:34:29] iter = 07970, loss = 2.7339
2024-10-30 23:34:33: [2024-10-30 23:34:33] iter = 07980, loss = 1.9153
2024-10-30 23:34:36: [2024-10-30 23:34:36] iter = 07990, loss = 3.3032
2024-10-30 23:34:40: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
2024-10-30 23:34:40: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 23:34:40: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 80365}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 23:37:02: Evaluate 5 random ConvNet, ACCmean = 0.7909 ACCstd = 0.0050
-------------------------
2024-10-30 23:37:02: Evaluate 5 random ConvNet, SENmean = 0.7885 SENstd = 0.0059
-------------------------
2024-10-30 23:37:02: Evaluate 5 random ConvNet, SPEmean = 0.9791 SPEstd = 0.0005
-------------------------
2024-10-30 23:37:02: Evaluate 5 random ConvNet, F!mean = 0.7766 F!std = 0.0051
-------------------------
2024-10-30 23:37:02: Evaluate 5 random ConvNet, mean = 0.7909 std = 0.0050
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 23:37:03: [2024-10-30 23:37:03] iter = 08000, loss = 1.9269
2024-10-30 23:37:06: [2024-10-30 23:37:06] iter = 08010, loss = 2.6755
2024-10-30 23:37:10: [2024-10-30 23:37:10] iter = 08020, loss = 4.0500
2024-10-30 23:37:13: [2024-10-30 23:37:13] iter = 08030, loss = 2.8699
2024-10-30 23:37:16: [2024-10-30 23:37:16] iter = 08040, loss = 2.6253
2024-10-30 23:37:19: [2024-10-30 23:37:19] iter = 08050, loss = 2.0681
2024-10-30 23:37:23: [2024-10-30 23:37:23] iter = 08060, loss = 2.0471
2024-10-30 23:37:27: [2024-10-30 23:37:27] iter = 08070, loss = 1.9479
2024-10-30 23:37:31: [2024-10-30 23:37:31] iter = 08080, loss = 2.1366
2024-10-30 23:37:34: [2024-10-30 23:37:34] iter = 08090, loss = 2.1376
2024-10-30 23:37:37: [2024-10-30 23:37:37] iter = 08100, loss = 2.0127
2024-10-30 23:37:41: [2024-10-30 23:37:41] iter = 08110, loss = 3.1951
2024-10-30 23:37:44: [2024-10-30 23:37:44] iter = 08120, loss = 2.5898
2024-10-30 23:37:47: [2024-10-30 23:37:47] iter = 08130, loss = 2.2915
2024-10-30 23:37:50: [2024-10-30 23:37:50] iter = 08140, loss = 1.9018
2024-10-30 23:37:53: [2024-10-30 23:37:53] iter = 08150, loss = 2.1236
2024-10-30 23:37:57: [2024-10-30 23:37:57] iter = 08160, loss = 1.9742
2024-10-30 23:38:00: [2024-10-30 23:38:00] iter = 08170, loss = 1.7017
2024-10-30 23:38:04: [2024-10-30 23:38:04] iter = 08180, loss = 1.8585
2024-10-30 23:38:08: [2024-10-30 23:38:08] iter = 08190, loss = 1.9330
2024-10-30 23:38:11: [2024-10-30 23:38:11] iter = 08200, loss = 2.4276
2024-10-30 23:38:15: [2024-10-30 23:38:15] iter = 08210, loss = 1.7697
2024-10-30 23:38:18: [2024-10-30 23:38:18] iter = 08220, loss = 2.2321
2024-10-30 23:38:21: [2024-10-30 23:38:21] iter = 08230, loss = 1.9902
2024-10-30 23:38:24: [2024-10-30 23:38:24] iter = 08240, loss = 1.7921
2024-10-30 23:38:27: [2024-10-30 23:38:27] iter = 08250, loss = 2.1470
2024-10-30 23:38:31: [2024-10-30 23:38:31] iter = 08260, loss = 2.5582
2024-10-30 23:38:35: [2024-10-30 23:38:35] iter = 08270, loss = 2.4264
2024-10-30 23:38:38: [2024-10-30 23:38:38] iter = 08280, loss = 2.4863
2024-10-30 23:38:41: [2024-10-30 23:38:41] iter = 08290, loss = 3.4073
2024-10-30 23:38:44: [2024-10-30 23:38:44] iter = 08300, loss = 2.0148
2024-10-30 23:38:47: [2024-10-30 23:38:47] iter = 08310, loss = 2.3223
2024-10-30 23:38:51: [2024-10-30 23:38:51] iter = 08320, loss = 1.9366
2024-10-30 23:38:55: [2024-10-30 23:38:55] iter = 08330, loss = 2.4977
2024-10-30 23:38:59: [2024-10-30 23:38:59] iter = 08340, loss = 1.8976
2024-10-30 23:39:02: [2024-10-30 23:39:02] iter = 08350, loss = 3.4666
2024-10-30 23:39:05: [2024-10-30 23:39:05] iter = 08360, loss = 2.2099
2024-10-30 23:39:09: [2024-10-30 23:39:09] iter = 08370, loss = 2.3293
2024-10-30 23:39:12: [2024-10-30 23:39:12] iter = 08380, loss = 2.2245
2024-10-30 23:39:15: [2024-10-30 23:39:15] iter = 08390, loss = 1.9909
2024-10-30 23:39:18: [2024-10-30 23:39:18] iter = 08400, loss = 2.0469
2024-10-30 23:39:21: [2024-10-30 23:39:21] iter = 08410, loss = 2.7929
2024-10-30 23:39:23: [2024-10-30 23:39:23] iter = 08420, loss = 2.5033
2024-10-30 23:39:26: [2024-10-30 23:39:26] iter = 08430, loss = 2.2432
2024-10-30 23:39:30: [2024-10-30 23:39:30] iter = 08440, loss = 2.1716
2024-10-30 23:39:33: [2024-10-30 23:39:33] iter = 08450, loss = 2.2353
2024-10-30 23:39:36: [2024-10-30 23:39:36] iter = 08460, loss = 2.1621
2024-10-30 23:39:40: [2024-10-30 23:39:40] iter = 08470, loss = 2.4217
2024-10-30 23:39:44: [2024-10-30 23:39:44] iter = 08480, loss = 2.4131
2024-10-30 23:39:47: [2024-10-30 23:39:47] iter = 08490, loss = 3.0552
2024-10-30 23:39:51: [2024-10-30 23:39:51] iter = 08500, loss = 2.2913
2024-10-30 23:39:55: [2024-10-30 23:39:55] iter = 08510, loss = 2.7481
2024-10-30 23:39:58: [2024-10-30 23:39:58] iter = 08520, loss = 2.0078
2024-10-30 23:40:02: [2024-10-30 23:40:02] iter = 08530, loss = 3.3034
2024-10-30 23:40:06: [2024-10-30 23:40:06] iter = 08540, loss = 2.0908
2024-10-30 23:40:10: [2024-10-30 23:40:10] iter = 08550, loss = 2.3677
2024-10-30 23:40:14: [2024-10-30 23:40:14] iter = 08560, loss = 1.6689
2024-10-30 23:40:18: [2024-10-30 23:40:18] iter = 08570, loss = 2.1445
2024-10-30 23:40:23: [2024-10-30 23:40:23] iter = 08580, loss = 2.9335
2024-10-30 23:40:26: [2024-10-30 23:40:26] iter = 08590, loss = 1.8828
2024-10-30 23:40:29: [2024-10-30 23:40:29] iter = 08600, loss = 2.3539
2024-10-30 23:40:32: [2024-10-30 23:40:32] iter = 08610, loss = 1.9521
2024-10-30 23:40:35: [2024-10-30 23:40:35] iter = 08620, loss = 2.6776
2024-10-30 23:40:38: [2024-10-30 23:40:38] iter = 08630, loss = 2.3778
2024-10-30 23:40:41: [2024-10-30 23:40:41] iter = 08640, loss = 2.8439
2024-10-30 23:40:45: [2024-10-30 23:40:45] iter = 08650, loss = 2.5937
2024-10-30 23:40:49: [2024-10-30 23:40:49] iter = 08660, loss = 2.0137
2024-10-30 23:40:53: [2024-10-30 23:40:53] iter = 08670, loss = 2.3529
2024-10-30 23:40:56: [2024-10-30 23:40:56] iter = 08680, loss = 2.7245
2024-10-30 23:40:59: [2024-10-30 23:40:59] iter = 08690, loss = 2.1290
2024-10-30 23:41:03: [2024-10-30 23:41:03] iter = 08700, loss = 1.8217
2024-10-30 23:41:07: [2024-10-30 23:41:07] iter = 08710, loss = 2.0501
2024-10-30 23:41:10: [2024-10-30 23:41:10] iter = 08720, loss = 2.7184
2024-10-30 23:41:13: [2024-10-30 23:41:13] iter = 08730, loss = 2.0608
2024-10-30 23:41:17: [2024-10-30 23:41:17] iter = 08740, loss = 2.6017
2024-10-30 23:41:20: [2024-10-30 23:41:20] iter = 08750, loss = 2.4491
2024-10-30 23:41:24: [2024-10-30 23:41:24] iter = 08760, loss = 2.1064
2024-10-30 23:41:28: [2024-10-30 23:41:28] iter = 08770, loss = 2.0106
2024-10-30 23:41:31: [2024-10-30 23:41:31] iter = 08780, loss = 3.3177
2024-10-30 23:41:35: [2024-10-30 23:41:35] iter = 08790, loss = 1.8094
2024-10-30 23:41:39: [2024-10-30 23:41:39] iter = 08800, loss = 2.3189
2024-10-30 23:41:42: [2024-10-30 23:41:42] iter = 08810, loss = 2.7745
2024-10-30 23:41:45: [2024-10-30 23:41:45] iter = 08820, loss = 2.3078
2024-10-30 23:41:50: [2024-10-30 23:41:49] iter = 08830, loss = 2.6070
2024-10-30 23:41:53: [2024-10-30 23:41:53] iter = 08840, loss = 2.3524
2024-10-30 23:41:57: [2024-10-30 23:41:57] iter = 08850, loss = 2.1253
2024-10-30 23:42:01: [2024-10-30 23:42:01] iter = 08860, loss = 2.2147
2024-10-30 23:42:05: [2024-10-30 23:42:05] iter = 08870, loss = 1.8888
2024-10-30 23:42:10: [2024-10-30 23:42:10] iter = 08880, loss = 2.3570
2024-10-30 23:42:13: [2024-10-30 23:42:13] iter = 08890, loss = 4.0971
2024-10-30 23:42:16: [2024-10-30 23:42:16] iter = 08900, loss = 1.9765
2024-10-30 23:42:20: [2024-10-30 23:42:20] iter = 08910, loss = 2.6280
2024-10-30 23:42:23: [2024-10-30 23:42:23] iter = 08920, loss = 2.5671
2024-10-30 23:42:27: [2024-10-30 23:42:27] iter = 08930, loss = 2.1072
2024-10-30 23:42:31: [2024-10-30 23:42:31] iter = 08940, loss = 2.2557
2024-10-30 23:42:35: [2024-10-30 23:42:35] iter = 08950, loss = 2.3533
2024-10-30 23:42:39: [2024-10-30 23:42:39] iter = 08960, loss = 2.3133
2024-10-30 23:42:42: [2024-10-30 23:42:42] iter = 08970, loss = 2.8527
2024-10-30 23:42:46: [2024-10-30 23:42:46] iter = 08980, loss = 1.8063
2024-10-30 23:42:49: [2024-10-30 23:42:49] iter = 08990, loss = 2.3514
2024-10-30 23:42:52: [2024-10-30 23:42:52] iter = 09000, loss = 1.9317
2024-10-30 23:42:55: [2024-10-30 23:42:55] iter = 09010, loss = 1.9730
2024-10-30 23:42:58: [2024-10-30 23:42:58] iter = 09020, loss = 1.8892
2024-10-30 23:43:03: [2024-10-30 23:43:03] iter = 09030, loss = 3.8760
2024-10-30 23:43:06: [2024-10-30 23:43:06] iter = 09040, loss = 1.9016
2024-10-30 23:43:09: [2024-10-30 23:43:09] iter = 09050, loss = 2.3778
2024-10-30 23:43:12: [2024-10-30 23:43:12] iter = 09060, loss = 2.0195
2024-10-30 23:43:16: [2024-10-30 23:43:16] iter = 09070, loss = 1.8813
2024-10-30 23:43:20: [2024-10-30 23:43:20] iter = 09080, loss = 2.7869
2024-10-30 23:43:24: [2024-10-30 23:43:24] iter = 09090, loss = 1.9936
2024-10-30 23:43:28: [2024-10-30 23:43:28] iter = 09100, loss = 2.7951
2024-10-30 23:43:31: [2024-10-30 23:43:31] iter = 09110, loss = 2.0610
2024-10-30 23:43:35: [2024-10-30 23:43:35] iter = 09120, loss = 2.9738
2024-10-30 23:43:38: [2024-10-30 23:43:38] iter = 09130, loss = 6.4049
2024-10-30 23:43:41: [2024-10-30 23:43:41] iter = 09140, loss = 2.3114
2024-10-30 23:43:44: [2024-10-30 23:43:44] iter = 09150, loss = 2.1514
2024-10-30 23:43:47: [2024-10-30 23:43:47] iter = 09160, loss = 2.6634
2024-10-30 23:43:50: [2024-10-30 23:43:50] iter = 09170, loss = 2.3594
2024-10-30 23:43:53: [2024-10-30 23:43:53] iter = 09180, loss = 2.0399
2024-10-30 23:43:57: [2024-10-30 23:43:57] iter = 09190, loss = 2.1087
2024-10-30 23:43:59: [2024-10-30 23:43:59] iter = 09200, loss = 2.5537
2024-10-30 23:44:02: [2024-10-30 23:44:02] iter = 09210, loss = 2.5729
2024-10-30 23:44:05: [2024-10-30 23:44:05] iter = 09220, loss = 1.9657
2024-10-30 23:44:09: [2024-10-30 23:44:09] iter = 09230, loss = 2.9131
2024-10-30 23:44:12: [2024-10-30 23:44:12] iter = 09240, loss = 3.5398
2024-10-30 23:44:16: [2024-10-30 23:44:16] iter = 09250, loss = 2.1132
2024-10-30 23:44:19: [2024-10-30 23:44:19] iter = 09260, loss = 2.6000
2024-10-30 23:44:22: [2024-10-30 23:44:22] iter = 09270, loss = 2.1473
2024-10-30 23:44:26: [2024-10-30 23:44:26] iter = 09280, loss = 2.2904
2024-10-30 23:44:30: [2024-10-30 23:44:30] iter = 09290, loss = 1.9381
2024-10-30 23:44:34: [2024-10-30 23:44:34] iter = 09300, loss = 1.9242
2024-10-30 23:44:37: [2024-10-30 23:44:37] iter = 09310, loss = 1.9667
2024-10-30 23:44:41: [2024-10-30 23:44:41] iter = 09320, loss = 2.2203
2024-10-30 23:44:45: [2024-10-30 23:44:45] iter = 09330, loss = 2.7733
2024-10-30 23:44:49: [2024-10-30 23:44:49] iter = 09340, loss = 2.6965
2024-10-30 23:44:53: [2024-10-30 23:44:53] iter = 09350, loss = 1.9584
2024-10-30 23:44:56: [2024-10-30 23:44:56] iter = 09360, loss = 3.0007
2024-10-30 23:44:59: [2024-10-30 23:44:59] iter = 09370, loss = 5.2838
2024-10-30 23:45:01: [2024-10-30 23:45:01] iter = 09380, loss = 2.8002
2024-10-30 23:45:04: [2024-10-30 23:45:04] iter = 09390, loss = 3.1340
2024-10-30 23:45:07: [2024-10-30 23:45:07] iter = 09400, loss = 2.3862
2024-10-30 23:45:11: [2024-10-30 23:45:11] iter = 09410, loss = 2.2900
2024-10-30 23:45:14: [2024-10-30 23:45:14] iter = 09420, loss = 2.0139
2024-10-30 23:45:17: [2024-10-30 23:45:17] iter = 09430, loss = 2.0129
2024-10-30 23:45:21: [2024-10-30 23:45:21] iter = 09440, loss = 2.6436
2024-10-30 23:45:26: [2024-10-30 23:45:26] iter = 09450, loss = 2.0737
2024-10-30 23:45:29: [2024-10-30 23:45:29] iter = 09460, loss = 2.1058
2024-10-30 23:45:33: [2024-10-30 23:45:33] iter = 09470, loss = 2.2089
2024-10-30 23:45:37: [2024-10-30 23:45:37] iter = 09480, loss = 2.3845
2024-10-30 23:45:41: [2024-10-30 23:45:41] iter = 09490, loss = 1.8936
2024-10-30 23:45:45: [2024-10-30 23:45:45] iter = 09500, loss = 4.0335
2024-10-30 23:45:49: [2024-10-30 23:45:49] iter = 09510, loss = 2.7268
2024-10-30 23:45:52: [2024-10-30 23:45:52] iter = 09520, loss = 1.8332
2024-10-30 23:45:57: [2024-10-30 23:45:57] iter = 09530, loss = 2.1208
2024-10-30 23:46:01: [2024-10-30 23:46:01] iter = 09540, loss = 3.1545
2024-10-30 23:46:04: [2024-10-30 23:46:04] iter = 09550, loss = 2.4063
2024-10-30 23:46:08: [2024-10-30 23:46:08] iter = 09560, loss = 3.0136
2024-10-30 23:46:13: [2024-10-30 23:46:13] iter = 09570, loss = 2.7506
2024-10-30 23:46:17: [2024-10-30 23:46:17] iter = 09580, loss = 2.7066
2024-10-30 23:46:21: [2024-10-30 23:46:21] iter = 09590, loss = 2.9022
2024-10-30 23:46:24: [2024-10-30 23:46:24] iter = 09600, loss = 2.7836
2024-10-30 23:46:28: [2024-10-30 23:46:28] iter = 09610, loss = 3.0550
2024-10-30 23:46:33: [2024-10-30 23:46:33] iter = 09620, loss = 2.0474
2024-10-30 23:46:36: [2024-10-30 23:46:36] iter = 09630, loss = 2.2131
2024-10-30 23:46:40: [2024-10-30 23:46:40] iter = 09640, loss = 2.0440
2024-10-30 23:46:44: [2024-10-30 23:46:44] iter = 09650, loss = 2.4866
2024-10-30 23:46:48: [2024-10-30 23:46:48] iter = 09660, loss = 2.6100
2024-10-30 23:46:52: [2024-10-30 23:46:52] iter = 09670, loss = 2.5498
2024-10-30 23:46:56: [2024-10-30 23:46:56] iter = 09680, loss = 2.7835
2024-10-30 23:47:00: [2024-10-30 23:47:00] iter = 09690, loss = 2.0333
2024-10-30 23:47:04: [2024-10-30 23:47:04] iter = 09700, loss = 2.3276
2024-10-30 23:47:08: [2024-10-30 23:47:08] iter = 09710, loss = 2.1201
2024-10-30 23:47:11: [2024-10-30 23:47:11] iter = 09720, loss = 2.8208
2024-10-30 23:47:15: [2024-10-30 23:47:15] iter = 09730, loss = 2.3089
2024-10-30 23:47:18: [2024-10-30 23:47:18] iter = 09740, loss = 2.7135
2024-10-30 23:47:22: [2024-10-30 23:47:22] iter = 09750, loss = 1.8638
2024-10-30 23:47:26: [2024-10-30 23:47:26] iter = 09760, loss = 1.8539
2024-10-30 23:47:30: [2024-10-30 23:47:30] iter = 09770, loss = 2.2208
2024-10-30 23:47:34: [2024-10-30 23:47:34] iter = 09780, loss = 3.1399
2024-10-30 23:47:37: [2024-10-30 23:47:37] iter = 09790, loss = 1.8688
2024-10-30 23:47:41: [2024-10-30 23:47:41] iter = 09800, loss = 1.8339
2024-10-30 23:47:44: [2024-10-30 23:47:44] iter = 09810, loss = 1.8883
2024-10-30 23:47:48: [2024-10-30 23:47:48] iter = 09820, loss = 2.3325
2024-10-30 23:47:51: [2024-10-30 23:47:51] iter = 09830, loss = 2.2056
2024-10-30 23:47:55: [2024-10-30 23:47:55] iter = 09840, loss = 2.1247
2024-10-30 23:47:58: [2024-10-30 23:47:58] iter = 09850, loss = 2.0087
2024-10-30 23:48:02: [2024-10-30 23:48:02] iter = 09860, loss = 1.9806
2024-10-30 23:48:05: [2024-10-30 23:48:05] iter = 09870, loss = 2.4057
2024-10-30 23:48:08: [2024-10-30 23:48:08] iter = 09880, loss = 2.0669
2024-10-30 23:48:12: [2024-10-30 23:48:12] iter = 09890, loss = 2.0015
2024-10-30 23:48:15: [2024-10-30 23:48:15] iter = 09900, loss = 2.1363
2024-10-30 23:48:19: [2024-10-30 23:48:19] iter = 09910, loss = 2.3138
2024-10-30 23:48:22: [2024-10-30 23:48:22] iter = 09920, loss = 2.8037
2024-10-30 23:48:25: [2024-10-30 23:48:25] iter = 09930, loss = 2.9714
2024-10-30 23:48:28: [2024-10-30 23:48:28] iter = 09940, loss = 2.3482
2024-10-30 23:48:31: [2024-10-30 23:48:31] iter = 09950, loss = 2.2534
2024-10-30 23:48:34: [2024-10-30 23:48:34] iter = 09960, loss = 1.9482
2024-10-30 23:48:38: [2024-10-30 23:48:38] iter = 09970, loss = 2.9240
2024-10-30 23:48:42: [2024-10-30 23:48:42] iter = 09980, loss = 2.0156
2024-10-30 23:48:46: [2024-10-30 23:48:46] iter = 09990, loss = 1.9320
2024-10-30 23:48:49: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
2024-10-30 23:48:49: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-30 23:48:49: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 29203}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 23:51:05: Evaluate 5 random ConvNet, ACCmean = 0.7830 ACCstd = 0.0050
-------------------------
2024-10-30 23:51:05: Evaluate 5 random ConvNet, SENmean = 0.7786 SENstd = 0.0058
-------------------------
2024-10-30 23:51:05: Evaluate 5 random ConvNet, SPEmean = 0.9783 SPEstd = 0.0005
-------------------------
2024-10-30 23:51:05: Evaluate 5 random ConvNet, F!mean = 0.7649 F!std = 0.0054
-------------------------
2024-10-30 23:51:05: Evaluate 5 random ConvNet, mean = 0.7830 std = 0.0050
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 23:51:05: [2024-10-30 23:51:05] iter = 10000, loss = 3.2044
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-30 23:51:09: [2024-10-30 23:51:09] iter = 10010, loss = 2.0287
2024-10-30 23:51:12: [2024-10-30 23:51:12] iter = 10020, loss = 1.9076
2024-10-30 23:51:15: [2024-10-30 23:51:15] iter = 10030, loss = 2.0205
2024-10-30 23:51:17: [2024-10-30 23:51:17] iter = 10040, loss = 2.2034
2024-10-30 23:51:20: [2024-10-30 23:51:20] iter = 10050, loss = 3.7089
2024-10-30 23:51:23: [2024-10-30 23:51:23] iter = 10060, loss = 1.9059
2024-10-30 23:51:26: [2024-10-30 23:51:26] iter = 10070, loss = 3.4808
2024-10-30 23:51:30: [2024-10-30 23:51:30] iter = 10080, loss = 2.6698
2024-10-30 23:51:34: [2024-10-30 23:51:34] iter = 10090, loss = 2.6229
2024-10-30 23:51:38: [2024-10-30 23:51:38] iter = 10100, loss = 2.2447
2024-10-30 23:51:41: [2024-10-30 23:51:41] iter = 10110, loss = 2.7762
2024-10-30 23:51:45: [2024-10-30 23:51:45] iter = 10120, loss = 1.6403
2024-10-30 23:51:48: [2024-10-30 23:51:48] iter = 10130, loss = 4.8431
2024-10-30 23:51:51: [2024-10-30 23:51:51] iter = 10140, loss = 2.7677
2024-10-30 23:51:55: [2024-10-30 23:51:55] iter = 10150, loss = 5.2199
2024-10-30 23:51:59: [2024-10-30 23:51:59] iter = 10160, loss = 2.9804
2024-10-30 23:52:02: [2024-10-30 23:52:02] iter = 10170, loss = 2.0930
2024-10-30 23:52:05: [2024-10-30 23:52:05] iter = 10180, loss = 2.2088
2024-10-30 23:52:09: [2024-10-30 23:52:09] iter = 10190, loss = 3.0334
2024-10-30 23:52:12: [2024-10-30 23:52:12] iter = 10200, loss = 2.2454
2024-10-30 23:52:17: [2024-10-30 23:52:17] iter = 10210, loss = 1.8703
2024-10-30 23:52:21: [2024-10-30 23:52:21] iter = 10220, loss = 2.8398
2024-10-30 23:52:24: [2024-10-30 23:52:24] iter = 10230, loss = 2.1033
2024-10-30 23:52:28: [2024-10-30 23:52:28] iter = 10240, loss = 2.2611
2024-10-30 23:52:32: [2024-10-30 23:52:32] iter = 10250, loss = 2.4272
2024-10-30 23:52:36: [2024-10-30 23:52:36] iter = 10260, loss = 1.8922
2024-10-30 23:52:39: [2024-10-30 23:52:39] iter = 10270, loss = 1.8805
2024-10-30 23:52:43: [2024-10-30 23:52:43] iter = 10280, loss = 3.2735
2024-10-30 23:52:46: [2024-10-30 23:52:46] iter = 10290, loss = 2.4891
2024-10-30 23:52:51: [2024-10-30 23:52:51] iter = 10300, loss = 2.3016
2024-10-30 23:52:54: [2024-10-30 23:52:54] iter = 10310, loss = 2.1688
2024-10-30 23:52:58: [2024-10-30 23:52:58] iter = 10320, loss = 2.9071
2024-10-30 23:53:01: [2024-10-30 23:53:01] iter = 10330, loss = 1.8714
2024-10-30 23:53:04: [2024-10-30 23:53:04] iter = 10340, loss = 2.0479
2024-10-30 23:53:07: [2024-10-30 23:53:07] iter = 10350, loss = 2.6605
2024-10-30 23:53:11: [2024-10-30 23:53:11] iter = 10360, loss = 2.1158
2024-10-30 23:53:15: [2024-10-30 23:53:15] iter = 10370, loss = 2.0308
2024-10-30 23:53:18: [2024-10-30 23:53:18] iter = 10380, loss = 3.8500
2024-10-30 23:53:22: [2024-10-30 23:53:22] iter = 10390, loss = 1.8277
2024-10-30 23:53:26: [2024-10-30 23:53:26] iter = 10400, loss = 2.1012
2024-10-30 23:53:29: [2024-10-30 23:53:29] iter = 10410, loss = 2.0436
2024-10-30 23:53:33: [2024-10-30 23:53:33] iter = 10420, loss = 1.9280
2024-10-30 23:53:37: [2024-10-30 23:53:37] iter = 10430, loss = 2.0305
2024-10-30 23:53:41: [2024-10-30 23:53:41] iter = 10440, loss = 2.0146
2024-10-30 23:53:44: [2024-10-30 23:53:44] iter = 10450, loss = 2.1757
2024-10-30 23:53:47: [2024-10-30 23:53:47] iter = 10460, loss = 1.6270
2024-10-30 23:53:51: [2024-10-30 23:53:51] iter = 10470, loss = 2.6467
2024-10-30 23:53:55: [2024-10-30 23:53:55] iter = 10480, loss = 2.3664
2024-10-30 23:53:58: [2024-10-30 23:53:58] iter = 10490, loss = 1.7507
2024-10-30 23:54:01: [2024-10-30 23:54:01] iter = 10500, loss = 2.2482
2024-10-30 23:54:04: [2024-10-30 23:54:04] iter = 10510, loss = 2.3072
2024-10-30 23:54:08: [2024-10-30 23:54:08] iter = 10520, loss = 2.4721
2024-10-30 23:54:11: [2024-10-30 23:54:11] iter = 10530, loss = 2.5994
2024-10-30 23:54:15: [2024-10-30 23:54:15] iter = 10540, loss = 2.4269
2024-10-30 23:54:19: [2024-10-30 23:54:19] iter = 10550, loss = 2.3767
2024-10-30 23:54:22: [2024-10-30 23:54:22] iter = 10560, loss = 2.8957
2024-10-30 23:54:27: [2024-10-30 23:54:27] iter = 10570, loss = 1.9356
2024-10-30 23:54:30: [2024-10-30 23:54:30] iter = 10580, loss = 1.7080
2024-10-30 23:54:33: [2024-10-30 23:54:33] iter = 10590, loss = 2.3774
2024-10-30 23:54:36: [2024-10-30 23:54:36] iter = 10600, loss = 2.3516
2024-10-30 23:54:39: [2024-10-30 23:54:39] iter = 10610, loss = 2.2372
2024-10-30 23:54:43: [2024-10-30 23:54:43] iter = 10620, loss = 2.8959
2024-10-30 23:54:46: [2024-10-30 23:54:46] iter = 10630, loss = 2.2105
2024-10-30 23:54:50: [2024-10-30 23:54:50] iter = 10640, loss = 2.3539
2024-10-30 23:54:54: [2024-10-30 23:54:54] iter = 10650, loss = 2.5929
2024-10-30 23:54:57: [2024-10-30 23:54:57] iter = 10660, loss = 1.9242
2024-10-30 23:54:59: [2024-10-30 23:54:59] iter = 10670, loss = 2.2313
2024-10-30 23:55:02: [2024-10-30 23:55:02] iter = 10680, loss = 2.7582
2024-10-30 23:55:06: [2024-10-30 23:55:06] iter = 10690, loss = 2.0102
2024-10-30 23:55:10: [2024-10-30 23:55:10] iter = 10700, loss = 2.0456
2024-10-30 23:55:14: [2024-10-30 23:55:14] iter = 10710, loss = 2.1240
2024-10-30 23:55:17: [2024-10-30 23:55:17] iter = 10720, loss = 2.0136
2024-10-30 23:55:21: [2024-10-30 23:55:21] iter = 10730, loss = 2.4318
2024-10-30 23:55:25: [2024-10-30 23:55:25] iter = 10740, loss = 1.8441
2024-10-30 23:55:28: [2024-10-30 23:55:28] iter = 10750, loss = 2.5250
2024-10-30 23:55:32: [2024-10-30 23:55:32] iter = 10760, loss = 2.3020
2024-10-30 23:55:36: [2024-10-30 23:55:36] iter = 10770, loss = 2.8335
2024-10-30 23:55:39: [2024-10-30 23:55:39] iter = 10780, loss = 2.0506
2024-10-30 23:55:43: [2024-10-30 23:55:43] iter = 10790, loss = 1.9539
2024-10-30 23:55:47: [2024-10-30 23:55:47] iter = 10800, loss = 2.0026
2024-10-30 23:55:51: [2024-10-30 23:55:51] iter = 10810, loss = 2.1358
2024-10-30 23:55:54: [2024-10-30 23:55:54] iter = 10820, loss = 1.8820
2024-10-30 23:55:57: [2024-10-30 23:55:57] iter = 10830, loss = 3.3969
2024-10-30 23:56:01: [2024-10-30 23:56:01] iter = 10840, loss = 1.9734
2024-10-30 23:56:04: [2024-10-30 23:56:04] iter = 10850, loss = 2.1580
2024-10-30 23:56:08: [2024-10-30 23:56:08] iter = 10860, loss = 2.3049
2024-10-30 23:56:12: [2024-10-30 23:56:12] iter = 10870, loss = 3.1375
2024-10-30 23:56:15: [2024-10-30 23:56:15] iter = 10880, loss = 1.9768
2024-10-30 23:56:19: [2024-10-30 23:56:19] iter = 10890, loss = 2.2656
2024-10-30 23:56:23: [2024-10-30 23:56:23] iter = 10900, loss = 1.8821
2024-10-30 23:56:27: [2024-10-30 23:56:27] iter = 10910, loss = 2.3954
2024-10-30 23:56:30: [2024-10-30 23:56:30] iter = 10920, loss = 3.0510
2024-10-30 23:56:35: [2024-10-30 23:56:35] iter = 10930, loss = 2.1458
2024-10-30 23:56:38: [2024-10-30 23:56:38] iter = 10940, loss = 2.1240
2024-10-30 23:56:41: [2024-10-30 23:56:41] iter = 10950, loss = 2.4435
2024-10-30 23:56:45: [2024-10-30 23:56:45] iter = 10960, loss = 2.6682
2024-10-30 23:56:49: [2024-10-30 23:56:49] iter = 10970, loss = 2.4534
2024-10-30 23:56:52: [2024-10-30 23:56:52] iter = 10980, loss = 2.6503
2024-10-30 23:56:56: [2024-10-30 23:56:56] iter = 10990, loss = 2.3898
2024-10-30 23:57:00: [2024-10-30 23:57:00] iter = 11000, loss = 2.5567
2024-10-30 23:57:03: [2024-10-30 23:57:03] iter = 11010, loss = 2.5656
2024-10-30 23:57:07: [2024-10-30 23:57:07] iter = 11020, loss = 1.7960
2024-10-30 23:57:10: [2024-10-30 23:57:10] iter = 11030, loss = 9.9720
2024-10-30 23:57:14: [2024-10-30 23:57:14] iter = 11040, loss = 1.7976
2024-10-30 23:57:18: [2024-10-30 23:57:18] iter = 11050, loss = 2.2210
2024-10-30 23:57:20: [2024-10-30 23:57:20] iter = 11060, loss = 3.3740
2024-10-30 23:57:24: [2024-10-30 23:57:24] iter = 11070, loss = 2.1336
2024-10-30 23:57:28: [2024-10-30 23:57:28] iter = 11080, loss = 2.6868
2024-10-30 23:57:32: [2024-10-30 23:57:32] iter = 11090, loss = 2.1497
2024-10-30 23:57:35: [2024-10-30 23:57:35] iter = 11100, loss = 1.8491
2024-10-30 23:57:38: [2024-10-30 23:57:38] iter = 11110, loss = 1.8970
2024-10-30 23:57:42: [2024-10-30 23:57:42] iter = 11120, loss = 3.1116
2024-10-30 23:57:45: [2024-10-30 23:57:45] iter = 11130, loss = 3.6844
2024-10-30 23:57:48: [2024-10-30 23:57:48] iter = 11140, loss = 2.0365
2024-10-30 23:57:52: [2024-10-30 23:57:52] iter = 11150, loss = 2.2591
2024-10-30 23:57:56: [2024-10-30 23:57:56] iter = 11160, loss = 2.2917
2024-10-30 23:57:59: [2024-10-30 23:57:59] iter = 11170, loss = 3.0720
2024-10-30 23:58:03: [2024-10-30 23:58:03] iter = 11180, loss = 2.3628
2024-10-30 23:58:07: [2024-10-30 23:58:07] iter = 11190, loss = 2.2327
2024-10-30 23:58:10: [2024-10-30 23:58:10] iter = 11200, loss = 2.2041
2024-10-30 23:58:13: [2024-10-30 23:58:13] iter = 11210, loss = 2.9105
2024-10-30 23:58:17: [2024-10-30 23:58:17] iter = 11220, loss = 2.0693
2024-10-30 23:58:20: [2024-10-30 23:58:20] iter = 11230, loss = 2.1932
2024-10-30 23:58:24: [2024-10-30 23:58:24] iter = 11240, loss = 2.4737
2024-10-30 23:58:29: [2024-10-30 23:58:29] iter = 11250, loss = 2.0340
2024-10-30 23:58:32: [2024-10-30 23:58:32] iter = 11260, loss = 1.8854
2024-10-30 23:58:36: [2024-10-30 23:58:36] iter = 11270, loss = 3.5851
2024-10-30 23:58:40: [2024-10-30 23:58:40] iter = 11280, loss = 1.8914
2024-10-30 23:58:43: [2024-10-30 23:58:43] iter = 11290, loss = 2.1761
2024-10-30 23:58:46: [2024-10-30 23:58:46] iter = 11300, loss = 2.2165
2024-10-30 23:58:50: [2024-10-30 23:58:50] iter = 11310, loss = 2.6437
2024-10-30 23:58:54: [2024-10-30 23:58:54] iter = 11320, loss = 2.1698
2024-10-30 23:58:58: [2024-10-30 23:58:58] iter = 11330, loss = 2.3369
2024-10-30 23:59:01: [2024-10-30 23:59:01] iter = 11340, loss = 2.6737
2024-10-30 23:59:04: [2024-10-30 23:59:04] iter = 11350, loss = 1.9017
2024-10-30 23:59:08: [2024-10-30 23:59:08] iter = 11360, loss = 3.2962
2024-10-30 23:59:11: [2024-10-30 23:59:11] iter = 11370, loss = 1.9526
2024-10-30 23:59:14: [2024-10-30 23:59:14] iter = 11380, loss = 2.4849
2024-10-30 23:59:18: [2024-10-30 23:59:18] iter = 11390, loss = 2.2056
2024-10-30 23:59:21: [2024-10-30 23:59:21] iter = 11400, loss = 2.2453
2024-10-30 23:59:23: [2024-10-30 23:59:23] iter = 11410, loss = 2.0314
2024-10-30 23:59:26: [2024-10-30 23:59:26] iter = 11420, loss = 2.0346
2024-10-30 23:59:29: [2024-10-30 23:59:29] iter = 11430, loss = 2.1814
2024-10-30 23:59:33: [2024-10-30 23:59:33] iter = 11440, loss = 2.5629
2024-10-30 23:59:36: [2024-10-30 23:59:36] iter = 11450, loss = 2.1906
2024-10-30 23:59:39: [2024-10-30 23:59:39] iter = 11460, loss = 1.7404
2024-10-30 23:59:43: [2024-10-30 23:59:43] iter = 11470, loss = 1.7479
2024-10-30 23:59:48: [2024-10-30 23:59:48] iter = 11480, loss = 1.8491
2024-10-30 23:59:52: [2024-10-30 23:59:52] iter = 11490, loss = 2.5524
2024-10-30 23:59:55: [2024-10-30 23:59:55] iter = 11500, loss = 2.4652
2024-10-30 23:59:59: [2024-10-30 23:59:59] iter = 11510, loss = 5.5238
2024-10-31 00:00:01: [2024-10-31 00:00:01] iter = 11520, loss = 2.2289
2024-10-31 00:00:05: [2024-10-31 00:00:05] iter = 11530, loss = 2.3860
2024-10-31 00:00:08: [2024-10-31 00:00:08] iter = 11540, loss = 2.1261
2024-10-31 00:00:12: [2024-10-31 00:00:12] iter = 11550, loss = 2.0666
2024-10-31 00:00:15: [2024-10-31 00:00:15] iter = 11560, loss = 2.1923
2024-10-31 00:00:18: [2024-10-31 00:00:18] iter = 11570, loss = 4.9124
2024-10-31 00:00:22: [2024-10-31 00:00:22] iter = 11580, loss = 1.9928
2024-10-31 00:00:25: [2024-10-31 00:00:25] iter = 11590, loss = 1.7776
2024-10-31 00:00:28: [2024-10-31 00:00:28] iter = 11600, loss = 1.8818
2024-10-31 00:00:32: [2024-10-31 00:00:32] iter = 11610, loss = 2.5038
2024-10-31 00:00:35: [2024-10-31 00:00:35] iter = 11620, loss = 2.5849
2024-10-31 00:00:38: [2024-10-31 00:00:38] iter = 11630, loss = 2.7522
2024-10-31 00:00:42: [2024-10-31 00:00:42] iter = 11640, loss = 2.3744
2024-10-31 00:00:46: [2024-10-31 00:00:46] iter = 11650, loss = 2.1538
2024-10-31 00:00:48: [2024-10-31 00:00:48] iter = 11660, loss = 1.9117
2024-10-31 00:00:51: [2024-10-31 00:00:51] iter = 11670, loss = 2.6675
2024-10-31 00:00:55: [2024-10-31 00:00:55] iter = 11680, loss = 2.1985
2024-10-31 00:00:57: [2024-10-31 00:00:57] iter = 11690, loss = 1.9821
2024-10-31 00:01:00: [2024-10-31 00:01:00] iter = 11700, loss = 2.6432
2024-10-31 00:01:03: [2024-10-31 00:01:03] iter = 11710, loss = 2.0136
2024-10-31 00:01:06: [2024-10-31 00:01:06] iter = 11720, loss = 3.5314
2024-10-31 00:01:10: [2024-10-31 00:01:10] iter = 11730, loss = 2.0048
2024-10-31 00:01:13: [2024-10-31 00:01:13] iter = 11740, loss = 2.3913
2024-10-31 00:01:15: [2024-10-31 00:01:15] iter = 11750, loss = 2.1412
2024-10-31 00:01:19: [2024-10-31 00:01:19] iter = 11760, loss = 6.7912
2024-10-31 00:01:22: [2024-10-31 00:01:22] iter = 11770, loss = 2.4504
2024-10-31 00:01:25: [2024-10-31 00:01:25] iter = 11780, loss = 2.6771
2024-10-31 00:01:29: [2024-10-31 00:01:29] iter = 11790, loss = 2.3910
2024-10-31 00:01:32: [2024-10-31 00:01:32] iter = 11800, loss = 2.2324
2024-10-31 00:01:35: [2024-10-31 00:01:35] iter = 11810, loss = 2.1768
2024-10-31 00:01:38: [2024-10-31 00:01:38] iter = 11820, loss = 1.8554
2024-10-31 00:01:42: [2024-10-31 00:01:42] iter = 11830, loss = 4.2818
2024-10-31 00:01:45: [2024-10-31 00:01:45] iter = 11840, loss = 1.8477
2024-10-31 00:01:49: [2024-10-31 00:01:49] iter = 11850, loss = 2.4061
2024-10-31 00:01:52: [2024-10-31 00:01:52] iter = 11860, loss = 3.1434
2024-10-31 00:01:55: [2024-10-31 00:01:55] iter = 11870, loss = 2.0048
2024-10-31 00:01:59: [2024-10-31 00:01:59] iter = 11880, loss = 2.5079
2024-10-31 00:02:02: [2024-10-31 00:02:01] iter = 11890, loss = 2.1401
2024-10-31 00:02:03: [2024-10-31 00:02:03] iter = 11900, loss = 2.1545
2024-10-31 00:02:07: [2024-10-31 00:02:07] iter = 11910, loss = 1.8679
2024-10-31 00:02:11: [2024-10-31 00:02:11] iter = 11920, loss = 2.2246
2024-10-31 00:02:14: [2024-10-31 00:02:14] iter = 11930, loss = 2.3707
2024-10-31 00:02:17: [2024-10-31 00:02:17] iter = 11940, loss = 1.8955
2024-10-31 00:02:21: [2024-10-31 00:02:21] iter = 11950, loss = 2.5346
2024-10-31 00:02:23: [2024-10-31 00:02:23] iter = 11960, loss = 2.1215
2024-10-31 00:02:27: [2024-10-31 00:02:27] iter = 11970, loss = 2.1867
2024-10-31 00:02:30: [2024-10-31 00:02:30] iter = 11980, loss = 2.2524
2024-10-31 00:02:34: [2024-10-31 00:02:34] iter = 11990, loss = 2.1967
2024-10-31 00:02:37: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
2024-10-31 00:02:37: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 00:02:37: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 57329}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:04:54: Evaluate 5 random ConvNet, ACCmean = 0.7748 ACCstd = 0.0052
-------------------------
2024-10-31 00:04:54: Evaluate 5 random ConvNet, SENmean = 0.7715 SENstd = 0.0059
-------------------------
2024-10-31 00:04:54: Evaluate 5 random ConvNet, SPEmean = 0.9775 SPEstd = 0.0005
-------------------------
2024-10-31 00:04:54: Evaluate 5 random ConvNet, F!mean = 0.7581 F!std = 0.0052
-------------------------
2024-10-31 00:04:54: Evaluate 5 random ConvNet, mean = 0.7748 std = 0.0052
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:04:55: [2024-10-31 00:04:55] iter = 12000, loss = 2.6437
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:04:58: [2024-10-31 00:04:58] iter = 12010, loss = 2.4725
2024-10-31 00:05:01: [2024-10-31 00:05:01] iter = 12020, loss = 2.3097
2024-10-31 00:05:05: [2024-10-31 00:05:05] iter = 12030, loss = 2.5729
2024-10-31 00:05:07: [2024-10-31 00:05:07] iter = 12040, loss = 1.8510
2024-10-31 00:05:10: [2024-10-31 00:05:10] iter = 12050, loss = 3.4895
2024-10-31 00:05:14: [2024-10-31 00:05:14] iter = 12060, loss = 2.5232
2024-10-31 00:05:17: [2024-10-31 00:05:17] iter = 12070, loss = 2.2297
2024-10-31 00:05:20: [2024-10-31 00:05:20] iter = 12080, loss = 2.1503
2024-10-31 00:05:24: [2024-10-31 00:05:24] iter = 12090, loss = 4.7412
2024-10-31 00:05:27: [2024-10-31 00:05:27] iter = 12100, loss = 6.0925
2024-10-31 00:05:30: [2024-10-31 00:05:30] iter = 12110, loss = 3.2349
2024-10-31 00:05:34: [2024-10-31 00:05:34] iter = 12120, loss = 2.2218
2024-10-31 00:05:37: [2024-10-31 00:05:37] iter = 12130, loss = 2.2930
2024-10-31 00:05:41: [2024-10-31 00:05:41] iter = 12140, loss = 2.6859
2024-10-31 00:05:44: [2024-10-31 00:05:44] iter = 12150, loss = 3.0062
2024-10-31 00:05:47: [2024-10-31 00:05:47] iter = 12160, loss = 2.1907
2024-10-31 00:05:51: [2024-10-31 00:05:51] iter = 12170, loss = 3.3392
2024-10-31 00:05:55: [2024-10-31 00:05:55] iter = 12180, loss = 2.4399
2024-10-31 00:05:58: [2024-10-31 00:05:58] iter = 12190, loss = 3.3902
2024-10-31 00:06:02: [2024-10-31 00:06:02] iter = 12200, loss = 2.1620
2024-10-31 00:06:05: [2024-10-31 00:06:05] iter = 12210, loss = 1.8919
2024-10-31 00:06:09: [2024-10-31 00:06:09] iter = 12220, loss = 2.4846
2024-10-31 00:06:13: [2024-10-31 00:06:13] iter = 12230, loss = 2.9328
2024-10-31 00:06:16: [2024-10-31 00:06:16] iter = 12240, loss = 1.9758
2024-10-31 00:06:19: [2024-10-31 00:06:19] iter = 12250, loss = 1.8685
2024-10-31 00:06:22: [2024-10-31 00:06:22] iter = 12260, loss = 2.1588
2024-10-31 00:06:25: [2024-10-31 00:06:25] iter = 12270, loss = 2.0423
2024-10-31 00:06:28: [2024-10-31 00:06:28] iter = 12280, loss = 1.9005
2024-10-31 00:06:31: [2024-10-31 00:06:31] iter = 12290, loss = 3.3989
2024-10-31 00:06:36: [2024-10-31 00:06:36] iter = 12300, loss = 2.3831
2024-10-31 00:06:40: [2024-10-31 00:06:40] iter = 12310, loss = 2.5440
2024-10-31 00:06:44: [2024-10-31 00:06:44] iter = 12320, loss = 2.0522
2024-10-31 00:06:47: [2024-10-31 00:06:47] iter = 12330, loss = 2.1667
2024-10-31 00:06:50: [2024-10-31 00:06:50] iter = 12340, loss = 2.5267
2024-10-31 00:06:54: [2024-10-31 00:06:54] iter = 12350, loss = 2.4085
2024-10-31 00:06:58: [2024-10-31 00:06:58] iter = 12360, loss = 3.3216
2024-10-31 00:07:01: [2024-10-31 00:07:01] iter = 12370, loss = 2.2755
2024-10-31 00:07:05: [2024-10-31 00:07:05] iter = 12380, loss = 3.2519
2024-10-31 00:07:09: [2024-10-31 00:07:09] iter = 12390, loss = 2.0615
2024-10-31 00:07:12: [2024-10-31 00:07:12] iter = 12400, loss = 2.2879
2024-10-31 00:07:16: [2024-10-31 00:07:16] iter = 12410, loss = 2.4135
2024-10-31 00:07:20: [2024-10-31 00:07:20] iter = 12420, loss = 2.9705
2024-10-31 00:07:23: [2024-10-31 00:07:23] iter = 12430, loss = 2.1787
2024-10-31 00:07:27: [2024-10-31 00:07:27] iter = 12440, loss = 1.8858
2024-10-31 00:07:30: [2024-10-31 00:07:30] iter = 12450, loss = 2.2959
2024-10-31 00:07:35: [2024-10-31 00:07:35] iter = 12460, loss = 1.8297
2024-10-31 00:07:38: [2024-10-31 00:07:38] iter = 12470, loss = 1.7354
2024-10-31 00:07:41: [2024-10-31 00:07:41] iter = 12480, loss = 2.2570
2024-10-31 00:07:44: [2024-10-31 00:07:44] iter = 12490, loss = 1.9119
2024-10-31 00:07:47: [2024-10-31 00:07:47] iter = 12500, loss = 2.3842
2024-10-31 00:07:51: [2024-10-31 00:07:51] iter = 12510, loss = 1.9091
2024-10-31 00:07:55: [2024-10-31 00:07:55] iter = 12520, loss = 3.9745
2024-10-31 00:07:58: [2024-10-31 00:07:58] iter = 12530, loss = 1.9716
2024-10-31 00:08:01: [2024-10-31 00:08:01] iter = 12540, loss = 2.1298
2024-10-31 00:08:06: [2024-10-31 00:08:06] iter = 12550, loss = 1.9945
2024-10-31 00:08:09: [2024-10-31 00:08:09] iter = 12560, loss = 2.2730
2024-10-31 00:08:12: [2024-10-31 00:08:12] iter = 12570, loss = 2.0225
2024-10-31 00:08:15: [2024-10-31 00:08:15] iter = 12580, loss = 1.7960
2024-10-31 00:08:18: [2024-10-31 00:08:18] iter = 12590, loss = 2.0225
2024-10-31 00:08:22: [2024-10-31 00:08:22] iter = 12600, loss = 4.6080
2024-10-31 00:08:26: [2024-10-31 00:08:26] iter = 12610, loss = 2.4247
2024-10-31 00:08:29: [2024-10-31 00:08:29] iter = 12620, loss = 3.7934
2024-10-31 00:08:33: [2024-10-31 00:08:33] iter = 12630, loss = 2.4233
2024-10-31 00:08:36: [2024-10-31 00:08:36] iter = 12640, loss = 1.9754
2024-10-31 00:08:40: [2024-10-31 00:08:40] iter = 12650, loss = 1.7265
2024-10-31 00:08:43: [2024-10-31 00:08:43] iter = 12660, loss = 1.7899
2024-10-31 00:08:47: [2024-10-31 00:08:47] iter = 12670, loss = 2.9922
2024-10-31 00:08:50: [2024-10-31 00:08:50] iter = 12680, loss = 3.2864
2024-10-31 00:08:54: [2024-10-31 00:08:54] iter = 12690, loss = 5.1009
2024-10-31 00:08:58: [2024-10-31 00:08:58] iter = 12700, loss = 2.4176
2024-10-31 00:09:01: [2024-10-31 00:09:01] iter = 12710, loss = 3.0235
2024-10-31 00:09:05: [2024-10-31 00:09:05] iter = 12720, loss = 2.3506
2024-10-31 00:09:09: [2024-10-31 00:09:09] iter = 12730, loss = 2.2864
2024-10-31 00:09:12: [2024-10-31 00:09:12] iter = 12740, loss = 1.8658
2024-10-31 00:09:15: [2024-10-31 00:09:15] iter = 12750, loss = 1.9674
2024-10-31 00:09:19: [2024-10-31 00:09:19] iter = 12760, loss = 2.3706
2024-10-31 00:09:23: [2024-10-31 00:09:23] iter = 12770, loss = 2.1171
2024-10-31 00:09:26: [2024-10-31 00:09:26] iter = 12780, loss = 2.2844
2024-10-31 00:09:30: [2024-10-31 00:09:30] iter = 12790, loss = 2.5002
2024-10-31 00:09:35: [2024-10-31 00:09:35] iter = 12800, loss = 2.3286
2024-10-31 00:09:38: [2024-10-31 00:09:38] iter = 12810, loss = 2.8400
2024-10-31 00:09:41: [2024-10-31 00:09:41] iter = 12820, loss = 1.9630
2024-10-31 00:09:45: [2024-10-31 00:09:45] iter = 12830, loss = 2.3216
2024-10-31 00:09:48: [2024-10-31 00:09:48] iter = 12840, loss = 1.8887
2024-10-31 00:09:52: [2024-10-31 00:09:52] iter = 12850, loss = 2.3211
2024-10-31 00:09:55: [2024-10-31 00:09:55] iter = 12860, loss = 3.8706
2024-10-31 00:09:57: [2024-10-31 00:09:57] iter = 12870, loss = 2.7405
2024-10-31 00:10:00: [2024-10-31 00:10:00] iter = 12880, loss = 1.8577
2024-10-31 00:10:04: [2024-10-31 00:10:04] iter = 12890, loss = 2.1374
2024-10-31 00:10:07: [2024-10-31 00:10:07] iter = 12900, loss = 2.2456
2024-10-31 00:10:11: [2024-10-31 00:10:11] iter = 12910, loss = 1.7729
2024-10-31 00:10:12: [2024-10-31 00:10:12] iter = 12920, loss = 2.0993
2024-10-31 00:10:15: [2024-10-31 00:10:15] iter = 12930, loss = 2.5987
2024-10-31 00:10:18: [2024-10-31 00:10:18] iter = 12940, loss = 2.1551
2024-10-31 00:10:22: [2024-10-31 00:10:22] iter = 12950, loss = 3.2679
2024-10-31 00:10:27: [2024-10-31 00:10:27] iter = 12960, loss = 1.8548
2024-10-31 00:10:31: [2024-10-31 00:10:31] iter = 12970, loss = 3.0697
2024-10-31 00:10:34: [2024-10-31 00:10:34] iter = 12980, loss = 3.3412
2024-10-31 00:10:37: [2024-10-31 00:10:37] iter = 12990, loss = 2.0083
2024-10-31 00:10:40: [2024-10-31 00:10:40] iter = 13000, loss = 2.4375
2024-10-31 00:10:42: [2024-10-31 00:10:42] iter = 13010, loss = 2.0455
2024-10-31 00:10:45: [2024-10-31 00:10:45] iter = 13020, loss = 1.8541
2024-10-31 00:10:48: [2024-10-31 00:10:48] iter = 13030, loss = 4.5692
2024-10-31 00:10:51: [2024-10-31 00:10:51] iter = 13040, loss = 2.0295
2024-10-31 00:10:55: [2024-10-31 00:10:55] iter = 13050, loss = 1.6755
2024-10-31 00:10:57: [2024-10-31 00:10:57] iter = 13060, loss = 1.9914
2024-10-31 00:11:01: [2024-10-31 00:11:01] iter = 13070, loss = 2.3376
2024-10-31 00:11:04: [2024-10-31 00:11:04] iter = 13080, loss = 1.8144
2024-10-31 00:11:08: [2024-10-31 00:11:08] iter = 13090, loss = 2.0150
2024-10-31 00:11:11: [2024-10-31 00:11:11] iter = 13100, loss = 2.0393
2024-10-31 00:11:15: [2024-10-31 00:11:15] iter = 13110, loss = 1.6952
2024-10-31 00:11:19: [2024-10-31 00:11:19] iter = 13120, loss = 1.8720
2024-10-31 00:11:23: [2024-10-31 00:11:23] iter = 13130, loss = 2.4356
2024-10-31 00:11:27: [2024-10-31 00:11:27] iter = 13140, loss = 2.1327
2024-10-31 00:11:30: [2024-10-31 00:11:30] iter = 13150, loss = 2.0956
2024-10-31 00:11:33: [2024-10-31 00:11:33] iter = 13160, loss = 2.8150
2024-10-31 00:11:37: [2024-10-31 00:11:37] iter = 13170, loss = 2.1852
2024-10-31 00:11:39: [2024-10-31 00:11:39] iter = 13180, loss = 2.8656
2024-10-31 00:11:43: [2024-10-31 00:11:43] iter = 13190, loss = 2.1259
2024-10-31 00:11:46: [2024-10-31 00:11:46] iter = 13200, loss = 2.6142
2024-10-31 00:11:50: [2024-10-31 00:11:50] iter = 13210, loss = 2.4240
2024-10-31 00:11:54: [2024-10-31 00:11:54] iter = 13220, loss = 2.6084
2024-10-31 00:11:57: [2024-10-31 00:11:57] iter = 13230, loss = 1.9925
2024-10-31 00:12:00: [2024-10-31 00:12:00] iter = 13240, loss = 2.1155
2024-10-31 00:12:03: [2024-10-31 00:12:03] iter = 13250, loss = 2.3043
2024-10-31 00:12:07: [2024-10-31 00:12:07] iter = 13260, loss = 1.8701
2024-10-31 00:12:10: [2024-10-31 00:12:10] iter = 13270, loss = 2.1455
2024-10-31 00:12:14: [2024-10-31 00:12:14] iter = 13280, loss = 2.0210
2024-10-31 00:12:18: [2024-10-31 00:12:18] iter = 13290, loss = 2.2298
2024-10-31 00:12:21: [2024-10-31 00:12:21] iter = 13300, loss = 3.6914
2024-10-31 00:12:26: [2024-10-31 00:12:26] iter = 13310, loss = 4.3908
2024-10-31 00:12:30: [2024-10-31 00:12:30] iter = 13320, loss = 2.5620
2024-10-31 00:12:34: [2024-10-31 00:12:33] iter = 13330, loss = 2.0220
2024-10-31 00:12:37: [2024-10-31 00:12:37] iter = 13340, loss = 3.1551
2024-10-31 00:12:40: [2024-10-31 00:12:40] iter = 13350, loss = 2.6586
2024-10-31 00:12:43: [2024-10-31 00:12:43] iter = 13360, loss = 2.0735
2024-10-31 00:12:47: [2024-10-31 00:12:47] iter = 13370, loss = 2.1527
2024-10-31 00:12:50: [2024-10-31 00:12:50] iter = 13380, loss = 2.9661
2024-10-31 00:12:54: [2024-10-31 00:12:54] iter = 13390, loss = 2.1556
2024-10-31 00:12:58: [2024-10-31 00:12:58] iter = 13400, loss = 2.2547
2024-10-31 00:13:01: [2024-10-31 00:13:01] iter = 13410, loss = 2.4956
2024-10-31 00:13:04: [2024-10-31 00:13:04] iter = 13420, loss = 2.9867
2024-10-31 00:13:08: [2024-10-31 00:13:08] iter = 13430, loss = 2.5905
2024-10-31 00:13:12: [2024-10-31 00:13:12] iter = 13440, loss = 1.9729
2024-10-31 00:13:15: [2024-10-31 00:13:15] iter = 13450, loss = 1.9185
2024-10-31 00:13:19: [2024-10-31 00:13:19] iter = 13460, loss = 2.3230
2024-10-31 00:13:23: [2024-10-31 00:13:23] iter = 13470, loss = 2.5075
2024-10-31 00:13:27: [2024-10-31 00:13:27] iter = 13480, loss = 2.0216
2024-10-31 00:13:30: [2024-10-31 00:13:30] iter = 13490, loss = 2.1614
2024-10-31 00:13:34: [2024-10-31 00:13:34] iter = 13500, loss = 3.0350
2024-10-31 00:13:37: [2024-10-31 00:13:37] iter = 13510, loss = 2.0520
2024-10-31 00:13:41: [2024-10-31 00:13:41] iter = 13520, loss = 3.1621
2024-10-31 00:13:45: [2024-10-31 00:13:45] iter = 13530, loss = 2.0199
2024-10-31 00:13:49: [2024-10-31 00:13:49] iter = 13540, loss = 1.7406
2024-10-31 00:13:52: [2024-10-31 00:13:52] iter = 13550, loss = 2.2881
2024-10-31 00:13:55: [2024-10-31 00:13:55] iter = 13560, loss = 2.1588
2024-10-31 00:13:59: [2024-10-31 00:13:59] iter = 13570, loss = 2.4025
2024-10-31 00:14:03: [2024-10-31 00:14:02] iter = 13580, loss = 2.9816
2024-10-31 00:14:06: [2024-10-31 00:14:06] iter = 13590, loss = 1.9367
2024-10-31 00:14:10: [2024-10-31 00:14:10] iter = 13600, loss = 1.8821
2024-10-31 00:14:14: [2024-10-31 00:14:14] iter = 13610, loss = 2.3598
2024-10-31 00:14:17: [2024-10-31 00:14:17] iter = 13620, loss = 2.1639
2024-10-31 00:14:20: [2024-10-31 00:14:20] iter = 13630, loss = 1.9634
2024-10-31 00:14:24: [2024-10-31 00:14:24] iter = 13640, loss = 2.7407
2024-10-31 00:14:28: [2024-10-31 00:14:28] iter = 13650, loss = 1.9664
2024-10-31 00:14:31: [2024-10-31 00:14:31] iter = 13660, loss = 1.8127
2024-10-31 00:14:35: [2024-10-31 00:14:35] iter = 13670, loss = 2.1053
2024-10-31 00:14:39: [2024-10-31 00:14:39] iter = 13680, loss = 2.1084
2024-10-31 00:14:42: [2024-10-31 00:14:42] iter = 13690, loss = 2.3317
2024-10-31 00:14:46: [2024-10-31 00:14:46] iter = 13700, loss = 1.7450
2024-10-31 00:14:50: [2024-10-31 00:14:50] iter = 13710, loss = 1.9520
2024-10-31 00:14:54: [2024-10-31 00:14:54] iter = 13720, loss = 3.6275
2024-10-31 00:14:58: [2024-10-31 00:14:58] iter = 13730, loss = 2.3748
2024-10-31 00:15:03: [2024-10-31 00:15:03] iter = 13740, loss = 1.8422
2024-10-31 00:15:07: [2024-10-31 00:15:07] iter = 13750, loss = 2.4524
2024-10-31 00:15:10: [2024-10-31 00:15:10] iter = 13760, loss = 2.0263
2024-10-31 00:15:14: [2024-10-31 00:15:14] iter = 13770, loss = 2.2320
2024-10-31 00:15:18: [2024-10-31 00:15:18] iter = 13780, loss = 3.0010
2024-10-31 00:15:22: [2024-10-31 00:15:22] iter = 13790, loss = 1.8915
2024-10-31 00:15:25: [2024-10-31 00:15:25] iter = 13800, loss = 2.2828
2024-10-31 00:15:28: [2024-10-31 00:15:28] iter = 13810, loss = 1.8673
2024-10-31 00:15:31: [2024-10-31 00:15:31] iter = 13820, loss = 1.9405
2024-10-31 00:15:34: [2024-10-31 00:15:34] iter = 13830, loss = 1.8073
2024-10-31 00:15:37: [2024-10-31 00:15:37] iter = 13840, loss = 2.6108
2024-10-31 00:15:40: [2024-10-31 00:15:40] iter = 13850, loss = 2.2520
2024-10-31 00:15:45: [2024-10-31 00:15:45] iter = 13860, loss = 1.8231
2024-10-31 00:15:48: [2024-10-31 00:15:48] iter = 13870, loss = 7.2463
2024-10-31 00:15:51: [2024-10-31 00:15:51] iter = 13880, loss = 2.0980
2024-10-31 00:15:55: [2024-10-31 00:15:55] iter = 13890, loss = 1.8957
2024-10-31 00:15:58: [2024-10-31 00:15:58] iter = 13900, loss = 2.3436
2024-10-31 00:16:01: [2024-10-31 00:16:01] iter = 13910, loss = 1.8542
2024-10-31 00:16:04: [2024-10-31 00:16:04] iter = 13920, loss = 3.2201
2024-10-31 00:16:08: [2024-10-31 00:16:08] iter = 13930, loss = 2.6838
2024-10-31 00:16:11: [2024-10-31 00:16:11] iter = 13940, loss = 1.9514
2024-10-31 00:16:14: [2024-10-31 00:16:14] iter = 13950, loss = 2.3707
2024-10-31 00:16:17: [2024-10-31 00:16:17] iter = 13960, loss = 1.9813
2024-10-31 00:16:20: [2024-10-31 00:16:20] iter = 13970, loss = 1.7855
2024-10-31 00:16:24: [2024-10-31 00:16:24] iter = 13980, loss = 2.6350
2024-10-31 00:16:27: [2024-10-31 00:16:27] iter = 13990, loss = 2.1354
2024-10-31 00:16:30: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
2024-10-31 00:16:30: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 00:16:30: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 90688}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:18:48: Evaluate 5 random ConvNet, ACCmean = 0.7794 ACCstd = 0.0022
-------------------------
2024-10-31 00:18:48: Evaluate 5 random ConvNet, SENmean = 0.7729 SENstd = 0.0014
-------------------------
2024-10-31 00:18:48: Evaluate 5 random ConvNet, SPEmean = 0.9779 SPEstd = 0.0002
-------------------------
2024-10-31 00:18:48: Evaluate 5 random ConvNet, F!mean = 0.7611 F!std = 0.0016
-------------------------
2024-10-31 00:18:48: Evaluate 5 random ConvNet, mean = 0.7794 std = 0.0022
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:18:49: [2024-10-31 00:18:49] iter = 14000, loss = 2.2305
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:18:52: [2024-10-31 00:18:52] iter = 14010, loss = 2.0646
2024-10-31 00:18:55: [2024-10-31 00:18:55] iter = 14020, loss = 2.9597
2024-10-31 00:18:58: [2024-10-31 00:18:58] iter = 14030, loss = 1.9256
2024-10-31 00:19:01: [2024-10-31 00:19:01] iter = 14040, loss = 3.3553
2024-10-31 00:19:04: [2024-10-31 00:19:04] iter = 14050, loss = 2.0688
2024-10-31 00:19:08: [2024-10-31 00:19:08] iter = 14060, loss = 2.4932
2024-10-31 00:19:12: [2024-10-31 00:19:12] iter = 14070, loss = 2.4660
2024-10-31 00:19:16: [2024-10-31 00:19:16] iter = 14080, loss = 2.1451
2024-10-31 00:19:19: [2024-10-31 00:19:19] iter = 14090, loss = 2.7992
2024-10-31 00:19:23: [2024-10-31 00:19:23] iter = 14100, loss = 2.1506
2024-10-31 00:19:26: [2024-10-31 00:19:26] iter = 14110, loss = 2.1703
2024-10-31 00:19:30: [2024-10-31 00:19:30] iter = 14120, loss = 2.4260
2024-10-31 00:19:34: [2024-10-31 00:19:34] iter = 14130, loss = 2.7307
2024-10-31 00:19:37: [2024-10-31 00:19:37] iter = 14140, loss = 1.8495
2024-10-31 00:19:40: [2024-10-31 00:19:40] iter = 14150, loss = 2.5975
2024-10-31 00:19:44: [2024-10-31 00:19:44] iter = 14160, loss = 3.4115
2024-10-31 00:19:48: [2024-10-31 00:19:48] iter = 14170, loss = 2.2141
2024-10-31 00:19:51: [2024-10-31 00:19:51] iter = 14180, loss = 2.5596
2024-10-31 00:19:55: [2024-10-31 00:19:55] iter = 14190, loss = 2.2508
2024-10-31 00:19:59: [2024-10-31 00:19:59] iter = 14200, loss = 2.0961
2024-10-31 00:20:03: [2024-10-31 00:20:03] iter = 14210, loss = 1.9593
2024-10-31 00:20:06: [2024-10-31 00:20:06] iter = 14220, loss = 1.9364
2024-10-31 00:20:10: [2024-10-31 00:20:10] iter = 14230, loss = 2.3006
2024-10-31 00:20:14: [2024-10-31 00:20:14] iter = 14240, loss = 2.4807
2024-10-31 00:20:18: [2024-10-31 00:20:18] iter = 14250, loss = 2.7057
2024-10-31 00:20:21: [2024-10-31 00:20:21] iter = 14260, loss = 2.0793
2024-10-31 00:20:25: [2024-10-31 00:20:25] iter = 14270, loss = 2.0176
2024-10-31 00:20:28: [2024-10-31 00:20:28] iter = 14280, loss = 2.2214
2024-10-31 00:20:32: [2024-10-31 00:20:32] iter = 14290, loss = 1.8287
2024-10-31 00:20:36: [2024-10-31 00:20:36] iter = 14300, loss = 1.9255
2024-10-31 00:20:40: [2024-10-31 00:20:40] iter = 14310, loss = 2.1018
2024-10-31 00:20:44: [2024-10-31 00:20:44] iter = 14320, loss = 1.9118
2024-10-31 00:20:47: [2024-10-31 00:20:47] iter = 14330, loss = 2.6673
2024-10-31 00:20:51: [2024-10-31 00:20:51] iter = 14340, loss = 2.5437
2024-10-31 00:20:54: [2024-10-31 00:20:54] iter = 14350, loss = 2.6588
2024-10-31 00:20:57: [2024-10-31 00:20:57] iter = 14360, loss = 2.2084
2024-10-31 00:21:00: [2024-10-31 00:21:00] iter = 14370, loss = 2.5159
2024-10-31 00:21:03: [2024-10-31 00:21:03] iter = 14380, loss = 2.6485
2024-10-31 00:21:06: [2024-10-31 00:21:06] iter = 14390, loss = 1.9702
2024-10-31 00:21:10: [2024-10-31 00:21:10] iter = 14400, loss = 4.1328
2024-10-31 00:21:14: [2024-10-31 00:21:14] iter = 14410, loss = 3.9721
2024-10-31 00:21:17: [2024-10-31 00:21:17] iter = 14420, loss = 2.4843
2024-10-31 00:21:19: [2024-10-31 00:21:19] iter = 14430, loss = 3.1884
2024-10-31 00:21:23: [2024-10-31 00:21:23] iter = 14440, loss = 2.9081
2024-10-31 00:21:26: [2024-10-31 00:21:26] iter = 14450, loss = 2.5511
2024-10-31 00:21:29: [2024-10-31 00:21:29] iter = 14460, loss = 2.9922
2024-10-31 00:21:33: [2024-10-31 00:21:33] iter = 14470, loss = 2.3166
2024-10-31 00:21:37: [2024-10-31 00:21:37] iter = 14480, loss = 2.3162
2024-10-31 00:21:41: [2024-10-31 00:21:41] iter = 14490, loss = 1.8596
2024-10-31 00:21:44: [2024-10-31 00:21:44] iter = 14500, loss = 2.0766
2024-10-31 00:21:48: [2024-10-31 00:21:48] iter = 14510, loss = 2.7899
2024-10-31 00:21:51: [2024-10-31 00:21:51] iter = 14520, loss = 1.9482
2024-10-31 00:21:54: [2024-10-31 00:21:54] iter = 14530, loss = 2.3333
2024-10-31 00:21:58: [2024-10-31 00:21:58] iter = 14540, loss = 2.2508
2024-10-31 00:22:01: [2024-10-31 00:22:01] iter = 14550, loss = 2.8714
2024-10-31 00:22:04: [2024-10-31 00:22:04] iter = 14560, loss = 2.4982
2024-10-31 00:22:07: [2024-10-31 00:22:07] iter = 14570, loss = 1.8661
2024-10-31 00:22:10: [2024-10-31 00:22:10] iter = 14580, loss = 2.1247
2024-10-31 00:22:13: [2024-10-31 00:22:13] iter = 14590, loss = 1.9527
2024-10-31 00:22:16: [2024-10-31 00:22:16] iter = 14600, loss = 1.9774
2024-10-31 00:22:20: [2024-10-31 00:22:20] iter = 14610, loss = 5.6668
2024-10-31 00:22:24: [2024-10-31 00:22:24] iter = 14620, loss = 1.9155
2024-10-31 00:22:28: [2024-10-31 00:22:28] iter = 14630, loss = 2.8753
2024-10-31 00:22:32: [2024-10-31 00:22:32] iter = 14640, loss = 2.9646
2024-10-31 00:22:36: [2024-10-31 00:22:36] iter = 14650, loss = 1.9265
2024-10-31 00:22:40: [2024-10-31 00:22:40] iter = 14660, loss = 1.9751
2024-10-31 00:22:43: [2024-10-31 00:22:43] iter = 14670, loss = 2.2288
2024-10-31 00:22:47: [2024-10-31 00:22:47] iter = 14680, loss = 2.1589
2024-10-31 00:22:50: [2024-10-31 00:22:50] iter = 14690, loss = 2.3064
2024-10-31 00:22:53: [2024-10-31 00:22:53] iter = 14700, loss = 2.4169
2024-10-31 00:22:56: [2024-10-31 00:22:56] iter = 14710, loss = 2.2462
2024-10-31 00:22:59: [2024-10-31 00:22:59] iter = 14720, loss = 2.8146
2024-10-31 00:23:02: [2024-10-31 00:23:02] iter = 14730, loss = 1.8526
2024-10-31 00:23:06: [2024-10-31 00:23:06] iter = 14740, loss = 2.3902
2024-10-31 00:23:10: [2024-10-31 00:23:10] iter = 14750, loss = 1.9300
2024-10-31 00:23:14: [2024-10-31 00:23:14] iter = 14760, loss = 1.9476
2024-10-31 00:23:18: [2024-10-31 00:23:18] iter = 14770, loss = 1.8189
2024-10-31 00:23:22: [2024-10-31 00:23:22] iter = 14780, loss = 2.0775
2024-10-31 00:23:25: [2024-10-31 00:23:25] iter = 14790, loss = 3.0023
2024-10-31 00:23:28: [2024-10-31 00:23:28] iter = 14800, loss = 9.4858
2024-10-31 00:23:32: [2024-10-31 00:23:32] iter = 14810, loss = 2.1536
2024-10-31 00:23:36: [2024-10-31 00:23:36] iter = 14820, loss = 2.0098
2024-10-31 00:23:40: [2024-10-31 00:23:40] iter = 14830, loss = 2.3537
2024-10-31 00:23:43: [2024-10-31 00:23:43] iter = 14840, loss = 1.9222
2024-10-31 00:23:46: [2024-10-31 00:23:46] iter = 14850, loss = 2.2017
2024-10-31 00:23:50: [2024-10-31 00:23:50] iter = 14860, loss = 2.1948
2024-10-31 00:23:55: [2024-10-31 00:23:55] iter = 14870, loss = 3.0730
2024-10-31 00:23:58: [2024-10-31 00:23:58] iter = 14880, loss = 1.9784
2024-10-31 00:24:01: [2024-10-31 00:24:01] iter = 14890, loss = 4.0524
2024-10-31 00:24:05: [2024-10-31 00:24:05] iter = 14900, loss = 2.0451
2024-10-31 00:24:08: [2024-10-31 00:24:08] iter = 14910, loss = 2.9639
2024-10-31 00:24:12: [2024-10-31 00:24:12] iter = 14920, loss = 2.5499
2024-10-31 00:24:16: [2024-10-31 00:24:16] iter = 14930, loss = 3.0816
2024-10-31 00:24:21: [2024-10-31 00:24:21] iter = 14940, loss = 2.0819
2024-10-31 00:24:24: [2024-10-31 00:24:24] iter = 14950, loss = 3.0932
2024-10-31 00:24:28: [2024-10-31 00:24:28] iter = 14960, loss = 2.7282
2024-10-31 00:24:30: [2024-10-31 00:24:30] iter = 14970, loss = 2.4648
2024-10-31 00:24:34: [2024-10-31 00:24:34] iter = 14980, loss = 2.2778
2024-10-31 00:24:38: [2024-10-31 00:24:38] iter = 14990, loss = 2.0955
2024-10-31 00:24:42: [2024-10-31 00:24:42] iter = 15000, loss = 2.7050
2024-10-31 00:24:45: [2024-10-31 00:24:45] iter = 15010, loss = 1.8918
2024-10-31 00:24:49: [2024-10-31 00:24:49] iter = 15020, loss = 2.2778
2024-10-31 00:24:53: [2024-10-31 00:24:53] iter = 15030, loss = 3.0502
2024-10-31 00:24:56: [2024-10-31 00:24:56] iter = 15040, loss = 2.1351
2024-10-31 00:25:00: [2024-10-31 00:25:00] iter = 15050, loss = 2.0684
2024-10-31 00:25:03: [2024-10-31 00:25:03] iter = 15060, loss = 1.8861
2024-10-31 00:25:07: [2024-10-31 00:25:07] iter = 15070, loss = 1.9160
2024-10-31 00:25:11: [2024-10-31 00:25:11] iter = 15080, loss = 1.7729
2024-10-31 00:25:14: [2024-10-31 00:25:14] iter = 15090, loss = 2.8799
2024-10-31 00:25:18: [2024-10-31 00:25:18] iter = 15100, loss = 2.1276
2024-10-31 00:25:21: [2024-10-31 00:25:21] iter = 15110, loss = 4.3299
2024-10-31 00:25:25: [2024-10-31 00:25:25] iter = 15120, loss = 2.1392
2024-10-31 00:25:29: [2024-10-31 00:25:29] iter = 15130, loss = 2.6415
2024-10-31 00:25:33: [2024-10-31 00:25:33] iter = 15140, loss = 1.7523
2024-10-31 00:25:37: [2024-10-31 00:25:37] iter = 15150, loss = 2.2362
2024-10-31 00:25:41: [2024-10-31 00:25:41] iter = 15160, loss = 2.1442
2024-10-31 00:25:44: [2024-10-31 00:25:44] iter = 15170, loss = 2.4026
2024-10-31 00:25:47: [2024-10-31 00:25:47] iter = 15180, loss = 3.1064
2024-10-31 00:25:51: [2024-10-31 00:25:51] iter = 15190, loss = 2.7728
2024-10-31 00:25:55: [2024-10-31 00:25:55] iter = 15200, loss = 2.1715
2024-10-31 00:25:59: [2024-10-31 00:25:59] iter = 15210, loss = 5.1494
2024-10-31 00:26:03: [2024-10-31 00:26:03] iter = 15220, loss = 2.1449
2024-10-31 00:26:06: [2024-10-31 00:26:06] iter = 15230, loss = 2.5634
2024-10-31 00:26:10: [2024-10-31 00:26:10] iter = 15240, loss = 1.8886
2024-10-31 00:26:13: [2024-10-31 00:26:13] iter = 15250, loss = 2.6327
2024-10-31 00:26:17: [2024-10-31 00:26:17] iter = 15260, loss = 1.9809
2024-10-31 00:26:19: [2024-10-31 00:26:19] iter = 15270, loss = 2.0643
2024-10-31 00:26:23: [2024-10-31 00:26:23] iter = 15280, loss = 2.1775
2024-10-31 00:26:26: [2024-10-31 00:26:26] iter = 15290, loss = 2.5214
2024-10-31 00:26:30: [2024-10-31 00:26:30] iter = 15300, loss = 1.8827
2024-10-31 00:26:33: [2024-10-31 00:26:33] iter = 15310, loss = 2.2809
2024-10-31 00:26:36: [2024-10-31 00:26:36] iter = 15320, loss = 3.1090
2024-10-31 00:26:40: [2024-10-31 00:26:40] iter = 15330, loss = 2.1833
2024-10-31 00:26:42: [2024-10-31 00:26:42] iter = 15340, loss = 2.7956
2024-10-31 00:26:45: [2024-10-31 00:26:45] iter = 15350, loss = 2.6166
2024-10-31 00:26:49: [2024-10-31 00:26:49] iter = 15360, loss = 1.7972
2024-10-31 00:26:52: [2024-10-31 00:26:52] iter = 15370, loss = 2.7099
2024-10-31 00:26:56: [2024-10-31 00:26:56] iter = 15380, loss = 1.7783
2024-10-31 00:26:59: [2024-10-31 00:26:59] iter = 15390, loss = 2.2392
2024-10-31 00:27:02: [2024-10-31 00:27:02] iter = 15400, loss = 2.0764
2024-10-31 00:27:06: [2024-10-31 00:27:06] iter = 15410, loss = 1.8575
2024-10-31 00:27:09: [2024-10-31 00:27:09] iter = 15420, loss = 1.8383
2024-10-31 00:27:12: [2024-10-31 00:27:12] iter = 15430, loss = 2.6355
2024-10-31 00:27:15: [2024-10-31 00:27:15] iter = 15440, loss = 2.0859
2024-10-31 00:27:19: [2024-10-31 00:27:19] iter = 15450, loss = 2.0630
2024-10-31 00:27:22: [2024-10-31 00:27:22] iter = 15460, loss = 2.3053
2024-10-31 00:27:26: [2024-10-31 00:27:26] iter = 15470, loss = 2.1265
2024-10-31 00:27:30: [2024-10-31 00:27:30] iter = 15480, loss = 5.3640
2024-10-31 00:27:33: [2024-10-31 00:27:33] iter = 15490, loss = 1.9415
2024-10-31 00:27:36: [2024-10-31 00:27:36] iter = 15500, loss = 2.0808
2024-10-31 00:27:39: [2024-10-31 00:27:39] iter = 15510, loss = 2.5345
2024-10-31 00:27:43: [2024-10-31 00:27:43] iter = 15520, loss = 1.9999
2024-10-31 00:27:46: [2024-10-31 00:27:46] iter = 15530, loss = 2.3671
2024-10-31 00:27:49: [2024-10-31 00:27:49] iter = 15540, loss = 1.6097
2024-10-31 00:27:52: [2024-10-31 00:27:52] iter = 15550, loss = 1.9495
2024-10-31 00:27:55: [2024-10-31 00:27:55] iter = 15560, loss = 2.8041
2024-10-31 00:27:58: [2024-10-31 00:27:58] iter = 15570, loss = 2.0313
2024-10-31 00:28:02: [2024-10-31 00:28:02] iter = 15580, loss = 1.9900
2024-10-31 00:28:06: [2024-10-31 00:28:06] iter = 15590, loss = 2.7354
2024-10-31 00:28:10: [2024-10-31 00:28:10] iter = 15600, loss = 5.0835
2024-10-31 00:28:14: [2024-10-31 00:28:14] iter = 15610, loss = 1.9921
2024-10-31 00:28:18: [2024-10-31 00:28:18] iter = 15620, loss = 2.3889
2024-10-31 00:28:21: [2024-10-31 00:28:21] iter = 15630, loss = 2.2306
2024-10-31 00:28:24: [2024-10-31 00:28:24] iter = 15640, loss = 1.8450
2024-10-31 00:28:28: [2024-10-31 00:28:28] iter = 15650, loss = 1.9912
2024-10-31 00:28:31: [2024-10-31 00:28:31] iter = 15660, loss = 2.1355
2024-10-31 00:28:36: [2024-10-31 00:28:36] iter = 15670, loss = 2.7600
2024-10-31 00:28:40: [2024-10-31 00:28:40] iter = 15680, loss = 2.2852
2024-10-31 00:28:43: [2024-10-31 00:28:43] iter = 15690, loss = 1.8790
2024-10-31 00:28:47: [2024-10-31 00:28:47] iter = 15700, loss = 2.3437
2024-10-31 00:28:51: [2024-10-31 00:28:51] iter = 15710, loss = 1.8339
2024-10-31 00:28:54: [2024-10-31 00:28:54] iter = 15720, loss = 2.7063
2024-10-31 00:28:58: [2024-10-31 00:28:58] iter = 15730, loss = 2.3498
2024-10-31 00:29:01: [2024-10-31 00:29:01] iter = 15740, loss = 3.6437
2024-10-31 00:29:05: [2024-10-31 00:29:05] iter = 15750, loss = 3.0568
2024-10-31 00:29:09: [2024-10-31 00:29:09] iter = 15760, loss = 1.9594
2024-10-31 00:29:13: [2024-10-31 00:29:13] iter = 15770, loss = 1.9314
2024-10-31 00:29:17: [2024-10-31 00:29:17] iter = 15780, loss = 2.7392
2024-10-31 00:29:20: [2024-10-31 00:29:20] iter = 15790, loss = 3.1366
2024-10-31 00:29:23: [2024-10-31 00:29:23] iter = 15800, loss = 2.4719
2024-10-31 00:29:27: [2024-10-31 00:29:27] iter = 15810, loss = 2.4522
2024-10-31 00:29:32: [2024-10-31 00:29:32] iter = 15820, loss = 2.2248
2024-10-31 00:29:35: [2024-10-31 00:29:35] iter = 15830, loss = 2.1825
2024-10-31 00:29:39: [2024-10-31 00:29:39] iter = 15840, loss = 2.3744
2024-10-31 00:29:42: [2024-10-31 00:29:42] iter = 15850, loss = 2.2048
2024-10-31 00:29:46: [2024-10-31 00:29:46] iter = 15860, loss = 3.5756
2024-10-31 00:29:50: [2024-10-31 00:29:50] iter = 15870, loss = 2.6145
2024-10-31 00:29:53: [2024-10-31 00:29:53] iter = 15880, loss = 2.2198
2024-10-31 00:29:56: [2024-10-31 00:29:56] iter = 15890, loss = 2.4281
2024-10-31 00:30:00: [2024-10-31 00:30:00] iter = 15900, loss = 2.1699
2024-10-31 00:30:03: [2024-10-31 00:30:03] iter = 15910, loss = 1.9214
2024-10-31 00:30:07: [2024-10-31 00:30:07] iter = 15920, loss = 2.6538
2024-10-31 00:30:10: [2024-10-31 00:30:10] iter = 15930, loss = 2.7064
2024-10-31 00:30:13: [2024-10-31 00:30:13] iter = 15940, loss = 2.6639
2024-10-31 00:30:18: [2024-10-31 00:30:18] iter = 15950, loss = 2.0210
2024-10-31 00:30:21: [2024-10-31 00:30:21] iter = 15960, loss = 2.6103
2024-10-31 00:30:25: [2024-10-31 00:30:25] iter = 15970, loss = 2.2784
2024-10-31 00:30:29: [2024-10-31 00:30:29] iter = 15980, loss = 1.8410
2024-10-31 00:30:33: [2024-10-31 00:30:33] iter = 15990, loss = 1.9525
2024-10-31 00:30:37: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
2024-10-31 00:30:37: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 00:30:37: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 37546}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:32:53: Evaluate 5 random ConvNet, ACCmean = 0.7833 ACCstd = 0.0034
-------------------------
2024-10-31 00:32:53: Evaluate 5 random ConvNet, SENmean = 0.7760 SENstd = 0.0033
-------------------------
2024-10-31 00:32:53: Evaluate 5 random ConvNet, SPEmean = 0.9783 SPEstd = 0.0003
-------------------------
2024-10-31 00:32:53: Evaluate 5 random ConvNet, F!mean = 0.7633 F!std = 0.0038
-------------------------
2024-10-31 00:32:53: Evaluate 5 random ConvNet, mean = 0.7833 std = 0.0034
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:32:54: [2024-10-31 00:32:54] iter = 16000, loss = 2.2932
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:32:57: [2024-10-31 00:32:57] iter = 16010, loss = 2.3342
2024-10-31 00:33:00: [2024-10-31 00:33:00] iter = 16020, loss = 2.4125
2024-10-31 00:33:03: [2024-10-31 00:33:03] iter = 16030, loss = 2.1098
2024-10-31 00:33:07: [2024-10-31 00:33:07] iter = 16040, loss = 2.0129
2024-10-31 00:33:10: [2024-10-31 00:33:10] iter = 16050, loss = 2.6620
2024-10-31 00:33:13: [2024-10-31 00:33:13] iter = 16060, loss = 2.6559
2024-10-31 00:33:16: [2024-10-31 00:33:16] iter = 16070, loss = 4.3230
2024-10-31 00:33:19: [2024-10-31 00:33:19] iter = 16080, loss = 2.0119
2024-10-31 00:33:23: [2024-10-31 00:33:23] iter = 16090, loss = 3.0595
2024-10-31 00:33:26: [2024-10-31 00:33:26] iter = 16100, loss = 2.9572
2024-10-31 00:33:30: [2024-10-31 00:33:30] iter = 16110, loss = 2.2568
2024-10-31 00:33:33: [2024-10-31 00:33:33] iter = 16120, loss = 3.0230
2024-10-31 00:33:37: [2024-10-31 00:33:37] iter = 16130, loss = 2.2247
2024-10-31 00:33:41: [2024-10-31 00:33:41] iter = 16140, loss = 1.7476
2024-10-31 00:33:44: [2024-10-31 00:33:44] iter = 16150, loss = 5.2461
2024-10-31 00:33:47: [2024-10-31 00:33:47] iter = 16160, loss = 2.5149
2024-10-31 00:33:50: [2024-10-31 00:33:50] iter = 16170, loss = 2.6078
2024-10-31 00:33:53: [2024-10-31 00:33:53] iter = 16180, loss = 2.7626
2024-10-31 00:33:56: [2024-10-31 00:33:56] iter = 16190, loss = 2.5881
2024-10-31 00:33:59: [2024-10-31 00:33:59] iter = 16200, loss = 6.3326
2024-10-31 00:34:03: [2024-10-31 00:34:03] iter = 16210, loss = 2.1653
2024-10-31 00:34:06: [2024-10-31 00:34:06] iter = 16220, loss = 2.1451
2024-10-31 00:34:10: [2024-10-31 00:34:10] iter = 16230, loss = 2.0656
2024-10-31 00:34:13: [2024-10-31 00:34:13] iter = 16240, loss = 2.2444
2024-10-31 00:34:17: [2024-10-31 00:34:17] iter = 16250, loss = 2.3737
2024-10-31 00:34:20: [2024-10-31 00:34:20] iter = 16260, loss = 2.0654
2024-10-31 00:34:24: [2024-10-31 00:34:24] iter = 16270, loss = 2.3022
2024-10-31 00:34:27: [2024-10-31 00:34:27] iter = 16280, loss = 3.3681
2024-10-31 00:34:31: [2024-10-31 00:34:31] iter = 16290, loss = 2.0094
2024-10-31 00:34:35: [2024-10-31 00:34:35] iter = 16300, loss = 2.4347
2024-10-31 00:34:38: [2024-10-31 00:34:38] iter = 16310, loss = 2.0855
2024-10-31 00:34:42: [2024-10-31 00:34:42] iter = 16320, loss = 2.6931
2024-10-31 00:34:45: [2024-10-31 00:34:45] iter = 16330, loss = 3.0266
2024-10-31 00:34:49: [2024-10-31 00:34:49] iter = 16340, loss = 2.3623
2024-10-31 00:34:53: [2024-10-31 00:34:53] iter = 16350, loss = 2.4661
2024-10-31 00:34:57: [2024-10-31 00:34:57] iter = 16360, loss = 3.4720
2024-10-31 00:35:00: [2024-10-31 00:35:00] iter = 16370, loss = 2.3240
2024-10-31 00:35:04: [2024-10-31 00:35:04] iter = 16380, loss = 4.8435
2024-10-31 00:35:08: [2024-10-31 00:35:08] iter = 16390, loss = 2.1004
2024-10-31 00:35:11: [2024-10-31 00:35:11] iter = 16400, loss = 2.1532
2024-10-31 00:35:14: [2024-10-31 00:35:14] iter = 16410, loss = 1.8544
2024-10-31 00:35:17: [2024-10-31 00:35:17] iter = 16420, loss = 1.7668
2024-10-31 00:35:20: [2024-10-31 00:35:20] iter = 16430, loss = 2.3150
2024-10-31 00:35:23: [2024-10-31 00:35:23] iter = 16440, loss = 2.0995
2024-10-31 00:35:27: [2024-10-31 00:35:27] iter = 16450, loss = 2.0078
2024-10-31 00:35:30: [2024-10-31 00:35:30] iter = 16460, loss = 2.8963
2024-10-31 00:35:35: [2024-10-31 00:35:35] iter = 16470, loss = 2.3544
2024-10-31 00:35:38: [2024-10-31 00:35:38] iter = 16480, loss = 2.4043
2024-10-31 00:35:42: [2024-10-31 00:35:42] iter = 16490, loss = 1.7994
2024-10-31 00:35:46: [2024-10-31 00:35:46] iter = 16500, loss = 2.5070
2024-10-31 00:35:50: [2024-10-31 00:35:50] iter = 16510, loss = 2.2576
2024-10-31 00:35:53: [2024-10-31 00:35:53] iter = 16520, loss = 2.2756
2024-10-31 00:35:57: [2024-10-31 00:35:57] iter = 16530, loss = 2.6387
2024-10-31 00:36:00: [2024-10-31 00:36:00] iter = 16540, loss = 2.1817
2024-10-31 00:36:04: [2024-10-31 00:36:04] iter = 16550, loss = 3.1226
2024-10-31 00:36:07: [2024-10-31 00:36:07] iter = 16560, loss = 2.2932
2024-10-31 00:36:11: [2024-10-31 00:36:11] iter = 16570, loss = 2.3406
2024-10-31 00:36:15: [2024-10-31 00:36:15] iter = 16580, loss = 2.1048
2024-10-31 00:36:18: [2024-10-31 00:36:18] iter = 16590, loss = 1.8683
2024-10-31 00:36:22: [2024-10-31 00:36:22] iter = 16600, loss = 1.8935
2024-10-31 00:36:26: [2024-10-31 00:36:26] iter = 16610, loss = 2.6870
2024-10-31 00:36:30: [2024-10-31 00:36:30] iter = 16620, loss = 2.3630
2024-10-31 00:36:33: [2024-10-31 00:36:33] iter = 16630, loss = 1.9636
2024-10-31 00:36:37: [2024-10-31 00:36:37] iter = 16640, loss = 2.0645
2024-10-31 00:36:41: [2024-10-31 00:36:41] iter = 16650, loss = 2.2477
2024-10-31 00:36:45: [2024-10-31 00:36:45] iter = 16660, loss = 2.0999
2024-10-31 00:36:48: [2024-10-31 00:36:48] iter = 16670, loss = 2.1611
2024-10-31 00:36:52: [2024-10-31 00:36:52] iter = 16680, loss = 1.9667
2024-10-31 00:36:55: [2024-10-31 00:36:55] iter = 16690, loss = 2.4907
2024-10-31 00:36:59: [2024-10-31 00:36:59] iter = 16700, loss = 3.2082
2024-10-31 00:37:03: [2024-10-31 00:37:03] iter = 16710, loss = 2.7486
2024-10-31 00:37:06: [2024-10-31 00:37:06] iter = 16720, loss = 3.2371
2024-10-31 00:37:11: [2024-10-31 00:37:11] iter = 16730, loss = 1.9001
2024-10-31 00:37:13: [2024-10-31 00:37:13] iter = 16740, loss = 2.6894
2024-10-31 00:37:17: [2024-10-31 00:37:17] iter = 16750, loss = 3.9076
2024-10-31 00:37:21: [2024-10-31 00:37:21] iter = 16760, loss = 2.2921
2024-10-31 00:37:25: [2024-10-31 00:37:25] iter = 16770, loss = 2.3434
2024-10-31 00:37:28: [2024-10-31 00:37:28] iter = 16780, loss = 2.2493
2024-10-31 00:37:30: [2024-10-31 00:37:30] iter = 16790, loss = 2.4565
2024-10-31 00:37:34: [2024-10-31 00:37:34] iter = 16800, loss = 1.6648
2024-10-31 00:37:38: [2024-10-31 00:37:38] iter = 16810, loss = 2.3609
2024-10-31 00:37:42: [2024-10-31 00:37:42] iter = 16820, loss = 3.3671
2024-10-31 00:37:45: [2024-10-31 00:37:45] iter = 16830, loss = 2.6749
2024-10-31 00:37:50: [2024-10-31 00:37:50] iter = 16840, loss = 2.3000
2024-10-31 00:37:53: [2024-10-31 00:37:53] iter = 16850, loss = 2.3996
2024-10-31 00:37:57: [2024-10-31 00:37:57] iter = 16860, loss = 2.1338
2024-10-31 00:38:02: [2024-10-31 00:38:02] iter = 16870, loss = 2.4043
2024-10-31 00:38:06: [2024-10-31 00:38:06] iter = 16880, loss = 2.1467
2024-10-31 00:38:09: [2024-10-31 00:38:09] iter = 16890, loss = 2.8578
2024-10-31 00:38:12: [2024-10-31 00:38:12] iter = 16900, loss = 2.0432
2024-10-31 00:38:15: [2024-10-31 00:38:15] iter = 16910, loss = 2.0389
2024-10-31 00:38:18: [2024-10-31 00:38:18] iter = 16920, loss = 2.5841
2024-10-31 00:38:21: [2024-10-31 00:38:21] iter = 16930, loss = 2.1673
2024-10-31 00:38:25: [2024-10-31 00:38:25] iter = 16940, loss = 2.7970
2024-10-31 00:38:29: [2024-10-31 00:38:29] iter = 16950, loss = 1.9634
2024-10-31 00:38:32: [2024-10-31 00:38:32] iter = 16960, loss = 1.8420
2024-10-31 00:38:36: [2024-10-31 00:38:36] iter = 16970, loss = 2.0506
2024-10-31 00:38:39: [2024-10-31 00:38:39] iter = 16980, loss = 2.8762
2024-10-31 00:38:42: [2024-10-31 00:38:42] iter = 16990, loss = 2.4190
2024-10-31 00:38:45: [2024-10-31 00:38:45] iter = 17000, loss = 2.3353
2024-10-31 00:38:48: [2024-10-31 00:38:48] iter = 17010, loss = 2.6762
2024-10-31 00:38:51: [2024-10-31 00:38:51] iter = 17020, loss = 2.7702
2024-10-31 00:38:55: [2024-10-31 00:38:55] iter = 17030, loss = 2.2405
2024-10-31 00:38:58: [2024-10-31 00:38:58] iter = 17040, loss = 1.7873
2024-10-31 00:39:02: [2024-10-31 00:39:02] iter = 17050, loss = 2.8149
2024-10-31 00:39:05: [2024-10-31 00:39:05] iter = 17060, loss = 1.6848
2024-10-31 00:39:08: [2024-10-31 00:39:08] iter = 17070, loss = 2.3270
2024-10-31 00:39:12: [2024-10-31 00:39:12] iter = 17080, loss = 3.2502
2024-10-31 00:39:16: [2024-10-31 00:39:16] iter = 17090, loss = 2.0082
2024-10-31 00:39:20: [2024-10-31 00:39:20] iter = 17100, loss = 1.8210
2024-10-31 00:39:24: [2024-10-31 00:39:24] iter = 17110, loss = 1.8288
2024-10-31 00:39:27: [2024-10-31 00:39:27] iter = 17120, loss = 2.2701
2024-10-31 00:39:31: [2024-10-31 00:39:31] iter = 17130, loss = 1.9402
2024-10-31 00:39:35: [2024-10-31 00:39:35] iter = 17140, loss = 1.9284
2024-10-31 00:39:39: [2024-10-31 00:39:39] iter = 17150, loss = 3.0204
2024-10-31 00:39:41: [2024-10-31 00:39:41] iter = 17160, loss = 2.7370
2024-10-31 00:39:45: [2024-10-31 00:39:45] iter = 17170, loss = 2.3309
2024-10-31 00:39:48: [2024-10-31 00:39:48] iter = 17180, loss = 2.1399
2024-10-31 00:39:53: [2024-10-31 00:39:53] iter = 17190, loss = 2.1429
2024-10-31 00:39:57: [2024-10-31 00:39:57] iter = 17200, loss = 2.2947
2024-10-31 00:40:01: [2024-10-31 00:40:01] iter = 17210, loss = 2.1329
2024-10-31 00:40:04: [2024-10-31 00:40:04] iter = 17220, loss = 1.9840
2024-10-31 00:40:08: [2024-10-31 00:40:08] iter = 17230, loss = 1.9780
2024-10-31 00:40:12: [2024-10-31 00:40:12] iter = 17240, loss = 3.5924
2024-10-31 00:40:15: [2024-10-31 00:40:15] iter = 17250, loss = 2.5010
2024-10-31 00:40:18: [2024-10-31 00:40:18] iter = 17260, loss = 3.4682
2024-10-31 00:40:21: [2024-10-31 00:40:21] iter = 17270, loss = 2.4367
2024-10-31 00:40:25: [2024-10-31 00:40:24] iter = 17280, loss = 1.8880
2024-10-31 00:40:28: [2024-10-31 00:40:28] iter = 17290, loss = 2.2845
2024-10-31 00:40:31: [2024-10-31 00:40:31] iter = 17300, loss = 1.8998
2024-10-31 00:40:34: [2024-10-31 00:40:34] iter = 17310, loss = 2.1063
2024-10-31 00:40:37: [2024-10-31 00:40:37] iter = 17320, loss = 3.0571
2024-10-31 00:40:39: [2024-10-31 00:40:39] iter = 17330, loss = 2.0100
2024-10-31 00:40:44: [2024-10-31 00:40:44] iter = 17340, loss = 1.9874
2024-10-31 00:40:46: [2024-10-31 00:40:46] iter = 17350, loss = 2.8224
2024-10-31 00:40:49: [2024-10-31 00:40:49] iter = 17360, loss = 1.6870
2024-10-31 00:40:53: [2024-10-31 00:40:53] iter = 17370, loss = 2.5303
2024-10-31 00:40:56: [2024-10-31 00:40:56] iter = 17380, loss = 3.1717
2024-10-31 00:40:59: [2024-10-31 00:40:59] iter = 17390, loss = 1.8110
2024-10-31 00:41:02: [2024-10-31 00:41:02] iter = 17400, loss = 1.7649
2024-10-31 00:41:05: [2024-10-31 00:41:05] iter = 17410, loss = 5.3609
2024-10-31 00:41:09: [2024-10-31 00:41:09] iter = 17420, loss = 2.0145
2024-10-31 00:41:13: [2024-10-31 00:41:12] iter = 17430, loss = 1.9674
2024-10-31 00:41:17: [2024-10-31 00:41:17] iter = 17440, loss = 1.6715
2024-10-31 00:41:19: [2024-10-31 00:41:19] iter = 17450, loss = 2.4802
2024-10-31 00:41:21: [2024-10-31 00:41:21] iter = 17460, loss = 2.1084
2024-10-31 00:41:25: [2024-10-31 00:41:25] iter = 17470, loss = 2.7787
2024-10-31 00:41:28: [2024-10-31 00:41:28] iter = 17480, loss = 3.2401
2024-10-31 00:41:32: [2024-10-31 00:41:32] iter = 17490, loss = 3.0574
2024-10-31 00:41:36: [2024-10-31 00:41:36] iter = 17500, loss = 2.4670
2024-10-31 00:41:39: [2024-10-31 00:41:39] iter = 17510, loss = 2.0913
2024-10-31 00:41:43: [2024-10-31 00:41:43] iter = 17520, loss = 2.2926
2024-10-31 00:41:46: [2024-10-31 00:41:46] iter = 17530, loss = 1.8007
2024-10-31 00:41:49: [2024-10-31 00:41:49] iter = 17540, loss = 2.3892
2024-10-31 00:41:53: [2024-10-31 00:41:53] iter = 17550, loss = 2.1268
2024-10-31 00:41:56: [2024-10-31 00:41:56] iter = 17560, loss = 1.9098
2024-10-31 00:41:59: [2024-10-31 00:41:59] iter = 17570, loss = 2.2043
2024-10-31 00:42:02: [2024-10-31 00:42:02] iter = 17580, loss = 1.9299
2024-10-31 00:42:05: [2024-10-31 00:42:05] iter = 17590, loss = 1.8248
2024-10-31 00:42:08: [2024-10-31 00:42:08] iter = 17600, loss = 2.6252
2024-10-31 00:42:12: [2024-10-31 00:42:12] iter = 17610, loss = 1.8760
2024-10-31 00:42:14: [2024-10-31 00:42:14] iter = 17620, loss = 2.1342
2024-10-31 00:42:16: [2024-10-31 00:42:16] iter = 17630, loss = 1.6671
2024-10-31 00:42:19: [2024-10-31 00:42:19] iter = 17640, loss = 2.5308
2024-10-31 00:42:23: [2024-10-31 00:42:23] iter = 17650, loss = 2.3131
2024-10-31 00:42:26: [2024-10-31 00:42:26] iter = 17660, loss = 1.7691
2024-10-31 00:42:29: [2024-10-31 00:42:29] iter = 17670, loss = 2.8221
2024-10-31 00:42:33: [2024-10-31 00:42:33] iter = 17680, loss = 1.9569
2024-10-31 00:42:36: [2024-10-31 00:42:36] iter = 17690, loss = 2.6323
2024-10-31 00:42:39: [2024-10-31 00:42:39] iter = 17700, loss = 2.0206
2024-10-31 00:42:43: [2024-10-31 00:42:43] iter = 17710, loss = 1.9175
2024-10-31 00:42:46: [2024-10-31 00:42:46] iter = 17720, loss = 2.0903
2024-10-31 00:42:49: [2024-10-31 00:42:49] iter = 17730, loss = 2.5380
2024-10-31 00:42:53: [2024-10-31 00:42:53] iter = 17740, loss = 1.9677
2024-10-31 00:42:56: [2024-10-31 00:42:56] iter = 17750, loss = 2.6082
2024-10-31 00:42:59: [2024-10-31 00:42:59] iter = 17760, loss = 1.6488
2024-10-31 00:43:02: [2024-10-31 00:43:02] iter = 17770, loss = 2.4371
2024-10-31 00:43:04: [2024-10-31 00:43:04] iter = 17780, loss = 3.2310
2024-10-31 00:43:08: [2024-10-31 00:43:08] iter = 17790, loss = 2.5013
2024-10-31 00:43:12: [2024-10-31 00:43:12] iter = 17800, loss = 2.1318
2024-10-31 00:43:16: [2024-10-31 00:43:16] iter = 17810, loss = 2.5220
2024-10-31 00:43:19: [2024-10-31 00:43:19] iter = 17820, loss = 2.1437
2024-10-31 00:43:24: [2024-10-31 00:43:24] iter = 17830, loss = 2.2749
2024-10-31 00:43:27: [2024-10-31 00:43:27] iter = 17840, loss = 2.2787
2024-10-31 00:43:30: [2024-10-31 00:43:30] iter = 17850, loss = 3.1608
2024-10-31 00:43:33: [2024-10-31 00:43:33] iter = 17860, loss = 1.9048
2024-10-31 00:43:37: [2024-10-31 00:43:37] iter = 17870, loss = 3.0859
2024-10-31 00:43:40: [2024-10-31 00:43:40] iter = 17880, loss = 2.0559
2024-10-31 00:43:43: [2024-10-31 00:43:43] iter = 17890, loss = 2.2571
2024-10-31 00:43:46: [2024-10-31 00:43:46] iter = 17900, loss = 2.0790
2024-10-31 00:43:49: [2024-10-31 00:43:49] iter = 17910, loss = 2.5099
2024-10-31 00:43:52: [2024-10-31 00:43:52] iter = 17920, loss = 2.2432
2024-10-31 00:43:56: [2024-10-31 00:43:56] iter = 17930, loss = 2.6382
2024-10-31 00:43:59: [2024-10-31 00:43:59] iter = 17940, loss = 1.9350
2024-10-31 00:44:03: [2024-10-31 00:44:03] iter = 17950, loss = 1.9522
2024-10-31 00:44:07: [2024-10-31 00:44:07] iter = 17960, loss = 1.9299
2024-10-31 00:44:10: [2024-10-31 00:44:10] iter = 17970, loss = 2.2005
2024-10-31 00:44:13: [2024-10-31 00:44:13] iter = 17980, loss = 2.9077
2024-10-31 00:44:17: [2024-10-31 00:44:17] iter = 17990, loss = 2.0223
2024-10-31 00:44:20: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
2024-10-31 00:44:20: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 00:44:20: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 60486}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:46:34: Evaluate 5 random ConvNet, ACCmean = 0.7813 ACCstd = 0.0019
-------------------------
2024-10-31 00:46:34: Evaluate 5 random ConvNet, SENmean = 0.7786 SENstd = 0.0013
-------------------------
2024-10-31 00:46:34: Evaluate 5 random ConvNet, SPEmean = 0.9781 SPEstd = 0.0002
-------------------------
2024-10-31 00:46:34: Evaluate 5 random ConvNet, F!mean = 0.7657 F!std = 0.0013
-------------------------
2024-10-31 00:46:34: Evaluate 5 random ConvNet, mean = 0.7813 std = 0.0019
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 00:46:35: [2024-10-31 00:46:35] iter = 18000, loss = 2.0644
2024-10-31 00:46:39: [2024-10-31 00:46:39] iter = 18010, loss = 1.9944
2024-10-31 00:46:43: [2024-10-31 00:46:43] iter = 18020, loss = 2.1110
2024-10-31 00:46:46: [2024-10-31 00:46:46] iter = 18030, loss = 2.1772
2024-10-31 00:46:50: [2024-10-31 00:46:50] iter = 18040, loss = 2.0644
2024-10-31 00:46:53: [2024-10-31 00:46:53] iter = 18050, loss = 2.6516
2024-10-31 00:46:56: [2024-10-31 00:46:56] iter = 18060, loss = 3.6707
2024-10-31 00:46:59: [2024-10-31 00:46:59] iter = 18070, loss = 3.6212
2024-10-31 00:47:03: [2024-10-31 00:47:03] iter = 18080, loss = 3.6615
2024-10-31 00:47:07: [2024-10-31 00:47:07] iter = 18090, loss = 2.4327
2024-10-31 00:47:11: [2024-10-31 00:47:11] iter = 18100, loss = 2.0478
2024-10-31 00:47:14: [2024-10-31 00:47:14] iter = 18110, loss = 2.9292
2024-10-31 00:47:16: [2024-10-31 00:47:16] iter = 18120, loss = 2.5317
2024-10-31 00:47:19: [2024-10-31 00:47:19] iter = 18130, loss = 1.8538
2024-10-31 00:47:23: [2024-10-31 00:47:23] iter = 18140, loss = 2.0061
2024-10-31 00:47:26: [2024-10-31 00:47:26] iter = 18150, loss = 2.4299
2024-10-31 00:47:29: [2024-10-31 00:47:29] iter = 18160, loss = 2.1145
2024-10-31 00:47:33: [2024-10-31 00:47:33] iter = 18170, loss = 2.2376
2024-10-31 00:47:36: [2024-10-31 00:47:36] iter = 18180, loss = 2.0196
2024-10-31 00:47:39: [2024-10-31 00:47:39] iter = 18190, loss = 3.3760
2024-10-31 00:47:42: [2024-10-31 00:47:42] iter = 18200, loss = 2.0827
2024-10-31 00:47:45: [2024-10-31 00:47:45] iter = 18210, loss = 1.8504
2024-10-31 00:47:49: [2024-10-31 00:47:49] iter = 18220, loss = 2.0134
2024-10-31 00:47:52: [2024-10-31 00:47:52] iter = 18230, loss = 2.5564
2024-10-31 00:47:56: [2024-10-31 00:47:56] iter = 18240, loss = 2.4655
2024-10-31 00:47:59: [2024-10-31 00:47:59] iter = 18250, loss = 2.6829
2024-10-31 00:48:02: [2024-10-31 00:48:02] iter = 18260, loss = 1.6832
2024-10-31 00:48:06: [2024-10-31 00:48:06] iter = 18270, loss = 2.0632
2024-10-31 00:48:09: [2024-10-31 00:48:09] iter = 18280, loss = 1.7189
2024-10-31 00:48:12: [2024-10-31 00:48:12] iter = 18290, loss = 1.8399
2024-10-31 00:48:16: [2024-10-31 00:48:16] iter = 18300, loss = 1.9292
2024-10-31 00:48:20: [2024-10-31 00:48:20] iter = 18310, loss = 2.1381
2024-10-31 00:48:23: [2024-10-31 00:48:23] iter = 18320, loss = 2.5206
2024-10-31 00:48:27: [2024-10-31 00:48:27] iter = 18330, loss = 2.5958
2024-10-31 00:48:30: [2024-10-31 00:48:30] iter = 18340, loss = 1.8486
2024-10-31 00:48:34: [2024-10-31 00:48:34] iter = 18350, loss = 1.7881
2024-10-31 00:48:38: [2024-10-31 00:48:38] iter = 18360, loss = 2.4424
2024-10-31 00:48:41: [2024-10-31 00:48:41] iter = 18370, loss = 2.2233
2024-10-31 00:48:44: [2024-10-31 00:48:44] iter = 18380, loss = 2.3040
2024-10-31 00:48:48: [2024-10-31 00:48:48] iter = 18390, loss = 1.8190
2024-10-31 00:48:51: [2024-10-31 00:48:51] iter = 18400, loss = 1.8350
2024-10-31 00:48:54: [2024-10-31 00:48:54] iter = 18410, loss = 2.1353
2024-10-31 00:48:57: [2024-10-31 00:48:57] iter = 18420, loss = 2.1882
2024-10-31 00:49:00: [2024-10-31 00:49:00] iter = 18430, loss = 2.1414
2024-10-31 00:49:04: [2024-10-31 00:49:04] iter = 18440, loss = 1.9383
2024-10-31 00:49:06: [2024-10-31 00:49:06] iter = 18450, loss = 1.8494
2024-10-31 00:49:08: [2024-10-31 00:49:08] iter = 18460, loss = 2.3312
2024-10-31 00:49:11: [2024-10-31 00:49:11] iter = 18470, loss = 2.3139
2024-10-31 00:49:14: [2024-10-31 00:49:14] iter = 18480, loss = 2.8295
2024-10-31 00:49:18: [2024-10-31 00:49:18] iter = 18490, loss = 2.9847
2024-10-31 00:49:21: [2024-10-31 00:49:21] iter = 18500, loss = 1.8269
2024-10-31 00:49:26: [2024-10-31 00:49:25] iter = 18510, loss = 1.9561
2024-10-31 00:49:29: [2024-10-31 00:49:29] iter = 18520, loss = 2.1745
2024-10-31 00:49:33: [2024-10-31 00:49:33] iter = 18530, loss = 2.0272
2024-10-31 00:49:36: [2024-10-31 00:49:36] iter = 18540, loss = 1.9452
2024-10-31 00:49:39: [2024-10-31 00:49:39] iter = 18550, loss = 2.1233
2024-10-31 00:49:42: [2024-10-31 00:49:42] iter = 18560, loss = 2.0356
2024-10-31 00:49:45: [2024-10-31 00:49:45] iter = 18570, loss = 3.5463
2024-10-31 00:49:48: [2024-10-31 00:49:48] iter = 18580, loss = 2.4318
2024-10-31 00:49:51: [2024-10-31 00:49:51] iter = 18590, loss = 2.1456
2024-10-31 00:49:54: [2024-10-31 00:49:54] iter = 18600, loss = 2.2567
2024-10-31 00:49:57: [2024-10-31 00:49:57] iter = 18610, loss = 1.9644
2024-10-31 00:50:00: [2024-10-31 00:50:00] iter = 18620, loss = 1.8957
2024-10-31 00:50:03: [2024-10-31 00:50:03] iter = 18630, loss = 3.0591
2024-10-31 00:50:07: [2024-10-31 00:50:07] iter = 18640, loss = 2.6247
2024-10-31 00:50:10: [2024-10-31 00:50:10] iter = 18650, loss = 2.5667
2024-10-31 00:50:13: [2024-10-31 00:50:13] iter = 18660, loss = 2.5567
2024-10-31 00:50:16: [2024-10-31 00:50:16] iter = 18670, loss = 2.0667
2024-10-31 00:50:19: [2024-10-31 00:50:19] iter = 18680, loss = 2.1930
2024-10-31 00:50:23: [2024-10-31 00:50:23] iter = 18690, loss = 2.0193
2024-10-31 00:50:27: [2024-10-31 00:50:27] iter = 18700, loss = 2.3236
2024-10-31 00:50:30: [2024-10-31 00:50:30] iter = 18710, loss = 2.4534
2024-10-31 00:50:33: [2024-10-31 00:50:33] iter = 18720, loss = 2.4121
2024-10-31 00:50:36: [2024-10-31 00:50:36] iter = 18730, loss = 1.9699
2024-10-31 00:50:39: [2024-10-31 00:50:39] iter = 18740, loss = 1.9100
2024-10-31 00:50:42: [2024-10-31 00:50:42] iter = 18750, loss = 2.2297
2024-10-31 00:50:45: [2024-10-31 00:50:45] iter = 18760, loss = 2.0766
2024-10-31 00:50:49: [2024-10-31 00:50:49] iter = 18770, loss = 2.0434
2024-10-31 00:50:52: [2024-10-31 00:50:52] iter = 18780, loss = 2.3417
2024-10-31 00:50:55: [2024-10-31 00:50:55] iter = 18790, loss = 2.1489
2024-10-31 00:50:58: [2024-10-31 00:50:58] iter = 18800, loss = 1.9957
2024-10-31 00:51:02: [2024-10-31 00:51:02] iter = 18810, loss = 4.0294
2024-10-31 00:51:05: [2024-10-31 00:51:05] iter = 18820, loss = 1.7766
2024-10-31 00:51:08: [2024-10-31 00:51:08] iter = 18830, loss = 1.7977
2024-10-31 00:51:11: [2024-10-31 00:51:11] iter = 18840, loss = 3.3447
2024-10-31 00:51:15: [2024-10-31 00:51:15] iter = 18850, loss = 2.0593
2024-10-31 00:51:18: [2024-10-31 00:51:18] iter = 18860, loss = 2.2708
2024-10-31 00:51:21: [2024-10-31 00:51:21] iter = 18870, loss = 2.7264
2024-10-31 00:51:23: [2024-10-31 00:51:23] iter = 18880, loss = 2.8404
2024-10-31 00:51:27: [2024-10-31 00:51:27] iter = 18890, loss = 3.7140
2024-10-31 00:51:31: [2024-10-31 00:51:31] iter = 18900, loss = 1.8252
2024-10-31 00:51:34: [2024-10-31 00:51:34] iter = 18910, loss = 1.7569
2024-10-31 00:51:39: [2024-10-31 00:51:39] iter = 18920, loss = 2.1709
2024-10-31 00:51:42: [2024-10-31 00:51:42] iter = 18930, loss = 3.1768
2024-10-31 00:51:46: [2024-10-31 00:51:46] iter = 18940, loss = 1.9738
2024-10-31 00:51:49: [2024-10-31 00:51:49] iter = 18950, loss = 6.2499
2024-10-31 00:51:52: [2024-10-31 00:51:52] iter = 18960, loss = 2.7246
2024-10-31 00:51:56: [2024-10-31 00:51:56] iter = 18970, loss = 2.3413
2024-10-31 00:51:59: [2024-10-31 00:51:59] iter = 18980, loss = 2.8982
2024-10-31 00:52:03: [2024-10-31 00:52:03] iter = 18990, loss = 1.9789
2024-10-31 00:52:06: [2024-10-31 00:52:06] iter = 19000, loss = 3.0470
2024-10-31 00:52:10: [2024-10-31 00:52:10] iter = 19010, loss = 1.8580
2024-10-31 00:52:14: [2024-10-31 00:52:14] iter = 19020, loss = 1.7186
2024-10-31 00:52:18: [2024-10-31 00:52:18] iter = 19030, loss = 2.7871
2024-10-31 00:52:22: [2024-10-31 00:52:22] iter = 19040, loss = 2.9636
2024-10-31 00:52:26: [2024-10-31 00:52:26] iter = 19050, loss = 2.3157
2024-10-31 00:52:30: [2024-10-31 00:52:30] iter = 19060, loss = 2.1863
2024-10-31 00:52:34: [2024-10-31 00:52:34] iter = 19070, loss = 2.1544
2024-10-31 00:52:37: [2024-10-31 00:52:37] iter = 19080, loss = 1.8256
2024-10-31 00:52:40: [2024-10-31 00:52:40] iter = 19090, loss = 2.0684
2024-10-31 00:52:43: [2024-10-31 00:52:43] iter = 19100, loss = 1.9130
2024-10-31 00:52:47: [2024-10-31 00:52:47] iter = 19110, loss = 2.0360
2024-10-31 00:52:50: [2024-10-31 00:52:50] iter = 19120, loss = 2.0409
2024-10-31 00:52:54: [2024-10-31 00:52:54] iter = 19130, loss = 2.2888
2024-10-31 00:52:57: [2024-10-31 00:52:57] iter = 19140, loss = 1.8890
2024-10-31 00:53:00: [2024-10-31 00:53:00] iter = 19150, loss = 2.6642
2024-10-31 00:53:03: [2024-10-31 00:53:03] iter = 19160, loss = 2.2636
2024-10-31 00:53:06: [2024-10-31 00:53:06] iter = 19170, loss = 2.5901
2024-10-31 00:53:10: [2024-10-31 00:53:10] iter = 19180, loss = 2.4231
2024-10-31 00:53:13: [2024-10-31 00:53:13] iter = 19190, loss = 1.9167
2024-10-31 00:53:17: [2024-10-31 00:53:17] iter = 19200, loss = 2.0096
2024-10-31 00:53:20: [2024-10-31 00:53:20] iter = 19210, loss = 4.3502
2024-10-31 00:53:24: [2024-10-31 00:53:24] iter = 19220, loss = 2.5821
2024-10-31 00:53:26: [2024-10-31 00:53:26] iter = 19230, loss = 3.1791
2024-10-31 00:53:31: [2024-10-31 00:53:31] iter = 19240, loss = 2.6121
2024-10-31 00:53:35: [2024-10-31 00:53:35] iter = 19250, loss = 3.0061
2024-10-31 00:53:38: [2024-10-31 00:53:38] iter = 19260, loss = 2.4565
2024-10-31 00:53:42: [2024-10-31 00:53:42] iter = 19270, loss = 1.7143
2024-10-31 00:53:46: [2024-10-31 00:53:46] iter = 19280, loss = 1.8368
2024-10-31 00:53:49: [2024-10-31 00:53:49] iter = 19290, loss = 2.7026
2024-10-31 00:53:52: [2024-10-31 00:53:52] iter = 19300, loss = 2.2524
2024-10-31 00:53:56: [2024-10-31 00:53:56] iter = 19310, loss = 2.2154
2024-10-31 00:54:00: [2024-10-31 00:54:00] iter = 19320, loss = 2.1770
2024-10-31 00:54:02: [2024-10-31 00:54:02] iter = 19330, loss = 2.2653
2024-10-31 00:54:05: [2024-10-31 00:54:05] iter = 19340, loss = 2.3092
2024-10-31 00:54:10: [2024-10-31 00:54:10] iter = 19350, loss = 1.8282
2024-10-31 00:54:14: [2024-10-31 00:54:14] iter = 19360, loss = 2.3380
2024-10-31 00:54:17: [2024-10-31 00:54:17] iter = 19370, loss = 2.1800
2024-10-31 00:54:20: [2024-10-31 00:54:20] iter = 19380, loss = 1.9190
2024-10-31 00:54:24: [2024-10-31 00:54:24] iter = 19390, loss = 2.5307
2024-10-31 00:54:26: [2024-10-31 00:54:26] iter = 19400, loss = 2.3596
2024-10-31 00:54:30: [2024-10-31 00:54:30] iter = 19410, loss = 2.4615
2024-10-31 00:54:34: [2024-10-31 00:54:34] iter = 19420, loss = 2.0339
2024-10-31 00:54:37: [2024-10-31 00:54:37] iter = 19430, loss = 1.9735
2024-10-31 00:54:41: [2024-10-31 00:54:41] iter = 19440, loss = 1.5240
2024-10-31 00:54:45: [2024-10-31 00:54:45] iter = 19450, loss = 2.4406
2024-10-31 00:54:48: [2024-10-31 00:54:48] iter = 19460, loss = 2.0837
2024-10-31 00:54:51: [2024-10-31 00:54:51] iter = 19470, loss = 3.0900
2024-10-31 00:54:54: [2024-10-31 00:54:54] iter = 19480, loss = 3.3380
2024-10-31 00:54:57: [2024-10-31 00:54:57] iter = 19490, loss = 1.8674
2024-10-31 00:55:00: [2024-10-31 00:55:00] iter = 19500, loss = 3.2047
2024-10-31 00:55:04: [2024-10-31 00:55:04] iter = 19510, loss = 2.0880
2024-10-31 00:55:07: [2024-10-31 00:55:07] iter = 19520, loss = 1.8606
2024-10-31 00:55:10: [2024-10-31 00:55:10] iter = 19530, loss = 2.1435
2024-10-31 00:55:14: [2024-10-31 00:55:14] iter = 19540, loss = 2.2902
2024-10-31 00:55:17: [2024-10-31 00:55:17] iter = 19550, loss = 2.7133
2024-10-31 00:55:20: [2024-10-31 00:55:20] iter = 19560, loss = 2.1505
2024-10-31 00:55:23: [2024-10-31 00:55:23] iter = 19570, loss = 2.7216
2024-10-31 00:55:26: [2024-10-31 00:55:26] iter = 19580, loss = 2.2810
2024-10-31 00:55:29: [2024-10-31 00:55:29] iter = 19590, loss = 2.5968
2024-10-31 00:55:33: [2024-10-31 00:55:33] iter = 19600, loss = 1.9940
2024-10-31 00:55:36: [2024-10-31 00:55:36] iter = 19610, loss = 2.0127
2024-10-31 00:55:39: [2024-10-31 00:55:39] iter = 19620, loss = 2.2755
2024-10-31 00:55:42: [2024-10-31 00:55:42] iter = 19630, loss = 3.4157
2024-10-31 00:55:46: [2024-10-31 00:55:46] iter = 19640, loss = 2.2679
2024-10-31 00:55:48: [2024-10-31 00:55:48] iter = 19650, loss = 1.9228
2024-10-31 00:55:52: [2024-10-31 00:55:52] iter = 19660, loss = 2.6588
2024-10-31 00:55:55: [2024-10-31 00:55:55] iter = 19670, loss = 2.1324
2024-10-31 00:56:00: [2024-10-31 00:56:00] iter = 19680, loss = 2.3655
2024-10-31 00:56:04: [2024-10-31 00:56:04] iter = 19690, loss = 2.2475
2024-10-31 00:56:08: [2024-10-31 00:56:08] iter = 19700, loss = 2.3821
2024-10-31 00:56:11: [2024-10-31 00:56:11] iter = 19710, loss = 2.3446
2024-10-31 00:56:14: [2024-10-31 00:56:14] iter = 19720, loss = 1.8973
2024-10-31 00:56:18: [2024-10-31 00:56:18] iter = 19730, loss = 2.2214
2024-10-31 00:56:22: [2024-10-31 00:56:22] iter = 19740, loss = 1.7354
2024-10-31 00:56:26: [2024-10-31 00:56:26] iter = 19750, loss = 3.3901
2024-10-31 00:56:30: [2024-10-31 00:56:30] iter = 19760, loss = 1.8906
2024-10-31 00:56:34: [2024-10-31 00:56:34] iter = 19770, loss = 2.4066
2024-10-31 00:56:37: [2024-10-31 00:56:37] iter = 19780, loss = 2.1958
2024-10-31 00:56:41: [2024-10-31 00:56:41] iter = 19790, loss = 2.5439
2024-10-31 00:56:44: [2024-10-31 00:56:44] iter = 19800, loss = 4.1880
2024-10-31 00:56:48: [2024-10-31 00:56:48] iter = 19810, loss = 2.5453
2024-10-31 00:56:52: [2024-10-31 00:56:52] iter = 19820, loss = 1.9887
2024-10-31 00:56:56: [2024-10-31 00:56:56] iter = 19830, loss = 2.1004
2024-10-31 00:56:59: [2024-10-31 00:56:59] iter = 19840, loss = 1.7816
2024-10-31 00:57:03: [2024-10-31 00:57:03] iter = 19850, loss = 2.3337
2024-10-31 00:57:06: [2024-10-31 00:57:06] iter = 19860, loss = 2.1873
2024-10-31 00:57:11: [2024-10-31 00:57:11] iter = 19870, loss = 2.7539
2024-10-31 00:57:15: [2024-10-31 00:57:15] iter = 19880, loss = 1.9852
2024-10-31 00:57:19: [2024-10-31 00:57:19] iter = 19890, loss = 2.6439
2024-10-31 00:57:23: [2024-10-31 00:57:23] iter = 19900, loss = 3.0529
2024-10-31 00:57:27: [2024-10-31 00:57:27] iter = 19910, loss = 4.3429
2024-10-31 00:57:31: [2024-10-31 00:57:31] iter = 19920, loss = 2.1229
2024-10-31 00:57:34: [2024-10-31 00:57:34] iter = 19930, loss = 3.2668
2024-10-31 00:57:37: [2024-10-31 00:57:37] iter = 19940, loss = 2.7090
2024-10-31 00:57:42: [2024-10-31 00:57:42] iter = 19950, loss = 1.8867
2024-10-31 00:57:45: [2024-10-31 00:57:45] iter = 19960, loss = 2.4876
2024-10-31 00:57:49: [2024-10-31 00:57:49] iter = 19970, loss = 2.5244
2024-10-31 00:57:52: [2024-10-31 00:57:52] iter = 19980, loss = 2.0344
2024-10-31 00:57:56: [2024-10-31 00:57:56] iter = 19990, loss = 2.0937
2024-10-31 00:58:00: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
2024-10-31 00:58:00: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 00:58:00: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 80363}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:00:15: Evaluate 5 random ConvNet, ACCmean = 0.7741 ACCstd = 0.0022
-------------------------
2024-10-31 01:00:15: Evaluate 5 random ConvNet, SENmean = 0.7714 SENstd = 0.0033
-------------------------
2024-10-31 01:00:15: Evaluate 5 random ConvNet, SPEmean = 0.9774 SPEstd = 0.0002
-------------------------
2024-10-31 01:00:15: Evaluate 5 random ConvNet, F!mean = 0.7588 F!std = 0.0028
-------------------------
2024-10-31 01:00:15: Evaluate 5 random ConvNet, mean = 0.7741 std = 0.0022
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:00:16: [2024-10-31 01:00:16] iter = 20000, loss = 1.9890
2024-10-31 01:00:16: 
================== Exp 4 ==================
 
2024-10-31 01:00:16: Hyper-parameters: 
{'dataset': 'OrganAMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'SS', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 1000, 'Iteration': 20000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': '/data/users/xiongyuxuan/DD/OBJDD/DatasetCondensation-master/data', 'save_path': 'result', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f5f8402db20>, 'dsa': True, 'logger': <Logger DM_IPC=10_Model_ConvNet_Data_OrganAMNIST (INFO)>}
2024-10-31 01:00:16: Evaluation model pool: ['ConvNet']
2024-10-31 01:00:18: class c = 0: 1956 real images
2024-10-31 01:00:18: class c = 1: 1390 real images
2024-10-31 01:00:18: class c = 2: 1357 real images
2024-10-31 01:00:18: class c = 3: 1474 real images
2024-10-31 01:00:18: class c = 4: 3963 real images
2024-10-31 01:00:18: class c = 5: 3817 real images
2024-10-31 01:00:18: class c = 6: 6164 real images
2024-10-31 01:00:18: class c = 7: 3919 real images
2024-10-31 01:00:18: class c = 8: 3929 real images
2024-10-31 01:00:18: class c = 9: 3031 real images
2024-10-31 01:00:18: class c = 10: 3561 real images
2024-10-31 01:00:18: real images channel 0, mean = 0.4680, std = 0.2974
main_DM.py:120: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-10-31 01:00:18: initialize synthetic data from random real images
2024-10-31 01:00:18: [2024-10-31 01:00:18] training begins
2024-10-31 01:00:18: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-10-31 01:00:18: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 01:00:18: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 16061}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:02:29: Evaluate 5 random ConvNet, ACCmean = 0.7117 ACCstd = 0.0040
-------------------------
2024-10-31 01:02:29: Evaluate 5 random ConvNet, SENmean = 0.6929 SENstd = 0.0037
-------------------------
2024-10-31 01:02:29: Evaluate 5 random ConvNet, SPEmean = 0.9713 SPEstd = 0.0004
-------------------------
2024-10-31 01:02:29: Evaluate 5 random ConvNet, F!mean = 0.6824 F!std = 0.0042
-------------------------
2024-10-31 01:02:29: Evaluate 5 random ConvNet, mean = 0.7117 std = 0.0040
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:02:29: [2024-10-31 01:02:29] iter = 00000, loss = 15.2978
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:02:33: [2024-10-31 01:02:33] iter = 00010, loss = 4.1784
2024-10-31 01:02:36: [2024-10-31 01:02:36] iter = 00020, loss = 3.8862
2024-10-31 01:02:39: [2024-10-31 01:02:39] iter = 00030, loss = 5.5227
2024-10-31 01:02:42: [2024-10-31 01:02:42] iter = 00040, loss = 3.2676
2024-10-31 01:02:46: [2024-10-31 01:02:46] iter = 00050, loss = 3.5650
2024-10-31 01:02:49: [2024-10-31 01:02:49] iter = 00060, loss = 3.0362
2024-10-31 01:02:53: [2024-10-31 01:02:53] iter = 00070, loss = 3.5253
2024-10-31 01:02:57: [2024-10-31 01:02:57] iter = 00080, loss = 3.0238
2024-10-31 01:03:00: [2024-10-31 01:03:00] iter = 00090, loss = 3.9578
2024-10-31 01:03:04: [2024-10-31 01:03:04] iter = 00100, loss = 2.0787
2024-10-31 01:03:07: [2024-10-31 01:03:07] iter = 00110, loss = 2.6890
2024-10-31 01:03:10: [2024-10-31 01:03:10] iter = 00120, loss = 2.6773
2024-10-31 01:03:13: [2024-10-31 01:03:13] iter = 00130, loss = 3.2007
2024-10-31 01:03:17: [2024-10-31 01:03:17] iter = 00140, loss = 2.3267
2024-10-31 01:03:21: [2024-10-31 01:03:21] iter = 00150, loss = 2.6060
2024-10-31 01:03:25: [2024-10-31 01:03:25] iter = 00160, loss = 2.3692
2024-10-31 01:03:28: [2024-10-31 01:03:28] iter = 00170, loss = 3.0122
2024-10-31 01:03:31: [2024-10-31 01:03:31] iter = 00180, loss = 2.3890
2024-10-31 01:03:35: [2024-10-31 01:03:35] iter = 00190, loss = 2.6593
2024-10-31 01:03:38: [2024-10-31 01:03:38] iter = 00200, loss = 2.7104
2024-10-31 01:03:41: [2024-10-31 01:03:41] iter = 00210, loss = 3.0444
2024-10-31 01:03:46: [2024-10-31 01:03:46] iter = 00220, loss = 2.4455
2024-10-31 01:03:49: [2024-10-31 01:03:49] iter = 00230, loss = 2.5262
2024-10-31 01:03:52: [2024-10-31 01:03:52] iter = 00240, loss = 2.8715
2024-10-31 01:03:56: [2024-10-31 01:03:56] iter = 00250, loss = 2.8044
2024-10-31 01:04:00: [2024-10-31 01:04:00] iter = 00260, loss = 2.1259
2024-10-31 01:04:04: [2024-10-31 01:04:04] iter = 00270, loss = 2.7298
2024-10-31 01:04:08: [2024-10-31 01:04:08] iter = 00280, loss = 3.4898
2024-10-31 01:04:12: [2024-10-31 01:04:12] iter = 00290, loss = 2.5172
2024-10-31 01:04:14: [2024-10-31 01:04:14] iter = 00300, loss = 2.8965
2024-10-31 01:04:17: [2024-10-31 01:04:17] iter = 00310, loss = 2.9435
2024-10-31 01:04:21: [2024-10-31 01:04:21] iter = 00320, loss = 2.2890
2024-10-31 01:04:24: [2024-10-31 01:04:24] iter = 00330, loss = 2.7962
2024-10-31 01:04:28: [2024-10-31 01:04:28] iter = 00340, loss = 2.5326
2024-10-31 01:04:32: [2024-10-31 01:04:32] iter = 00350, loss = 2.5227
2024-10-31 01:04:35: [2024-10-31 01:04:35] iter = 00360, loss = 3.8275
2024-10-31 01:04:38: [2024-10-31 01:04:38] iter = 00370, loss = 2.4793
2024-10-31 01:04:42: [2024-10-31 01:04:42] iter = 00380, loss = 2.1653
2024-10-31 01:04:45: [2024-10-31 01:04:45] iter = 00390, loss = 2.1771
2024-10-31 01:04:48: [2024-10-31 01:04:48] iter = 00400, loss = 2.0468
2024-10-31 01:04:52: [2024-10-31 01:04:52] iter = 00410, loss = 2.9787
2024-10-31 01:04:55: [2024-10-31 01:04:55] iter = 00420, loss = 3.4078
2024-10-31 01:04:59: [2024-10-31 01:04:59] iter = 00430, loss = 2.5387
2024-10-31 01:05:04: [2024-10-31 01:05:04] iter = 00440, loss = 3.3778
2024-10-31 01:05:09: [2024-10-31 01:05:09] iter = 00450, loss = 1.9577
2024-10-31 01:05:12: [2024-10-31 01:05:12] iter = 00460, loss = 2.3738
2024-10-31 01:05:16: [2024-10-31 01:05:16] iter = 00470, loss = 2.2782
2024-10-31 01:05:19: [2024-10-31 01:05:19] iter = 00480, loss = 2.5442
2024-10-31 01:05:22: [2024-10-31 01:05:22] iter = 00490, loss = 3.3123
2024-10-31 01:05:26: [2024-10-31 01:05:26] iter = 00500, loss = 2.2072
2024-10-31 01:05:30: [2024-10-31 01:05:30] iter = 00510, loss = 2.5613
2024-10-31 01:05:34: [2024-10-31 01:05:34] iter = 00520, loss = 2.6368
2024-10-31 01:05:37: [2024-10-31 01:05:37] iter = 00530, loss = 2.2689
2024-10-31 01:05:42: [2024-10-31 01:05:42] iter = 00540, loss = 2.1837
2024-10-31 01:05:45: [2024-10-31 01:05:45] iter = 00550, loss = 2.5197
2024-10-31 01:05:49: [2024-10-31 01:05:49] iter = 00560, loss = 2.0710
2024-10-31 01:05:53: [2024-10-31 01:05:53] iter = 00570, loss = 2.4152
2024-10-31 01:05:56: [2024-10-31 01:05:56] iter = 00580, loss = 2.3767
2024-10-31 01:05:59: [2024-10-31 01:05:59] iter = 00590, loss = 2.8930
2024-10-31 01:06:03: [2024-10-31 01:06:03] iter = 00600, loss = 2.3752
2024-10-31 01:06:06: [2024-10-31 01:06:06] iter = 00610, loss = 2.1984
2024-10-31 01:06:11: [2024-10-31 01:06:11] iter = 00620, loss = 3.8116
2024-10-31 01:06:14: [2024-10-31 01:06:14] iter = 00630, loss = 2.5580
2024-10-31 01:06:17: [2024-10-31 01:06:17] iter = 00640, loss = 2.3578
2024-10-31 01:06:20: [2024-10-31 01:06:20] iter = 00650, loss = 3.9631
2024-10-31 01:06:23: [2024-10-31 01:06:23] iter = 00660, loss = 2.8340
2024-10-31 01:06:26: [2024-10-31 01:06:26] iter = 00670, loss = 3.2157
2024-10-31 01:06:29: [2024-10-31 01:06:29] iter = 00680, loss = 2.0437
2024-10-31 01:06:32: [2024-10-31 01:06:32] iter = 00690, loss = 2.8620
2024-10-31 01:06:36: [2024-10-31 01:06:36] iter = 00700, loss = 2.2814
2024-10-31 01:06:39: [2024-10-31 01:06:39] iter = 00710, loss = 2.3115
2024-10-31 01:06:42: [2024-10-31 01:06:42] iter = 00720, loss = 2.9478
2024-10-31 01:06:45: [2024-10-31 01:06:45] iter = 00730, loss = 2.1223
2024-10-31 01:06:48: [2024-10-31 01:06:48] iter = 00740, loss = 3.0913
2024-10-31 01:06:51: [2024-10-31 01:06:51] iter = 00750, loss = 2.0425
2024-10-31 01:06:55: [2024-10-31 01:06:55] iter = 00760, loss = 2.1370
2024-10-31 01:06:59: [2024-10-31 01:06:59] iter = 00770, loss = 2.8919
2024-10-31 01:07:03: [2024-10-31 01:07:03] iter = 00780, loss = 3.9779
2024-10-31 01:07:06: [2024-10-31 01:07:06] iter = 00790, loss = 3.0814
2024-10-31 01:07:10: [2024-10-31 01:07:10] iter = 00800, loss = 3.2050
2024-10-31 01:07:15: [2024-10-31 01:07:15] iter = 00810, loss = 2.1977
2024-10-31 01:07:18: [2024-10-31 01:07:18] iter = 00820, loss = 2.0212
2024-10-31 01:07:22: [2024-10-31 01:07:22] iter = 00830, loss = 2.3814
2024-10-31 01:07:25: [2024-10-31 01:07:25] iter = 00840, loss = 1.9891
2024-10-31 01:07:29: [2024-10-31 01:07:29] iter = 00850, loss = 2.6124
2024-10-31 01:07:32: [2024-10-31 01:07:32] iter = 00860, loss = 2.1947
2024-10-31 01:07:35: [2024-10-31 01:07:35] iter = 00870, loss = 1.8912
2024-10-31 01:07:39: [2024-10-31 01:07:39] iter = 00880, loss = 2.2844
2024-10-31 01:07:42: [2024-10-31 01:07:42] iter = 00890, loss = 2.3475
2024-10-31 01:07:46: [2024-10-31 01:07:46] iter = 00900, loss = 2.4801
2024-10-31 01:07:50: [2024-10-31 01:07:50] iter = 00910, loss = 2.1788
2024-10-31 01:07:53: [2024-10-31 01:07:53] iter = 00920, loss = 3.4831
2024-10-31 01:07:56: [2024-10-31 01:07:56] iter = 00930, loss = 2.6280
2024-10-31 01:08:00: [2024-10-31 01:08:00] iter = 00940, loss = 2.4922
2024-10-31 01:08:03: [2024-10-31 01:08:03] iter = 00950, loss = 2.9058
2024-10-31 01:08:08: [2024-10-31 01:08:08] iter = 00960, loss = 2.3073
2024-10-31 01:08:12: [2024-10-31 01:08:12] iter = 00970, loss = 2.4025
2024-10-31 01:08:16: [2024-10-31 01:08:16] iter = 00980, loss = 3.1250
2024-10-31 01:08:19: [2024-10-31 01:08:19] iter = 00990, loss = 2.7881
2024-10-31 01:08:23: [2024-10-31 01:08:23] iter = 01000, loss = 2.0874
2024-10-31 01:08:27: [2024-10-31 01:08:27] iter = 01010, loss = 2.1119
2024-10-31 01:08:30: [2024-10-31 01:08:30] iter = 01020, loss = 1.9942
2024-10-31 01:08:33: [2024-10-31 01:08:33] iter = 01030, loss = 3.0757
2024-10-31 01:08:38: [2024-10-31 01:08:38] iter = 01040, loss = 2.2487
2024-10-31 01:08:41: [2024-10-31 01:08:41] iter = 01050, loss = 2.1926
2024-10-31 01:08:45: [2024-10-31 01:08:45] iter = 01060, loss = 2.5804
2024-10-31 01:08:49: [2024-10-31 01:08:49] iter = 01070, loss = 1.6779
2024-10-31 01:08:54: [2024-10-31 01:08:54] iter = 01080, loss = 2.3781
2024-10-31 01:08:57: [2024-10-31 01:08:57] iter = 01090, loss = 2.2389
2024-10-31 01:09:01: [2024-10-31 01:09:01] iter = 01100, loss = 1.9339
2024-10-31 01:09:04: [2024-10-31 01:09:04] iter = 01110, loss = 2.4999
2024-10-31 01:09:07: [2024-10-31 01:09:07] iter = 01120, loss = 2.4801
2024-10-31 01:09:10: [2024-10-31 01:09:10] iter = 01130, loss = 2.4647
2024-10-31 01:09:13: [2024-10-31 01:09:13] iter = 01140, loss = 2.6458
2024-10-31 01:09:16: [2024-10-31 01:09:16] iter = 01150, loss = 2.5365
2024-10-31 01:09:19: [2024-10-31 01:09:19] iter = 01160, loss = 2.2644
2024-10-31 01:09:23: [2024-10-31 01:09:23] iter = 01170, loss = 2.2135
2024-10-31 01:09:28: [2024-10-31 01:09:28] iter = 01180, loss = 2.7031
2024-10-31 01:09:30: [2024-10-31 01:09:30] iter = 01190, loss = 2.0635
2024-10-31 01:09:34: [2024-10-31 01:09:34] iter = 01200, loss = 2.8214
2024-10-31 01:09:37: [2024-10-31 01:09:37] iter = 01210, loss = 2.2867
2024-10-31 01:09:40: [2024-10-31 01:09:40] iter = 01220, loss = 4.1380
2024-10-31 01:09:43: [2024-10-31 01:09:43] iter = 01230, loss = 2.4175
2024-10-31 01:09:46: [2024-10-31 01:09:46] iter = 01240, loss = 2.0920
2024-10-31 01:09:50: [2024-10-31 01:09:50] iter = 01250, loss = 2.9644
2024-10-31 01:09:53: [2024-10-31 01:09:53] iter = 01260, loss = 4.2587
2024-10-31 01:09:56: [2024-10-31 01:09:56] iter = 01270, loss = 3.1813
2024-10-31 01:10:00: [2024-10-31 01:10:00] iter = 01280, loss = 1.9452
2024-10-31 01:10:03: [2024-10-31 01:10:03] iter = 01290, loss = 2.3399
2024-10-31 01:10:05: [2024-10-31 01:10:05] iter = 01300, loss = 2.4190
2024-10-31 01:10:08: [2024-10-31 01:10:08] iter = 01310, loss = 2.5127
2024-10-31 01:10:11: [2024-10-31 01:10:11] iter = 01320, loss = 3.1758
2024-10-31 01:10:14: [2024-10-31 01:10:14] iter = 01330, loss = 3.3819
2024-10-31 01:10:18: [2024-10-31 01:10:18] iter = 01340, loss = 2.2093
2024-10-31 01:10:21: [2024-10-31 01:10:21] iter = 01350, loss = 5.6862
2024-10-31 01:10:25: [2024-10-31 01:10:25] iter = 01360, loss = 2.4476
2024-10-31 01:10:28: [2024-10-31 01:10:28] iter = 01370, loss = 1.6743
2024-10-31 01:10:31: [2024-10-31 01:10:31] iter = 01380, loss = 1.9288
2024-10-31 01:10:35: [2024-10-31 01:10:35] iter = 01390, loss = 8.5621
2024-10-31 01:10:39: [2024-10-31 01:10:39] iter = 01400, loss = 2.5446
2024-10-31 01:10:43: [2024-10-31 01:10:43] iter = 01410, loss = 2.0463
2024-10-31 01:10:47: [2024-10-31 01:10:47] iter = 01420, loss = 2.7322
2024-10-31 01:10:50: [2024-10-31 01:10:50] iter = 01430, loss = 1.9906
2024-10-31 01:10:53: [2024-10-31 01:10:53] iter = 01440, loss = 2.9191
2024-10-31 01:10:57: [2024-10-31 01:10:57] iter = 01450, loss = 2.3710
2024-10-31 01:11:00: [2024-10-31 01:11:00] iter = 01460, loss = 1.9952
2024-10-31 01:11:03: [2024-10-31 01:11:03] iter = 01470, loss = 1.7990
2024-10-31 01:11:07: [2024-10-31 01:11:07] iter = 01480, loss = 3.8288
2024-10-31 01:11:11: [2024-10-31 01:11:11] iter = 01490, loss = 2.0026
2024-10-31 01:11:15: [2024-10-31 01:11:15] iter = 01500, loss = 2.4446
2024-10-31 01:11:19: [2024-10-31 01:11:19] iter = 01510, loss = 2.5081
2024-10-31 01:11:22: [2024-10-31 01:11:22] iter = 01520, loss = 3.5798
2024-10-31 01:11:26: [2024-10-31 01:11:26] iter = 01530, loss = 2.8878
2024-10-31 01:11:30: [2024-10-31 01:11:30] iter = 01540, loss = 2.0457
2024-10-31 01:11:34: [2024-10-31 01:11:34] iter = 01550, loss = 1.9405
2024-10-31 01:11:37: [2024-10-31 01:11:37] iter = 01560, loss = 2.2869
2024-10-31 01:11:41: [2024-10-31 01:11:41] iter = 01570, loss = 2.0164
2024-10-31 01:11:45: [2024-10-31 01:11:45] iter = 01580, loss = 2.7231
2024-10-31 01:11:48: [2024-10-31 01:11:48] iter = 01590, loss = 2.9541
2024-10-31 01:11:52: [2024-10-31 01:11:52] iter = 01600, loss = 2.1477
2024-10-31 01:11:56: [2024-10-31 01:11:56] iter = 01610, loss = 2.2710
2024-10-31 01:12:00: [2024-10-31 01:12:00] iter = 01620, loss = 5.3960
2024-10-31 01:12:04: [2024-10-31 01:12:04] iter = 01630, loss = 2.4913
2024-10-31 01:12:07: [2024-10-31 01:12:07] iter = 01640, loss = 2.2536
2024-10-31 01:12:10: [2024-10-31 01:12:10] iter = 01650, loss = 2.2965
2024-10-31 01:12:14: [2024-10-31 01:12:14] iter = 01660, loss = 3.7247
2024-10-31 01:12:19: [2024-10-31 01:12:19] iter = 01670, loss = 4.0570
2024-10-31 01:12:22: [2024-10-31 01:12:22] iter = 01680, loss = 2.5172
2024-10-31 01:12:26: [2024-10-31 01:12:26] iter = 01690, loss = 2.0610
2024-10-31 01:12:30: [2024-10-31 01:12:30] iter = 01700, loss = 3.6864
2024-10-31 01:12:32: [2024-10-31 01:12:32] iter = 01710, loss = 2.8882
2024-10-31 01:12:36: [2024-10-31 01:12:36] iter = 01720, loss = 2.1735
2024-10-31 01:12:39: [2024-10-31 01:12:39] iter = 01730, loss = 2.5341
2024-10-31 01:12:43: [2024-10-31 01:12:43] iter = 01740, loss = 4.0362
2024-10-31 01:12:47: [2024-10-31 01:12:47] iter = 01750, loss = 1.9803
2024-10-31 01:12:51: [2024-10-31 01:12:51] iter = 01760, loss = 3.4639
2024-10-31 01:12:56: [2024-10-31 01:12:56] iter = 01770, loss = 1.8728
2024-10-31 01:12:59: [2024-10-31 01:12:59] iter = 01780, loss = 2.9923
2024-10-31 01:13:02: [2024-10-31 01:13:02] iter = 01790, loss = 2.1377
2024-10-31 01:13:06: [2024-10-31 01:13:06] iter = 01800, loss = 2.8621
2024-10-31 01:13:09: [2024-10-31 01:13:09] iter = 01810, loss = 1.8768
2024-10-31 01:13:12: [2024-10-31 01:13:12] iter = 01820, loss = 2.1844
2024-10-31 01:13:15: [2024-10-31 01:13:15] iter = 01830, loss = 1.9748
2024-10-31 01:13:19: [2024-10-31 01:13:19] iter = 01840, loss = 2.2284
2024-10-31 01:13:21: [2024-10-31 01:13:21] iter = 01850, loss = 2.7186
2024-10-31 01:13:24: [2024-10-31 01:13:24] iter = 01860, loss = 2.8958
2024-10-31 01:13:28: [2024-10-31 01:13:28] iter = 01870, loss = 2.4178
2024-10-31 01:13:32: [2024-10-31 01:13:32] iter = 01880, loss = 3.0465
2024-10-31 01:13:34: [2024-10-31 01:13:34] iter = 01890, loss = 2.2429
2024-10-31 01:13:37: [2024-10-31 01:13:37] iter = 01900, loss = 2.2168
2024-10-31 01:13:42: [2024-10-31 01:13:42] iter = 01910, loss = 2.5359
2024-10-31 01:13:45: [2024-10-31 01:13:45] iter = 01920, loss = 2.0696
2024-10-31 01:13:48: [2024-10-31 01:13:48] iter = 01930, loss = 3.2253
2024-10-31 01:13:52: [2024-10-31 01:13:52] iter = 01940, loss = 1.8046
2024-10-31 01:13:55: [2024-10-31 01:13:55] iter = 01950, loss = 1.9083
2024-10-31 01:13:59: [2024-10-31 01:13:59] iter = 01960, loss = 1.9125
2024-10-31 01:14:02: [2024-10-31 01:14:02] iter = 01970, loss = 2.2669
2024-10-31 01:14:06: [2024-10-31 01:14:06] iter = 01980, loss = 2.7445
2024-10-31 01:14:09: [2024-10-31 01:14:09] iter = 01990, loss = 2.1265
2024-10-31 01:14:13: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 2000
2024-10-31 01:14:13: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 01:14:13: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 53015}

[2024-10-30 23:22:39] Evaluate_04: epoch = 1000 train time = 25 s train loss = 0.016728 train acc = 1.0000, test acc = 0.7922, test_sen =0.7913, test_spe =0.9792, test_f1 =0.7784
[2024-10-30 23:35:09] Evaluate_00: epoch = 1000 train time = 27 s train loss = 0.007223 train acc = 1.0000, test acc = 0.7936, test_sen =0.7899, test_spe =0.9793, test_f1 =0.7789
[2024-10-30 23:35:38] Evaluate_01: epoch = 1000 train time = 27 s train loss = 0.023117 train acc = 1.0000, test acc = 0.7832, test_sen =0.7792, test_spe =0.9783, test_f1 =0.7686
[2024-10-30 23:36:06] Evaluate_02: epoch = 1000 train time = 25 s train loss = 0.002543 train acc = 1.0000, test acc = 0.7967, test_sen =0.7967, test_spe =0.9797, test_f1 =0.7836
[2024-10-30 23:36:34] Evaluate_03: epoch = 1000 train time = 26 s train loss = 0.004135 train acc = 1.0000, test acc = 0.7873, test_sen =0.7854, test_spe =0.9787, test_f1 =0.7734
[2024-10-30 23:37:02] Evaluate_04: epoch = 1000 train time = 25 s train loss = 0.004984 train acc = 1.0000, test acc = 0.7940, test_sen =0.7914, test_spe =0.9794, test_f1 =0.7784
[2024-10-30 23:49:17] Evaluate_00: epoch = 1000 train time = 26 s train loss = 0.035382 train acc = 0.9909, test acc = 0.7763, test_sen =0.7686, test_spe =0.9776, test_f1 =0.7574
[2024-10-30 23:49:46] Evaluate_01: epoch = 1000 train time = 27 s train loss = 0.004884 train acc = 1.0000, test acc = 0.7849, test_sen =0.7800, test_spe =0.9784, test_f1 =0.7680
[2024-10-30 23:50:12] Evaluate_02: epoch = 1000 train time = 23 s train loss = 0.004706 train acc = 1.0000, test acc = 0.7901, test_sen =0.7860, test_spe =0.9790, test_f1 =0.7725
[2024-10-30 23:50:38] Evaluate_03: epoch = 1000 train time = 25 s train loss = 0.002774 train acc = 1.0000, test acc = 0.7786, test_sen =0.7774, test_spe =0.9779, test_f1 =0.7605
[2024-10-30 23:51:05] Evaluate_04: epoch = 1000 train time = 24 s train loss = 0.018156 train acc = 1.0000, test acc = 0.7852, test_sen =0.7814, test_spe =0.9785, test_f1 =0.7660
[2024-10-31 00:03:04] Evaluate_00: epoch = 1000 train time = 25 s train loss = 0.007751 train acc = 1.0000, test acc = 0.7720, test_sen =0.7683, test_spe =0.9772, test_f1 =0.7558
[2024-10-31 00:03:30] Evaluate_01: epoch = 1000 train time = 23 s train loss = 0.031211 train acc = 1.0000, test acc = 0.7746, test_sen =0.7691, test_spe =0.9775, test_f1 =0.7550
[2024-10-31 00:03:58] Evaluate_02: epoch = 1000 train time = 27 s train loss = 0.026139 train acc = 0.9909, test acc = 0.7848, test_sen =0.7833, test_spe =0.9785, test_f1 =0.7683
[2024-10-31 00:04:25] Evaluate_03: epoch = 1000 train time = 24 s train loss = 0.003332 train acc = 1.0000, test acc = 0.7704, test_sen =0.7673, test_spe =0.9771, test_f1 =0.7539
[2024-10-31 00:04:54] Evaluate_04: epoch = 1000 train time = 27 s train loss = 0.006797 train acc = 1.0000, test acc = 0.7720, test_sen =0.7697, test_spe =0.9772, test_f1 =0.7573
[2024-10-31 00:16:58] Evaluate_00: epoch = 1000 train time = 26 s train loss = 0.008087 train acc = 1.0000, test acc = 0.7825, test_sen =0.7751, test_spe =0.9783, test_f1 =0.7632
[2024-10-31 00:17:24] Evaluate_01: epoch = 1000 train time = 24 s train loss = 0.007142 train acc = 1.0000, test acc = 0.7794, test_sen =0.7723, test_spe =0.9780, test_f1 =0.7596
[2024-10-31 00:17:51] Evaluate_02: epoch = 1000 train time = 24 s train loss = 0.003298 train acc = 1.0000, test acc = 0.7762, test_sen =0.7715, test_spe =0.9776, test_f1 =0.7595
[2024-10-31 00:18:20] Evaluate_03: epoch = 1000 train time = 26 s train loss = 0.022483 train acc = 1.0000, test acc = 0.7779, test_sen =0.7718, test_spe =0.9777, test_f1 =0.7605
[2024-10-31 00:18:48] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.029705 train acc = 1.0000, test acc = 0.7809, test_sen =0.7740, test_spe =0.9781, test_f1 =0.7629
[2024-10-31 00:31:04] Evaluate_00: epoch = 1000 train time = 24 s train loss = 0.003029 train acc = 1.0000, test acc = 0.7834, test_sen =0.7759, test_spe =0.9784, test_f1 =0.7617
[2024-10-31 00:31:31] Evaluate_01: epoch = 1000 train time = 25 s train loss = 0.019564 train acc = 1.0000, test acc = 0.7773, test_sen =0.7707, test_spe =0.9777, test_f1 =0.7577
[2024-10-31 00:31:58] Evaluate_02: epoch = 1000 train time = 25 s train loss = 0.008729 train acc = 1.0000, test acc = 0.7837, test_sen =0.7764, test_spe =0.9784, test_f1 =0.7623
[2024-10-31 00:32:24] Evaluate_03: epoch = 1000 train time = 24 s train loss = 0.004336 train acc = 1.0000, test acc = 0.7845, test_sen =0.7763, test_spe =0.9784, test_f1 =0.7661
[2024-10-31 00:32:53] Evaluate_04: epoch = 1000 train time = 27 s train loss = 0.017991 train acc = 1.0000, test acc = 0.7877, test_sen =0.7809, test_spe =0.9788, test_f1 =0.7688
[2024-10-31 00:44:47] Evaluate_00: epoch = 1000 train time = 25 s train loss = 0.021304 train acc = 1.0000, test acc = 0.7825, test_sen =0.7802, test_spe =0.9783, test_f1 =0.7668
[2024-10-31 00:45:15] Evaluate_01: epoch = 1000 train time = 25 s train loss = 0.019462 train acc = 1.0000, test acc = 0.7785, test_sen =0.7783, test_spe =0.9778, test_f1 =0.7659
[2024-10-31 00:45:40] Evaluate_02: epoch = 1000 train time = 23 s train loss = 0.003212 train acc = 1.0000, test acc = 0.7827, test_sen =0.7789, test_spe =0.9783, test_f1 =0.7657
[2024-10-31 00:46:06] Evaluate_03: epoch = 1000 train time = 23 s train loss = 0.002725 train acc = 1.0000, test acc = 0.7797, test_sen =0.7763, test_spe =0.9779, test_f1 =0.7633
[2024-10-31 00:46:34] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.027635 train acc = 1.0000, test acc = 0.7833, test_sen =0.7791, test_spe =0.9783, test_f1 =0.7668
[2024-10-31 00:58:26] Evaluate_00: epoch = 1000 train time = 24 s train loss = 0.027898 train acc = 1.0000, test acc = 0.7733, test_sen =0.7708, test_spe =0.9773, test_f1 =0.7597
[2024-10-31 00:58:55] Evaluate_01: epoch = 1000 train time = 26 s train loss = 0.004438 train acc = 1.0000, test acc = 0.7772, test_sen =0.7763, test_spe =0.9777, test_f1 =0.7622
[2024-10-31 00:59:22] Evaluate_02: epoch = 1000 train time = 24 s train loss = 0.006021 train acc = 1.0000, test acc = 0.7706, test_sen =0.7663, test_spe =0.9770, test_f1 =0.7558
[2024-10-31 00:59:48] Evaluate_03: epoch = 1000 train time = 24 s train loss = 0.003898 train acc = 1.0000, test acc = 0.7739, test_sen =0.7709, test_spe =0.9774, test_f1 =0.7552
[2024-10-31 01:00:15] Evaluate_04: epoch = 1000 train time = 25 s train loss = 0.021065 train acc = 1.0000, test acc = 0.7753, test_sen =0.7730, test_spe =0.9774, test_f1 =0.7613
[2024-10-31 01:00:42] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.006198 train acc = 1.0000, test acc = 0.7161, test_sen =0.6977, test_spe =0.9718, test_f1 =0.6870
[2024-10-31 01:01:08] Evaluate_01: epoch = 1000 train time = 24 s train loss = 0.001670 train acc = 1.0000, test acc = 0.7064, test_sen =0.6885, test_spe =0.9708, test_f1 =0.6772
[2024-10-31 01:01:34] Evaluate_02: epoch = 1000 train time = 24 s train loss = 0.009555 train acc = 1.0000, test acc = 0.7085, test_sen =0.6907, test_spe =0.9710, test_f1 =0.6780
[2024-10-31 01:02:02] Evaluate_03: epoch = 1000 train time = 26 s train loss = 0.010391 train acc = 1.0000, test acc = 0.7163, test_sen =0.6970, test_spe =0.9718, test_f1 =0.6866
[2024-10-31 01:02:29] Evaluate_04: epoch = 1000 train time = 24 s train loss = 0.028041 train acc = 1.0000, test acc = 0.7113, test_sen =0.6906, test_spe =0.9712, test_f1 =0.6830
[2024-10-31 01:14:40] Evaluate_00: epoch = 1000 train time = 25 s train loss = 0.005765 train acc = 1.0000, test acc = 0.7735, test_sen =0.7703, test_spe =0.9773, test_f1 =0.7565
[2024-10-31 01:15:07] Evaluate_01: epoch = 1000 train time = 25 s train loss = 0.014978 train acc = 1.0000, test acc = 0.7767, test_sen =0.7749, test_spe =0.9777, test_f1 =0.7596
[2024-10-31 01:15:38] Evaluate_02: epoch = 1000 train time = 29 s train loss = 0.006732 train acc = 1.0000, test acc = 0.7712, test_sen =0.7724, test_spe =0.9770, test_f1 =0.7565
[2024-10-31 01:16:06] Evaluate_03: epoch = 1000 train time = 25 s train loss = 0.013128 train acc = 1.0000, test acc = 0.7731, test_sen =0.7738, test_spe =0.9773, test_f1 =0.7572
[2024-10-31 01:16:35] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.002775 train acc = 1.0000, test acc = 0.7798, test_sen =0.7770, test_spe =0.9779, test_f1 =0.7626/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:16:35: Evaluate 5 random ConvNet, ACCmean = 0.7749 ACCstd = 0.0030
-------------------------
2024-10-31 01:16:35: Evaluate 5 random ConvNet, SENmean = 0.7737 SENstd = 0.0023
-------------------------
2024-10-31 01:16:35: Evaluate 5 random ConvNet, SPEmean = 0.9774 SPEstd = 0.0003
-------------------------
2024-10-31 01:16:35: Evaluate 5 random ConvNet, F!mean = 0.7585 F!std = 0.0024
-------------------------
2024-10-31 01:16:35: Evaluate 5 random ConvNet, mean = 0.7749 std = 0.0030
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:16:35: [2024-10-31 01:16:35] iter = 02000, loss = 2.2332
2024-10-31 01:16:39: [2024-10-31 01:16:39] iter = 02010, loss = 2.9262
2024-10-31 01:16:43: [2024-10-31 01:16:43] iter = 02020, loss = 2.1821
2024-10-31 01:16:46: [2024-10-31 01:16:46] iter = 02030, loss = 2.7637
2024-10-31 01:16:49: [2024-10-31 01:16:49] iter = 02040, loss = 2.6783
2024-10-31 01:16:54: [2024-10-31 01:16:54] iter = 02050, loss = 3.0848
2024-10-31 01:16:57: [2024-10-31 01:16:57] iter = 02060, loss = 2.5430
2024-10-31 01:17:00: [2024-10-31 01:17:00] iter = 02070, loss = 2.3231
2024-10-31 01:17:03: [2024-10-31 01:17:03] iter = 02080, loss = 2.0052
2024-10-31 01:17:07: [2024-10-31 01:17:07] iter = 02090, loss = 1.9655
2024-10-31 01:17:11: [2024-10-31 01:17:11] iter = 02100, loss = 2.1080
2024-10-31 01:17:14: [2024-10-31 01:17:14] iter = 02110, loss = 3.8643
2024-10-31 01:17:18: [2024-10-31 01:17:18] iter = 02120, loss = 2.3921
2024-10-31 01:17:21: [2024-10-31 01:17:21] iter = 02130, loss = 3.5622
2024-10-31 01:17:25: [2024-10-31 01:17:25] iter = 02140, loss = 2.2159
2024-10-31 01:17:29: [2024-10-31 01:17:29] iter = 02150, loss = 2.7071
2024-10-31 01:17:32: [2024-10-31 01:17:32] iter = 02160, loss = 2.2178
2024-10-31 01:17:35: [2024-10-31 01:17:35] iter = 02170, loss = 2.0235
2024-10-31 01:17:39: [2024-10-31 01:17:39] iter = 02180, loss = 2.4365
2024-10-31 01:17:42: [2024-10-31 01:17:42] iter = 02190, loss = 2.5024
2024-10-31 01:17:46: [2024-10-31 01:17:46] iter = 02200, loss = 1.7845
2024-10-31 01:17:49: [2024-10-31 01:17:49] iter = 02210, loss = 2.0271
2024-10-31 01:17:53: [2024-10-31 01:17:53] iter = 02220, loss = 3.4113
2024-10-31 01:17:56: [2024-10-31 01:17:56] iter = 02230, loss = 3.5015
2024-10-31 01:17:59: [2024-10-31 01:17:59] iter = 02240, loss = 2.1294
2024-10-31 01:18:03: [2024-10-31 01:18:03] iter = 02250, loss = 2.2058
2024-10-31 01:18:05: [2024-10-31 01:18:05] iter = 02260, loss = 2.1797
2024-10-31 01:18:08: [2024-10-31 01:18:08] iter = 02270, loss = 1.8982
2024-10-31 01:18:12: [2024-10-31 01:18:12] iter = 02280, loss = 2.3828
2024-10-31 01:18:15: [2024-10-31 01:18:15] iter = 02290, loss = 1.8227
2024-10-31 01:18:20: [2024-10-31 01:18:20] iter = 02300, loss = 2.1651
2024-10-31 01:18:24: [2024-10-31 01:18:24] iter = 02310, loss = 2.3354
2024-10-31 01:18:29: [2024-10-31 01:18:29] iter = 02320, loss = 2.1434
2024-10-31 01:18:32: [2024-10-31 01:18:32] iter = 02330, loss = 2.1796
2024-10-31 01:18:36: [2024-10-31 01:18:36] iter = 02340, loss = 2.1226
2024-10-31 01:18:40: [2024-10-31 01:18:40] iter = 02350, loss = 2.6751
2024-10-31 01:18:43: [2024-10-31 01:18:43] iter = 02360, loss = 3.9495
2024-10-31 01:18:46: [2024-10-31 01:18:46] iter = 02370, loss = 2.6335
2024-10-31 01:18:50: [2024-10-31 01:18:50] iter = 02380, loss = 2.6643
2024-10-31 01:18:54: [2024-10-31 01:18:54] iter = 02390, loss = 2.5268
2024-10-31 01:18:58: [2024-10-31 01:18:58] iter = 02400, loss = 1.6883
2024-10-31 01:19:02: [2024-10-31 01:19:02] iter = 02410, loss = 2.1703
2024-10-31 01:19:06: [2024-10-31 01:19:06] iter = 02420, loss = 2.1298
2024-10-31 01:19:10: [2024-10-31 01:19:10] iter = 02430, loss = 2.0342
2024-10-31 01:19:14: [2024-10-31 01:19:14] iter = 02440, loss = 2.2073
2024-10-31 01:19:18: [2024-10-31 01:19:18] iter = 02450, loss = 2.2424
2024-10-31 01:19:22: [2024-10-31 01:19:22] iter = 02460, loss = 2.6987
2024-10-31 01:19:26: [2024-10-31 01:19:26] iter = 02470, loss = 2.1966
2024-10-31 01:19:29: [2024-10-31 01:19:29] iter = 02480, loss = 2.0065
2024-10-31 01:19:32: [2024-10-31 01:19:32] iter = 02490, loss = 2.1438
2024-10-31 01:19:35: [2024-10-31 01:19:35] iter = 02500, loss = 2.0879
2024-10-31 01:19:38: [2024-10-31 01:19:38] iter = 02510, loss = 2.8531
2024-10-31 01:19:41: [2024-10-31 01:19:41] iter = 02520, loss = 2.8870
2024-10-31 01:19:44: [2024-10-31 01:19:44] iter = 02530, loss = 1.7912
2024-10-31 01:19:48: [2024-10-31 01:19:48] iter = 02540, loss = 2.4445
2024-10-31 01:19:52: [2024-10-31 01:19:52] iter = 02550, loss = 2.3737
2024-10-31 01:19:56: [2024-10-31 01:19:56] iter = 02560, loss = 2.2748
2024-10-31 01:19:59: [2024-10-31 01:19:59] iter = 02570, loss = 2.5602
2024-10-31 01:20:03: [2024-10-31 01:20:03] iter = 02580, loss = 2.4547
2024-10-31 01:20:07: [2024-10-31 01:20:07] iter = 02590, loss = 2.1132
2024-10-31 01:20:10: [2024-10-31 01:20:10] iter = 02600, loss = 2.2347
2024-10-31 01:20:12: [2024-10-31 01:20:12] iter = 02610, loss = 2.0031
2024-10-31 01:20:16: [2024-10-31 01:20:16] iter = 02620, loss = 2.0314
2024-10-31 01:20:20: [2024-10-31 01:20:20] iter = 02630, loss = 2.5377
2024-10-31 01:20:24: [2024-10-31 01:20:24] iter = 02640, loss = 2.5191
2024-10-31 01:20:28: [2024-10-31 01:20:28] iter = 02650, loss = 2.9833
2024-10-31 01:20:32: [2024-10-31 01:20:32] iter = 02660, loss = 2.6427
2024-10-31 01:20:36: [2024-10-31 01:20:36] iter = 02670, loss = 3.6051
2024-10-31 01:20:40: [2024-10-31 01:20:40] iter = 02680, loss = 3.7026
2024-10-31 01:20:44: [2024-10-31 01:20:44] iter = 02690, loss = 2.5312
2024-10-31 01:20:48: [2024-10-31 01:20:48] iter = 02700, loss = 1.8842
2024-10-31 01:20:52: [2024-10-31 01:20:52] iter = 02710, loss = 2.3397
2024-10-31 01:20:56: [2024-10-31 01:20:56] iter = 02720, loss = 3.0779
2024-10-31 01:21:00: [2024-10-31 01:21:00] iter = 02730, loss = 1.8515
2024-10-31 01:21:03: [2024-10-31 01:21:03] iter = 02740, loss = 2.0342
2024-10-31 01:21:07: [2024-10-31 01:21:07] iter = 02750, loss = 2.6759
2024-10-31 01:21:11: [2024-10-31 01:21:11] iter = 02760, loss = 1.9198
2024-10-31 01:21:14: [2024-10-31 01:21:14] iter = 02770, loss = 7.8697
2024-10-31 01:21:18: [2024-10-31 01:21:18] iter = 02780, loss = 3.1065
2024-10-31 01:21:22: [2024-10-31 01:21:22] iter = 02790, loss = 2.1918
2024-10-31 01:21:25: [2024-10-31 01:21:25] iter = 02800, loss = 2.5821
2024-10-31 01:21:28: [2024-10-31 01:21:28] iter = 02810, loss = 1.9097
2024-10-31 01:21:32: [2024-10-31 01:21:32] iter = 02820, loss = 3.9587
2024-10-31 01:21:36: [2024-10-31 01:21:36] iter = 02830, loss = 2.5387
2024-10-31 01:21:39: [2024-10-31 01:21:39] iter = 02840, loss = 2.9326
2024-10-31 01:21:43: [2024-10-31 01:21:43] iter = 02850, loss = 2.2977
2024-10-31 01:21:47: [2024-10-31 01:21:47] iter = 02860, loss = 3.5402
2024-10-31 01:21:49: [2024-10-31 01:21:49] iter = 02870, loss = 4.3853
2024-10-31 01:21:52: [2024-10-31 01:21:52] iter = 02880, loss = 2.2111
2024-10-31 01:21:55: [2024-10-31 01:21:55] iter = 02890, loss = 2.3397
2024-10-31 01:21:59: [2024-10-31 01:21:59] iter = 02900, loss = 2.7357
2024-10-31 01:22:02: [2024-10-31 01:22:02] iter = 02910, loss = 2.5633
2024-10-31 01:22:05: [2024-10-31 01:22:05] iter = 02920, loss = 2.0375
2024-10-31 01:22:09: [2024-10-31 01:22:09] iter = 02930, loss = 2.7129
2024-10-31 01:22:12: [2024-10-31 01:22:12] iter = 02940, loss = 5.3883
2024-10-31 01:22:16: [2024-10-31 01:22:16] iter = 02950, loss = 2.4939
2024-10-31 01:22:20: [2024-10-31 01:22:20] iter = 02960, loss = 1.6539
2024-10-31 01:22:23: [2024-10-31 01:22:23] iter = 02970, loss = 2.3377
2024-10-31 01:22:27: [2024-10-31 01:22:27] iter = 02980, loss = 2.1927
2024-10-31 01:22:31: [2024-10-31 01:22:31] iter = 02990, loss = 2.4245
2024-10-31 01:22:34: [2024-10-31 01:22:34] iter = 03000, loss = 2.1013
2024-10-31 01:22:38: [2024-10-31 01:22:38] iter = 03010, loss = 3.2209
2024-10-31 01:22:41: [2024-10-31 01:22:41] iter = 03020, loss = 2.7572
2024-10-31 01:22:45: [2024-10-31 01:22:45] iter = 03030, loss = 3.2991
2024-10-31 01:22:49: [2024-10-31 01:22:49] iter = 03040, loss = 2.2610
2024-10-31 01:22:52: [2024-10-31 01:22:52] iter = 03050, loss = 2.3365
2024-10-31 01:22:56: [2024-10-31 01:22:56] iter = 03060, loss = 2.6495
2024-10-31 01:23:00: [2024-10-31 01:23:00] iter = 03070, loss = 1.9493
2024-10-31 01:23:03: [2024-10-31 01:23:03] iter = 03080, loss = 2.0043
2024-10-31 01:23:06: [2024-10-31 01:23:06] iter = 03090, loss = 3.3293
2024-10-31 01:23:10: [2024-10-31 01:23:10] iter = 03100, loss = 2.3760
2024-10-31 01:23:14: [2024-10-31 01:23:14] iter = 03110, loss = 2.3165
2024-10-31 01:23:18: [2024-10-31 01:23:18] iter = 03120, loss = 2.0704
2024-10-31 01:23:21: [2024-10-31 01:23:21] iter = 03130, loss = 2.5601
2024-10-31 01:23:25: [2024-10-31 01:23:25] iter = 03140, loss = 2.5232
2024-10-31 01:23:27: [2024-10-31 01:23:27] iter = 03150, loss = 1.9368
2024-10-31 01:23:31: [2024-10-31 01:23:31] iter = 03160, loss = 2.0212
2024-10-31 01:23:34: [2024-10-31 01:23:34] iter = 03170, loss = 1.8464
2024-10-31 01:23:38: [2024-10-31 01:23:38] iter = 03180, loss = 2.7841
2024-10-31 01:23:41: [2024-10-31 01:23:41] iter = 03190, loss = 2.5266
2024-10-31 01:23:45: [2024-10-31 01:23:45] iter = 03200, loss = 2.5547
2024-10-31 01:23:47: [2024-10-31 01:23:47] iter = 03210, loss = 2.3478
2024-10-31 01:23:51: [2024-10-31 01:23:51] iter = 03220, loss = 2.8871
2024-10-31 01:23:54: [2024-10-31 01:23:54] iter = 03230, loss = 4.0280
2024-10-31 01:23:57: [2024-10-31 01:23:57] iter = 03240, loss = 1.9594
2024-10-31 01:24:01: [2024-10-31 01:24:01] iter = 03250, loss = 2.2746
2024-10-31 01:24:04: [2024-10-31 01:24:04] iter = 03260, loss = 2.7623
2024-10-31 01:24:08: [2024-10-31 01:24:08] iter = 03270, loss = 2.0757
2024-10-31 01:24:11: [2024-10-31 01:24:11] iter = 03280, loss = 2.5752
2024-10-31 01:24:15: [2024-10-31 01:24:15] iter = 03290, loss = 1.9408
2024-10-31 01:24:18: [2024-10-31 01:24:18] iter = 03300, loss = 2.3935
2024-10-31 01:24:21: [2024-10-31 01:24:21] iter = 03310, loss = 2.3718
2024-10-31 01:24:24: [2024-10-31 01:24:24] iter = 03320, loss = 2.0731
2024-10-31 01:24:28: [2024-10-31 01:24:28] iter = 03330, loss = 2.6583
2024-10-31 01:24:31: [2024-10-31 01:24:31] iter = 03340, loss = 2.1548
2024-10-31 01:24:35: [2024-10-31 01:24:35] iter = 03350, loss = 1.7446
2024-10-31 01:24:38: [2024-10-31 01:24:38] iter = 03360, loss = 5.0561
2024-10-31 01:24:41: [2024-10-31 01:24:41] iter = 03370, loss = 2.0828
2024-10-31 01:24:45: [2024-10-31 01:24:45] iter = 03380, loss = 2.9689
2024-10-31 01:24:48: [2024-10-31 01:24:48] iter = 03390, loss = 2.2006
2024-10-31 01:24:51: [2024-10-31 01:24:51] iter = 03400, loss = 2.1362
2024-10-31 01:24:55: [2024-10-31 01:24:55] iter = 03410, loss = 3.4238
2024-10-31 01:24:59: [2024-10-31 01:24:59] iter = 03420, loss = 2.1840
2024-10-31 01:25:03: [2024-10-31 01:25:03] iter = 03430, loss = 2.2465
2024-10-31 01:25:06: [2024-10-31 01:25:06] iter = 03440, loss = 2.4594
2024-10-31 01:25:10: [2024-10-31 01:25:10] iter = 03450, loss = 2.2922
2024-10-31 01:25:14: [2024-10-31 01:25:14] iter = 03460, loss = 2.7772
2024-10-31 01:25:17: [2024-10-31 01:25:17] iter = 03470, loss = 2.2430
2024-10-31 01:25:21: [2024-10-31 01:25:21] iter = 03480, loss = 2.6222
2024-10-31 01:25:25: [2024-10-31 01:25:25] iter = 03490, loss = 2.6802
2024-10-31 01:25:28: [2024-10-31 01:25:28] iter = 03500, loss = 2.6066
2024-10-31 01:25:32: [2024-10-31 01:25:32] iter = 03510, loss = 1.9033
2024-10-31 01:25:35: [2024-10-31 01:25:35] iter = 03520, loss = 2.0077
2024-10-31 01:25:38: [2024-10-31 01:25:38] iter = 03530, loss = 2.0139
2024-10-31 01:25:41: [2024-10-31 01:25:41] iter = 03540, loss = 2.1770
2024-10-31 01:25:45: [2024-10-31 01:25:45] iter = 03550, loss = 1.8484
2024-10-31 01:25:49: [2024-10-31 01:25:49] iter = 03560, loss = 1.9264
2024-10-31 01:25:52: [2024-10-31 01:25:52] iter = 03570, loss = 2.6750
2024-10-31 01:25:57: [2024-10-31 01:25:57] iter = 03580, loss = 1.9022
2024-10-31 01:26:02: [2024-10-31 01:26:02] iter = 03590, loss = 1.9615
2024-10-31 01:26:06: [2024-10-31 01:26:06] iter = 03600, loss = 1.9479
2024-10-31 01:26:09: [2024-10-31 01:26:09] iter = 03610, loss = 2.7042
2024-10-31 01:26:12: [2024-10-31 01:26:12] iter = 03620, loss = 1.9962
2024-10-31 01:26:15: [2024-10-31 01:26:15] iter = 03630, loss = 2.1456
2024-10-31 01:26:18: [2024-10-31 01:26:18] iter = 03640, loss = 2.0752
2024-10-31 01:26:21: [2024-10-31 01:26:21] iter = 03650, loss = 1.7274
2024-10-31 01:26:25: [2024-10-31 01:26:25] iter = 03660, loss = 2.6389
2024-10-31 01:26:28: [2024-10-31 01:26:28] iter = 03670, loss = 1.9431
2024-10-31 01:26:32: [2024-10-31 01:26:32] iter = 03680, loss = 1.8421
2024-10-31 01:26:36: [2024-10-31 01:26:36] iter = 03690, loss = 2.2423
2024-10-31 01:26:39: [2024-10-31 01:26:39] iter = 03700, loss = 2.3680
2024-10-31 01:26:42: [2024-10-31 01:26:42] iter = 03710, loss = 3.9837
2024-10-31 01:26:46: [2024-10-31 01:26:46] iter = 03720, loss = 1.9823
2024-10-31 01:26:49: [2024-10-31 01:26:49] iter = 03730, loss = 1.8926
2024-10-31 01:26:53: [2024-10-31 01:26:53] iter = 03740, loss = 2.9109
2024-10-31 01:26:57: [2024-10-31 01:26:57] iter = 03750, loss = 2.0491
2024-10-31 01:27:01: [2024-10-31 01:27:01] iter = 03760, loss = 1.8453
2024-10-31 01:27:04: [2024-10-31 01:27:04] iter = 03770, loss = 2.2156
2024-10-31 01:27:07: [2024-10-31 01:27:07] iter = 03780, loss = 3.5161
2024-10-31 01:27:11: [2024-10-31 01:27:11] iter = 03790, loss = 2.1953
2024-10-31 01:27:15: [2024-10-31 01:27:15] iter = 03800, loss = 2.1641
2024-10-31 01:27:20: [2024-10-31 01:27:20] iter = 03810, loss = 2.0913
2024-10-31 01:27:24: [2024-10-31 01:27:24] iter = 03820, loss = 3.4214
2024-10-31 01:27:28: [2024-10-31 01:27:28] iter = 03830, loss = 1.7103
2024-10-31 01:27:31: [2024-10-31 01:27:31] iter = 03840, loss = 2.3451
2024-10-31 01:27:35: [2024-10-31 01:27:35] iter = 03850, loss = 2.3817
2024-10-31 01:27:38: [2024-10-31 01:27:38] iter = 03860, loss = 2.1841
2024-10-31 01:27:42: [2024-10-31 01:27:42] iter = 03870, loss = 1.8864
2024-10-31 01:27:45: [2024-10-31 01:27:45] iter = 03880, loss = 5.8270
2024-10-31 01:27:49: [2024-10-31 01:27:49] iter = 03890, loss = 2.4801
2024-10-31 01:27:52: [2024-10-31 01:27:52] iter = 03900, loss = 2.0786
2024-10-31 01:27:55: [2024-10-31 01:27:55] iter = 03910, loss = 2.8801
2024-10-31 01:27:59: [2024-10-31 01:27:59] iter = 03920, loss = 2.3554
2024-10-31 01:28:03: [2024-10-31 01:28:03] iter = 03930, loss = 1.9379
2024-10-31 01:28:07: [2024-10-31 01:28:07] iter = 03940, loss = 2.9922
2024-10-31 01:28:10: [2024-10-31 01:28:10] iter = 03950, loss = 6.1041
2024-10-31 01:28:14: [2024-10-31 01:28:14] iter = 03960, loss = 1.9491
2024-10-31 01:28:17: [2024-10-31 01:28:17] iter = 03970, loss = 2.7777
2024-10-31 01:28:20: [2024-10-31 01:28:20] iter = 03980, loss = 2.3296
2024-10-31 01:28:24: [2024-10-31 01:28:24] iter = 03990, loss = 2.1999
2024-10-31 01:28:26: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 4000
2024-10-31 01:28:26: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 01:28:26: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 6935}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:30:30: Evaluate 5 random ConvNet, ACCmean = 0.7809 ACCstd = 0.0048
-------------------------
2024-10-31 01:30:30: Evaluate 5 random ConvNet, SENmean = 0.7828 SENstd = 0.0051
-------------------------
2024-10-31 01:30:30: Evaluate 5 random ConvNet, SPEmean = 0.9781 SPEstd = 0.0005
-------------------------
2024-10-31 01:30:30: Evaluate 5 random ConvNet, F!mean = 0.7678 F!std = 0.0051
-------------------------
2024-10-31 01:30:30: Evaluate 5 random ConvNet, mean = 0.7809 std = 0.0048
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:30:30: [2024-10-31 01:30:30] iter = 04000, loss = 3.5228
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:30:33: [2024-10-31 01:30:33] iter = 04010, loss = 1.7443
2024-10-31 01:30:36: [2024-10-31 01:30:36] iter = 04020, loss = 2.2016
2024-10-31 01:30:40: [2024-10-31 01:30:40] iter = 04030, loss = 2.5818
2024-10-31 01:30:43: [2024-10-31 01:30:43] iter = 04040, loss = 6.2904
2024-10-31 01:30:46: [2024-10-31 01:30:46] iter = 04050, loss = 2.1704
2024-10-31 01:30:50: [2024-10-31 01:30:50] iter = 04060, loss = 2.3307
2024-10-31 01:30:52: [2024-10-31 01:30:52] iter = 04070, loss = 2.0853
2024-10-31 01:30:56: [2024-10-31 01:30:56] iter = 04080, loss = 1.8099
2024-10-31 01:31:00: [2024-10-31 01:31:00] iter = 04090, loss = 3.5340
2024-10-31 01:31:02: [2024-10-31 01:31:02] iter = 04100, loss = 2.7706
2024-10-31 01:31:06: [2024-10-31 01:31:06] iter = 04110, loss = 1.8959
2024-10-31 01:31:10: [2024-10-31 01:31:10] iter = 04120, loss = 2.0033
2024-10-31 01:31:13: [2024-10-31 01:31:13] iter = 04130, loss = 2.4824
2024-10-31 01:31:17: [2024-10-31 01:31:17] iter = 04140, loss = 3.0450
2024-10-31 01:31:20: [2024-10-31 01:31:20] iter = 04150, loss = 3.1111
2024-10-31 01:31:23: [2024-10-31 01:31:23] iter = 04160, loss = 4.9691
2024-10-31 01:31:27: [2024-10-31 01:31:27] iter = 04170, loss = 2.7214
2024-10-31 01:31:30: [2024-10-31 01:31:30] iter = 04180, loss = 2.1174
2024-10-31 01:31:33: [2024-10-31 01:31:33] iter = 04190, loss = 2.0310
2024-10-31 01:31:36: [2024-10-31 01:31:36] iter = 04200, loss = 1.9035
2024-10-31 01:31:40: [2024-10-31 01:31:40] iter = 04210, loss = 2.4224
2024-10-31 01:31:43: [2024-10-31 01:31:43] iter = 04220, loss = 2.3955
2024-10-31 01:31:47: [2024-10-31 01:31:47] iter = 04230, loss = 1.9392
2024-10-31 01:31:50: [2024-10-31 01:31:50] iter = 04240, loss = 2.3715
2024-10-31 01:31:53: [2024-10-31 01:31:53] iter = 04250, loss = 2.1779
2024-10-31 01:31:56: [2024-10-31 01:31:56] iter = 04260, loss = 2.1555
2024-10-31 01:31:59: [2024-10-31 01:31:59] iter = 04270, loss = 2.8470
2024-10-31 01:32:02: [2024-10-31 01:32:02] iter = 04280, loss = 2.2560
2024-10-31 01:32:05: [2024-10-31 01:32:05] iter = 04290, loss = 1.9566
2024-10-31 01:32:08: [2024-10-31 01:32:08] iter = 04300, loss = 2.5015
2024-10-31 01:32:11: [2024-10-31 01:32:11] iter = 04310, loss = 2.2373
2024-10-31 01:32:15: [2024-10-31 01:32:15] iter = 04320, loss = 3.8017
2024-10-31 01:32:18: [2024-10-31 01:32:18] iter = 04330, loss = 2.0448
2024-10-31 01:32:22: [2024-10-31 01:32:22] iter = 04340, loss = 1.8774
2024-10-31 01:32:24: [2024-10-31 01:32:24] iter = 04350, loss = 2.0113
2024-10-31 01:32:27: [2024-10-31 01:32:27] iter = 04360, loss = 2.3586
2024-10-31 01:32:30: [2024-10-31 01:32:30] iter = 04370, loss = 1.8868
2024-10-31 01:32:33: [2024-10-31 01:32:33] iter = 04380, loss = 2.4339
2024-10-31 01:32:36: [2024-10-31 01:32:36] iter = 04390, loss = 4.9125
2024-10-31 01:32:39: [2024-10-31 01:32:39] iter = 04400, loss = 1.9731
2024-10-31 01:32:43: [2024-10-31 01:32:43] iter = 04410, loss = 2.0102
2024-10-31 01:32:45: [2024-10-31 01:32:45] iter = 04420, loss = 2.2873
2024-10-31 01:32:49: [2024-10-31 01:32:49] iter = 04430, loss = 2.1825
2024-10-31 01:32:52: [2024-10-31 01:32:52] iter = 04440, loss = 2.1512
2024-10-31 01:32:55: [2024-10-31 01:32:55] iter = 04450, loss = 2.2109
2024-10-31 01:32:58: [2024-10-31 01:32:58] iter = 04460, loss = 2.2806
2024-10-31 01:33:01: [2024-10-31 01:33:01] iter = 04470, loss = 3.1496
2024-10-31 01:33:05: [2024-10-31 01:33:05] iter = 04480, loss = 1.7735
2024-10-31 01:33:08: [2024-10-31 01:33:08] iter = 04490, loss = 1.6195
2024-10-31 01:33:12: [2024-10-31 01:33:12] iter = 04500, loss = 5.6160
2024-10-31 01:33:15: [2024-10-31 01:33:15] iter = 04510, loss = 2.7252
2024-10-31 01:33:19: [2024-10-31 01:33:19] iter = 04520, loss = 1.8871
2024-10-31 01:33:23: [2024-10-31 01:33:23] iter = 04530, loss = 2.2417
2024-10-31 01:33:26: [2024-10-31 01:33:26] iter = 04540, loss = 2.0788
2024-10-31 01:33:29: [2024-10-31 01:33:29] iter = 04550, loss = 2.5819
2024-10-31 01:33:34: [2024-10-31 01:33:34] iter = 04560, loss = 2.1667
2024-10-31 01:33:37: [2024-10-31 01:33:37] iter = 04570, loss = 2.1194
2024-10-31 01:33:40: [2024-10-31 01:33:40] iter = 04580, loss = 2.3836
2024-10-31 01:33:44: [2024-10-31 01:33:44] iter = 04590, loss = 2.2453
2024-10-31 01:33:48: [2024-10-31 01:33:48] iter = 04600, loss = 3.0663
2024-10-31 01:33:51: [2024-10-31 01:33:51] iter = 04610, loss = 2.2186
2024-10-31 01:33:55: [2024-10-31 01:33:55] iter = 04620, loss = 2.3138
2024-10-31 01:33:58: [2024-10-31 01:33:58] iter = 04630, loss = 1.9049
2024-10-31 01:34:01: [2024-10-31 01:34:01] iter = 04640, loss = 2.1587
2024-10-31 01:34:05: [2024-10-31 01:34:05] iter = 04650, loss = 1.8963
2024-10-31 01:34:08: [2024-10-31 01:34:08] iter = 04660, loss = 2.2228
2024-10-31 01:34:12: [2024-10-31 01:34:12] iter = 04670, loss = 2.7974
2024-10-31 01:34:15: [2024-10-31 01:34:15] iter = 04680, loss = 1.8629
2024-10-31 01:34:19: [2024-10-31 01:34:19] iter = 04690, loss = 2.0206
2024-10-31 01:34:22: [2024-10-31 01:34:22] iter = 04700, loss = 2.0176
2024-10-31 01:34:26: [2024-10-31 01:34:26] iter = 04710, loss = 1.9610
2024-10-31 01:34:29: [2024-10-31 01:34:29] iter = 04720, loss = 1.9586
2024-10-31 01:34:32: [2024-10-31 01:34:32] iter = 04730, loss = 3.2292
2024-10-31 01:34:36: [2024-10-31 01:34:36] iter = 04740, loss = 2.4785
2024-10-31 01:34:39: [2024-10-31 01:34:39] iter = 04750, loss = 2.0379
2024-10-31 01:34:41: [2024-10-31 01:34:41] iter = 04760, loss = 2.8449
2024-10-31 01:34:45: [2024-10-31 01:34:45] iter = 04770, loss = 2.2270
2024-10-31 01:34:48: [2024-10-31 01:34:48] iter = 04780, loss = 2.4283
2024-10-31 01:34:52: [2024-10-31 01:34:52] iter = 04790, loss = 3.8415
2024-10-31 01:34:55: [2024-10-31 01:34:55] iter = 04800, loss = 2.7142
2024-10-31 01:34:58: [2024-10-31 01:34:58] iter = 04810, loss = 2.7389
2024-10-31 01:35:02: [2024-10-31 01:35:02] iter = 04820, loss = 2.0567
2024-10-31 01:35:05: [2024-10-31 01:35:05] iter = 04830, loss = 2.1766
2024-10-31 01:35:09: [2024-10-31 01:35:09] iter = 04840, loss = 2.6658
2024-10-31 01:35:12: [2024-10-31 01:35:12] iter = 04850, loss = 1.7572
2024-10-31 01:35:15: [2024-10-31 01:35:15] iter = 04860, loss = 2.4397
2024-10-31 01:35:18: [2024-10-31 01:35:18] iter = 04870, loss = 2.7464
2024-10-31 01:35:22: [2024-10-31 01:35:22] iter = 04880, loss = 1.9142
2024-10-31 01:35:25: [2024-10-31 01:35:25] iter = 04890, loss = 2.1651
2024-10-31 01:35:28: [2024-10-31 01:35:28] iter = 04900, loss = 2.3502
2024-10-31 01:35:32: [2024-10-31 01:35:32] iter = 04910, loss = 3.5351
2024-10-31 01:35:35: [2024-10-31 01:35:35] iter = 04920, loss = 2.0088
2024-10-31 01:35:38: [2024-10-31 01:35:38] iter = 04930, loss = 1.7561
2024-10-31 01:35:42: [2024-10-31 01:35:42] iter = 04940, loss = 2.4878
2024-10-31 01:35:45: [2024-10-31 01:35:45] iter = 04950, loss = 2.0341
2024-10-31 01:35:48: [2024-10-31 01:35:48] iter = 04960, loss = 2.0878
2024-10-31 01:35:51: [2024-10-31 01:35:51] iter = 04970, loss = 1.7514
2024-10-31 01:35:55: [2024-10-31 01:35:55] iter = 04980, loss = 1.9388
2024-10-31 01:35:57: [2024-10-31 01:35:57] iter = 04990, loss = 2.0284
2024-10-31 01:36:00: [2024-10-31 01:36:00] iter = 05000, loss = 1.8085
2024-10-31 01:36:04: [2024-10-31 01:36:04] iter = 05010, loss = 2.3067
2024-10-31 01:36:07: [2024-10-31 01:36:07] iter = 05020, loss = 2.2389
2024-10-31 01:36:10: [2024-10-31 01:36:10] iter = 05030, loss = 2.0696
2024-10-31 01:36:13: [2024-10-31 01:36:13] iter = 05040, loss = 2.4538
2024-10-31 01:36:17: [2024-10-31 01:36:17] iter = 05050, loss = 1.9910
2024-10-31 01:36:20: [2024-10-31 01:36:20] iter = 05060, loss = 4.6012
2024-10-31 01:36:23: [2024-10-31 01:36:23] iter = 05070, loss = 2.0577
2024-10-31 01:36:26: [2024-10-31 01:36:26] iter = 05080, loss = 4.7181
2024-10-31 01:36:29: [2024-10-31 01:36:29] iter = 05090, loss = 1.9688
2024-10-31 01:36:32: [2024-10-31 01:36:32] iter = 05100, loss = 2.1699
2024-10-31 01:36:35: [2024-10-31 01:36:35] iter = 05110, loss = 1.8271
2024-10-31 01:36:38: [2024-10-31 01:36:38] iter = 05120, loss = 2.1538
2024-10-31 01:36:41: [2024-10-31 01:36:41] iter = 05130, loss = 2.2434
2024-10-31 01:36:43: [2024-10-31 01:36:43] iter = 05140, loss = 2.6035
2024-10-31 01:36:45: [2024-10-31 01:36:45] iter = 05150, loss = 2.2389
2024-10-31 01:36:49: [2024-10-31 01:36:49] iter = 05160, loss = 2.4089
2024-10-31 01:36:52: [2024-10-31 01:36:52] iter = 05170, loss = 3.2233
2024-10-31 01:36:55: [2024-10-31 01:36:55] iter = 05180, loss = 3.9473
2024-10-31 01:36:57: [2024-10-31 01:36:57] iter = 05190, loss = 2.5526
2024-10-31 01:37:01: [2024-10-31 01:37:01] iter = 05200, loss = 1.7507
2024-10-31 01:37:03: [2024-10-31 01:37:03] iter = 05210, loss = 2.4733
2024-10-31 01:37:06: [2024-10-31 01:37:06] iter = 05220, loss = 2.3634
2024-10-31 01:37:10: [2024-10-31 01:37:10] iter = 05230, loss = 1.8543
2024-10-31 01:37:13: [2024-10-31 01:37:13] iter = 05240, loss = 1.8226
2024-10-31 01:37:17: [2024-10-31 01:37:17] iter = 05250, loss = 3.0340
2024-10-31 01:37:20: [2024-10-31 01:37:20] iter = 05260, loss = 2.9459
2024-10-31 01:37:23: [2024-10-31 01:37:23] iter = 05270, loss = 1.8746
2024-10-31 01:37:25: [2024-10-31 01:37:25] iter = 05280, loss = 2.2705
2024-10-31 01:37:28: [2024-10-31 01:37:28] iter = 05290, loss = 2.0549
2024-10-31 01:37:31: [2024-10-31 01:37:31] iter = 05300, loss = 2.1700
2024-10-31 01:37:35: [2024-10-31 01:37:35] iter = 05310, loss = 4.2158
2024-10-31 01:37:39: [2024-10-31 01:37:39] iter = 05320, loss = 2.2878
2024-10-31 01:37:43: [2024-10-31 01:37:43] iter = 05330, loss = 2.0413
2024-10-31 01:37:46: [2024-10-31 01:37:46] iter = 05340, loss = 6.8451
2024-10-31 01:37:49: [2024-10-31 01:37:49] iter = 05350, loss = 2.4980
2024-10-31 01:37:51: [2024-10-31 01:37:51] iter = 05360, loss = 2.4849
2024-10-31 01:37:54: [2024-10-31 01:37:54] iter = 05370, loss = 1.9344
2024-10-31 01:37:57: [2024-10-31 01:37:57] iter = 05380, loss = 2.0213
2024-10-31 01:38:00: [2024-10-31 01:38:00] iter = 05390, loss = 2.0434
2024-10-31 01:38:02: [2024-10-31 01:38:02] iter = 05400, loss = 1.9413
2024-10-31 01:38:05: [2024-10-31 01:38:05] iter = 05410, loss = 4.8316
2024-10-31 01:38:08: [2024-10-31 01:38:08] iter = 05420, loss = 3.7937
2024-10-31 01:38:12: [2024-10-31 01:38:12] iter = 05430, loss = 1.9366
2024-10-31 01:38:15: [2024-10-31 01:38:15] iter = 05440, loss = 2.0665
2024-10-31 01:38:18: [2024-10-31 01:38:18] iter = 05450, loss = 2.7383
2024-10-31 01:38:21: [2024-10-31 01:38:21] iter = 05460, loss = 2.8698
2024-10-31 01:38:25: [2024-10-31 01:38:25] iter = 05470, loss = 2.3363
2024-10-31 01:38:27: [2024-10-31 01:38:27] iter = 05480, loss = 1.8472
2024-10-31 01:38:31: [2024-10-31 01:38:31] iter = 05490, loss = 1.7138
2024-10-31 01:38:34: [2024-10-31 01:38:34] iter = 05500, loss = 2.0184
2024-10-31 01:38:38: [2024-10-31 01:38:38] iter = 05510, loss = 2.0994
2024-10-31 01:38:42: [2024-10-31 01:38:42] iter = 05520, loss = 2.0247
2024-10-31 01:38:45: [2024-10-31 01:38:45] iter = 05530, loss = 1.8114
2024-10-31 01:38:49: [2024-10-31 01:38:49] iter = 05540, loss = 4.3767
2024-10-31 01:38:52: [2024-10-31 01:38:52] iter = 05550, loss = 2.4292
2024-10-31 01:38:55: [2024-10-31 01:38:55] iter = 05560, loss = 1.6943
2024-10-31 01:38:58: [2024-10-31 01:38:58] iter = 05570, loss = 2.1273
2024-10-31 01:39:02: [2024-10-31 01:39:02] iter = 05580, loss = 3.4097
2024-10-31 01:39:05: [2024-10-31 01:39:05] iter = 05590, loss = 2.2098
2024-10-31 01:39:09: [2024-10-31 01:39:09] iter = 05600, loss = 2.0139
2024-10-31 01:39:13: [2024-10-31 01:39:13] iter = 05610, loss = 2.2202
2024-10-31 01:39:16: [2024-10-31 01:39:16] iter = 05620, loss = 7.1394
2024-10-31 01:39:19: [2024-10-31 01:39:19] iter = 05630, loss = 1.8218
2024-10-31 01:39:23: [2024-10-31 01:39:23] iter = 05640, loss = 1.9743
2024-10-31 01:39:27: [2024-10-31 01:39:27] iter = 05650, loss = 9.0836
2024-10-31 01:39:30: [2024-10-31 01:39:30] iter = 05660, loss = 3.2816
2024-10-31 01:39:34: [2024-10-31 01:39:34] iter = 05670, loss = 2.3383
2024-10-31 01:39:38: [2024-10-31 01:39:38] iter = 05680, loss = 2.4982
2024-10-31 01:39:41: [2024-10-31 01:39:41] iter = 05690, loss = 4.3955
2024-10-31 01:39:44: [2024-10-31 01:39:44] iter = 05700, loss = 2.1228
2024-10-31 01:39:48: [2024-10-31 01:39:48] iter = 05710, loss = 2.1801
2024-10-31 01:39:52: [2024-10-31 01:39:52] iter = 05720, loss = 2.1088
2024-10-31 01:39:55: [2024-10-31 01:39:55] iter = 05730, loss = 2.5217
2024-10-31 01:39:59: [2024-10-31 01:39:59] iter = 05740, loss = 2.2401
2024-10-31 01:40:02: [2024-10-31 01:40:02] iter = 05750, loss = 2.0204
2024-10-31 01:40:06: [2024-10-31 01:40:06] iter = 05760, loss = 2.1398
2024-10-31 01:40:10: [2024-10-31 01:40:10] iter = 05770, loss = 1.8640
2024-10-31 01:40:14: [2024-10-31 01:40:14] iter = 05780, loss = 3.3887
2024-10-31 01:40:17: [2024-10-31 01:40:17] iter = 05790, loss = 3.1682
2024-10-31 01:40:21: [2024-10-31 01:40:21] iter = 05800, loss = 2.2937
2024-10-31 01:40:24: [2024-10-31 01:40:24] iter = 05810, loss = 2.5552
2024-10-31 01:40:28: [2024-10-31 01:40:28] iter = 05820, loss = 2.0843
2024-10-31 01:40:32: [2024-10-31 01:40:32] iter = 05830, loss = 2.2041
2024-10-31 01:40:35: [2024-10-31 01:40:35] iter = 05840, loss = 4.0002
2024-10-31 01:40:38: [2024-10-31 01:40:38] iter = 05850, loss = 1.8365
2024-10-31 01:40:42: [2024-10-31 01:40:42] iter = 05860, loss = 1.7293
2024-10-31 01:40:46: [2024-10-31 01:40:46] iter = 05870, loss = 2.0745
2024-10-31 01:40:49: [2024-10-31 01:40:49] iter = 05880, loss = 2.0550
2024-10-31 01:40:53: [2024-10-31 01:40:53] iter = 05890, loss = 2.9755
2024-10-31 01:40:57: [2024-10-31 01:40:57] iter = 05900, loss = 2.4406
2024-10-31 01:41:00: [2024-10-31 01:41:00] iter = 05910, loss = 2.9100
2024-10-31 01:41:04: [2024-10-31 01:41:04] iter = 05920, loss = 2.1424
2024-10-31 01:41:08: [2024-10-31 01:41:08] iter = 05930, loss = 1.8150
2024-10-31 01:41:11: [2024-10-31 01:41:11] iter = 05940, loss = 2.1004
2024-10-31 01:41:14: [2024-10-31 01:41:14] iter = 05950, loss = 2.0317
2024-10-31 01:41:18: [2024-10-31 01:41:18] iter = 05960, loss = 2.3569
2024-10-31 01:41:22: [2024-10-31 01:41:22] iter = 05970, loss = 2.8762
2024-10-31 01:41:26: [2024-10-31 01:41:26] iter = 05980, loss = 2.3362
2024-10-31 01:41:29: [2024-10-31 01:41:29] iter = 05990, loss = 2.2626
2024-10-31 01:41:32: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 6000
2024-10-31 01:41:32: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 01:41:32: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 92261}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:43:43: Evaluate 5 random ConvNet, ACCmean = 0.7792 ACCstd = 0.0012
-------------------------
2024-10-31 01:43:43: Evaluate 5 random ConvNet, SENmean = 0.7750 SENstd = 0.0023
-------------------------
2024-10-31 01:43:43: Evaluate 5 random ConvNet, SPEmean = 0.9779 SPEstd = 0.0001
-------------------------
2024-10-31 01:43:43: Evaluate 5 random ConvNet, F!mean = 0.7624 F!std = 0.0026
-------------------------
2024-10-31 01:43:43: Evaluate 5 random ConvNet, mean = 0.7792 std = 0.0012
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:43:43: [2024-10-31 01:43:43] iter = 06000, loss = 1.8673
2024-10-31 01:43:46: [2024-10-31 01:43:46] iter = 06010, loss = 2.7723
2024-10-31 01:43:50: [2024-10-31 01:43:50] iter = 06020, loss = 2.4678
2024-10-31 01:43:53: [2024-10-31 01:43:53] iter = 06030, loss = 2.4752
2024-10-31 01:43:57: [2024-10-31 01:43:57] iter = 06040, loss = 2.0978
2024-10-31 01:44:00: [2024-10-31 01:44:00] iter = 06050, loss = 1.9340
2024-10-31 01:44:03: [2024-10-31 01:44:03] iter = 06060, loss = 1.9609
2024-10-31 01:44:07: [2024-10-31 01:44:07] iter = 06070, loss = 2.4003
2024-10-31 01:44:10: [2024-10-31 01:44:10] iter = 06080, loss = 2.6793
2024-10-31 01:44:13: [2024-10-31 01:44:13] iter = 06090, loss = 3.3353
2024-10-31 01:44:17: [2024-10-31 01:44:17] iter = 06100, loss = 2.9355
2024-10-31 01:44:21: [2024-10-31 01:44:21] iter = 06110, loss = 6.0527
2024-10-31 01:44:24: [2024-10-31 01:44:24] iter = 06120, loss = 4.6620
2024-10-31 01:44:28: [2024-10-31 01:44:28] iter = 06130, loss = 2.5151
2024-10-31 01:44:31: [2024-10-31 01:44:31] iter = 06140, loss = 2.2577
2024-10-31 01:44:35: [2024-10-31 01:44:35] iter = 06150, loss = 1.9265
2024-10-31 01:44:38: [2024-10-31 01:44:38] iter = 06160, loss = 2.8731
2024-10-31 01:44:41: [2024-10-31 01:44:41] iter = 06170, loss = 2.3650
2024-10-31 01:44:43: [2024-10-31 01:44:43] iter = 06180, loss = 2.0890
2024-10-31 01:44:45: [2024-10-31 01:44:45] iter = 06190, loss = 2.3571
2024-10-31 01:44:48: [2024-10-31 01:44:48] iter = 06200, loss = 2.6386
2024-10-31 01:44:50: [2024-10-31 01:44:50] iter = 06210, loss = 1.9347
2024-10-31 01:44:54: [2024-10-31 01:44:54] iter = 06220, loss = 1.6992
2024-10-31 01:44:57: [2024-10-31 01:44:57] iter = 06230, loss = 1.9556
2024-10-31 01:45:00: [2024-10-31 01:45:00] iter = 06240, loss = 2.2519
2024-10-31 01:45:04: [2024-10-31 01:45:04] iter = 06250, loss = 2.0436
2024-10-31 01:45:07: [2024-10-31 01:45:07] iter = 06260, loss = 1.9412
2024-10-31 01:45:10: [2024-10-31 01:45:10] iter = 06270, loss = 2.3768
2024-10-31 01:45:13: [2024-10-31 01:45:13] iter = 06280, loss = 2.0330
2024-10-31 01:45:17: [2024-10-31 01:45:17] iter = 06290, loss = 2.2852
2024-10-31 01:45:21: [2024-10-31 01:45:21] iter = 06300, loss = 2.1994
2024-10-31 01:45:24: [2024-10-31 01:45:24] iter = 06310, loss = 2.3535
2024-10-31 01:45:27: [2024-10-31 01:45:27] iter = 06320, loss = 2.6040
2024-10-31 01:45:31: [2024-10-31 01:45:31] iter = 06330, loss = 1.9214
2024-10-31 01:45:34: [2024-10-31 01:45:34] iter = 06340, loss = 2.0434
2024-10-31 01:45:37: [2024-10-31 01:45:37] iter = 06350, loss = 2.8512
2024-10-31 01:45:40: [2024-10-31 01:45:40] iter = 06360, loss = 3.4393
2024-10-31 01:45:44: [2024-10-31 01:45:44] iter = 06370, loss = 2.3596
2024-10-31 01:45:47: [2024-10-31 01:45:47] iter = 06380, loss = 1.9775
2024-10-31 01:45:50: [2024-10-31 01:45:50] iter = 06390, loss = 1.7720
2024-10-31 01:45:53: [2024-10-31 01:45:53] iter = 06400, loss = 2.3417
2024-10-31 01:45:56: [2024-10-31 01:45:56] iter = 06410, loss = 1.9817
2024-10-31 01:45:59: [2024-10-31 01:45:59] iter = 06420, loss = 2.0852
2024-10-31 01:46:03: [2024-10-31 01:46:03] iter = 06430, loss = 2.7722
2024-10-31 01:46:06: [2024-10-31 01:46:06] iter = 06440, loss = 2.1745
2024-10-31 01:46:10: [2024-10-31 01:46:10] iter = 06450, loss = 2.7226
2024-10-31 01:46:13: [2024-10-31 01:46:13] iter = 06460, loss = 2.5693
2024-10-31 01:46:16: [2024-10-31 01:46:16] iter = 06470, loss = 1.7817
2024-10-31 01:46:19: [2024-10-31 01:46:19] iter = 06480, loss = 2.3359
2024-10-31 01:46:22: [2024-10-31 01:46:22] iter = 06490, loss = 1.8737
2024-10-31 01:46:25: [2024-10-31 01:46:25] iter = 06500, loss = 2.0419
2024-10-31 01:46:28: [2024-10-31 01:46:28] iter = 06510, loss = 3.5654
2024-10-31 01:46:31: [2024-10-31 01:46:31] iter = 06520, loss = 3.5280
2024-10-31 01:46:35: [2024-10-31 01:46:35] iter = 06530, loss = 1.9764
2024-10-31 01:46:38: [2024-10-31 01:46:38] iter = 06540, loss = 1.8720
2024-10-31 01:46:41: [2024-10-31 01:46:41] iter = 06550, loss = 2.5385
2024-10-31 01:46:45: [2024-10-31 01:46:45] iter = 06560, loss = 2.1237
2024-10-31 01:46:48: [2024-10-31 01:46:48] iter = 06570, loss = 1.8825
2024-10-31 01:46:51: [2024-10-31 01:46:51] iter = 06580, loss = 2.6018
2024-10-31 01:46:54: [2024-10-31 01:46:54] iter = 06590, loss = 1.9818
2024-10-31 01:46:58: [2024-10-31 01:46:58] iter = 06600, loss = 1.8227
2024-10-31 01:47:00: [2024-10-31 01:47:00] iter = 06610, loss = 2.6137
2024-10-31 01:47:03: [2024-10-31 01:47:03] iter = 06620, loss = 2.4553
2024-10-31 01:47:05: [2024-10-31 01:47:05] iter = 06630, loss = 2.6875
2024-10-31 01:47:08: [2024-10-31 01:47:08] iter = 06640, loss = 2.9080
2024-10-31 01:47:10: [2024-10-31 01:47:10] iter = 06650, loss = 2.1782
2024-10-31 01:47:12: [2024-10-31 01:47:12] iter = 06660, loss = 1.9605
2024-10-31 01:47:14: [2024-10-31 01:47:14] iter = 06670, loss = 1.9767
2024-10-31 01:47:17: [2024-10-31 01:47:17] iter = 06680, loss = 2.6677
2024-10-31 01:47:20: [2024-10-31 01:47:20] iter = 06690, loss = 2.4181
2024-10-31 01:47:23: [2024-10-31 01:47:23] iter = 06700, loss = 1.9169
2024-10-31 01:47:26: [2024-10-31 01:47:26] iter = 06710, loss = 2.9753
2024-10-31 01:47:28: [2024-10-31 01:47:28] iter = 06720, loss = 1.9259
2024-10-31 01:47:31: [2024-10-31 01:47:31] iter = 06730, loss = 2.1006
2024-10-31 01:47:35: [2024-10-31 01:47:35] iter = 06740, loss = 2.2100
2024-10-31 01:47:38: [2024-10-31 01:47:38] iter = 06750, loss = 3.5267
2024-10-31 01:47:41: [2024-10-31 01:47:41] iter = 06760, loss = 3.0136
2024-10-31 01:47:44: [2024-10-31 01:47:44] iter = 06770, loss = 1.9802
2024-10-31 01:47:47: [2024-10-31 01:47:47] iter = 06780, loss = 1.9777
2024-10-31 01:47:49: [2024-10-31 01:47:49] iter = 06790, loss = 2.0702
2024-10-31 01:47:52: [2024-10-31 01:47:52] iter = 06800, loss = 1.8537
2024-10-31 01:47:55: [2024-10-31 01:47:55] iter = 06810, loss = 1.8778
2024-10-31 01:47:58: [2024-10-31 01:47:58] iter = 06820, loss = 2.8030
2024-10-31 01:48:01: [2024-10-31 01:48:01] iter = 06830, loss = 2.5229
2024-10-31 01:48:04: [2024-10-31 01:48:04] iter = 06840, loss = 2.3423
2024-10-31 01:48:06: [2024-10-31 01:48:06] iter = 06850, loss = 1.8024
2024-10-31 01:48:09: [2024-10-31 01:48:09] iter = 06860, loss = 1.9850
2024-10-31 01:48:12: [2024-10-31 01:48:12] iter = 06870, loss = 2.3624
2024-10-31 01:48:16: [2024-10-31 01:48:16] iter = 06880, loss = 1.7852
2024-10-31 01:48:19: [2024-10-31 01:48:19] iter = 06890, loss = 1.9569
2024-10-31 01:48:21: [2024-10-31 01:48:21] iter = 06900, loss = 2.2584
2024-10-31 01:48:24: [2024-10-31 01:48:24] iter = 06910, loss = 2.8119
2024-10-31 01:48:27: [2024-10-31 01:48:27] iter = 06920, loss = 2.3648
2024-10-31 01:48:30: [2024-10-31 01:48:30] iter = 06930, loss = 2.8705
2024-10-31 01:48:33: [2024-10-31 01:48:33] iter = 06940, loss = 1.9854
2024-10-31 01:48:37: [2024-10-31 01:48:37] iter = 06950, loss = 2.5587
2024-10-31 01:48:40: [2024-10-31 01:48:40] iter = 06960, loss = 2.0028
2024-10-31 01:48:42: [2024-10-31 01:48:42] iter = 06970, loss = 4.8769
2024-10-31 01:48:45: [2024-10-31 01:48:45] iter = 06980, loss = 2.6681
2024-10-31 01:48:48: [2024-10-31 01:48:48] iter = 06990, loss = 2.2248
2024-10-31 01:48:51: [2024-10-31 01:48:51] iter = 07000, loss = 2.1250
2024-10-31 01:48:55: [2024-10-31 01:48:55] iter = 07010, loss = 1.7048
2024-10-31 01:48:58: [2024-10-31 01:48:58] iter = 07020, loss = 6.2768
2024-10-31 01:49:01: [2024-10-31 01:49:01] iter = 07030, loss = 2.5534
2024-10-31 01:49:05: [2024-10-31 01:49:05] iter = 07040, loss = 2.1810
2024-10-31 01:49:08: [2024-10-31 01:49:08] iter = 07050, loss = 2.6043
2024-10-31 01:49:10: [2024-10-31 01:49:10] iter = 07060, loss = 2.2401
2024-10-31 01:49:13: [2024-10-31 01:49:13] iter = 07070, loss = 1.7878
2024-10-31 01:49:16: [2024-10-31 01:49:16] iter = 07080, loss = 2.1520
2024-10-31 01:49:20: [2024-10-31 01:49:20] iter = 07090, loss = 2.1902
2024-10-31 01:49:22: [2024-10-31 01:49:22] iter = 07100, loss = 2.1183
2024-10-31 01:49:26: [2024-10-31 01:49:26] iter = 07110, loss = 2.7280
2024-10-31 01:49:29: [2024-10-31 01:49:29] iter = 07120, loss = 2.3993
2024-10-31 01:49:32: [2024-10-31 01:49:32] iter = 07130, loss = 2.3273
2024-10-31 01:49:35: [2024-10-31 01:49:35] iter = 07140, loss = 1.8015
2024-10-31 01:49:38: [2024-10-31 01:49:38] iter = 07150, loss = 2.0326
2024-10-31 01:49:42: [2024-10-31 01:49:42] iter = 07160, loss = 2.3962
2024-10-31 01:49:45: [2024-10-31 01:49:45] iter = 07170, loss = 2.3874
2024-10-31 01:49:48: [2024-10-31 01:49:48] iter = 07180, loss = 2.4358
2024-10-31 01:49:50: [2024-10-31 01:49:50] iter = 07190, loss = 2.8283
2024-10-31 01:49:53: [2024-10-31 01:49:53] iter = 07200, loss = 2.4097
2024-10-31 01:49:56: [2024-10-31 01:49:56] iter = 07210, loss = 2.3649
2024-10-31 01:49:59: [2024-10-31 01:49:59] iter = 07220, loss = 2.3700
2024-10-31 01:50:02: [2024-10-31 01:50:02] iter = 07230, loss = 2.6449
2024-10-31 01:50:05: [2024-10-31 01:50:05] iter = 07240, loss = 1.9130
2024-10-31 01:50:08: [2024-10-31 01:50:08] iter = 07250, loss = 1.9615
2024-10-31 01:50:11: [2024-10-31 01:50:11] iter = 07260, loss = 2.8109
2024-10-31 01:50:15: [2024-10-31 01:50:15] iter = 07270, loss = 1.8752
2024-10-31 01:50:19: [2024-10-31 01:50:19] iter = 07280, loss = 2.1837
2024-10-31 01:50:22: [2024-10-31 01:50:22] iter = 07290, loss = 1.8246
2024-10-31 01:50:25: [2024-10-31 01:50:25] iter = 07300, loss = 2.2682
2024-10-31 01:50:28: [2024-10-31 01:50:28] iter = 07310, loss = 1.7319
2024-10-31 01:50:32: [2024-10-31 01:50:32] iter = 07320, loss = 3.1076
2024-10-31 01:50:35: [2024-10-31 01:50:35] iter = 07330, loss = 2.1977
2024-10-31 01:50:38: [2024-10-31 01:50:38] iter = 07340, loss = 2.0698
2024-10-31 01:50:41: [2024-10-31 01:50:41] iter = 07350, loss = 3.9063
2024-10-31 01:50:44: [2024-10-31 01:50:44] iter = 07360, loss = 2.0771
2024-10-31 01:50:48: [2024-10-31 01:50:48] iter = 07370, loss = 1.9840
2024-10-31 01:50:51: [2024-10-31 01:50:51] iter = 07380, loss = 1.9926
2024-10-31 01:50:54: [2024-10-31 01:50:54] iter = 07390, loss = 2.7135
2024-10-31 01:50:57: [2024-10-31 01:50:57] iter = 07400, loss = 2.5876
2024-10-31 01:51:00: [2024-10-31 01:51:00] iter = 07410, loss = 2.0492
2024-10-31 01:51:03: [2024-10-31 01:51:03] iter = 07420, loss = 2.5307
2024-10-31 01:51:06: [2024-10-31 01:51:06] iter = 07430, loss = 2.0157
2024-10-31 01:51:10: [2024-10-31 01:51:10] iter = 07440, loss = 2.0684
2024-10-31 01:51:14: [2024-10-31 01:51:14] iter = 07450, loss = 2.2110
2024-10-31 01:51:18: [2024-10-31 01:51:18] iter = 07460, loss = 2.4764
2024-10-31 01:51:22: [2024-10-31 01:51:22] iter = 07470, loss = 2.2877
2024-10-31 01:51:26: [2024-10-31 01:51:26] iter = 07480, loss = 2.8833
2024-10-31 01:51:28: [2024-10-31 01:51:28] iter = 07490, loss = 2.1836
2024-10-31 01:51:32: [2024-10-31 01:51:32] iter = 07500, loss = 2.2711
2024-10-31 01:51:35: [2024-10-31 01:51:35] iter = 07510, loss = 1.9652
2024-10-31 01:51:37: [2024-10-31 01:51:37] iter = 07520, loss = 3.0559
2024-10-31 01:51:40: [2024-10-31 01:51:40] iter = 07530, loss = 3.0702
2024-10-31 01:51:44: [2024-10-31 01:51:44] iter = 07540, loss = 1.8060
2024-10-31 01:51:48: [2024-10-31 01:51:48] iter = 07550, loss = 1.8133
2024-10-31 01:51:51: [2024-10-31 01:51:51] iter = 07560, loss = 2.3760
2024-10-31 01:51:55: [2024-10-31 01:51:55] iter = 07570, loss = 1.9667
2024-10-31 01:51:59: [2024-10-31 01:51:59] iter = 07580, loss = 1.7697
2024-10-31 01:52:01: [2024-10-31 01:52:01] iter = 07590, loss = 2.0012
2024-10-31 01:52:04: [2024-10-31 01:52:04] iter = 07600, loss = 2.4155
2024-10-31 01:52:07: [2024-10-31 01:52:07] iter = 07610, loss = 1.7983
2024-10-31 01:52:11: [2024-10-31 01:52:11] iter = 07620, loss = 2.0635
2024-10-31 01:52:15: [2024-10-31 01:52:15] iter = 07630, loss = 1.9079
2024-10-31 01:52:18: [2024-10-31 01:52:18] iter = 07640, loss = 1.7319
2024-10-31 01:52:22: [2024-10-31 01:52:22] iter = 07650, loss = 2.3139
2024-10-31 01:52:25: [2024-10-31 01:52:25] iter = 07660, loss = 2.2483
2024-10-31 01:52:28: [2024-10-31 01:52:28] iter = 07670, loss = 4.7726
2024-10-31 01:52:32: [2024-10-31 01:52:32] iter = 07680, loss = 2.1536
2024-10-31 01:52:35: [2024-10-31 01:52:35] iter = 07690, loss = 2.2311
2024-10-31 01:52:39: [2024-10-31 01:52:39] iter = 07700, loss = 1.9955
2024-10-31 01:52:43: [2024-10-31 01:52:43] iter = 07710, loss = 2.2072
2024-10-31 01:52:46: [2024-10-31 01:52:46] iter = 07720, loss = 1.8073
2024-10-31 01:52:49: [2024-10-31 01:52:49] iter = 07730, loss = 3.0979
2024-10-31 01:52:52: [2024-10-31 01:52:52] iter = 07740, loss = 2.0894
2024-10-31 01:52:55: [2024-10-31 01:52:55] iter = 07750, loss = 2.9613
2024-10-31 01:52:59: [2024-10-31 01:52:59] iter = 07760, loss = 1.8952
2024-10-31 01:53:02: [2024-10-31 01:53:02] iter = 07770, loss = 2.1891
2024-10-31 01:53:06: [2024-10-31 01:53:06] iter = 07780, loss = 2.6924
2024-10-31 01:53:09: [2024-10-31 01:53:09] iter = 07790, loss = 2.0814
2024-10-31 01:53:13: [2024-10-31 01:53:13] iter = 07800, loss = 2.3448
2024-10-31 01:53:16: [2024-10-31 01:53:16] iter = 07810, loss = 2.1386
2024-10-31 01:53:19: [2024-10-31 01:53:19] iter = 07820, loss = 5.5000
2024-10-31 01:53:22: [2024-10-31 01:53:22] iter = 07830, loss = 2.1977
2024-10-31 01:53:25: [2024-10-31 01:53:25] iter = 07840, loss = 2.9346
2024-10-31 01:53:29: [2024-10-31 01:53:29] iter = 07850, loss = 2.1832
2024-10-31 01:53:32: [2024-10-31 01:53:32] iter = 07860, loss = 1.9627
2024-10-31 01:53:35: [2024-10-31 01:53:35] iter = 07870, loss = 2.7868
2024-10-31 01:53:38: [2024-10-31 01:53:38] iter = 07880, loss = 2.3746
2024-10-31 01:53:41: [2024-10-31 01:53:41] iter = 07890, loss = 3.7500
2024-10-31 01:53:44: [2024-10-31 01:53:44] iter = 07900, loss = 2.6368
2024-10-31 01:53:48: [2024-10-31 01:53:48] iter = 07910, loss = 1.9983
2024-10-31 01:53:51: [2024-10-31 01:53:51] iter = 07920, loss = 3.6243
2024-10-31 01:53:53: [2024-10-31 01:53:53] iter = 07930, loss = 2.7766
2024-10-31 01:53:57: [2024-10-31 01:53:57] iter = 07940, loss = 2.2936
2024-10-31 01:54:00: [2024-10-31 01:54:00] iter = 07950, loss = 2.2318
2024-10-31 01:54:03: [2024-10-31 01:54:03] iter = 07960, loss = 2.5215
2024-10-31 01:54:06: [2024-10-31 01:54:06] iter = 07970, loss = 2.1043
2024-10-31 01:54:09: [2024-10-31 01:54:09] iter = 07980, loss = 6.9226
2024-10-31 01:54:13: [2024-10-31 01:54:13] iter = 07990, loss = 2.1979
2024-10-31 01:54:16: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 8000
2024-10-31 01:54:16: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 01:54:16: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 56205}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:56:25: Evaluate 5 random ConvNet, ACCmean = 0.7736 ACCstd = 0.0017
-------------------------
2024-10-31 01:56:25: Evaluate 5 random ConvNet, SENmean = 0.7701 SENstd = 0.0019
-------------------------
2024-10-31 01:56:25: Evaluate 5 random ConvNet, SPEmean = 0.9772 SPEstd = 0.0002
-------------------------
2024-10-31 01:56:25: Evaluate 5 random ConvNet, F!mean = 0.7592 F!std = 0.0017
-------------------------
2024-10-31 01:56:25: Evaluate 5 random ConvNet, mean = 0.7736 std = 0.0017
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 01:56:26: [2024-10-31 01:56:26] iter = 08000, loss = 1.9060
2024-10-31 01:56:29: [2024-10-31 01:56:29] iter = 08010, loss = 2.3768
2024-10-31 01:56:33: [2024-10-31 01:56:33] iter = 08020, loss = 2.4745
2024-10-31 01:56:37: [2024-10-31 01:56:37] iter = 08030, loss = 2.7907
2024-10-31 01:56:40: [2024-10-31 01:56:40] iter = 08040, loss = 2.1092
2024-10-31 01:56:43: [2024-10-31 01:56:43] iter = 08050, loss = 4.1321
2024-10-31 01:56:46: [2024-10-31 01:56:46] iter = 08060, loss = 2.1810
2024-10-31 01:56:49: [2024-10-31 01:56:49] iter = 08070, loss = 2.8597
2024-10-31 01:56:52: [2024-10-31 01:56:52] iter = 08080, loss = 1.8807
2024-10-31 01:56:55: [2024-10-31 01:56:55] iter = 08090, loss = 1.8489
2024-10-31 01:56:58: [2024-10-31 01:56:58] iter = 08100, loss = 2.6423
2024-10-31 01:57:01: [2024-10-31 01:57:01] iter = 08110, loss = 2.2689
2024-10-31 01:57:04: [2024-10-31 01:57:04] iter = 08120, loss = 4.2639
2024-10-31 01:57:06: [2024-10-31 01:57:06] iter = 08130, loss = 2.1747
2024-10-31 01:57:10: [2024-10-31 01:57:10] iter = 08140, loss = 3.5021
2024-10-31 01:57:12: [2024-10-31 01:57:12] iter = 08150, loss = 2.3539
2024-10-31 01:57:16: [2024-10-31 01:57:16] iter = 08160, loss = 2.4649
2024-10-31 01:57:20: [2024-10-31 01:57:20] iter = 08170, loss = 2.4124
2024-10-31 01:57:23: [2024-10-31 01:57:23] iter = 08180, loss = 2.0929
2024-10-31 01:57:25: [2024-10-31 01:57:25] iter = 08190, loss = 2.3193
2024-10-31 01:57:29: [2024-10-31 01:57:29] iter = 08200, loss = 2.0562
2024-10-31 01:57:32: [2024-10-31 01:57:32] iter = 08210, loss = 2.7953
2024-10-31 01:57:35: [2024-10-31 01:57:35] iter = 08220, loss = 2.8074
2024-10-31 01:57:38: [2024-10-31 01:57:38] iter = 08230, loss = 1.8890
2024-10-31 01:57:41: [2024-10-31 01:57:41] iter = 08240, loss = 1.9830
2024-10-31 01:57:44: [2024-10-31 01:57:44] iter = 08250, loss = 2.0763
2024-10-31 01:57:47: [2024-10-31 01:57:47] iter = 08260, loss = 2.4571
2024-10-31 01:57:50: [2024-10-31 01:57:50] iter = 08270, loss = 2.1298
2024-10-31 01:57:53: [2024-10-31 01:57:53] iter = 08280, loss = 2.6360
2024-10-31 01:57:56: [2024-10-31 01:57:56] iter = 08290, loss = 1.7405
2024-10-31 01:58:00: [2024-10-31 01:58:00] iter = 08300, loss = 2.6348
2024-10-31 01:58:03: [2024-10-31 01:58:03] iter = 08310, loss = 2.0722
2024-10-31 01:58:05: [2024-10-31 01:58:05] iter = 08320, loss = 1.8510
2024-10-31 01:58:07: [2024-10-31 01:58:07] iter = 08330, loss = 2.0798
2024-10-31 01:58:11: [2024-10-31 01:58:11] iter = 08340, loss = 2.1653
2024-10-31 01:58:14: [2024-10-31 01:58:14] iter = 08350, loss = 2.7500
2024-10-31 01:58:18: [2024-10-31 01:58:18] iter = 08360, loss = 2.0145
2024-10-31 01:58:22: [2024-10-31 01:58:22] iter = 08370, loss = 3.7359
2024-10-31 01:58:26: [2024-10-31 01:58:26] iter = 08380, loss = 2.1958
2024-10-31 01:58:29: [2024-10-31 01:58:29] iter = 08390, loss = 2.2744
2024-10-31 01:58:33: [2024-10-31 01:58:33] iter = 08400, loss = 2.9684
2024-10-31 01:58:36: [2024-10-31 01:58:36] iter = 08410, loss = 2.0103
2024-10-31 01:58:39: [2024-10-31 01:58:39] iter = 08420, loss = 2.0243
2024-10-31 01:58:43: [2024-10-31 01:58:43] iter = 08430, loss = 9.9573
2024-10-31 01:58:46: [2024-10-31 01:58:46] iter = 08440, loss = 2.2032
2024-10-31 01:58:50: [2024-10-31 01:58:50] iter = 08450, loss = 2.2797
2024-10-31 01:58:53: [2024-10-31 01:58:53] iter = 08460, loss = 2.1628
2024-10-31 01:58:57: [2024-10-31 01:58:57] iter = 08470, loss = 2.4268
2024-10-31 01:59:00: [2024-10-31 01:59:00] iter = 08480, loss = 2.2937
2024-10-31 01:59:03: [2024-10-31 01:59:03] iter = 08490, loss = 1.8958
2024-10-31 01:59:06: [2024-10-31 01:59:06] iter = 08500, loss = 2.0338
2024-10-31 01:59:09: [2024-10-31 01:59:09] iter = 08510, loss = 1.7798
2024-10-31 01:59:12: [2024-10-31 01:59:12] iter = 08520, loss = 2.1696
2024-10-31 01:59:15: [2024-10-31 01:59:15] iter = 08530, loss = 2.3498
2024-10-31 01:59:19: [2024-10-31 01:59:19] iter = 08540, loss = 4.3871
2024-10-31 01:59:22: [2024-10-31 01:59:22] iter = 08550, loss = 2.0480
2024-10-31 01:59:25: [2024-10-31 01:59:25] iter = 08560, loss = 1.5465
2024-10-31 01:59:28: [2024-10-31 01:59:28] iter = 08570, loss = 2.0183
2024-10-31 01:59:32: [2024-10-31 01:59:32] iter = 08580, loss = 1.9889
2024-10-31 01:59:35: [2024-10-31 01:59:35] iter = 08590, loss = 3.9082
2024-10-31 01:59:37: [2024-10-31 01:59:37] iter = 08600, loss = 3.3390
2024-10-31 01:59:41: [2024-10-31 01:59:41] iter = 08610, loss = 2.2432
2024-10-31 01:59:44: [2024-10-31 01:59:44] iter = 08620, loss = 2.0769
2024-10-31 01:59:47: [2024-10-31 01:59:47] iter = 08630, loss = 2.0133
2024-10-31 01:59:50: [2024-10-31 01:59:50] iter = 08640, loss = 2.6981
2024-10-31 01:59:53: [2024-10-31 01:59:53] iter = 08650, loss = 2.0153
2024-10-31 01:59:56: [2024-10-31 01:59:56] iter = 08660, loss = 2.2751
2024-10-31 01:59:59: [2024-10-31 01:59:59] iter = 08670, loss = 2.0363
2024-10-31 02:00:03: [2024-10-31 02:00:02] iter = 08680, loss = 2.3181
2024-10-31 02:00:06: [2024-10-31 02:00:06] iter = 08690, loss = 1.8733
2024-10-31 02:00:09: [2024-10-31 02:00:09] iter = 08700, loss = 2.3502
2024-10-31 02:00:12: [2024-10-31 02:00:12] iter = 08710, loss = 2.1162
2024-10-31 02:00:16: [2024-10-31 02:00:16] iter = 08720, loss = 1.9397
2024-10-31 02:00:19: [2024-10-31 02:00:19] iter = 08730, loss = 2.6230
2024-10-31 02:00:22: [2024-10-31 02:00:22] iter = 08740, loss = 2.1855
2024-10-31 02:00:25: [2024-10-31 02:00:25] iter = 08750, loss = 2.4191
2024-10-31 02:00:29: [2024-10-31 02:00:29] iter = 08760, loss = 2.8127
2024-10-31 02:00:33: [2024-10-31 02:00:33] iter = 08770, loss = 1.8522
2024-10-31 02:00:36: [2024-10-31 02:00:36] iter = 08780, loss = 2.1101
2024-10-31 02:00:39: [2024-10-31 02:00:39] iter = 08790, loss = 1.7933
2024-10-31 02:00:42: [2024-10-31 02:00:42] iter = 08800, loss = 2.0018
2024-10-31 02:00:45: [2024-10-31 02:00:45] iter = 08810, loss = 2.1444
2024-10-31 02:00:49: [2024-10-31 02:00:49] iter = 08820, loss = 7.0276
2024-10-31 02:00:52: [2024-10-31 02:00:52] iter = 08830, loss = 2.5264
2024-10-31 02:00:55: [2024-10-31 02:00:55] iter = 08840, loss = 2.1075
2024-10-31 02:00:59: [2024-10-31 02:00:59] iter = 08850, loss = 1.9642
2024-10-31 02:01:03: [2024-10-31 02:01:03] iter = 08860, loss = 1.9599
2024-10-31 02:01:06: [2024-10-31 02:01:06] iter = 08870, loss = 1.9798
2024-10-31 02:01:10: [2024-10-31 02:01:10] iter = 08880, loss = 2.5862
2024-10-31 02:01:14: [2024-10-31 02:01:14] iter = 08890, loss = 2.6983
2024-10-31 02:01:16: [2024-10-31 02:01:16] iter = 08900, loss = 2.1598
2024-10-31 02:01:19: [2024-10-31 02:01:19] iter = 08910, loss = 2.1578
2024-10-31 02:01:22: [2024-10-31 02:01:22] iter = 08920, loss = 2.6328
2024-10-31 02:01:25: [2024-10-31 02:01:25] iter = 08930, loss = 2.5021
2024-10-31 02:01:28: [2024-10-31 02:01:28] iter = 08940, loss = 2.2465
2024-10-31 02:01:31: [2024-10-31 02:01:31] iter = 08950, loss = 2.4466
2024-10-31 02:01:35: [2024-10-31 02:01:35] iter = 08960, loss = 1.9659
2024-10-31 02:01:37: [2024-10-31 02:01:37] iter = 08970, loss = 2.7291
2024-10-31 02:01:40: [2024-10-31 02:01:40] iter = 08980, loss = 1.9052
2024-10-31 02:01:44: [2024-10-31 02:01:44] iter = 08990, loss = 2.3269
2024-10-31 02:01:47: [2024-10-31 02:01:47] iter = 09000, loss = 1.8848
2024-10-31 02:01:49: [2024-10-31 02:01:49] iter = 09010, loss = 2.0836
2024-10-31 02:01:54: [2024-10-31 02:01:54] iter = 09020, loss = 2.0465
2024-10-31 02:01:57: [2024-10-31 02:01:57] iter = 09030, loss = 1.8291
2024-10-31 02:02:00: [2024-10-31 02:02:00] iter = 09040, loss = 2.5433
2024-10-31 02:02:03: [2024-10-31 02:02:03] iter = 09050, loss = 3.1634
2024-10-31 02:02:07: [2024-10-31 02:02:07] iter = 09060, loss = 2.0532
2024-10-31 02:02:10: [2024-10-31 02:02:10] iter = 09070, loss = 8.2136
2024-10-31 02:02:13: [2024-10-31 02:02:13] iter = 09080, loss = 2.0330
2024-10-31 02:02:17: [2024-10-31 02:02:17] iter = 09090, loss = 1.7756
2024-10-31 02:02:21: [2024-10-31 02:02:21] iter = 09100, loss = 2.2023
2024-10-31 02:02:24: [2024-10-31 02:02:24] iter = 09110, loss = 2.1432
2024-10-31 02:02:27: [2024-10-31 02:02:27] iter = 09120, loss = 2.1952
2024-10-31 02:02:30: [2024-10-31 02:02:30] iter = 09130, loss = 2.7626
2024-10-31 02:02:33: [2024-10-31 02:02:33] iter = 09140, loss = 2.1752
2024-10-31 02:02:36: [2024-10-31 02:02:36] iter = 09150, loss = 4.7477
2024-10-31 02:02:39: [2024-10-31 02:02:39] iter = 09160, loss = 1.9295
2024-10-31 02:02:42: [2024-10-31 02:02:42] iter = 09170, loss = 1.8443
2024-10-31 02:02:45: [2024-10-31 02:02:45] iter = 09180, loss = 1.9426
2024-10-31 02:02:49: [2024-10-31 02:02:49] iter = 09190, loss = 2.1257
2024-10-31 02:02:53: [2024-10-31 02:02:53] iter = 09200, loss = 2.4976
2024-10-31 02:02:57: [2024-10-31 02:02:57] iter = 09210, loss = 1.8336
2024-10-31 02:03:01: [2024-10-31 02:03:01] iter = 09220, loss = 1.8722
2024-10-31 02:03:03: [2024-10-31 02:03:03] iter = 09230, loss = 1.9175
2024-10-31 02:03:07: [2024-10-31 02:03:07] iter = 09240, loss = 2.4490
2024-10-31 02:03:10: [2024-10-31 02:03:10] iter = 09250, loss = 1.9321
2024-10-31 02:03:13: [2024-10-31 02:03:13] iter = 09260, loss = 2.5535
2024-10-31 02:03:16: [2024-10-31 02:03:16] iter = 09270, loss = 1.8398
2024-10-31 02:03:20: [2024-10-31 02:03:20] iter = 09280, loss = 1.6028
2024-10-31 02:03:23: [2024-10-31 02:03:23] iter = 09290, loss = 1.9140
2024-10-31 02:03:26: [2024-10-31 02:03:26] iter = 09300, loss = 2.8605
2024-10-31 02:03:28: [2024-10-31 02:03:28] iter = 09310, loss = 1.9334
2024-10-31 02:03:30: [2024-10-31 02:03:30] iter = 09320, loss = 2.6143
2024-10-31 02:03:34: [2024-10-31 02:03:34] iter = 09330, loss = 2.9607
2024-10-31 02:03:37: [2024-10-31 02:03:37] iter = 09340, loss = 2.0163
2024-10-31 02:03:41: [2024-10-31 02:03:41] iter = 09350, loss = 1.6834
2024-10-31 02:03:43: [2024-10-31 02:03:43] iter = 09360, loss = 2.0543
2024-10-31 02:03:46: [2024-10-31 02:03:46] iter = 09370, loss = 1.9184
2024-10-31 02:03:49: [2024-10-31 02:03:49] iter = 09380, loss = 2.3212
2024-10-31 02:03:52: [2024-10-31 02:03:52] iter = 09390, loss = 2.1167
2024-10-31 02:03:56: [2024-10-31 02:03:56] iter = 09400, loss = 2.3619
2024-10-31 02:03:59: [2024-10-31 02:03:59] iter = 09410, loss = 2.1676
2024-10-31 02:04:03: [2024-10-31 02:04:03] iter = 09420, loss = 1.7834
2024-10-31 02:04:06: [2024-10-31 02:04:06] iter = 09430, loss = 1.8868
2024-10-31 02:04:10: [2024-10-31 02:04:10] iter = 09440, loss = 3.4076
2024-10-31 02:04:15: [2024-10-31 02:04:15] iter = 09450, loss = 2.4443
2024-10-31 02:04:18: [2024-10-31 02:04:18] iter = 09460, loss = 3.5800
2024-10-31 02:04:21: [2024-10-31 02:04:21] iter = 09470, loss = 4.5012
2024-10-31 02:04:25: [2024-10-31 02:04:25] iter = 09480, loss = 2.0474
2024-10-31 02:04:28: [2024-10-31 02:04:28] iter = 09490, loss = 2.3306
2024-10-31 02:04:31: [2024-10-31 02:04:31] iter = 09500, loss = 1.9389
2024-10-31 02:04:35: [2024-10-31 02:04:35] iter = 09510, loss = 1.7581
2024-10-31 02:04:39: [2024-10-31 02:04:39] iter = 09520, loss = 1.8738
2024-10-31 02:04:42: [2024-10-31 02:04:42] iter = 09530, loss = 2.2124
2024-10-31 02:04:46: [2024-10-31 02:04:46] iter = 09540, loss = 2.1248
2024-10-31 02:04:49: [2024-10-31 02:04:49] iter = 09550, loss = 2.0218
2024-10-31 02:04:53: [2024-10-31 02:04:53] iter = 09560, loss = 2.2025
2024-10-31 02:04:56: [2024-10-31 02:04:56] iter = 09570, loss = 1.9621
2024-10-31 02:04:59: [2024-10-31 02:04:59] iter = 09580, loss = 1.9471
2024-10-31 02:05:02: [2024-10-31 02:05:02] iter = 09590, loss = 2.6654
2024-10-31 02:05:05: [2024-10-31 02:05:05] iter = 09600, loss = 2.1242
2024-10-31 02:05:08: [2024-10-31 02:05:08] iter = 09610, loss = 2.2111
2024-10-31 02:05:12: [2024-10-31 02:05:12] iter = 09620, loss = 2.2672
2024-10-31 02:05:16: [2024-10-31 02:05:16] iter = 09630, loss = 3.0930
2024-10-31 02:05:19: [2024-10-31 02:05:19] iter = 09640, loss = 2.1696
2024-10-31 02:05:22: [2024-10-31 02:05:22] iter = 09650, loss = 2.2792
2024-10-31 02:05:25: [2024-10-31 02:05:25] iter = 09660, loss = 2.9605
2024-10-31 02:05:27: [2024-10-31 02:05:27] iter = 09670, loss = 2.0056
2024-10-31 02:05:29: [2024-10-31 02:05:29] iter = 09680, loss = 1.9676
2024-10-31 02:05:32: [2024-10-31 02:05:32] iter = 09690, loss = 2.5472
2024-10-31 02:05:35: [2024-10-31 02:05:35] iter = 09700, loss = 3.0723
2024-10-31 02:05:39: [2024-10-31 02:05:39] iter = 09710, loss = 1.6143
2024-10-31 02:05:43: [2024-10-31 02:05:43] iter = 09720, loss = 2.1147
2024-10-31 02:05:45: [2024-10-31 02:05:45] iter = 09730, loss = 1.8381
2024-10-31 02:05:48: [2024-10-31 02:05:48] iter = 09740, loss = 1.7808
2024-10-31 02:05:52: [2024-10-31 02:05:52] iter = 09750, loss = 3.5345
2024-10-31 02:05:55: [2024-10-31 02:05:55] iter = 09760, loss = 2.0030
2024-10-31 02:05:57: [2024-10-31 02:05:57] iter = 09770, loss = 2.1069
2024-10-31 02:06:00: [2024-10-31 02:06:00] iter = 09780, loss = 2.5049
2024-10-31 02:06:03: [2024-10-31 02:06:03] iter = 09790, loss = 2.1632
2024-10-31 02:06:05: [2024-10-31 02:06:05] iter = 09800, loss = 2.2914
2024-10-31 02:06:09: [2024-10-31 02:06:09] iter = 09810, loss = 2.1086
2024-10-31 02:06:12: [2024-10-31 02:06:12] iter = 09820, loss = 2.4606
2024-10-31 02:06:14: [2024-10-31 02:06:14] iter = 09830, loss = 2.0870
2024-10-31 02:06:17: [2024-10-31 02:06:17] iter = 09840, loss = 4.8190
2024-10-31 02:06:21: [2024-10-31 02:06:21] iter = 09850, loss = 2.0870
2024-10-31 02:06:24: [2024-10-31 02:06:24] iter = 09860, loss = 2.5317
2024-10-31 02:06:27: [2024-10-31 02:06:27] iter = 09870, loss = 1.9017
2024-10-31 02:06:31: [2024-10-31 02:06:31] iter = 09880, loss = 2.0638
2024-10-31 02:06:34: [2024-10-31 02:06:34] iter = 09890, loss = 3.0171
2024-10-31 02:06:37: [2024-10-31 02:06:37] iter = 09900, loss = 2.6826
2024-10-31 02:06:41: [2024-10-31 02:06:41] iter = 09910, loss = 1.8563
2024-10-31 02:06:45: [2024-10-31 02:06:45] iter = 09920, loss = 2.0463
2024-10-31 02:06:48: [2024-10-31 02:06:48] iter = 09930, loss = 2.0310
2024-10-31 02:06:50: [2024-10-31 02:06:50] iter = 09940, loss = 2.5951
2024-10-31 02:06:53: [2024-10-31 02:06:53] iter = 09950, loss = 2.6639
2024-10-31 02:06:56: [2024-10-31 02:06:56] iter = 09960, loss = 7.5700
2024-10-31 02:07:00: [2024-10-31 02:07:00] iter = 09970, loss = 2.6343
2024-10-31 02:07:03: [2024-10-31 02:07:03] iter = 09980, loss = 2.4600
2024-10-31 02:07:05: [2024-10-31 02:07:05] iter = 09990, loss = 2.4226
2024-10-31 02:07:08: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 10000
2024-10-31 02:07:08: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 02:07:08: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 28818}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:09:23: Evaluate 5 random ConvNet, ACCmean = 0.7906 ACCstd = 0.0065
-------------------------
2024-10-31 02:09:23: Evaluate 5 random ConvNet, SENmean = 0.7859 SENstd = 0.0062
-------------------------
2024-10-31 02:09:23: Evaluate 5 random ConvNet, SPEmean = 0.9790 SPEstd = 0.0006
-------------------------
2024-10-31 02:09:23: Evaluate 5 random ConvNet, F!mean = 0.7753 F!std = 0.0060
-------------------------
2024-10-31 02:09:23: Evaluate 5 random ConvNet, mean = 0.7906 std = 0.0065
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:09:24: [2024-10-31 02:09:24] iter = 10000, loss = 2.3186
2024-10-31 02:09:28: [2024-10-31 02:09:28] iter = 10010, loss = 2.1164
2024-10-31 02:09:31: [2024-10-31 02:09:31] iter = 10020, loss = 2.7748
2024-10-31 02:09:35: [2024-10-31 02:09:35] iter = 10030, loss = 2.0738
2024-10-31 02:09:39: [2024-10-31 02:09:39] iter = 10040, loss = 2.8121
2024-10-31 02:09:42: [2024-10-31 02:09:42] iter = 10050, loss = 2.0202
2024-10-31 02:09:46: [2024-10-31 02:09:46] iter = 10060, loss = 2.8445
2024-10-31 02:09:49: [2024-10-31 02:09:49] iter = 10070, loss = 2.0189
2024-10-31 02:09:53: [2024-10-31 02:09:53] iter = 10080, loss = 3.0170
2024-10-31 02:09:56: [2024-10-31 02:09:56] iter = 10090, loss = 2.1514
2024-10-31 02:09:59: [2024-10-31 02:09:59] iter = 10100, loss = 2.1506
2024-10-31 02:10:02: [2024-10-31 02:10:02] iter = 10110, loss = 2.4857
2024-10-31 02:10:07: [2024-10-31 02:10:07] iter = 10120, loss = 2.7196
2024-10-31 02:10:11: [2024-10-31 02:10:11] iter = 10130, loss = 2.2113
2024-10-31 02:10:15: [2024-10-31 02:10:15] iter = 10140, loss = 1.9815
2024-10-31 02:10:19: [2024-10-31 02:10:19] iter = 10150, loss = 2.1101
2024-10-31 02:10:22: [2024-10-31 02:10:22] iter = 10160, loss = 2.1655
2024-10-31 02:10:26: [2024-10-31 02:10:26] iter = 10170, loss = 2.3006
2024-10-31 02:10:29: [2024-10-31 02:10:29] iter = 10180, loss = 1.7891
2024-10-31 02:10:33: [2024-10-31 02:10:33] iter = 10190, loss = 3.0853
2024-10-31 02:10:36: [2024-10-31 02:10:36] iter = 10200, loss = 3.1407
2024-10-31 02:10:39: [2024-10-31 02:10:39] iter = 10210, loss = 2.2787
2024-10-31 02:10:43: [2024-10-31 02:10:43] iter = 10220, loss = 2.1989
2024-10-31 02:10:46: [2024-10-31 02:10:46] iter = 10230, loss = 2.2636
2024-10-31 02:10:50: [2024-10-31 02:10:50] iter = 10240, loss = 2.5186
2024-10-31 02:10:53: [2024-10-31 02:10:53] iter = 10250, loss = 2.2561
2024-10-31 02:10:57: [2024-10-31 02:10:57] iter = 10260, loss = 2.4611
2024-10-31 02:11:01: [2024-10-31 02:11:01] iter = 10270, loss = 2.0670
2024-10-31 02:11:04: [2024-10-31 02:11:04] iter = 10280, loss = 1.7602
2024-10-31 02:11:08: [2024-10-31 02:11:08] iter = 10290, loss = 1.9591
2024-10-31 02:11:12: [2024-10-31 02:11:12] iter = 10300, loss = 2.0528
2024-10-31 02:11:16: [2024-10-31 02:11:16] iter = 10310, loss = 2.2173
2024-10-31 02:11:19: [2024-10-31 02:11:19] iter = 10320, loss = 1.9852
2024-10-31 02:11:23: [2024-10-31 02:11:23] iter = 10330, loss = 3.6029
2024-10-31 02:11:28: [2024-10-31 02:11:28] iter = 10340, loss = 2.1623
2024-10-31 02:11:32: [2024-10-31 02:11:32] iter = 10350, loss = 3.0125
2024-10-31 02:11:37: [2024-10-31 02:11:37] iter = 10360, loss = 1.9064
2024-10-31 02:11:41: [2024-10-31 02:11:41] iter = 10370, loss = 2.2162
2024-10-31 02:11:45: [2024-10-31 02:11:45] iter = 10380, loss = 2.4702
2024-10-31 02:11:50: [2024-10-31 02:11:50] iter = 10390, loss = 1.9974
2024-10-31 02:11:53: [2024-10-31 02:11:53] iter = 10400, loss = 2.8732
2024-10-31 02:11:56: [2024-10-31 02:11:56] iter = 10410, loss = 1.7249
2024-10-31 02:12:00: [2024-10-31 02:12:00] iter = 10420, loss = 1.8891
2024-10-31 02:12:03: [2024-10-31 02:12:03] iter = 10430, loss = 1.9606
2024-10-31 02:12:07: [2024-10-31 02:12:07] iter = 10440, loss = 1.7137
2024-10-31 02:12:12: [2024-10-31 02:12:12] iter = 10450, loss = 2.4994
2024-10-31 02:12:15: [2024-10-31 02:12:15] iter = 10460, loss = 2.1451
2024-10-31 02:12:18: [2024-10-31 02:12:18] iter = 10470, loss = 3.2244
2024-10-31 02:12:22: [2024-10-31 02:12:22] iter = 10480, loss = 2.5451
2024-10-31 02:12:27: [2024-10-31 02:12:27] iter = 10490, loss = 2.1028
2024-10-31 02:12:31: [2024-10-31 02:12:31] iter = 10500, loss = 2.3031
2024-10-31 02:12:35: [2024-10-31 02:12:35] iter = 10510, loss = 1.7596
2024-10-31 02:12:39: [2024-10-31 02:12:39] iter = 10520, loss = 3.3735
2024-10-31 02:12:43: [2024-10-31 02:12:43] iter = 10530, loss = 2.1829
2024-10-31 02:12:46: [2024-10-31 02:12:46] iter = 10540, loss = 8.3507
2024-10-31 02:12:49: [2024-10-31 02:12:49] iter = 10550, loss = 2.0595
2024-10-31 02:12:52: [2024-10-31 02:12:52] iter = 10560, loss = 2.9417
2024-10-31 02:12:55: [2024-10-31 02:12:55] iter = 10570, loss = 2.3938
2024-10-31 02:12:58: [2024-10-31 02:12:58] iter = 10580, loss = 1.6280
2024-10-31 02:13:01: [2024-10-31 02:13:01] iter = 10590, loss = 2.3878
2024-10-31 02:13:04: [2024-10-31 02:13:04] iter = 10600, loss = 2.6585
2024-10-31 02:13:07: [2024-10-31 02:13:07] iter = 10610, loss = 2.5518
2024-10-31 02:13:10: [2024-10-31 02:13:10] iter = 10620, loss = 2.3085
2024-10-31 02:13:13: [2024-10-31 02:13:13] iter = 10630, loss = 2.8588
2024-10-31 02:13:16: [2024-10-31 02:13:16] iter = 10640, loss = 3.0844
2024-10-31 02:13:19: [2024-10-31 02:13:19] iter = 10650, loss = 1.7868
2024-10-31 02:13:22: [2024-10-31 02:13:22] iter = 10660, loss = 2.0287
2024-10-31 02:13:25: [2024-10-31 02:13:25] iter = 10670, loss = 2.1746
2024-10-31 02:13:28: [2024-10-31 02:13:28] iter = 10680, loss = 2.1768
2024-10-31 02:13:31: [2024-10-31 02:13:31] iter = 10690, loss = 2.3726
2024-10-31 02:13:34: [2024-10-31 02:13:34] iter = 10700, loss = 3.2774
2024-10-31 02:13:38: [2024-10-31 02:13:38] iter = 10710, loss = 2.3956
2024-10-31 02:13:41: [2024-10-31 02:13:41] iter = 10720, loss = 2.4470
2024-10-31 02:13:45: [2024-10-31 02:13:45] iter = 10730, loss = 2.0320
2024-10-31 02:13:48: [2024-10-31 02:13:48] iter = 10740, loss = 2.0491
2024-10-31 02:13:51: [2024-10-31 02:13:51] iter = 10750, loss = 1.8244
2024-10-31 02:13:53: [2024-10-31 02:13:53] iter = 10760, loss = 2.0750
2024-10-31 02:13:56: [2024-10-31 02:13:56] iter = 10770, loss = 2.6441
2024-10-31 02:13:59: [2024-10-31 02:13:59] iter = 10780, loss = 1.9596
2024-10-31 02:14:01: [2024-10-31 02:14:01] iter = 10790, loss = 2.3391
2024-10-31 02:14:05: [2024-10-31 02:14:05] iter = 10800, loss = 2.8810
2024-10-31 02:14:08: [2024-10-31 02:14:08] iter = 10810, loss = 2.1621
2024-10-31 02:14:10: [2024-10-31 02:14:10] iter = 10820, loss = 2.2524
2024-10-31 02:14:13: [2024-10-31 02:14:13] iter = 10830, loss = 1.9990
2024-10-31 02:14:16: [2024-10-31 02:14:16] iter = 10840, loss = 2.0898
2024-10-31 02:14:19: [2024-10-31 02:14:19] iter = 10850, loss = 3.0894
2024-10-31 02:14:23: [2024-10-31 02:14:23] iter = 10860, loss = 2.2164
2024-10-31 02:14:26: [2024-10-31 02:14:26] iter = 10870, loss = 1.8819
2024-10-31 02:14:30: [2024-10-31 02:14:30] iter = 10880, loss = 1.9047
2024-10-31 02:14:33: [2024-10-31 02:14:33] iter = 10890, loss = 3.5327
2024-10-31 02:14:36: [2024-10-31 02:14:36] iter = 10900, loss = 2.1897
2024-10-31 02:14:39: [2024-10-31 02:14:39] iter = 10910, loss = 2.2836
2024-10-31 02:14:41: [2024-10-31 02:14:41] iter = 10920, loss = 2.8952
2024-10-31 02:14:44: [2024-10-31 02:14:44] iter = 10930, loss = 2.4030
2024-10-31 02:14:47: [2024-10-31 02:14:47] iter = 10940, loss = 1.8871
2024-10-31 02:14:51: [2024-10-31 02:14:51] iter = 10950, loss = 2.8357
2024-10-31 02:14:54: [2024-10-31 02:14:54] iter = 10960, loss = 2.0346
2024-10-31 02:14:58: [2024-10-31 02:14:58] iter = 10970, loss = 2.2977
2024-10-31 02:15:01: [2024-10-31 02:15:01] iter = 10980, loss = 1.8874
2024-10-31 02:15:04: [2024-10-31 02:15:04] iter = 10990, loss = 2.1731
2024-10-31 02:15:07: [2024-10-31 02:15:07] iter = 11000, loss = 2.2326
2024-10-31 02:15:10: [2024-10-31 02:15:10] iter = 11010, loss = 2.1345
2024-10-31 02:15:13: [2024-10-31 02:15:13] iter = 11020, loss = 2.0221
2024-10-31 02:15:16: [2024-10-31 02:15:16] iter = 11030, loss = 2.1986
2024-10-31 02:15:19: [2024-10-31 02:15:19] iter = 11040, loss = 1.9962
2024-10-31 02:15:22: [2024-10-31 02:15:22] iter = 11050, loss = 1.8580
2024-10-31 02:15:25: [2024-10-31 02:15:25] iter = 11060, loss = 1.9145
2024-10-31 02:15:28: [2024-10-31 02:15:28] iter = 11070, loss = 2.2022
2024-10-31 02:15:31: [2024-10-31 02:15:31] iter = 11080, loss = 2.1473
2024-10-31 02:15:35: [2024-10-31 02:15:35] iter = 11090, loss = 2.9931
2024-10-31 02:15:39: [2024-10-31 02:15:39] iter = 11100, loss = 2.3733
2024-10-31 02:15:42: [2024-10-31 02:15:42] iter = 11110, loss = 2.5156
2024-10-31 02:15:46: [2024-10-31 02:15:46] iter = 11120, loss = 2.4039
2024-10-31 02:15:49: [2024-10-31 02:15:49] iter = 11130, loss = 2.0084
2024-10-31 02:15:52: [2024-10-31 02:15:52] iter = 11140, loss = 1.7631
2024-10-31 02:15:55: [2024-10-31 02:15:55] iter = 11150, loss = 4.0428
2024-10-31 02:15:58: [2024-10-31 02:15:58] iter = 11160, loss = 2.4862
2024-10-31 02:16:01: [2024-10-31 02:16:01] iter = 11170, loss = 1.9274
2024-10-31 02:16:04: [2024-10-31 02:16:04] iter = 11180, loss = 2.4021
2024-10-31 02:16:06: [2024-10-31 02:16:06] iter = 11190, loss = 1.8392
2024-10-31 02:16:09: [2024-10-31 02:16:09] iter = 11200, loss = 1.9257
2024-10-31 02:16:12: [2024-10-31 02:16:12] iter = 11210, loss = 2.1744
2024-10-31 02:16:15: [2024-10-31 02:16:15] iter = 11220, loss = 2.6592
2024-10-31 02:16:18: [2024-10-31 02:16:18] iter = 11230, loss = 2.2216
2024-10-31 02:16:21: [2024-10-31 02:16:21] iter = 11240, loss = 1.8801
2024-10-31 02:16:25: [2024-10-31 02:16:25] iter = 11250, loss = 2.0004
2024-10-31 02:16:28: [2024-10-31 02:16:28] iter = 11260, loss = 2.3445
2024-10-31 02:16:31: [2024-10-31 02:16:31] iter = 11270, loss = 2.5529
2024-10-31 02:16:34: [2024-10-31 02:16:34] iter = 11280, loss = 2.5232
2024-10-31 02:16:38: [2024-10-31 02:16:38] iter = 11290, loss = 1.8886
2024-10-31 02:16:40: [2024-10-31 02:16:40] iter = 11300, loss = 2.1220
2024-10-31 02:16:43: [2024-10-31 02:16:43] iter = 11310, loss = 5.2069
2024-10-31 02:16:46: [2024-10-31 02:16:46] iter = 11320, loss = 2.5935
2024-10-31 02:16:49: [2024-10-31 02:16:49] iter = 11330, loss = 2.6962
2024-10-31 02:16:53: [2024-10-31 02:16:53] iter = 11340, loss = 2.0090
2024-10-31 02:16:55: [2024-10-31 02:16:55] iter = 11350, loss = 1.8230
2024-10-31 02:16:58: [2024-10-31 02:16:58] iter = 11360, loss = 2.1202
2024-10-31 02:17:02: [2024-10-31 02:17:02] iter = 11370, loss = 2.0885
2024-10-31 02:17:06: [2024-10-31 02:17:06] iter = 11380, loss = 2.6206
2024-10-31 02:17:09: [2024-10-31 02:17:09] iter = 11390, loss = 2.1295
2024-10-31 02:17:12: [2024-10-31 02:17:12] iter = 11400, loss = 1.7914
2024-10-31 02:17:16: [2024-10-31 02:17:16] iter = 11410, loss = 2.6068
2024-10-31 02:17:19: [2024-10-31 02:17:19] iter = 11420, loss = 1.8948
2024-10-31 02:17:21: [2024-10-31 02:17:21] iter = 11430, loss = 2.1600
2024-10-31 02:17:24: [2024-10-31 02:17:24] iter = 11440, loss = 2.1802
2024-10-31 02:17:28: [2024-10-31 02:17:28] iter = 11450, loss = 3.1304
2024-10-31 02:17:31: [2024-10-31 02:17:31] iter = 11460, loss = 2.6263
2024-10-31 02:17:35: [2024-10-31 02:17:35] iter = 11470, loss = 2.2989
2024-10-31 02:17:38: [2024-10-31 02:17:38] iter = 11480, loss = 2.0717
2024-10-31 02:17:42: [2024-10-31 02:17:42] iter = 11490, loss = 2.7810
2024-10-31 02:17:44: [2024-10-31 02:17:44] iter = 11500, loss = 2.4035
2024-10-31 02:17:47: [2024-10-31 02:17:47] iter = 11510, loss = 2.3019
2024-10-31 02:17:50: [2024-10-31 02:17:50] iter = 11520, loss = 2.7136
2024-10-31 02:17:54: [2024-10-31 02:17:54] iter = 11530, loss = 4.1218
2024-10-31 02:17:57: [2024-10-31 02:17:57] iter = 11540, loss = 2.4886
2024-10-31 02:18:00: [2024-10-31 02:18:00] iter = 11550, loss = 1.7420
2024-10-31 02:18:03: [2024-10-31 02:18:03] iter = 11560, loss = 2.1312
2024-10-31 02:18:06: [2024-10-31 02:18:06] iter = 11570, loss = 1.9214
2024-10-31 02:18:09: [2024-10-31 02:18:09] iter = 11580, loss = 2.3284
2024-10-31 02:18:12: [2024-10-31 02:18:12] iter = 11590, loss = 2.7044
2024-10-31 02:18:16: [2024-10-31 02:18:16] iter = 11600, loss = 2.3375
2024-10-31 02:18:19: [2024-10-31 02:18:19] iter = 11610, loss = 3.0318
2024-10-31 02:18:22: [2024-10-31 02:18:22] iter = 11620, loss = 2.1197
2024-10-31 02:18:24: [2024-10-31 02:18:24] iter = 11630, loss = 3.9422
2024-10-31 02:18:27: [2024-10-31 02:18:27] iter = 11640, loss = 2.1135
2024-10-31 02:18:31: [2024-10-31 02:18:31] iter = 11650, loss = 3.1689
2024-10-31 02:18:34: [2024-10-31 02:18:34] iter = 11660, loss = 2.0923
2024-10-31 02:18:37: [2024-10-31 02:18:37] iter = 11670, loss = 2.0184
2024-10-31 02:18:40: [2024-10-31 02:18:40] iter = 11680, loss = 1.9127
2024-10-31 02:18:43: [2024-10-31 02:18:43] iter = 11690, loss = 1.8073
2024-10-31 02:18:46: [2024-10-31 02:18:46] iter = 11700, loss = 2.6950
2024-10-31 02:18:50: [2024-10-31 02:18:50] iter = 11710, loss = 2.6651
2024-10-31 02:18:53: [2024-10-31 02:18:53] iter = 11720, loss = 2.0168
2024-10-31 02:18:56: [2024-10-31 02:18:56] iter = 11730, loss = 1.9822
2024-10-31 02:18:59: [2024-10-31 02:18:59] iter = 11740, loss = 2.3149
2024-10-31 02:19:02: [2024-10-31 02:19:02] iter = 11750, loss = 2.3598
2024-10-31 02:19:05: [2024-10-31 02:19:05] iter = 11760, loss = 1.7532
2024-10-31 02:19:08: [2024-10-31 02:19:08] iter = 11770, loss = 1.8304
2024-10-31 02:19:11: [2024-10-31 02:19:11] iter = 11780, loss = 1.9626
2024-10-31 02:19:14: [2024-10-31 02:19:14] iter = 11790, loss = 2.6996
2024-10-31 02:19:18: [2024-10-31 02:19:18] iter = 11800, loss = 1.7453
2024-10-31 02:19:20: [2024-10-31 02:19:20] iter = 11810, loss = 2.1465
2024-10-31 02:19:22: [2024-10-31 02:19:22] iter = 11820, loss = 2.5071
2024-10-31 02:19:24: [2024-10-31 02:19:24] iter = 11830, loss = 2.8367
2024-10-31 02:19:27: [2024-10-31 02:19:27] iter = 11840, loss = 2.3370
2024-10-31 02:19:30: [2024-10-31 02:19:30] iter = 11850, loss = 2.0646
2024-10-31 02:19:33: [2024-10-31 02:19:33] iter = 11860, loss = 3.6846
2024-10-31 02:19:36: [2024-10-31 02:19:36] iter = 11870, loss = 2.3869
2024-10-31 02:19:39: [2024-10-31 02:19:39] iter = 11880, loss = 2.1518
2024-10-31 02:19:42: [2024-10-31 02:19:42] iter = 11890, loss = 2.0955
2024-10-31 02:19:46: [2024-10-31 02:19:46] iter = 11900, loss = 2.9056
2024-10-31 02:19:49: [2024-10-31 02:19:49] iter = 11910, loss = 2.2025
2024-10-31 02:19:53: [2024-10-31 02:19:53] iter = 11920, loss = 2.2370
2024-10-31 02:19:56: [2024-10-31 02:19:56] iter = 11930, loss = 2.4460
2024-10-31 02:19:59: [2024-10-31 02:19:59] iter = 11940, loss = 5.7978
2024-10-31 02:20:02: [2024-10-31 02:20:02] iter = 11950, loss = 2.2197
2024-10-31 02:20:04: [2024-10-31 02:20:04] iter = 11960, loss = 2.3659
2024-10-31 02:20:07: [2024-10-31 02:20:07] iter = 11970, loss = 2.2977
2024-10-31 02:20:10: [2024-10-31 02:20:10] iter = 11980, loss = 2.1420
2024-10-31 02:20:13: [2024-10-31 02:20:13] iter = 11990, loss = 2.2358
2024-10-31 02:20:15: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 12000
2024-10-31 02:20:15: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 02:20:15: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 15362}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:22:00: Evaluate 5 random ConvNet, ACCmean = 0.7984 ACCstd = 0.0026
-------------------------
2024-10-31 02:22:00: Evaluate 5 random ConvNet, SENmean = 0.7956 SENstd = 0.0027
-------------------------
2024-10-31 02:22:00: Evaluate 5 random ConvNet, SPEmean = 0.9798 SPEstd = 0.0003
-------------------------
2024-10-31 02:22:00: Evaluate 5 random ConvNet, F!mean = 0.7830 F!std = 0.0025
-------------------------
2024-10-31 02:22:00: Evaluate 5 random ConvNet, mean = 0.7984 std = 0.0026
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:22:00: [2024-10-31 02:22:00] iter = 12000, loss = 1.9474
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:22:03: [2024-10-31 02:22:03] iter = 12010, loss = 1.9931
2024-10-31 02:22:06: [2024-10-31 02:22:06] iter = 12020, loss = 2.7096
2024-10-31 02:22:09: [2024-10-31 02:22:09] iter = 12030, loss = 2.3109
2024-10-31 02:22:13: [2024-10-31 02:22:13] iter = 12040, loss = 2.8766
2024-10-31 02:22:16: [2024-10-31 02:22:16] iter = 12050, loss = 1.9214
2024-10-31 02:22:18: [2024-10-31 02:22:18] iter = 12060, loss = 2.0332
2024-10-31 02:22:21: [2024-10-31 02:22:21] iter = 12070, loss = 2.1289
2024-10-31 02:22:24: [2024-10-31 02:22:24] iter = 12080, loss = 2.2480
2024-10-31 02:22:28: [2024-10-31 02:22:28] iter = 12090, loss = 1.8589
2024-10-31 02:22:31: [2024-10-31 02:22:31] iter = 12100, loss = 3.1021
2024-10-31 02:22:34: [2024-10-31 02:22:34] iter = 12110, loss = 2.9531
2024-10-31 02:22:38: [2024-10-31 02:22:38] iter = 12120, loss = 1.9443
2024-10-31 02:22:41: [2024-10-31 02:22:41] iter = 12130, loss = 2.1326
2024-10-31 02:22:44: [2024-10-31 02:22:44] iter = 12140, loss = 2.2730
2024-10-31 02:22:47: [2024-10-31 02:22:47] iter = 12150, loss = 4.7921
2024-10-31 02:22:51: [2024-10-31 02:22:51] iter = 12160, loss = 3.1359
2024-10-31 02:22:54: [2024-10-31 02:22:54] iter = 12170, loss = 3.7296
2024-10-31 02:22:57: [2024-10-31 02:22:57] iter = 12180, loss = 1.7421
2024-10-31 02:23:00: [2024-10-31 02:23:00] iter = 12190, loss = 2.0782
2024-10-31 02:23:03: [2024-10-31 02:23:03] iter = 12200, loss = 2.1133
2024-10-31 02:23:05: [2024-10-31 02:23:05] iter = 12210, loss = 2.8823
2024-10-31 02:23:09: [2024-10-31 02:23:09] iter = 12220, loss = 2.2484
2024-10-31 02:23:11: [2024-10-31 02:23:11] iter = 12230, loss = 2.4642
2024-10-31 02:23:13: [2024-10-31 02:23:13] iter = 12240, loss = 2.0652
2024-10-31 02:23:16: [2024-10-31 02:23:16] iter = 12250, loss = 1.9493
2024-10-31 02:23:20: [2024-10-31 02:23:20] iter = 12260, loss = 2.9927
2024-10-31 02:23:23: [2024-10-31 02:23:23] iter = 12270, loss = 2.0089
2024-10-31 02:23:27: [2024-10-31 02:23:27] iter = 12280, loss = 2.1941
2024-10-31 02:23:30: [2024-10-31 02:23:30] iter = 12290, loss = 3.6806
2024-10-31 02:23:34: [2024-10-31 02:23:34] iter = 12300, loss = 2.9422
2024-10-31 02:23:36: [2024-10-31 02:23:36] iter = 12310, loss = 2.0035
2024-10-31 02:23:39: [2024-10-31 02:23:39] iter = 12320, loss = 1.9889
2024-10-31 02:23:41: [2024-10-31 02:23:41] iter = 12330, loss = 1.8914
2024-10-31 02:23:45: [2024-10-31 02:23:45] iter = 12340, loss = 3.5515
2024-10-31 02:23:48: [2024-10-31 02:23:48] iter = 12350, loss = 2.2181
2024-10-31 02:23:50: [2024-10-31 02:23:50] iter = 12360, loss = 2.2083
2024-10-31 02:23:54: [2024-10-31 02:23:54] iter = 12370, loss = 2.2200
2024-10-31 02:23:57: [2024-10-31 02:23:57] iter = 12380, loss = 2.1173
2024-10-31 02:24:00: [2024-10-31 02:24:00] iter = 12390, loss = 3.0574
2024-10-31 02:24:03: [2024-10-31 02:24:03] iter = 12400, loss = 1.9321
2024-10-31 02:24:06: [2024-10-31 02:24:06] iter = 12410, loss = 1.9403
2024-10-31 02:24:09: [2024-10-31 02:24:09] iter = 12420, loss = 2.1199
2024-10-31 02:24:12: [2024-10-31 02:24:12] iter = 12430, loss = 3.0292
2024-10-31 02:24:15: [2024-10-31 02:24:15] iter = 12440, loss = 2.3395
2024-10-31 02:24:17: [2024-10-31 02:24:17] iter = 12450, loss = 2.0566
2024-10-31 02:24:20: [2024-10-31 02:24:20] iter = 12460, loss = 3.1111
2024-10-31 02:24:22: [2024-10-31 02:24:22] iter = 12470, loss = 2.7753
2024-10-31 02:24:25: [2024-10-31 02:24:25] iter = 12480, loss = 1.8514
2024-10-31 02:24:28: [2024-10-31 02:24:28] iter = 12490, loss = 1.9825
2024-10-31 02:24:30: [2024-10-31 02:24:30] iter = 12500, loss = 2.0920
2024-10-31 02:24:33: [2024-10-31 02:24:33] iter = 12510, loss = 2.1899
2024-10-31 02:24:36: [2024-10-31 02:24:36] iter = 12520, loss = 3.3119
2024-10-31 02:24:38: [2024-10-31 02:24:38] iter = 12530, loss = 1.8916
2024-10-31 02:24:41: [2024-10-31 02:24:41] iter = 12540, loss = 4.0781
2024-10-31 02:24:44: [2024-10-31 02:24:44] iter = 12550, loss = 1.8458
2024-10-31 02:24:47: [2024-10-31 02:24:47] iter = 12560, loss = 2.3513
2024-10-31 02:24:50: [2024-10-31 02:24:50] iter = 12570, loss = 2.6049
2024-10-31 02:24:52: [2024-10-31 02:24:52] iter = 12580, loss = 2.5768
2024-10-31 02:24:55: [2024-10-31 02:24:55] iter = 12590, loss = 1.8929
2024-10-31 02:24:58: [2024-10-31 02:24:58] iter = 12600, loss = 2.1833
2024-10-31 02:25:01: [2024-10-31 02:25:01] iter = 12610, loss = 3.4286
2024-10-31 02:25:04: [2024-10-31 02:25:04] iter = 12620, loss = 2.3701
2024-10-31 02:25:07: [2024-10-31 02:25:07] iter = 12630, loss = 2.2161
2024-10-31 02:25:10: [2024-10-31 02:25:10] iter = 12640, loss = 2.5468
2024-10-31 02:25:14: [2024-10-31 02:25:14] iter = 12650, loss = 2.3287
2024-10-31 02:25:17: [2024-10-31 02:25:17] iter = 12660, loss = 2.2098
2024-10-31 02:25:20: [2024-10-31 02:25:20] iter = 12670, loss = 2.3192
2024-10-31 02:25:22: [2024-10-31 02:25:22] iter = 12680, loss = 1.9622
2024-10-31 02:25:25: [2024-10-31 02:25:25] iter = 12690, loss = 2.3817
2024-10-31 02:25:27: [2024-10-31 02:25:27] iter = 12700, loss = 4.8638
2024-10-31 02:25:30: [2024-10-31 02:25:30] iter = 12710, loss = 1.9731
2024-10-31 02:25:33: [2024-10-31 02:25:33] iter = 12720, loss = 2.8917
2024-10-31 02:25:36: [2024-10-31 02:25:36] iter = 12730, loss = 2.1984
2024-10-31 02:25:38: [2024-10-31 02:25:38] iter = 12740, loss = 2.9909
2024-10-31 02:25:41: [2024-10-31 02:25:41] iter = 12750, loss = 2.5204
2024-10-31 02:25:43: [2024-10-31 02:25:43] iter = 12760, loss = 2.3403
2024-10-31 02:25:45: [2024-10-31 02:25:45] iter = 12770, loss = 2.0417
2024-10-31 02:25:47: [2024-10-31 02:25:47] iter = 12780, loss = 2.4209
2024-10-31 02:25:50: [2024-10-31 02:25:50] iter = 12790, loss = 4.3480
2024-10-31 02:25:52: [2024-10-31 02:25:52] iter = 12800, loss = 2.1512
2024-10-31 02:25:55: [2024-10-31 02:25:55] iter = 12810, loss = 2.3664
2024-10-31 02:25:57: [2024-10-31 02:25:57] iter = 12820, loss = 2.1962
2024-10-31 02:25:59: [2024-10-31 02:25:59] iter = 12830, loss = 2.1115
2024-10-31 02:26:01: [2024-10-31 02:26:01] iter = 12840, loss = 1.9818
2024-10-31 02:26:04: [2024-10-31 02:26:04] iter = 12850, loss = 2.3715
2024-10-31 02:26:06: [2024-10-31 02:26:06] iter = 12860, loss = 2.2071
2024-10-31 02:26:09: [2024-10-31 02:26:09] iter = 12870, loss = 2.6256
2024-10-31 02:26:11: [2024-10-31 02:26:11] iter = 12880, loss = 2.3065
2024-10-31 02:26:14: [2024-10-31 02:26:14] iter = 12890, loss = 2.2729
2024-10-31 02:26:17: [2024-10-31 02:26:17] iter = 12900, loss = 2.0394
2024-10-31 02:26:19: [2024-10-31 02:26:19] iter = 12910, loss = 1.9385
2024-10-31 02:26:22: [2024-10-31 02:26:22] iter = 12920, loss = 2.2306
2024-10-31 02:26:24: [2024-10-31 02:26:24] iter = 12930, loss = 2.5738
2024-10-31 02:26:27: [2024-10-31 02:26:27] iter = 12940, loss = 2.4011
2024-10-31 02:26:29: [2024-10-31 02:26:29] iter = 12950, loss = 3.0900
2024-10-31 02:26:31: [2024-10-31 02:26:31] iter = 12960, loss = 2.3705
2024-10-31 02:26:35: [2024-10-31 02:26:35] iter = 12970, loss = 2.2228
2024-10-31 02:26:37: [2024-10-31 02:26:37] iter = 12980, loss = 1.8303
2024-10-31 02:26:39: [2024-10-31 02:26:39] iter = 12990, loss = 2.0988
2024-10-31 02:26:42: [2024-10-31 02:26:42] iter = 13000, loss = 2.3546
2024-10-31 02:26:44: [2024-10-31 02:26:44] iter = 13010, loss = 2.0632
2024-10-31 02:26:47: [2024-10-31 02:26:47] iter = 13020, loss = 3.9780
2024-10-31 02:26:49: [2024-10-31 02:26:49] iter = 13030, loss = 3.2367
2024-10-31 02:26:50: [2024-10-31 02:26:50] iter = 13040, loss = 4.7908
2024-10-31 02:26:52: [2024-10-31 02:26:52] iter = 13050, loss = 2.8191
2024-10-31 02:26:54: [2024-10-31 02:26:54] iter = 13060, loss = 2.4619
2024-10-31 02:26:56: [2024-10-31 02:26:56] iter = 13070, loss = 2.6044
2024-10-31 02:26:58: [2024-10-31 02:26:58] iter = 13080, loss = 2.7266
2024-10-31 02:27:01: [2024-10-31 02:27:01] iter = 13090, loss = 3.1805
2024-10-31 02:27:03: [2024-10-31 02:27:03] iter = 13100, loss = 2.1433
2024-10-31 02:27:06: [2024-10-31 02:27:06] iter = 13110, loss = 2.2572
2024-10-31 02:27:08: [2024-10-31 02:27:08] iter = 13120, loss = 2.8577
2024-10-31 02:27:11: [2024-10-31 02:27:11] iter = 13130, loss = 1.8784
2024-10-31 02:27:12: [2024-10-31 02:27:12] iter = 13140, loss = 1.9887
2024-10-31 02:27:15: [2024-10-31 02:27:15] iter = 13150, loss = 1.9046
2024-10-31 02:27:17: [2024-10-31 02:27:17] iter = 13160, loss = 2.9478
2024-10-31 02:27:20: [2024-10-31 02:27:20] iter = 13170, loss = 2.8430
2024-10-31 02:27:22: [2024-10-31 02:27:22] iter = 13180, loss = 2.3853
2024-10-31 02:27:25: [2024-10-31 02:27:25] iter = 13190, loss = 1.9109
2024-10-31 02:27:28: [2024-10-31 02:27:28] iter = 13200, loss = 2.2695
2024-10-31 02:27:30: [2024-10-31 02:27:30] iter = 13210, loss = 3.2158
2024-10-31 02:27:33: [2024-10-31 02:27:33] iter = 13220, loss = 2.3670
2024-10-31 02:27:36: [2024-10-31 02:27:36] iter = 13230, loss = 2.3503
2024-10-31 02:27:38: [2024-10-31 02:27:38] iter = 13240, loss = 2.3091
2024-10-31 02:27:41: [2024-10-31 02:27:41] iter = 13250, loss = 2.3370
2024-10-31 02:27:43: [2024-10-31 02:27:43] iter = 13260, loss = 2.4275
2024-10-31 02:27:45: [2024-10-31 02:27:45] iter = 13270, loss = 1.8911
2024-10-31 02:27:47: [2024-10-31 02:27:47] iter = 13280, loss = 2.2799
2024-10-31 02:27:50: [2024-10-31 02:27:50] iter = 13290, loss = 1.7534
2024-10-31 02:27:52: [2024-10-31 02:27:52] iter = 13300, loss = 1.9950
2024-10-31 02:27:54: [2024-10-31 02:27:54] iter = 13310, loss = 2.0747
2024-10-31 02:27:56: [2024-10-31 02:27:56] iter = 13320, loss = 1.7608
2024-10-31 02:27:59: [2024-10-31 02:27:59] iter = 13330, loss = 2.1995
2024-10-31 02:28:01: [2024-10-31 02:28:01] iter = 13340, loss = 1.7785
2024-10-31 02:28:04: [2024-10-31 02:28:04] iter = 13350, loss = 2.1250
2024-10-31 02:28:06: [2024-10-31 02:28:06] iter = 13360, loss = 2.6291
2024-10-31 02:28:09: [2024-10-31 02:28:09] iter = 13370, loss = 2.2869
2024-10-31 02:28:11: [2024-10-31 02:28:11] iter = 13380, loss = 2.5449
2024-10-31 02:28:13: [2024-10-31 02:28:13] iter = 13390, loss = 2.0957
2024-10-31 02:28:16: [2024-10-31 02:28:16] iter = 13400, loss = 2.8475
2024-10-31 02:28:18: [2024-10-31 02:28:18] iter = 13410, loss = 2.2504
2024-10-31 02:28:21: [2024-10-31 02:28:21] iter = 13420, loss = 2.1569
2024-10-31 02:28:23: [2024-10-31 02:28:23] iter = 13430, loss = 2.7799
2024-10-31 02:28:26: [2024-10-31 02:28:26] iter = 13440, loss = 2.1669
2024-10-31 02:28:28: [2024-10-31 02:28:28] iter = 13450, loss = 2.3648
2024-10-31 02:28:30: [2024-10-31 02:28:30] iter = 13460, loss = 3.1090
2024-10-31 02:28:32: [2024-10-31 02:28:32] iter = 13470, loss = 2.0687
2024-10-31 02:28:34: [2024-10-31 02:28:34] iter = 13480, loss = 2.0909
2024-10-31 02:28:36: [2024-10-31 02:28:36] iter = 13490, loss = 2.0145
2024-10-31 02:28:38: [2024-10-31 02:28:38] iter = 13500, loss = 2.4878
2024-10-31 02:28:41: [2024-10-31 02:28:41] iter = 13510, loss = 2.8218
2024-10-31 02:28:44: [2024-10-31 02:28:44] iter = 13520, loss = 2.9969
2024-10-31 02:28:47: [2024-10-31 02:28:47] iter = 13530, loss = 1.9565
2024-10-31 02:28:50: [2024-10-31 02:28:50] iter = 13540, loss = 2.0251
2024-10-31 02:28:53: [2024-10-31 02:28:53] iter = 13550, loss = 2.0538
2024-10-31 02:28:55: [2024-10-31 02:28:55] iter = 13560, loss = 5.8971
2024-10-31 02:28:57: [2024-10-31 02:28:57] iter = 13570, loss = 1.7970
2024-10-31 02:28:59: [2024-10-31 02:28:59] iter = 13580, loss = 2.1368
2024-10-31 02:29:02: [2024-10-31 02:29:02] iter = 13590, loss = 2.7661
2024-10-31 02:29:04: [2024-10-31 02:29:04] iter = 13600, loss = 2.0217
2024-10-31 02:29:07: [2024-10-31 02:29:07] iter = 13610, loss = 2.7962
2024-10-31 02:29:10: [2024-10-31 02:29:10] iter = 13620, loss = 1.8732
2024-10-31 02:29:13: [2024-10-31 02:29:13] iter = 13630, loss = 2.0022
2024-10-31 02:29:16: [2024-10-31 02:29:16] iter = 13640, loss = 2.4156
2024-10-31 02:29:19: [2024-10-31 02:29:19] iter = 13650, loss = 2.3636
2024-10-31 02:29:21: [2024-10-31 02:29:21] iter = 13660, loss = 2.3617
2024-10-31 02:29:23: [2024-10-31 02:29:23] iter = 13670, loss = 4.1981
2024-10-31 02:29:25: [2024-10-31 02:29:25] iter = 13680, loss = 3.8749
2024-10-31 02:29:28: [2024-10-31 02:29:28] iter = 13690, loss = 2.2804
2024-10-31 02:29:30: [2024-10-31 02:29:30] iter = 13700, loss = 3.8648
2024-10-31 02:29:33: [2024-10-31 02:29:33] iter = 13710, loss = 1.8093
2024-10-31 02:29:36: [2024-10-31 02:29:36] iter = 13720, loss = 2.7694
2024-10-31 02:29:38: [2024-10-31 02:29:38] iter = 13730, loss = 3.0256
2024-10-31 02:29:40: [2024-10-31 02:29:40] iter = 13740, loss = 2.3750
2024-10-31 02:29:42: [2024-10-31 02:29:42] iter = 13750, loss = 2.2827
2024-10-31 02:29:45: [2024-10-31 02:29:45] iter = 13760, loss = 2.0186
2024-10-31 02:29:46: [2024-10-31 02:29:46] iter = 13770, loss = 2.5287
2024-10-31 02:29:48: [2024-10-31 02:29:48] iter = 13780, loss = 2.3891
2024-10-31 02:29:51: [2024-10-31 02:29:51] iter = 13790, loss = 1.9263
2024-10-31 02:29:53: [2024-10-31 02:29:53] iter = 13800, loss = 3.0817
2024-10-31 02:29:55: [2024-10-31 02:29:55] iter = 13810, loss = 2.3739
2024-10-31 02:29:58: [2024-10-31 02:29:58] iter = 13820, loss = 1.8435
2024-10-31 02:30:01: [2024-10-31 02:30:01] iter = 13830, loss = 1.8537
2024-10-31 02:30:04: [2024-10-31 02:30:04] iter = 13840, loss = 2.1468
2024-10-31 02:30:06: [2024-10-31 02:30:06] iter = 13850, loss = 2.0614
2024-10-31 02:30:09: [2024-10-31 02:30:09] iter = 13860, loss = 3.0087
2024-10-31 02:30:12: [2024-10-31 02:30:12] iter = 13870, loss = 3.3564
2024-10-31 02:30:15: [2024-10-31 02:30:15] iter = 13880, loss = 2.0560
2024-10-31 02:30:17: [2024-10-31 02:30:17] iter = 13890, loss = 1.9094
2024-10-31 02:30:19: [2024-10-31 02:30:19] iter = 13900, loss = 2.7048
2024-10-31 02:30:22: [2024-10-31 02:30:22] iter = 13910, loss = 1.8986
2024-10-31 02:30:24: [2024-10-31 02:30:24] iter = 13920, loss = 2.3152
2024-10-31 02:30:27: [2024-10-31 02:30:27] iter = 13930, loss = 2.3212
2024-10-31 02:30:30: [2024-10-31 02:30:30] iter = 13940, loss = 2.8532
2024-10-31 02:30:32: [2024-10-31 02:30:32] iter = 13950, loss = 2.4133
2024-10-31 02:30:35: [2024-10-31 02:30:35] iter = 13960, loss = 1.8932
2024-10-31 02:30:37: [2024-10-31 02:30:37] iter = 13970, loss = 2.8406
2024-10-31 02:30:40: [2024-10-31 02:30:40] iter = 13980, loss = 2.4814
2024-10-31 02:30:43: [2024-10-31 02:30:43] iter = 13990, loss = 2.4645
2024-10-31 02:30:45: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 14000
2024-10-31 02:30:45: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 02:30:45: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 45361}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:32:21: Evaluate 5 random ConvNet, ACCmean = 0.7787 ACCstd = 0.0017
-------------------------
2024-10-31 02:32:21: Evaluate 5 random ConvNet, SENmean = 0.7758 SENstd = 0.0024
-------------------------
2024-10-31 02:32:21: Evaluate 5 random ConvNet, SPEmean = 0.9779 SPEstd = 0.0002
-------------------------
2024-10-31 02:32:21: Evaluate 5 random ConvNet, F!mean = 0.7625 F!std = 0.0019
-------------------------
2024-10-31 02:32:21: Evaluate 5 random ConvNet, mean = 0.7787 std = 0.0017
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:32:21: [2024-10-31 02:32:21] iter = 14000, loss = 2.3284
2024-10-31 02:32:24: [2024-10-31 02:32:24] iter = 14010, loss = 1.8828
2024-10-31 02:32:27: [2024-10-31 02:32:27] iter = 14020, loss = 2.1049
2024-10-31 02:32:30: [2024-10-31 02:32:30] iter = 14030, loss = 2.7680
2024-10-31 02:32:32: [2024-10-31 02:32:32] iter = 14040, loss = 2.2056
2024-10-31 02:32:36: [2024-10-31 02:32:36] iter = 14050, loss = 1.7729
2024-10-31 02:32:39: [2024-10-31 02:32:39] iter = 14060, loss = 3.5358
2024-10-31 02:32:42: [2024-10-31 02:32:42] iter = 14070, loss = 2.0884
2024-10-31 02:32:45: [2024-10-31 02:32:45] iter = 14080, loss = 1.9749
2024-10-31 02:32:48: [2024-10-31 02:32:48] iter = 14090, loss = 1.9294
2024-10-31 02:32:51: [2024-10-31 02:32:51] iter = 14100, loss = 2.1889
2024-10-31 02:32:53: [2024-10-31 02:32:53] iter = 14110, loss = 2.5150
2024-10-31 02:32:56: [2024-10-31 02:32:56] iter = 14120, loss = 1.9498
2024-10-31 02:32:59: [2024-10-31 02:32:59] iter = 14130, loss = 3.0158
2024-10-31 02:33:02: [2024-10-31 02:33:02] iter = 14140, loss = 2.2436
2024-10-31 02:33:05: [2024-10-31 02:33:05] iter = 14150, loss = 2.0590
2024-10-31 02:33:08: [2024-10-31 02:33:08] iter = 14160, loss = 3.1784
2024-10-31 02:33:11: [2024-10-31 02:33:11] iter = 14170, loss = 2.2869
2024-10-31 02:33:13: [2024-10-31 02:33:13] iter = 14180, loss = 2.0176
2024-10-31 02:33:16: [2024-10-31 02:33:16] iter = 14190, loss = 2.1377
2024-10-31 02:33:20: [2024-10-31 02:33:20] iter = 14200, loss = 2.0272
2024-10-31 02:33:23: [2024-10-31 02:33:23] iter = 14210, loss = 1.9969
2024-10-31 02:33:26: [2024-10-31 02:33:26] iter = 14220, loss = 2.1623
2024-10-31 02:33:30: [2024-10-31 02:33:30] iter = 14230, loss = 1.8607
2024-10-31 02:33:33: [2024-10-31 02:33:33] iter = 14240, loss = 3.6319
2024-10-31 02:33:36: [2024-10-31 02:33:36] iter = 14250, loss = 2.3817
2024-10-31 02:33:39: [2024-10-31 02:33:39] iter = 14260, loss = 2.4818
2024-10-31 02:33:41: [2024-10-31 02:33:41] iter = 14270, loss = 2.0838
2024-10-31 02:33:44: [2024-10-31 02:33:44] iter = 14280, loss = 2.3760
2024-10-31 02:33:47: [2024-10-31 02:33:47] iter = 14290, loss = 2.7325
2024-10-31 02:33:50: [2024-10-31 02:33:50] iter = 14300, loss = 1.9539
2024-10-31 02:33:53: [2024-10-31 02:33:53] iter = 14310, loss = 1.6478
2024-10-31 02:33:56: [2024-10-31 02:33:56] iter = 14320, loss = 2.3310
2024-10-31 02:34:00: [2024-10-31 02:34:00] iter = 14330, loss = 2.1878
2024-10-31 02:34:03: [2024-10-31 02:34:03] iter = 14340, loss = 2.1584
2024-10-31 02:34:05: [2024-10-31 02:34:05] iter = 14350, loss = 2.5824
2024-10-31 02:34:07: [2024-10-31 02:34:07] iter = 14360, loss = 3.3467
2024-10-31 02:34:10: [2024-10-31 02:34:10] iter = 14370, loss = 2.1495
2024-10-31 02:34:12: [2024-10-31 02:34:12] iter = 14380, loss = 1.9108
2024-10-31 02:34:14: [2024-10-31 02:34:14] iter = 14390, loss = 2.2155
2024-10-31 02:34:17: [2024-10-31 02:34:17] iter = 14400, loss = 2.4768
2024-10-31 02:34:19: [2024-10-31 02:34:19] iter = 14410, loss = 2.0759
2024-10-31 02:34:22: [2024-10-31 02:34:22] iter = 14420, loss = 1.8537
2024-10-31 02:34:24: [2024-10-31 02:34:24] iter = 14430, loss = 1.7851
2024-10-31 02:34:27: [2024-10-31 02:34:27] iter = 14440, loss = 2.1864
2024-10-31 02:34:29: [2024-10-31 02:34:29] iter = 14450, loss = 2.4331
2024-10-31 02:34:31: [2024-10-31 02:34:31] iter = 14460, loss = 1.9412
2024-10-31 02:34:34: [2024-10-31 02:34:34] iter = 14470, loss = 2.4903
2024-10-31 02:34:36: [2024-10-31 02:34:36] iter = 14480, loss = 2.0392
2024-10-31 02:34:39: [2024-10-31 02:34:39] iter = 14490, loss = 1.9950
2024-10-31 02:34:43: [2024-10-31 02:34:43] iter = 14500, loss = 2.2209
2024-10-31 02:34:46: [2024-10-31 02:34:46] iter = 14510, loss = 2.6935
2024-10-31 02:34:49: [2024-10-31 02:34:49] iter = 14520, loss = 2.2510
2024-10-31 02:34:51: [2024-10-31 02:34:51] iter = 14530, loss = 2.5817
2024-10-31 02:34:54: [2024-10-31 02:34:54] iter = 14540, loss = 1.8021
2024-10-31 02:34:57: [2024-10-31 02:34:57] iter = 14550, loss = 1.9142
2024-10-31 02:35:01: [2024-10-31 02:35:01] iter = 14560, loss = 3.0100
2024-10-31 02:35:05: [2024-10-31 02:35:05] iter = 14570, loss = 2.2941
2024-10-31 02:35:07: [2024-10-31 02:35:07] iter = 14580, loss = 3.5343
2024-10-31 02:35:11: [2024-10-31 02:35:11] iter = 14590, loss = 2.3964
2024-10-31 02:35:13: [2024-10-31 02:35:13] iter = 14600, loss = 1.8912
2024-10-31 02:35:16: [2024-10-31 02:35:16] iter = 14610, loss = 2.5844
2024-10-31 02:35:19: [2024-10-31 02:35:19] iter = 14620, loss = 2.9121
2024-10-31 02:35:22: [2024-10-31 02:35:22] iter = 14630, loss = 1.8969
2024-10-31 02:35:25: [2024-10-31 02:35:25] iter = 14640, loss = 4.3829
2024-10-31 02:35:29: [2024-10-31 02:35:29] iter = 14650, loss = 2.3452
2024-10-31 02:35:32: [2024-10-31 02:35:32] iter = 14660, loss = 4.3258
2024-10-31 02:35:35: [2024-10-31 02:35:35] iter = 14670, loss = 2.0892
2024-10-31 02:35:39: [2024-10-31 02:35:39] iter = 14680, loss = 2.4971
2024-10-31 02:35:41: [2024-10-31 02:35:41] iter = 14690, loss = 2.3749
2024-10-31 02:35:45: [2024-10-31 02:35:45] iter = 14700, loss = 2.1110
2024-10-31 02:35:48: [2024-10-31 02:35:48] iter = 14710, loss = 2.0169
2024-10-31 02:35:52: [2024-10-31 02:35:52] iter = 14720, loss = 2.6737
2024-10-31 02:35:55: [2024-10-31 02:35:55] iter = 14730, loss = 2.0944
2024-10-31 02:36:00: [2024-10-31 02:36:00] iter = 14740, loss = 2.2216
2024-10-31 02:36:03: [2024-10-31 02:36:03] iter = 14750, loss = 1.6259
2024-10-31 02:36:06: [2024-10-31 02:36:06] iter = 14760, loss = 2.0394
2024-10-31 02:36:09: [2024-10-31 02:36:09] iter = 14770, loss = 3.5503
2024-10-31 02:36:13: [2024-10-31 02:36:13] iter = 14780, loss = 2.3603
2024-10-31 02:36:16: [2024-10-31 02:36:16] iter = 14790, loss = 2.5458
2024-10-31 02:36:19: [2024-10-31 02:36:19] iter = 14800, loss = 2.1073
2024-10-31 02:36:22: [2024-10-31 02:36:22] iter = 14810, loss = 1.7864
2024-10-31 02:36:25: [2024-10-31 02:36:25] iter = 14820, loss = 3.6536
2024-10-31 02:36:28: [2024-10-31 02:36:28] iter = 14830, loss = 2.5187
2024-10-31 02:36:32: [2024-10-31 02:36:32] iter = 14840, loss = 1.9214
2024-10-31 02:36:35: [2024-10-31 02:36:35] iter = 14850, loss = 2.1515
2024-10-31 02:36:39: [2024-10-31 02:36:39] iter = 14860, loss = 1.8273
2024-10-31 02:36:42: [2024-10-31 02:36:42] iter = 14870, loss = 2.3265
2024-10-31 02:36:45: [2024-10-31 02:36:45] iter = 14880, loss = 6.1449
2024-10-31 02:36:47: [2024-10-31 02:36:47] iter = 14890, loss = 1.8912
2024-10-31 02:36:51: [2024-10-31 02:36:51] iter = 14900, loss = 2.0855
2024-10-31 02:36:55: [2024-10-31 02:36:55] iter = 14910, loss = 2.2112
2024-10-31 02:36:58: [2024-10-31 02:36:58] iter = 14920, loss = 3.3156
2024-10-31 02:37:01: [2024-10-31 02:37:01] iter = 14930, loss = 2.2042
2024-10-31 02:37:03: [2024-10-31 02:37:03] iter = 14940, loss = 2.5199
2024-10-31 02:37:06: [2024-10-31 02:37:06] iter = 14950, loss = 2.2292
2024-10-31 02:37:09: [2024-10-31 02:37:09] iter = 14960, loss = 3.0010
2024-10-31 02:37:11: [2024-10-31 02:37:11] iter = 14970, loss = 2.4937
2024-10-31 02:37:14: [2024-10-31 02:37:14] iter = 14980, loss = 3.7067
2024-10-31 02:37:16: [2024-10-31 02:37:16] iter = 14990, loss = 2.3238
2024-10-31 02:37:19: [2024-10-31 02:37:19] iter = 15000, loss = 2.3570
2024-10-31 02:37:22: [2024-10-31 02:37:22] iter = 15010, loss = 2.0667
2024-10-31 02:37:24: [2024-10-31 02:37:24] iter = 15020, loss = 2.1094
2024-10-31 02:37:26: [2024-10-31 02:37:26] iter = 15030, loss = 2.5453
2024-10-31 02:37:29: [2024-10-31 02:37:29] iter = 15040, loss = 2.5378
2024-10-31 02:37:31: [2024-10-31 02:37:31] iter = 15050, loss = 2.0167
2024-10-31 02:37:34: [2024-10-31 02:37:34] iter = 15060, loss = 2.5739
2024-10-31 02:37:36: [2024-10-31 02:37:36] iter = 15070, loss = 2.2701
2024-10-31 02:37:39: [2024-10-31 02:37:39] iter = 15080, loss = 1.8882
2024-10-31 02:37:41: [2024-10-31 02:37:41] iter = 15090, loss = 1.9626
2024-10-31 02:37:43: [2024-10-31 02:37:43] iter = 15100, loss = 2.0396
2024-10-31 02:37:46: [2024-10-31 02:37:46] iter = 15110, loss = 2.1717
2024-10-31 02:37:48: [2024-10-31 02:37:48] iter = 15120, loss = 2.6273
2024-10-31 02:37:51: [2024-10-31 02:37:51] iter = 15130, loss = 1.9811
2024-10-31 02:37:54: [2024-10-31 02:37:54] iter = 15140, loss = 2.1177
2024-10-31 02:37:56: [2024-10-31 02:37:56] iter = 15150, loss = 4.5692
2024-10-31 02:37:59: [2024-10-31 02:37:59] iter = 15160, loss = 3.4090
2024-10-31 02:38:02: [2024-10-31 02:38:02] iter = 15170, loss = 2.3125
2024-10-31 02:38:05: [2024-10-31 02:38:05] iter = 15180, loss = 1.9891
2024-10-31 02:38:08: [2024-10-31 02:38:08] iter = 15190, loss = 4.3194
2024-10-31 02:38:10: [2024-10-31 02:38:10] iter = 15200, loss = 2.1780
2024-10-31 02:38:13: [2024-10-31 02:38:13] iter = 15210, loss = 3.5997
2024-10-31 02:38:15: [2024-10-31 02:38:15] iter = 15220, loss = 2.3224
2024-10-31 02:38:18: [2024-10-31 02:38:18] iter = 15230, loss = 2.7962
2024-10-31 02:38:19: [2024-10-31 02:38:19] iter = 15240, loss = 2.0259
2024-10-31 02:38:22: [2024-10-31 02:38:22] iter = 15250, loss = 1.8772
2024-10-31 02:38:25: [2024-10-31 02:38:25] iter = 15260, loss = 2.7031
2024-10-31 02:38:28: [2024-10-31 02:38:28] iter = 15270, loss = 2.6302
2024-10-31 02:38:30: [2024-10-31 02:38:30] iter = 15280, loss = 2.8615
2024-10-31 02:38:33: [2024-10-31 02:38:33] iter = 15290, loss = 4.5554
2024-10-31 02:38:36: [2024-10-31 02:38:35] iter = 15300, loss = 5.3193
2024-10-31 02:38:39: [2024-10-31 02:38:39] iter = 15310, loss = 2.3228
2024-10-31 02:38:42: [2024-10-31 02:38:42] iter = 15320, loss = 2.5885
2024-10-31 02:38:44: [2024-10-31 02:38:44] iter = 15330, loss = 4.6791
2024-10-31 02:38:47: [2024-10-31 02:38:47] iter = 15340, loss = 2.4238
2024-10-31 02:38:49: [2024-10-31 02:38:49] iter = 15350, loss = 2.7690
2024-10-31 02:38:51: [2024-10-31 02:38:51] iter = 15360, loss = 1.9268
2024-10-31 02:38:53: [2024-10-31 02:38:53] iter = 15370, loss = 2.4929
2024-10-31 02:38:56: [2024-10-31 02:38:56] iter = 15380, loss = 2.0637
2024-10-31 02:38:59: [2024-10-31 02:38:59] iter = 15390, loss = 2.4028
2024-10-31 02:39:01: [2024-10-31 02:39:01] iter = 15400, loss = 3.5800
2024-10-31 02:39:04: [2024-10-31 02:39:04] iter = 15410, loss = 2.5466
2024-10-31 02:39:07: [2024-10-31 02:39:07] iter = 15420, loss = 2.5570
2024-10-31 02:39:10: [2024-10-31 02:39:10] iter = 15430, loss = 1.9219
2024-10-31 02:39:13: [2024-10-31 02:39:13] iter = 15440, loss = 1.9096
2024-10-31 02:39:15: [2024-10-31 02:39:15] iter = 15450, loss = 2.2785
2024-10-31 02:39:18: [2024-10-31 02:39:18] iter = 15460, loss = 2.5909
2024-10-31 02:39:21: [2024-10-31 02:39:21] iter = 15470, loss = 1.8482
2024-10-31 02:39:23: [2024-10-31 02:39:23] iter = 15480, loss = 2.0652
2024-10-31 02:39:26: [2024-10-31 02:39:26] iter = 15490, loss = 2.0383
2024-10-31 02:39:29: [2024-10-31 02:39:29] iter = 15500, loss = 2.2019
2024-10-31 02:39:32: [2024-10-31 02:39:32] iter = 15510, loss = 1.7254
2024-10-31 02:39:35: [2024-10-31 02:39:35] iter = 15520, loss = 2.3672
2024-10-31 02:39:39: [2024-10-31 02:39:39] iter = 15530, loss = 1.9762
2024-10-31 02:39:42: [2024-10-31 02:39:42] iter = 15540, loss = 2.2616
2024-10-31 02:39:44: [2024-10-31 02:39:44] iter = 15550, loss = 4.2504
2024-10-31 02:39:47: [2024-10-31 02:39:47] iter = 15560, loss = 2.0806
2024-10-31 02:39:51: [2024-10-31 02:39:51] iter = 15570, loss = 1.6170
2024-10-31 02:39:54: [2024-10-31 02:39:54] iter = 15580, loss = 1.9833
2024-10-31 02:39:56: [2024-10-31 02:39:56] iter = 15590, loss = 1.7744
2024-10-31 02:39:59: [2024-10-31 02:39:59] iter = 15600, loss = 2.3351
2024-10-31 02:40:02: [2024-10-31 02:40:02] iter = 15610, loss = 2.5532
2024-10-31 02:40:06: [2024-10-31 02:40:06] iter = 15620, loss = 2.2037
2024-10-31 02:40:08: [2024-10-31 02:40:08] iter = 15630, loss = 2.1231
2024-10-31 02:40:10: [2024-10-31 02:40:10] iter = 15640, loss = 2.8623
2024-10-31 02:40:13: [2024-10-31 02:40:13] iter = 15650, loss = 2.2517
2024-10-31 02:40:16: [2024-10-31 02:40:16] iter = 15660, loss = 1.8838
2024-10-31 02:40:19: [2024-10-31 02:40:19] iter = 15670, loss = 2.0164
2024-10-31 02:40:22: [2024-10-31 02:40:22] iter = 15680, loss = 1.5573
2024-10-31 02:40:25: [2024-10-31 02:40:25] iter = 15690, loss = 2.0826
2024-10-31 02:40:28: [2024-10-31 02:40:28] iter = 15700, loss = 1.8219
2024-10-31 02:40:30: [2024-10-31 02:40:30] iter = 15710, loss = 2.2151
2024-10-31 02:40:33: [2024-10-31 02:40:33] iter = 15720, loss = 1.9231
2024-10-31 02:40:37: [2024-10-31 02:40:37] iter = 15730, loss = 3.5559
2024-10-31 02:40:41: [2024-10-31 02:40:41] iter = 15740, loss = 2.2340
2024-10-31 02:40:44: [2024-10-31 02:40:44] iter = 15750, loss = 2.3302
2024-10-31 02:40:48: [2024-10-31 02:40:48] iter = 15760, loss = 2.2047
2024-10-31 02:40:50: [2024-10-31 02:40:50] iter = 15770, loss = 2.1017
2024-10-31 02:40:53: [2024-10-31 02:40:53] iter = 15780, loss = 2.3268
2024-10-31 02:40:56: [2024-10-31 02:40:56] iter = 15790, loss = 2.2100
2024-10-31 02:40:58: [2024-10-31 02:40:58] iter = 15800, loss = 2.2391
2024-10-31 02:41:00: [2024-10-31 02:41:00] iter = 15810, loss = 3.7988
2024-10-31 02:41:03: [2024-10-31 02:41:03] iter = 15820, loss = 2.5496
2024-10-31 02:41:06: [2024-10-31 02:41:06] iter = 15830, loss = 2.3972
2024-10-31 02:41:09: [2024-10-31 02:41:09] iter = 15840, loss = 2.0452
2024-10-31 02:41:11: [2024-10-31 02:41:11] iter = 15850, loss = 2.3278
2024-10-31 02:41:14: [2024-10-31 02:41:14] iter = 15860, loss = 2.1295
2024-10-31 02:41:16: [2024-10-31 02:41:16] iter = 15870, loss = 1.8692
2024-10-31 02:41:19: [2024-10-31 02:41:19] iter = 15880, loss = 2.3010
2024-10-31 02:41:21: [2024-10-31 02:41:21] iter = 15890, loss = 2.5550
2024-10-31 02:41:23: [2024-10-31 02:41:23] iter = 15900, loss = 2.1572
2024-10-31 02:41:25: [2024-10-31 02:41:25] iter = 15910, loss = 2.4386
2024-10-31 02:41:28: [2024-10-31 02:41:28] iter = 15920, loss = 1.8877
2024-10-31 02:41:30: [2024-10-31 02:41:30] iter = 15930, loss = 2.2880
2024-10-31 02:41:33: [2024-10-31 02:41:33] iter = 15940, loss = 2.7062
2024-10-31 02:41:36: [2024-10-31 02:41:36] iter = 15950, loss = 2.0576
2024-10-31 02:41:39: [2024-10-31 02:41:39] iter = 15960, loss = 2.3294
2024-10-31 02:41:42: [2024-10-31 02:41:42] iter = 15970, loss = 2.7262
2024-10-31 02:41:44: [2024-10-31 02:41:44] iter = 15980, loss = 1.9438
2024-10-31 02:41:47: [2024-10-31 02:41:47] iter = 15990, loss = 2.3032
2024-10-31 02:41:49: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 16000
2024-10-31 02:41:49: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 02:41:49: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 9869}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:43:34: Evaluate 5 random ConvNet, ACCmean = 0.7740 ACCstd = 0.0039
-------------------------
2024-10-31 02:43:34: Evaluate 5 random ConvNet, SENmean = 0.7681 SENstd = 0.0028
-------------------------
2024-10-31 02:43:34: Evaluate 5 random ConvNet, SPEmean = 0.9774 SPEstd = 0.0004
-------------------------
2024-10-31 02:43:34: Evaluate 5 random ConvNet, F!mean = 0.7560 F!std = 0.0040
-------------------------
2024-10-31 02:43:34: Evaluate 5 random ConvNet, mean = 0.7740 std = 0.0039
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:43:34: [2024-10-31 02:43:34] iter = 16000, loss = 2.0957
2024-10-31 02:43:38: [2024-10-31 02:43:38] iter = 16010, loss = 2.2937
2024-10-31 02:43:40: [2024-10-31 02:43:40] iter = 16020, loss = 1.7205
2024-10-31 02:43:44: [2024-10-31 02:43:44] iter = 16030, loss = 1.9061
2024-10-31 02:43:47: [2024-10-31 02:43:47] iter = 16040, loss = 1.8715
2024-10-31 02:43:50: [2024-10-31 02:43:50] iter = 16050, loss = 1.8847
2024-10-31 02:43:53: [2024-10-31 02:43:53] iter = 16060, loss = 2.1095
2024-10-31 02:43:56: [2024-10-31 02:43:56] iter = 16070, loss = 2.1035
2024-10-31 02:43:58: [2024-10-31 02:43:58] iter = 16080, loss = 1.9973
2024-10-31 02:44:02: [2024-10-31 02:44:02] iter = 16090, loss = 2.0575
2024-10-31 02:44:04: [2024-10-31 02:44:04] iter = 16100, loss = 2.0030
2024-10-31 02:44:07: [2024-10-31 02:44:07] iter = 16110, loss = 1.9230
2024-10-31 02:44:09: [2024-10-31 02:44:09] iter = 16120, loss = 2.0941
2024-10-31 02:44:12: [2024-10-31 02:44:12] iter = 16130, loss = 2.1953
2024-10-31 02:44:15: [2024-10-31 02:44:15] iter = 16140, loss = 2.0117
2024-10-31 02:44:17: [2024-10-31 02:44:17] iter = 16150, loss = 2.8228
2024-10-31 02:44:20: [2024-10-31 02:44:20] iter = 16160, loss = 2.3749
2024-10-31 02:44:23: [2024-10-31 02:44:23] iter = 16170, loss = 2.5500
2024-10-31 02:44:25: [2024-10-31 02:44:25] iter = 16180, loss = 2.2344
2024-10-31 02:44:28: [2024-10-31 02:44:28] iter = 16190, loss = 1.9242
2024-10-31 02:44:31: [2024-10-31 02:44:31] iter = 16200, loss = 2.3646
2024-10-31 02:44:34: [2024-10-31 02:44:34] iter = 16210, loss = 1.9043
2024-10-31 02:44:37: [2024-10-31 02:44:37] iter = 16220, loss = 1.7711
2024-10-31 02:44:40: [2024-10-31 02:44:40] iter = 16230, loss = 2.2388
2024-10-31 02:44:43: [2024-10-31 02:44:43] iter = 16240, loss = 2.1798
2024-10-31 02:44:45: [2024-10-31 02:44:45] iter = 16250, loss = 2.5346
2024-10-31 02:44:48: [2024-10-31 02:44:48] iter = 16260, loss = 2.2556
2024-10-31 02:44:51: [2024-10-31 02:44:51] iter = 16270, loss = 2.5020
2024-10-31 02:44:55: [2024-10-31 02:44:55] iter = 16280, loss = 2.2930
2024-10-31 02:44:58: [2024-10-31 02:44:58] iter = 16290, loss = 2.6921
2024-10-31 02:45:00: [2024-10-31 02:45:00] iter = 16300, loss = 1.9014
2024-10-31 02:45:03: [2024-10-31 02:45:03] iter = 16310, loss = 2.3328
2024-10-31 02:45:06: [2024-10-31 02:45:06] iter = 16320, loss = 2.0452
2024-10-31 02:45:09: [2024-10-31 02:45:09] iter = 16330, loss = 2.1727
2024-10-31 02:45:12: [2024-10-31 02:45:12] iter = 16340, loss = 3.6648
2024-10-31 02:45:15: [2024-10-31 02:45:15] iter = 16350, loss = 2.7771
2024-10-31 02:45:17: [2024-10-31 02:45:17] iter = 16360, loss = 6.9389
2024-10-31 02:45:20: [2024-10-31 02:45:20] iter = 16370, loss = 2.2033
2024-10-31 02:45:23: [2024-10-31 02:45:23] iter = 16380, loss = 2.2984
2024-10-31 02:45:26: [2024-10-31 02:45:26] iter = 16390, loss = 1.9600
2024-10-31 02:45:28: [2024-10-31 02:45:28] iter = 16400, loss = 2.0465
2024-10-31 02:45:31: [2024-10-31 02:45:31] iter = 16410, loss = 2.0917
2024-10-31 02:45:34: [2024-10-31 02:45:34] iter = 16420, loss = 2.1241
2024-10-31 02:45:36: [2024-10-31 02:45:36] iter = 16430, loss = 1.8629
2024-10-31 02:45:38: [2024-10-31 02:45:38] iter = 16440, loss = 2.1993
2024-10-31 02:45:41: [2024-10-31 02:45:41] iter = 16450, loss = 4.0188
2024-10-31 02:45:44: [2024-10-31 02:45:44] iter = 16460, loss = 1.8881
2024-10-31 02:45:46: [2024-10-31 02:45:46] iter = 16470, loss = 2.3686
2024-10-31 02:45:49: [2024-10-31 02:45:49] iter = 16480, loss = 3.6040
2024-10-31 02:45:51: [2024-10-31 02:45:51] iter = 16490, loss = 2.7392
2024-10-31 02:45:54: [2024-10-31 02:45:54] iter = 16500, loss = 1.9510
2024-10-31 02:45:57: [2024-10-31 02:45:57] iter = 16510, loss = 1.9709
2024-10-31 02:45:58: [2024-10-31 02:45:58] iter = 16520, loss = 2.3652
2024-10-31 02:46:01: [2024-10-31 02:46:01] iter = 16530, loss = 5.2765
2024-10-31 02:46:03: [2024-10-31 02:46:03] iter = 16540, loss = 3.0078
2024-10-31 02:46:05: [2024-10-31 02:46:05] iter = 16550, loss = 4.3140
2024-10-31 02:46:07: [2024-10-31 02:46:07] iter = 16560, loss = 2.3312
2024-10-31 02:46:10: [2024-10-31 02:46:10] iter = 16570, loss = 2.3895
2024-10-31 02:46:12: [2024-10-31 02:46:12] iter = 16580, loss = 5.7893
2024-10-31 02:46:14: [2024-10-31 02:46:14] iter = 16590, loss = 2.2492
2024-10-31 02:46:17: [2024-10-31 02:46:17] iter = 16600, loss = 2.0404
2024-10-31 02:46:18: [2024-10-31 02:46:18] iter = 16610, loss = 2.2448
2024-10-31 02:46:20: [2024-10-31 02:46:20] iter = 16620, loss = 2.3447
2024-10-31 02:46:22: [2024-10-31 02:46:22] iter = 16630, loss = 1.9469
2024-10-31 02:46:25: [2024-10-31 02:46:25] iter = 16640, loss = 1.9723
2024-10-31 02:46:28: [2024-10-31 02:46:28] iter = 16650, loss = 2.6049
2024-10-31 02:46:31: [2024-10-31 02:46:31] iter = 16660, loss = 1.7754
2024-10-31 02:46:32: [2024-10-31 02:46:32] iter = 16670, loss = 2.0739
2024-10-31 02:46:35: [2024-10-31 02:46:35] iter = 16680, loss = 2.4703
2024-10-31 02:46:38: [2024-10-31 02:46:38] iter = 16690, loss = 2.6193
2024-10-31 02:46:40: [2024-10-31 02:46:40] iter = 16700, loss = 2.5564
2024-10-31 02:46:43: [2024-10-31 02:46:43] iter = 16710, loss = 2.2195
2024-10-31 02:46:45: [2024-10-31 02:46:45] iter = 16720, loss = 1.7309
2024-10-31 02:46:48: [2024-10-31 02:46:48] iter = 16730, loss = 2.3190
2024-10-31 02:46:50: [2024-10-31 02:46:50] iter = 16740, loss = 2.2468
2024-10-31 02:46:53: [2024-10-31 02:46:53] iter = 16750, loss = 2.1498
2024-10-31 02:46:56: [2024-10-31 02:46:56] iter = 16760, loss = 2.1924
2024-10-31 02:46:58: [2024-10-31 02:46:58] iter = 16770, loss = 2.7070
2024-10-31 02:47:02: [2024-10-31 02:47:02] iter = 16780, loss = 2.1113
2024-10-31 02:47:04: [2024-10-31 02:47:04] iter = 16790, loss = 2.4028
2024-10-31 02:47:08: [2024-10-31 02:47:08] iter = 16800, loss = 1.9476
2024-10-31 02:47:10: [2024-10-31 02:47:10] iter = 16810, loss = 2.0018
2024-10-31 02:47:13: [2024-10-31 02:47:13] iter = 16820, loss = 10.3218
2024-10-31 02:47:15: [2024-10-31 02:47:15] iter = 16830, loss = 3.0975
2024-10-31 02:47:17: [2024-10-31 02:47:17] iter = 16840, loss = 1.7284
2024-10-31 02:47:20: [2024-10-31 02:47:20] iter = 16850, loss = 2.1886
2024-10-31 02:47:23: [2024-10-31 02:47:23] iter = 16860, loss = 1.7377
2024-10-31 02:47:25: [2024-10-31 02:47:25] iter = 16870, loss = 2.0007
2024-10-31 02:47:27: [2024-10-31 02:47:27] iter = 16880, loss = 2.3252
2024-10-31 02:47:30: [2024-10-31 02:47:30] iter = 16890, loss = 1.9916
2024-10-31 02:47:33: [2024-10-31 02:47:33] iter = 16900, loss = 2.3538
2024-10-31 02:47:35: [2024-10-31 02:47:35] iter = 16910, loss = 1.8854
2024-10-31 02:47:39: [2024-10-31 02:47:39] iter = 16920, loss = 4.7570
2024-10-31 02:47:41: [2024-10-31 02:47:41] iter = 16930, loss = 4.3179
2024-10-31 02:47:44: [2024-10-31 02:47:44] iter = 16940, loss = 2.6474
2024-10-31 02:47:47: [2024-10-31 02:47:47] iter = 16950, loss = 2.2991
2024-10-31 02:47:50: [2024-10-31 02:47:50] iter = 16960, loss = 3.0350
2024-10-31 02:47:52: [2024-10-31 02:47:52] iter = 16970, loss = 2.0340
2024-10-31 02:47:56: [2024-10-31 02:47:56] iter = 16980, loss = 2.1857
2024-10-31 02:47:58: [2024-10-31 02:47:58] iter = 16990, loss = 2.0160
2024-10-31 02:48:01: [2024-10-31 02:48:01] iter = 17000, loss = 2.3134
2024-10-31 02:48:03: [2024-10-31 02:48:03] iter = 17010, loss = 2.5183
2024-10-31 02:48:06: [2024-10-31 02:48:06] iter = 17020, loss = 2.2822
2024-10-31 02:48:08: [2024-10-31 02:48:08] iter = 17030, loss = 5.0121
2024-10-31 02:48:11: [2024-10-31 02:48:11] iter = 17040, loss = 1.9625
2024-10-31 02:48:14: [2024-10-31 02:48:14] iter = 17050, loss = 2.0693
2024-10-31 02:48:16: [2024-10-31 02:48:16] iter = 17060, loss = 2.2880
2024-10-31 02:48:19: [2024-10-31 02:48:19] iter = 17070, loss = 1.9931
2024-10-31 02:48:21: [2024-10-31 02:48:21] iter = 17080, loss = 2.1857
2024-10-31 02:48:23: [2024-10-31 02:48:23] iter = 17090, loss = 2.3997
2024-10-31 02:48:26: [2024-10-31 02:48:26] iter = 17100, loss = 2.3949
2024-10-31 02:48:28: [2024-10-31 02:48:28] iter = 17110, loss = 2.7643
2024-10-31 02:48:31: [2024-10-31 02:48:31] iter = 17120, loss = 3.4494
2024-10-31 02:48:34: [2024-10-31 02:48:34] iter = 17130, loss = 1.9625
2024-10-31 02:48:37: [2024-10-31 02:48:37] iter = 17140, loss = 1.9966
2024-10-31 02:48:39: [2024-10-31 02:48:39] iter = 17150, loss = 2.2911
2024-10-31 02:48:41: [2024-10-31 02:48:41] iter = 17160, loss = 4.3457
2024-10-31 02:48:44: [2024-10-31 02:48:44] iter = 17170, loss = 2.0445
2024-10-31 02:48:46: [2024-10-31 02:48:46] iter = 17180, loss = 2.1573
2024-10-31 02:48:49: [2024-10-31 02:48:49] iter = 17190, loss = 2.4502
2024-10-31 02:48:52: [2024-10-31 02:48:52] iter = 17200, loss = 2.1044
2024-10-31 02:48:55: [2024-10-31 02:48:55] iter = 17210, loss = 2.1906
2024-10-31 02:48:58: [2024-10-31 02:48:58] iter = 17220, loss = 2.2221
2024-10-31 02:49:01: [2024-10-31 02:49:01] iter = 17230, loss = 1.8501
2024-10-31 02:49:04: [2024-10-31 02:49:04] iter = 17240, loss = 1.8582
2024-10-31 02:49:06: [2024-10-31 02:49:06] iter = 17250, loss = 3.3156
2024-10-31 02:49:09: [2024-10-31 02:49:09] iter = 17260, loss = 1.9231
2024-10-31 02:49:11: [2024-10-31 02:49:11] iter = 17270, loss = 2.6023
2024-10-31 02:49:14: [2024-10-31 02:49:14] iter = 17280, loss = 1.9730
2024-10-31 02:49:17: [2024-10-31 02:49:17] iter = 17290, loss = 1.8168
2024-10-31 02:49:19: [2024-10-31 02:49:19] iter = 17300, loss = 1.9294
2024-10-31 02:49:22: [2024-10-31 02:49:22] iter = 17310, loss = 2.8589
2024-10-31 02:49:24: [2024-10-31 02:49:24] iter = 17320, loss = 2.3340
2024-10-31 02:49:27: [2024-10-31 02:49:27] iter = 17330, loss = 2.1699
2024-10-31 02:49:30: [2024-10-31 02:49:30] iter = 17340, loss = 2.1343
2024-10-31 02:49:32: [2024-10-31 02:49:32] iter = 17350, loss = 1.9759
2024-10-31 02:49:34: [2024-10-31 02:49:34] iter = 17360, loss = 2.4928
2024-10-31 02:49:37: [2024-10-31 02:49:37] iter = 17370, loss = 8.0013
2024-10-31 02:49:39: [2024-10-31 02:49:39] iter = 17380, loss = 1.8357
2024-10-31 02:49:42: [2024-10-31 02:49:42] iter = 17390, loss = 1.9463
2024-10-31 02:49:45: [2024-10-31 02:49:45] iter = 17400, loss = 6.9983
2024-10-31 02:49:48: [2024-10-31 02:49:48] iter = 17410, loss = 2.1700
2024-10-31 02:49:51: [2024-10-31 02:49:51] iter = 17420, loss = 2.1365
2024-10-31 02:49:55: [2024-10-31 02:49:55] iter = 17430, loss = 3.2557
2024-10-31 02:49:57: [2024-10-31 02:49:57] iter = 17440, loss = 2.0851
2024-10-31 02:50:00: [2024-10-31 02:50:00] iter = 17450, loss = 1.9972
2024-10-31 02:50:02: [2024-10-31 02:50:02] iter = 17460, loss = 2.1625
2024-10-31 02:50:05: [2024-10-31 02:50:05] iter = 17470, loss = 1.9196
2024-10-31 02:50:08: [2024-10-31 02:50:08] iter = 17480, loss = 2.0263
2024-10-31 02:50:10: [2024-10-31 02:50:10] iter = 17490, loss = 2.1522
2024-10-31 02:50:12: [2024-10-31 02:50:12] iter = 17500, loss = 1.6695
2024-10-31 02:50:14: [2024-10-31 02:50:14] iter = 17510, loss = 2.0838
2024-10-31 02:50:17: [2024-10-31 02:50:17] iter = 17520, loss = 2.1650
2024-10-31 02:50:19: [2024-10-31 02:50:19] iter = 17530, loss = 2.2586
2024-10-31 02:50:22: [2024-10-31 02:50:22] iter = 17540, loss = 2.6017
2024-10-31 02:50:25: [2024-10-31 02:50:25] iter = 17550, loss = 2.0977
2024-10-31 02:50:28: [2024-10-31 02:50:28] iter = 17560, loss = 1.9691
2024-10-31 02:50:30: [2024-10-31 02:50:30] iter = 17570, loss = 2.2159
2024-10-31 02:50:33: [2024-10-31 02:50:33] iter = 17580, loss = 2.0059
2024-10-31 02:50:35: [2024-10-31 02:50:35] iter = 17590, loss = 4.7753
2024-10-31 02:50:38: [2024-10-31 02:50:38] iter = 17600, loss = 2.5352
2024-10-31 02:50:41: [2024-10-31 02:50:41] iter = 17610, loss = 2.6400
2024-10-31 02:50:44: [2024-10-31 02:50:44] iter = 17620, loss = 2.4190
2024-10-31 02:50:46: [2024-10-31 02:50:46] iter = 17630, loss = 3.6716
2024-10-31 02:50:49: [2024-10-31 02:50:49] iter = 17640, loss = 1.8006
2024-10-31 02:50:52: [2024-10-31 02:50:52] iter = 17650, loss = 2.3161
2024-10-31 02:50:55: [2024-10-31 02:50:55] iter = 17660, loss = 2.4125
2024-10-31 02:50:58: [2024-10-31 02:50:58] iter = 17670, loss = 2.0568
2024-10-31 02:51:00: [2024-10-31 02:51:00] iter = 17680, loss = 2.0148
2024-10-31 02:51:03: [2024-10-31 02:51:03] iter = 17690, loss = 4.6314
2024-10-31 02:51:06: [2024-10-31 02:51:06] iter = 17700, loss = 2.1749
2024-10-31 02:51:08: [2024-10-31 02:51:08] iter = 17710, loss = 1.8770
2024-10-31 02:51:11: [2024-10-31 02:51:11] iter = 17720, loss = 1.8781
2024-10-31 02:51:13: [2024-10-31 02:51:13] iter = 17730, loss = 1.8768
2024-10-31 02:51:16: [2024-10-31 02:51:16] iter = 17740, loss = 2.2143
2024-10-31 02:51:19: [2024-10-31 02:51:19] iter = 17750, loss = 1.9916
2024-10-31 02:51:21: [2024-10-31 02:51:21] iter = 17760, loss = 1.6233
2024-10-31 02:51:24: [2024-10-31 02:51:24] iter = 17770, loss = 1.9094
2024-10-31 02:51:27: [2024-10-31 02:51:27] iter = 17780, loss = 2.8578
2024-10-31 02:51:29: [2024-10-31 02:51:29] iter = 17790, loss = 2.1751
2024-10-31 02:51:32: [2024-10-31 02:51:32] iter = 17800, loss = 2.0700
2024-10-31 02:51:35: [2024-10-31 02:51:35] iter = 17810, loss = 3.6231
2024-10-31 02:51:38: [2024-10-31 02:51:38] iter = 17820, loss = 2.7179
2024-10-31 02:51:41: [2024-10-31 02:51:41] iter = 17830, loss = 2.7842
2024-10-31 02:51:44: [2024-10-31 02:51:44] iter = 17840, loss = 2.1878
2024-10-31 02:51:47: [2024-10-31 02:51:47] iter = 17850, loss = 2.2561
2024-10-31 02:51:49: [2024-10-31 02:51:49] iter = 17860, loss = 2.7453
2024-10-31 02:51:52: [2024-10-31 02:51:52] iter = 17870, loss = 1.7393
2024-10-31 02:51:55: [2024-10-31 02:51:55] iter = 17880, loss = 2.1527
2024-10-31 02:51:58: [2024-10-31 02:51:58] iter = 17890, loss = 1.9311
2024-10-31 02:52:01: [2024-10-31 02:52:01] iter = 17900, loss = 2.2870
2024-10-31 02:52:04: [2024-10-31 02:52:04] iter = 17910, loss = 2.4412
2024-10-31 02:52:06: [2024-10-31 02:52:06] iter = 17920, loss = 3.7027
2024-10-31 02:52:09: [2024-10-31 02:52:09] iter = 17930, loss = 1.9723
2024-10-31 02:52:11: [2024-10-31 02:52:11] iter = 17940, loss = 2.4808
2024-10-31 02:52:14: [2024-10-31 02:52:14] iter = 17950, loss = 2.3312
2024-10-31 02:52:16: [2024-10-31 02:52:16] iter = 17960, loss = 1.9209
2024-10-31 02:52:18: [2024-10-31 02:52:18] iter = 17970, loss = 2.1421
2024-10-31 02:52:21: [2024-10-31 02:52:21] iter = 17980, loss = 2.6580
2024-10-31 02:52:24: [2024-10-31 02:52:24] iter = 17990, loss = 1.6471
2024-10-31 02:52:26: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 18000
2024-10-31 02:52:26: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 02:52:26: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 46683}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:54:13: Evaluate 5 random ConvNet, ACCmean = 0.7892 ACCstd = 0.0016
-------------------------
2024-10-31 02:54:13: Evaluate 5 random ConvNet, SENmean = 0.7866 SENstd = 0.0019
-------------------------
2024-10-31 02:54:13: Evaluate 5 random ConvNet, SPEmean = 0.9789 SPEstd = 0.0002
-------------------------
2024-10-31 02:54:13: Evaluate 5 random ConvNet, F!mean = 0.7737 F!std = 0.0020
-------------------------
2024-10-31 02:54:13: Evaluate 5 random ConvNet, mean = 0.7892 std = 0.0016
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 02:54:14: [2024-10-31 02:54:14] iter = 18000, loss = 6.9309
2024-10-31 02:54:17: [2024-10-31 02:54:17] iter = 18010, loss = 2.6555
2024-10-31 02:54:20: [2024-10-31 02:54:20] iter = 18020, loss = 2.0173
2024-10-31 02:54:22: [2024-10-31 02:54:22] iter = 18030, loss = 2.8651
2024-10-31 02:54:25: [2024-10-31 02:54:25] iter = 18040, loss = 2.1871
2024-10-31 02:54:29: [2024-10-31 02:54:29] iter = 18050, loss = 2.1802
2024-10-31 02:54:32: [2024-10-31 02:54:32] iter = 18060, loss = 2.7472
2024-10-31 02:54:35: [2024-10-31 02:54:35] iter = 18070, loss = 1.7323
2024-10-31 02:54:37: [2024-10-31 02:54:37] iter = 18080, loss = 2.1620
2024-10-31 02:54:40: [2024-10-31 02:54:40] iter = 18090, loss = 2.5166
2024-10-31 02:54:43: [2024-10-31 02:54:43] iter = 18100, loss = 1.7195
2024-10-31 02:54:46: [2024-10-31 02:54:46] iter = 18110, loss = 2.5670
2024-10-31 02:54:49: [2024-10-31 02:54:49] iter = 18120, loss = 2.6766
2024-10-31 02:54:51: [2024-10-31 02:54:51] iter = 18130, loss = 2.6789
2024-10-31 02:54:54: [2024-10-31 02:54:54] iter = 18140, loss = 2.7631
2024-10-31 02:54:57: [2024-10-31 02:54:57] iter = 18150, loss = 1.9913
2024-10-31 02:54:59: [2024-10-31 02:54:59] iter = 18160, loss = 2.4716
2024-10-31 02:55:02: [2024-10-31 02:55:02] iter = 18170, loss = 1.9801
2024-10-31 02:55:05: [2024-10-31 02:55:05] iter = 18180, loss = 1.8393
2024-10-31 02:55:08: [2024-10-31 02:55:08] iter = 18190, loss = 2.4850
2024-10-31 02:55:10: [2024-10-31 02:55:10] iter = 18200, loss = 2.4070
2024-10-31 02:55:14: [2024-10-31 02:55:14] iter = 18210, loss = 2.2515
2024-10-31 02:55:17: [2024-10-31 02:55:17] iter = 18220, loss = 3.1980
2024-10-31 02:55:19: [2024-10-31 02:55:19] iter = 18230, loss = 3.4547
2024-10-31 02:55:22: [2024-10-31 02:55:22] iter = 18240, loss = 3.2822
2024-10-31 02:55:25: [2024-10-31 02:55:25] iter = 18250, loss = 2.6790
2024-10-31 02:55:27: [2024-10-31 02:55:27] iter = 18260, loss = 1.9788
2024-10-31 02:55:30: [2024-10-31 02:55:30] iter = 18270, loss = 1.9560
2024-10-31 02:55:32: [2024-10-31 02:55:32] iter = 18280, loss = 2.0340
2024-10-31 02:55:35: [2024-10-31 02:55:35] iter = 18290, loss = 3.1747
2024-10-31 02:55:38: [2024-10-31 02:55:38] iter = 18300, loss = 2.5346
2024-10-31 02:55:40: [2024-10-31 02:55:40] iter = 18310, loss = 2.1774
2024-10-31 02:55:42: [2024-10-31 02:55:42] iter = 18320, loss = 2.0679
2024-10-31 02:55:45: [2024-10-31 02:55:45] iter = 18330, loss = 1.8490
2024-10-31 02:55:47: [2024-10-31 02:55:47] iter = 18340, loss = 2.4703
2024-10-31 02:55:49: [2024-10-31 02:55:49] iter = 18350, loss = 1.9810
2024-10-31 02:55:53: [2024-10-31 02:55:53] iter = 18360, loss = 1.7229
2024-10-31 02:55:55: [2024-10-31 02:55:55] iter = 18370, loss = 2.0242
2024-10-31 02:55:58: [2024-10-31 02:55:58] iter = 18380, loss = 1.9007
2024-10-31 02:55:59: [2024-10-31 02:55:59] iter = 18390, loss = 2.5245
2024-10-31 02:56:01: [2024-10-31 02:56:01] iter = 18400, loss = 1.7881
2024-10-31 02:56:03: [2024-10-31 02:56:03] iter = 18410, loss = 2.2831
2024-10-31 02:56:04: [2024-10-31 02:56:04] iter = 18420, loss = 1.8876
2024-10-31 02:56:06: [2024-10-31 02:56:06] iter = 18430, loss = 1.9869
2024-10-31 02:56:08: [2024-10-31 02:56:08] iter = 18440, loss = 1.8753
2024-10-31 02:56:10: [2024-10-31 02:56:10] iter = 18450, loss = 2.1216
2024-10-31 02:56:12: [2024-10-31 02:56:12] iter = 18460, loss = 2.1757
2024-10-31 02:56:14: [2024-10-31 02:56:14] iter = 18470, loss = 1.7045
2024-10-31 02:56:16: [2024-10-31 02:56:16] iter = 18480, loss = 2.1069
2024-10-31 02:56:18: [2024-10-31 02:56:18] iter = 18490, loss = 2.2354
2024-10-31 02:56:20: [2024-10-31 02:56:20] iter = 18500, loss = 1.7716
2024-10-31 02:56:22: [2024-10-31 02:56:22] iter = 18510, loss = 1.8640
2024-10-31 02:56:24: [2024-10-31 02:56:24] iter = 18520, loss = 2.2465
2024-10-31 02:56:26: [2024-10-31 02:56:26] iter = 18530, loss = 2.0856
2024-10-31 02:56:28: [2024-10-31 02:56:28] iter = 18540, loss = 2.1274
2024-10-31 02:56:30: [2024-10-31 02:56:30] iter = 18550, loss = 2.1457
2024-10-31 02:56:32: [2024-10-31 02:56:32] iter = 18560, loss = 1.8429
2024-10-31 02:56:34: [2024-10-31 02:56:34] iter = 18570, loss = 2.3131
2024-10-31 02:56:36: [2024-10-31 02:56:36] iter = 18580, loss = 2.0065
2024-10-31 02:56:38: [2024-10-31 02:56:38] iter = 18590, loss = 1.9860
2024-10-31 02:56:40: [2024-10-31 02:56:39] iter = 18600, loss = 1.8317
2024-10-31 02:56:41: [2024-10-31 02:56:41] iter = 18610, loss = 1.9091
2024-10-31 02:56:44: [2024-10-31 02:56:44] iter = 18620, loss = 2.0439
2024-10-31 02:56:46: [2024-10-31 02:56:46] iter = 18630, loss = 5.0087
2024-10-31 02:56:48: [2024-10-31 02:56:48] iter = 18640, loss = 1.8816
2024-10-31 02:56:50: [2024-10-31 02:56:50] iter = 18650, loss = 2.4828
2024-10-31 02:56:53: [2024-10-31 02:56:53] iter = 18660, loss = 2.0555
2024-10-31 02:56:55: [2024-10-31 02:56:55] iter = 18670, loss = 2.0384
2024-10-31 02:56:57: [2024-10-31 02:56:57] iter = 18680, loss = 2.1172
2024-10-31 02:56:59: [2024-10-31 02:56:59] iter = 18690, loss = 2.3923
2024-10-31 02:57:02: [2024-10-31 02:57:02] iter = 18700, loss = 2.0043
2024-10-31 02:57:04: [2024-10-31 02:57:04] iter = 18710, loss = 2.2686
2024-10-31 02:57:06: [2024-10-31 02:57:06] iter = 18720, loss = 3.2853
2024-10-31 02:57:08: [2024-10-31 02:57:08] iter = 18730, loss = 2.1242
2024-10-31 02:57:10: [2024-10-31 02:57:10] iter = 18740, loss = 2.4461
2024-10-31 02:57:13: [2024-10-31 02:57:13] iter = 18750, loss = 3.4109
2024-10-31 02:57:15: [2024-10-31 02:57:15] iter = 18760, loss = 1.7412
2024-10-31 02:57:17: [2024-10-31 02:57:17] iter = 18770, loss = 2.0319
2024-10-31 02:57:19: [2024-10-31 02:57:19] iter = 18780, loss = 3.8445
2024-10-31 02:57:21: [2024-10-31 02:57:21] iter = 18790, loss = 2.2087
2024-10-31 02:57:23: [2024-10-31 02:57:23] iter = 18800, loss = 2.0638
2024-10-31 02:57:25: [2024-10-31 02:57:25] iter = 18810, loss = 1.9221
2024-10-31 02:57:27: [2024-10-31 02:57:27] iter = 18820, loss = 2.5689
2024-10-31 02:57:29: [2024-10-31 02:57:29] iter = 18830, loss = 2.1316
2024-10-31 02:57:31: [2024-10-31 02:57:31] iter = 18840, loss = 2.1792
2024-10-31 02:57:33: [2024-10-31 02:57:33] iter = 18850, loss = 3.9673
2024-10-31 02:57:35: [2024-10-31 02:57:35] iter = 18860, loss = 2.1930
2024-10-31 02:57:37: [2024-10-31 02:57:37] iter = 18870, loss = 1.9244
2024-10-31 02:57:39: [2024-10-31 02:57:39] iter = 18880, loss = 2.5417
2024-10-31 02:57:41: [2024-10-31 02:57:41] iter = 18890, loss = 2.4570
2024-10-31 02:57:43: [2024-10-31 02:57:43] iter = 18900, loss = 2.1747
2024-10-31 02:57:45: [2024-10-31 02:57:45] iter = 18910, loss = 2.2415
2024-10-31 02:57:47: [2024-10-31 02:57:47] iter = 18920, loss = 1.9574
2024-10-31 02:57:50: [2024-10-31 02:57:50] iter = 18930, loss = 2.0147
2024-10-31 02:57:52: [2024-10-31 02:57:52] iter = 18940, loss = 4.5915
2024-10-31 02:57:55: [2024-10-31 02:57:55] iter = 18950, loss = 3.0534
2024-10-31 02:57:57: [2024-10-31 02:57:57] iter = 18960, loss = 3.5406
2024-10-31 02:57:59: [2024-10-31 02:57:59] iter = 18970, loss = 2.0495
2024-10-31 02:58:02: [2024-10-31 02:58:02] iter = 18980, loss = 1.9460
2024-10-31 02:58:04: [2024-10-31 02:58:04] iter = 18990, loss = 1.7349
2024-10-31 02:58:07: [2024-10-31 02:58:07] iter = 19000, loss = 2.0726
2024-10-31 02:58:09: [2024-10-31 02:58:09] iter = 19010, loss = 3.1929
2024-10-31 02:58:12: [2024-10-31 02:58:12] iter = 19020, loss = 1.8434
2024-10-31 02:58:14: [2024-10-31 02:58:14] iter = 19030, loss = 3.7088
2024-10-31 02:58:16: [2024-10-31 02:58:16] iter = 19040, loss = 2.2827
2024-10-31 02:58:19: [2024-10-31 02:58:19] iter = 19050, loss = 1.8990
2024-10-31 02:58:21: [2024-10-31 02:58:21] iter = 19060, loss = 6.4049
2024-10-31 02:58:23: [2024-10-31 02:58:23] iter = 19070, loss = 1.8331
2024-10-31 02:58:25: [2024-10-31 02:58:25] iter = 19080, loss = 1.8796
2024-10-31 02:58:28: [2024-10-31 02:58:28] iter = 19090, loss = 2.2431
2024-10-31 02:58:30: [2024-10-31 02:58:30] iter = 19100, loss = 2.4022
2024-10-31 02:58:32: [2024-10-31 02:58:32] iter = 19110, loss = 2.2762
2024-10-31 02:58:34: [2024-10-31 02:58:34] iter = 19120, loss = 2.0897
2024-10-31 02:58:37: [2024-10-31 02:58:37] iter = 19130, loss = 2.5864
2024-10-31 02:58:39: [2024-10-31 02:58:39] iter = 19140, loss = 2.1837
2024-10-31 02:58:42: [2024-10-31 02:58:42] iter = 19150, loss = 2.4837
2024-10-31 02:58:44: [2024-10-31 02:58:44] iter = 19160, loss = 1.9245
2024-10-31 02:58:47: [2024-10-31 02:58:47] iter = 19170, loss = 1.6847
2024-10-31 02:58:49: [2024-10-31 02:58:49] iter = 19180, loss = 2.9825
2024-10-31 02:58:51: [2024-10-31 02:58:51] iter = 19190, loss = 3.0104
2024-10-31 02:58:53: [2024-10-31 02:58:53] iter = 19200, loss = 2.3245
2024-10-31 02:58:56: [2024-10-31 02:58:56] iter = 19210, loss = 2.4315
2024-10-31 02:58:59: [2024-10-31 02:58:59] iter = 19220, loss = 2.0799
2024-10-31 02:59:01: [2024-10-31 02:59:01] iter = 19230, loss = 2.4114
2024-10-31 02:59:04: [2024-10-31 02:59:04] iter = 19240, loss = 2.6800
2024-10-31 02:59:06: [2024-10-31 02:59:06] iter = 19250, loss = 2.5673
2024-10-31 02:59:09: [2024-10-31 02:59:09] iter = 19260, loss = 3.2792
2024-10-31 02:59:11: [2024-10-31 02:59:11] iter = 19270, loss = 1.9255
2024-10-31 02:59:13: [2024-10-31 02:59:13] iter = 19280, loss = 2.0017
2024-10-31 02:59:15: [2024-10-31 02:59:15] iter = 19290, loss = 4.6605
2024-10-31 02:59:18: [2024-10-31 02:59:18] iter = 19300, loss = 2.3769
2024-10-31 02:59:20: [2024-10-31 02:59:20] iter = 19310, loss = 2.1802
2024-10-31 02:59:22: [2024-10-31 02:59:22] iter = 19320, loss = 1.9348
2024-10-31 02:59:24: [2024-10-31 02:59:24] iter = 19330, loss = 2.1946
2024-10-31 02:59:27: [2024-10-31 02:59:27] iter = 19340, loss = 2.2598
2024-10-31 02:59:30: [2024-10-31 02:59:30] iter = 19350, loss = 2.3625
2024-10-31 02:59:32: [2024-10-31 02:59:32] iter = 19360, loss = 3.0065
2024-10-31 02:59:35: [2024-10-31 02:59:35] iter = 19370, loss = 2.2727
2024-10-31 02:59:37: [2024-10-31 02:59:37] iter = 19380, loss = 3.7689
2024-10-31 02:59:40: [2024-10-31 02:59:40] iter = 19390, loss = 2.3188
2024-10-31 02:59:43: [2024-10-31 02:59:43] iter = 19400, loss = 5.3563
2024-10-31 02:59:45: [2024-10-31 02:59:45] iter = 19410, loss = 2.5986
2024-10-31 02:59:47: [2024-10-31 02:59:47] iter = 19420, loss = 2.7259
2024-10-31 02:59:50: [2024-10-31 02:59:50] iter = 19430, loss = 1.7374
2024-10-31 02:59:52: [2024-10-31 02:59:52] iter = 19440, loss = 1.8912
2024-10-31 02:59:54: [2024-10-31 02:59:54] iter = 19450, loss = 2.5970
2024-10-31 02:59:57: [2024-10-31 02:59:57] iter = 19460, loss = 2.1357
2024-10-31 02:59:59: [2024-10-31 02:59:59] iter = 19470, loss = 2.2739
2024-10-31 03:00:01: [2024-10-31 03:00:01] iter = 19480, loss = 2.5306
2024-10-31 03:00:03: [2024-10-31 03:00:03] iter = 19490, loss = 2.5770
2024-10-31 03:00:05: [2024-10-31 03:00:05] iter = 19500, loss = 2.9607
2024-10-31 03:00:07: [2024-10-31 03:00:07] iter = 19510, loss = 2.3422
2024-10-31 03:00:10: [2024-10-31 03:00:10] iter = 19520, loss = 2.6622
2024-10-31 03:00:12: [2024-10-31 03:00:12] iter = 19530, loss = 2.1916
2024-10-31 03:00:15: [2024-10-31 03:00:15] iter = 19540, loss = 3.8630
2024-10-31 03:00:17: [2024-10-31 03:00:17] iter = 19550, loss = 4.6627
2024-10-31 03:00:19: [2024-10-31 03:00:19] iter = 19560, loss = 2.3310
2024-10-31 03:00:21: [2024-10-31 03:00:21] iter = 19570, loss = 2.0093
2024-10-31 03:00:23: [2024-10-31 03:00:23] iter = 19580, loss = 2.1930
2024-10-31 03:00:25: [2024-10-31 03:00:25] iter = 19590, loss = 2.6138
2024-10-31 03:00:27: [2024-10-31 03:00:27] iter = 19600, loss = 2.8005
2024-10-31 03:00:30: [2024-10-31 03:00:30] iter = 19610, loss = 2.2293
2024-10-31 03:00:32: [2024-10-31 03:00:32] iter = 19620, loss = 2.2355
2024-10-31 03:00:34: [2024-10-31 03:00:34] iter = 19630, loss = 2.8113
2024-10-31 03:00:36: [2024-10-31 03:00:36] iter = 19640, loss = 2.6090
2024-10-31 03:00:38: [2024-10-31 03:00:38] iter = 19650, loss = 1.9073
2024-10-31 03:00:40: [2024-10-31 03:00:40] iter = 19660, loss = 1.6999
2024-10-31 03:00:42: [2024-10-31 03:00:42] iter = 19670, loss = 2.3027
2024-10-31 03:00:44: [2024-10-31 03:00:44] iter = 19680, loss = 2.1535
2024-10-31 03:00:46: [2024-10-31 03:00:46] iter = 19690, loss = 2.1898
2024-10-31 03:00:48: [2024-10-31 03:00:48] iter = 19700, loss = 2.4595
2024-10-31 03:00:50: [2024-10-31 03:00:50] iter = 19710, loss = 2.0644
2024-10-31 03:00:52: [2024-10-31 03:00:52] iter = 19720, loss = 2.3996
2024-10-31 03:00:54: [2024-10-31 03:00:54] iter = 19730, loss = 2.4329
2024-10-31 03:00:56: [2024-10-31 03:00:56] iter = 19740, loss = 2.2512
2024-10-31 03:00:59: [2024-10-31 03:00:59] iter = 19750, loss = 2.7338
2024-10-31 03:01:01: [2024-10-31 03:01:01] iter = 19760, loss = 3.0089
2024-10-31 03:01:04: [2024-10-31 03:01:04] iter = 19770, loss = 2.5354
2024-10-31 03:01:06: [2024-10-31 03:01:06] iter = 19780, loss = 2.1266
2024-10-31 03:01:08: [2024-10-31 03:01:08] iter = 19790, loss = 2.6615
2024-10-31 03:01:10: [2024-10-31 03:01:10] iter = 19800, loss = 2.0739
2024-10-31 03:01:12: [2024-10-31 03:01:12] iter = 19810, loss = 2.6195
2024-10-31 03:01:15: [2024-10-31 03:01:15] iter = 19820, loss = 1.9853
2024-10-31 03:01:17: [2024-10-31 03:01:17] iter = 19830, loss = 2.9318
2024-10-31 03:01:19: [2024-10-31 03:01:19] iter = 19840, loss = 4.1846
2024-10-31 03:01:21: [2024-10-31 03:01:21] iter = 19850, loss = 1.8399
2024-10-31 03:01:24: [2024-10-31 03:01:24] iter = 19860, loss = 1.9778
2024-10-31 03:01:26: [2024-10-31 03:01:26] iter = 19870, loss = 2.1068
2024-10-31 03:01:28: [2024-10-31 03:01:28] iter = 19880, loss = 2.1298
2024-10-31 03:01:30: [2024-10-31 03:01:30] iter = 19890, loss = 2.2888
2024-10-31 03:01:33: [2024-10-31 03:01:33] iter = 19900, loss = 2.9328
2024-10-31 03:01:36: [2024-10-31 03:01:36] iter = 19910, loss = 2.8563
2024-10-31 03:01:38: [2024-10-31 03:01:38] iter = 19920, loss = 5.4360
2024-10-31 03:01:40: [2024-10-31 03:01:40] iter = 19930, loss = 3.7338
2024-10-31 03:01:42: [2024-10-31 03:01:42] iter = 19940, loss = 2.2790
2024-10-31 03:01:45: [2024-10-31 03:01:45] iter = 19950, loss = 4.0319
2024-10-31 03:01:47: [2024-10-31 03:01:47] iter = 19960, loss = 1.8744
2024-10-31 03:01:49: [2024-10-31 03:01:49] iter = 19970, loss = 2.1651
2024-10-31 03:01:51: [2024-10-31 03:01:51] iter = 19980, loss = 1.9048
2024-10-31 03:01:53: [2024-10-31 03:01:53] iter = 19990, loss = 2.7809
2024-10-31 03:01:55: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 20000
2024-10-31 03:01:55: DSA augmentation strategy: 
color_crop_cutout_flip_scale_rotate
2024-10-31 03:01:55: DSA augmentation parameters: 
{'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'Siamese': True, 'latestseed': 15852}
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 03:03:09: Evaluate 5 random ConvNet, ACCmean = 0.7693 ACCstd = 0.0035
-------------------------
2024-10-31 03:03:09: Evaluate 5 random ConvNet, SENmean = 0.7670 SENstd = 0.0029
-------------------------
2024-10-31 03:03:09: Evaluate 5 random ConvNet, SPEmean = 0.9768 SPEstd = 0.0003
-------------------------
2024-10-31 03:03:09: Evaluate 5 random ConvNet, F!mean = 0.7523 F!std = 0.0041
-------------------------
2024-10-31 03:03:09: Evaluate 5 random ConvNet, mean = 0.7693 std = 0.0035
-------------------------
/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/nn/functional.py:4289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
2024-10-31 03:03:09: [2024-10-31 03:03:09] iter = 20000, loss = 2.5947
2024-10-31 03:03:09: 
==================== Final Results ====================

2024-10-31 03:03:09: Run 5 experiments, train on ConvNet, evaluate 25 random ConvNet, mean  = 77.79%  std = 0.73%

[2024-10-31 01:28:50] Evaluate_00: epoch = 1000 train time = 22 s train loss = 0.005088 train acc = 1.0000, test acc = 0.7778, test_sen =0.7795, test_spe =0.9778, test_f1 =0.7646
[2024-10-31 01:29:15] Evaluate_01: epoch = 1000 train time = 23 s train loss = 0.022473 train acc = 1.0000, test acc = 0.7795, test_sen =0.7807, test_spe =0.9780, test_f1 =0.7655
[2024-10-31 01:29:40] Evaluate_02: epoch = 1000 train time = 23 s train loss = 0.024454 train acc = 0.9909, test acc = 0.7874, test_sen =0.7909, test_spe =0.9787, test_f1 =0.7752
[2024-10-31 01:30:06] Evaluate_03: epoch = 1000 train time = 23 s train loss = 0.022878 train acc = 1.0000, test acc = 0.7743, test_sen =0.7766, test_spe =0.9774, test_f1 =0.7614
[2024-10-31 01:30:30] Evaluate_04: epoch = 1000 train time = 22 s train loss = 0.008891 train acc = 1.0000, test acc = 0.7854, test_sen =0.7864, test_spe =0.9785, test_f1 =0.7724
[2024-10-31 01:42:02] Evaluate_00: epoch = 1000 train time = 27 s train loss = 0.021997 train acc = 1.0000, test acc = 0.7780, test_sen =0.7722, test_spe =0.9777, test_f1 =0.7615
[2024-10-31 01:42:28] Evaluate_01: epoch = 1000 train time = 24 s train loss = 0.008318 train acc = 1.0000, test acc = 0.7776, test_sen =0.7728, test_spe =0.9777, test_f1 =0.7588
[2024-10-31 01:42:51] Evaluate_02: epoch = 1000 train time = 21 s train loss = 0.003920 train acc = 1.0000, test acc = 0.7794, test_sen =0.7755, test_spe =0.9780, test_f1 =0.7610
[2024-10-31 01:43:18] Evaluate_03: epoch = 1000 train time = 23 s train loss = 0.003192 train acc = 1.0000, test acc = 0.7810, test_sen =0.7786, test_spe =0.9781, test_f1 =0.7661
[2024-10-31 01:43:43] Evaluate_04: epoch = 1000 train time = 24 s train loss = 0.004826 train acc = 1.0000, test acc = 0.7800, test_sen =0.7758, test_spe =0.9779, test_f1 =0.7644
[2024-10-31 01:54:42] Evaluate_00: epoch = 1000 train time = 24 s train loss = 0.004395 train acc = 1.0000, test acc = 0.7755, test_sen =0.7714, test_spe =0.9774, test_f1 =0.7595
[2024-10-31 01:55:06] Evaluate_01: epoch = 1000 train time = 22 s train loss = 0.002995 train acc = 1.0000, test acc = 0.7750, test_sen =0.7693, test_spe =0.9773, test_f1 =0.7604
[2024-10-31 01:55:34] Evaluate_02: epoch = 1000 train time = 25 s train loss = 0.016443 train acc = 1.0000, test acc = 0.7741, test_sen =0.7718, test_spe =0.9772, test_f1 =0.7616
[2024-10-31 01:55:58] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.006780 train acc = 1.0000, test acc = 0.7706, test_sen =0.7713, test_spe =0.9770, test_f1 =0.7570
[2024-10-31 01:56:25] Evaluate_04: epoch = 1000 train time = 25 s train loss = 0.002777 train acc = 1.0000, test acc = 0.7728, test_sen =0.7667, test_spe =0.9771, test_f1 =0.7576
[2024-10-31 02:07:36] Evaluate_00: epoch = 1000 train time = 25 s train loss = 0.034221 train acc = 1.0000, test acc = 0.7786, test_sen =0.7739, test_spe =0.9778, test_f1 =0.7645
[2024-10-31 02:08:03] Evaluate_01: epoch = 1000 train time = 25 s train loss = 0.004894 train acc = 1.0000, test acc = 0.7890, test_sen =0.7868, test_spe =0.9789, test_f1 =0.7731
[2024-10-31 02:08:29] Evaluate_02: epoch = 1000 train time = 23 s train loss = 0.003825 train acc = 1.0000, test acc = 0.7953, test_sen =0.7892, test_spe =0.9794, test_f1 =0.7784
[2024-10-31 02:08:55] Evaluate_03: epoch = 1000 train time = 23 s train loss = 0.024734 train acc = 1.0000, test acc = 0.7941, test_sen =0.7884, test_spe =0.9794, test_f1 =0.7788
[2024-10-31 02:09:23] Evaluate_04: epoch = 1000 train time = 26 s train loss = 0.030645 train acc = 0.9909, test acc = 0.7959, test_sen =0.7913, test_spe =0.9795, test_f1 =0.7815
[2024-10-31 02:20:37] Evaluate_00: epoch = 1000 train time = 20 s train loss = 0.006224 train acc = 1.0000, test acc = 0.7982, test_sen =0.7947, test_spe =0.9798, test_f1 =0.7828
[2024-10-31 02:21:00] Evaluate_01: epoch = 1000 train time = 20 s train loss = 0.018276 train acc = 1.0000, test acc = 0.7964, test_sen =0.7927, test_spe =0.9796, test_f1 =0.7816
[2024-10-31 02:21:21] Evaluate_02: epoch = 1000 train time = 20 s train loss = 0.009724 train acc = 1.0000, test acc = 0.7998, test_sen =0.7963, test_spe =0.9799, test_f1 =0.7834
[2024-10-31 02:21:40] Evaluate_03: epoch = 1000 train time = 17 s train loss = 0.006153 train acc = 1.0000, test acc = 0.8026, test_sen =0.8004, test_spe =0.9802, test_f1 =0.7875
[2024-10-31 02:22:00] Evaluate_04: epoch = 1000 train time = 18 s train loss = 0.002455 train acc = 1.0000, test acc = 0.7952, test_sen =0.7939, test_spe =0.9795, test_f1 =0.7798
[2024-10-31 02:31:05] Evaluate_00: epoch = 1000 train time = 17 s train loss = 0.003179 train acc = 1.0000, test acc = 0.7770, test_sen =0.7721, test_spe =0.9777, test_f1 =0.7596
[2024-10-31 02:31:23] Evaluate_01: epoch = 1000 train time = 17 s train loss = 0.014140 train acc = 1.0000, test acc = 0.7815, test_sen =0.7796, test_spe =0.9782, test_f1 =0.7648
[2024-10-31 02:31:43] Evaluate_02: epoch = 1000 train time = 18 s train loss = 0.026595 train acc = 1.0000, test acc = 0.7797, test_sen =0.7752, test_spe =0.9780, test_f1 =0.7646
[2024-10-31 02:32:02] Evaluate_03: epoch = 1000 train time = 17 s train loss = 0.005998 train acc = 1.0000, test acc = 0.7773, test_sen =0.7763, test_spe =0.9778, test_f1 =0.7618
[2024-10-31 02:32:21] Evaluate_04: epoch = 1000 train time = 17 s train loss = 0.002611 train acc = 1.0000, test acc = 0.7782, test_sen =0.7757, test_spe =0.9779, test_f1 =0.7617
[2024-10-31 02:42:11] Evaluate_00: epoch = 1000 train time = 20 s train loss = 0.003732 train acc = 1.0000, test acc = 0.7726, test_sen =0.7645, test_spe =0.9772, test_f1 =0.7519
[2024-10-31 02:42:31] Evaluate_01: epoch = 1000 train time = 18 s train loss = 0.035071 train acc = 0.9909, test acc = 0.7760, test_sen =0.7706, test_spe =0.9776, test_f1 =0.7578
[2024-10-31 02:42:53] Evaluate_02: epoch = 1000 train time = 20 s train loss = 0.005073 train acc = 1.0000, test acc = 0.7673, test_sen =0.7654, test_spe =0.9767, test_f1 =0.7506
[2024-10-31 02:43:14] Evaluate_03: epoch = 1000 train time = 19 s train loss = 0.011712 train acc = 1.0000, test acc = 0.7788, test_sen =0.7718, test_spe =0.9779, test_f1 =0.7608
[2024-10-31 02:43:34] Evaluate_04: epoch = 1000 train time = 18 s train loss = 0.003940 train acc = 1.0000, test acc = 0.7755, test_sen =0.7683, test_spe =0.9774, test_f1 =0.7587
[2024-10-31 02:52:46] Evaluate_00: epoch = 1000 train time = 18 s train loss = 0.003409 train acc = 1.0000, test acc = 0.7893, test_sen =0.7874, test_spe =0.9789, test_f1 =0.7734
[2024-10-31 02:53:07] Evaluate_01: epoch = 1000 train time = 19 s train loss = 0.006526 train acc = 1.0000, test acc = 0.7876, test_sen =0.7854, test_spe =0.9788, test_f1 =0.7718
[2024-10-31 02:53:29] Evaluate_02: epoch = 1000 train time = 20 s train loss = 0.008784 train acc = 1.0000, test acc = 0.7921, test_sen =0.7898, test_spe =0.9792, test_f1 =0.7766
[2024-10-31 02:53:53] Evaluate_03: epoch = 1000 train time = 22 s train loss = 0.004470 train acc = 1.0000, test acc = 0.7878, test_sen =0.7841, test_spe =0.9788, test_f1 =0.7715
[2024-10-31 02:54:13] Evaluate_04: epoch = 1000 train time = 19 s train loss = 0.003513 train acc = 1.0000, test acc = 0.7890, test_sen =0.7863, test_spe =0.9789, test_f1 =0.7751
[2024-10-31 03:02:10] Evaluate_00: epoch = 1000 train time = 13 s train loss = 0.019109 train acc = 1.0000, test acc = 0.7703, test_sen =0.7683, test_spe =0.9769, test_f1 =0.7549
[2024-10-31 03:02:25] Evaluate_01: epoch = 1000 train time = 14 s train loss = 0.024332 train acc = 1.0000, test acc = 0.7717, test_sen =0.7691, test_spe =0.9771, test_f1 =0.7543
[2024-10-31 03:02:38] Evaluate_02: epoch = 1000 train time = 12 s train loss = 0.003148 train acc = 1.0000, test acc = 0.7737, test_sen =0.7703, test_spe =0.9772, test_f1 =0.7570
[2024-10-31 03:02:52] Evaluate_03: epoch = 1000 train time = 12 s train loss = 0.003138 train acc = 1.0000, test acc = 0.7665, test_sen =0.7642, test_spe =0.9766, test_f1 =0.7499
[2024-10-31 03:03:09] Evaluate_04: epoch = 1000 train time = 15 s train loss = 0.003293 train acc = 1.0000, test acc = 0.7642, test_sen =0.7629, test_spe =0.9764, test_f1 =0.7456
