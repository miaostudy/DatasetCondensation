nohup: ignoring input
2024-11-21 18:29:28: eval_it_pool: [0, 500, 1000]
2024-11-21 18:29:28: 
================== Exp 0 ==================
 
2024-11-21 18:29:28: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganCMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fc239531a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganCMNIST (INFO)>}
2024-11-21 18:29:28: Evaluation model pool: ['ConvNet']
2024-11-21 18:29:30: class c = 0: 1148 real images
2024-11-21 18:29:30: class c = 1: 619 real images
2024-11-21 18:29:30: class c = 2: 595 real images
2024-11-21 18:29:30: class c = 3: 600 real images
2024-11-21 18:29:30: class c = 4: 1088 real images
2024-11-21 18:29:30: class c = 5: 1170 real images
2024-11-21 18:29:30: class c = 6: 2986 real images
2024-11-21 18:29:30: class c = 7: 1002 real images
2024-11-21 18:29:30: class c = 8: 1022 real images
2024-11-21 18:29:30: class c = 9: 1173 real images
2024-11-21 18:29:30: class c = 10: 1572 real images
2024-11-21 18:29:30: real images channel 0, mean = 0.4942, std = 0.2834
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
main_base.py:125: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975312/work/torch/csrc/utils/tensor_new.cpp:230.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 18:29:30: initialize synthetic data from random noise
2024-11-21 18:29:30: [2024-11-21 18:29:30] training begins
2024-11-21 18:29:30: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 18:29:30: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 18:29:47: Evaluate 5 random ConvNet, ACCmean = 0.0970 ACCstd = 0.0123
-------------------------
2024-11-21 18:29:47: Evaluate 5 random ConvNet, F1mean = 0.0770 F!std = 0.0062
-------------------------
2024-11-21 18:29:55: [2024-11-21 18:29:55] iter = 0000, loss = 296.9778
2024-11-21 18:31:46: [2024-11-21 18:31:46] iter = 0010, loss = 147.5263
2024-11-21 18:33:43: [2024-11-21 18:33:43] iter = 0020, loss = 86.9163
2024-11-21 18:35:40: [2024-11-21 18:35:40] iter = 0030, loss = 70.8166
2024-11-21 18:37:35: [2024-11-21 18:37:35] iter = 0040, loss = 67.5052
2024-11-21 18:39:32: [2024-11-21 18:39:32] iter = 0050, loss = 58.6787
2024-11-21 18:41:27: [2024-11-21 18:41:27] iter = 0060, loss = 53.9652
2024-11-21 18:43:23: [2024-11-21 18:43:22] iter = 0070, loss = 51.9328
2024-11-21 18:45:18: [2024-11-21 18:45:18] iter = 0080, loss = 52.2726
2024-11-21 18:47:14: [2024-11-21 18:47:14] iter = 0090, loss = 47.6037
2024-11-21 18:49:11: [2024-11-21 18:49:11] iter = 0100, loss = 45.7170
2024-11-21 18:51:06: [2024-11-21 18:51:06] iter = 0110, loss = 44.5582
2024-11-21 18:53:03: [2024-11-21 18:53:03] iter = 0120, loss = 45.6166
2024-11-21 18:55:00: [2024-11-21 18:55:00] iter = 0130, loss = 42.2760
2024-11-21 18:56:58: [2024-11-21 18:56:58] iter = 0140, loss = 39.7786
2024-11-21 18:58:56: [2024-11-21 18:58:56] iter = 0150, loss = 38.0497
2024-11-21 19:00:54: [2024-11-21 19:00:54] iter = 0160, loss = 39.0346
2024-11-21 19:02:52: [2024-11-21 19:02:52] iter = 0170, loss = 37.8220
2024-11-21 19:04:49: [2024-11-21 19:04:49] iter = 0180, loss = 36.5306
2024-11-21 19:06:45: [2024-11-21 19:06:45] iter = 0190, loss = 39.3056
2024-11-21 19:08:41: [2024-11-21 19:08:41] iter = 0200, loss = 38.3568
2024-11-21 19:10:38: [2024-11-21 19:10:38] iter = 0210, loss = 36.8946
2024-11-21 19:12:32: [2024-11-21 19:12:32] iter = 0220, loss = 35.2336
2024-11-21 19:14:30: [2024-11-21 19:14:30] iter = 0230, loss = 33.0974
2024-11-21 19:16:26: [2024-11-21 19:16:26] iter = 0240, loss = 34.6422
2024-11-21 19:18:22: [2024-11-21 19:18:22] iter = 0250, loss = 33.0432
2024-11-21 19:20:18: [2024-11-21 19:20:18] iter = 0260, loss = 33.7235
2024-11-21 19:22:14: [2024-11-21 19:22:14] iter = 0270, loss = 33.4435
2024-11-21 19:24:11: [2024-11-21 19:24:11] iter = 0280, loss = 32.8545
2024-11-21 19:26:09: [2024-11-21 19:26:09] iter = 0290, loss = 31.8489
2024-11-21 19:28:06: [2024-11-21 19:28:06] iter = 0300, loss = 32.2072
2024-11-21 19:30:03: [2024-11-21 19:30:03] iter = 0310, loss = 31.3013
2024-11-21 19:32:03: [2024-11-21 19:32:03] iter = 0320, loss = 32.7916
2024-11-21 19:34:01: [2024-11-21 19:34:01] iter = 0330, loss = 31.8842
2024-11-21 19:35:59: [2024-11-21 19:35:59] iter = 0340, loss = 33.4172
2024-11-21 19:37:57: [2024-11-21 19:37:57] iter = 0350, loss = 31.4176
2024-11-21 19:39:55: [2024-11-21 19:39:55] iter = 0360, loss = 31.0421
2024-11-21 19:41:52: [2024-11-21 19:41:52] iter = 0370, loss = 32.3324
2024-11-21 19:43:50: [2024-11-21 19:43:50] iter = 0380, loss = 30.2136
2024-11-21 19:45:48: [2024-11-21 19:45:48] iter = 0390, loss = 31.3185
2024-11-21 19:47:45: [2024-11-21 19:47:45] iter = 0400, loss = 29.9238
2024-11-21 19:49:44: [2024-11-21 19:49:44] iter = 0410, loss = 29.5629
2024-11-21 19:51:42: [2024-11-21 19:51:42] iter = 0420, loss = 30.2370
2024-11-21 19:53:41: [2024-11-21 19:53:41] iter = 0430, loss = 29.8993
2024-11-21 19:55:39: [2024-11-21 19:55:39] iter = 0440, loss = 31.2058
2024-11-21 19:57:36: [2024-11-21 19:57:36] iter = 0450, loss = 29.5845
2024-11-21 19:59:35: [2024-11-21 19:59:35] iter = 0460, loss = 32.7283
2024-11-21 20:01:34: [2024-11-21 20:01:34] iter = 0470, loss = 28.4972
2024-11-21 20:03:32: [2024-11-21 20:03:32] iter = 0480, loss = 31.7362
2024-11-21 20:05:30: [2024-11-21 20:05:30] iter = 0490, loss = 28.8275
2024-11-21 20:07:16: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-21 20:07:16: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 20:07:38: Evaluate 5 random ConvNet, ACCmean = 0.7969 ACCstd = 0.0031
-------------------------
2024-11-21 20:07:38: Evaluate 5 random ConvNet, F1mean = 0.7745 F!std = 0.0033
-------------------------
2024-11-21 20:07:50: [2024-11-21 20:07:50] iter = 0500, loss = 30.3994
2024-11-21 20:09:51: [2024-11-21 20:09:51] iter = 0510, loss = 31.6125
2024-11-21 20:11:48: [2024-11-21 20:11:48] iter = 0520, loss = 32.3872
2024-11-21 20:13:46: [2024-11-21 20:13:46] iter = 0530, loss = 29.0319
2024-11-21 20:15:43: [2024-11-21 20:15:43] iter = 0540, loss = 30.3023
2024-11-21 20:17:42: [2024-11-21 20:17:42] iter = 0550, loss = 28.8600
2024-11-21 20:19:39: [2024-11-21 20:19:39] iter = 0560, loss = 29.9291
2024-11-21 20:21:36: [2024-11-21 20:21:36] iter = 0570, loss = 29.5970
2024-11-21 20:23:33: [2024-11-21 20:23:33] iter = 0580, loss = 28.6380
2024-11-21 20:25:29: [2024-11-21 20:25:29] iter = 0590, loss = 29.1875
2024-11-21 20:27:26: [2024-11-21 20:27:26] iter = 0600, loss = 29.8589
2024-11-21 20:29:24: [2024-11-21 20:29:24] iter = 0610, loss = 30.9675
2024-11-21 20:31:20: [2024-11-21 20:31:20] iter = 0620, loss = 29.0438
2024-11-21 20:33:18: [2024-11-21 20:33:18] iter = 0630, loss = 29.8661
2024-11-21 20:35:16: [2024-11-21 20:35:16] iter = 0640, loss = 29.9534
2024-11-21 20:37:17: [2024-11-21 20:37:17] iter = 0650, loss = 29.6283
2024-11-21 20:39:14: [2024-11-21 20:39:14] iter = 0660, loss = 30.0480
2024-11-21 20:41:19: [2024-11-21 20:41:19] iter = 0670, loss = 30.4554
2024-11-21 20:43:32: [2024-11-21 20:43:32] iter = 0680, loss = 29.6058
2024-11-21 20:45:38: [2024-11-21 20:45:38] iter = 0690, loss = 28.5858
2024-11-21 20:48:17: [2024-11-21 20:48:17] iter = 0700, loss = 28.9196
2024-11-21 20:50:53: [2024-11-21 20:50:53] iter = 0710, loss = 30.5269
2024-11-21 20:53:38: [2024-11-21 20:53:38] iter = 0720, loss = 31.4573
2024-11-21 20:56:21: [2024-11-21 20:56:21] iter = 0730, loss = 30.2840
2024-11-21 20:59:05: [2024-11-21 20:59:05] iter = 0740, loss = 29.6687
2024-11-21 21:01:49: [2024-11-21 21:01:49] iter = 0750, loss = 29.1506
2024-11-21 21:04:37: [2024-11-21 21:04:37] iter = 0760, loss = 30.2316
2024-11-21 21:07:24: [2024-11-21 21:07:24] iter = 0770, loss = 28.2090
2024-11-21 21:10:09: [2024-11-21 21:10:09] iter = 0780, loss = 29.6561
2024-11-21 21:12:50: [2024-11-21 21:12:50] iter = 0790, loss = 29.6960
2024-11-21 21:15:32: [2024-11-21 21:15:32] iter = 0800, loss = 30.9087
2024-11-21 21:18:23: [2024-11-21 21:18:23] iter = 0810, loss = 29.2957
2024-11-21 21:21:08: [2024-11-21 21:21:08] iter = 0820, loss = 28.6790
2024-11-21 21:23:49: [2024-11-21 21:23:49] iter = 0830, loss = 28.6013
2024-11-21 21:26:34: [2024-11-21 21:26:34] iter = 0840, loss = 28.5156
2024-11-21 21:29:19: [2024-11-21 21:29:19] iter = 0850, loss = 28.6468
2024-11-21 21:32:05: [2024-11-21 21:32:05] iter = 0860, loss = 29.7766
2024-11-21 21:34:46: [2024-11-21 21:34:46] iter = 0870, loss = 28.3957
2024-11-21 21:37:31: [2024-11-21 21:37:31] iter = 0880, loss = 29.9647
2024-11-21 21:40:15: [2024-11-21 21:40:15] iter = 0890, loss = 29.2373
2024-11-21 21:42:58: [2024-11-21 21:42:58] iter = 0900, loss = 29.2107
2024-11-21 21:45:37: [2024-11-21 21:45:37] iter = 0910, loss = 29.9389
2024-11-21 21:48:23: [2024-11-21 21:48:23] iter = 0920, loss = 27.9213
2024-11-21 21:51:04: [2024-11-21 21:51:04] iter = 0930, loss = 28.9416
2024-11-21 21:53:39: [2024-11-21 21:53:39] iter = 0940, loss = 28.9080
2024-11-21 21:56:06: [2024-11-21 21:56:06] iter = 0950, loss = 28.0337
2024-11-21 21:58:33: [2024-11-21 21:58:33] iter = 0960, loss = 29.1360
2024-11-21 22:00:55: [2024-11-21 22:00:55] iter = 0970, loss = 29.3015
2024-11-21 22:03:16: [2024-11-21 22:03:16] iter = 0980, loss = 27.8733
2024-11-21 22:05:40: [2024-11-21 22:05:40] iter = 0990, loss = 30.3347
2024-11-21 22:07:48: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-21 22:07:48: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 22:08:23: Evaluate 5 random ConvNet, ACCmean = 0.7965 ACCstd = 0.0031
-------------------------
2024-11-21 22:08:23: Evaluate 5 random ConvNet, F1mean = 0.7759 F!std = 0.0033
-------------------------
2024-11-21 22:08:40: [2024-11-21 22:08:40] iter = 1000, loss = 28.4655
2024-11-21 22:08:40: 
================== Exp 1 ==================
 
2024-11-21 22:08:40: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganCMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fc239531a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganCMNIST (INFO)>, 'dc_aug_param': None}
2024-11-21 22:08:40: Evaluation model pool: ['ConvNet']
2024-11-21 22:08:41: class c = 0: 1148 real images
2024-11-21 22:08:41: class c = 1: 619 real images
2024-11-21 22:08:41: class c = 2: 595 real images
2024-11-21 22:08:41: class c = 3: 600 real images
2024-11-21 22:08:41: class c = 4: 1088 real images
2024-11-21 22:08:41: class c = 5: 1170 real images
2024-11-21 22:08:41: class c = 6: 2986 real images
2024-11-21 22:08:41: class c = 7: 1002 real images
2024-11-21 22:08:41: class c = 8: 1022 real images
2024-11-21 22:08:41: class c = 9: 1173 real images
2024-11-21 22:08:41: class c = 10: 1572 real images
2024-11-21 22:08:41: real images channel 0, mean = 0.4942, std = 0.2834
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 22:08:41: initialize synthetic data from random noise
2024-11-21 22:08:41: [2024-11-21 22:08:41] training begins
2024-11-21 22:08:41: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 22:08:41: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 22:09:10: Evaluate 5 random ConvNet, ACCmean = 0.1078 ACCstd = 0.0170
-------------------------
2024-11-21 22:09:10: Evaluate 5 random ConvNet, F1mean = 0.0938 F!std = 0.0147
-------------------------
2024-11-21 22:09:25: [2024-11-21 22:09:25] iter = 0000, loss = 285.9827
2024-11-21 22:11:56: [2024-11-21 22:11:56] iter = 0010, loss = 152.6585
2024-11-21 22:14:22: [2024-11-21 22:14:22] iter = 0020, loss = 93.2966
2024-11-21 22:16:47: [2024-11-21 22:16:47] iter = 0030, loss = 76.3226
2024-11-21 22:19:19: [2024-11-21 22:19:19] iter = 0040, loss = 66.0159
2024-11-21 22:21:53: [2024-11-21 22:21:53] iter = 0050, loss = 58.5063
2024-11-21 22:24:19: [2024-11-21 22:24:19] iter = 0060, loss = 53.0843
2024-11-21 22:26:45: [2024-11-21 22:26:45] iter = 0070, loss = 48.0972
2024-11-21 22:29:11: [2024-11-21 22:29:11] iter = 0080, loss = 47.9006
2024-11-21 22:31:36: [2024-11-21 22:31:36] iter = 0090, loss = 44.2379
2024-11-21 22:34:10: [2024-11-21 22:34:10] iter = 0100, loss = 43.8984
2024-11-21 22:36:36: [2024-11-21 22:36:36] iter = 0110, loss = 43.1629
2024-11-21 22:39:03: [2024-11-21 22:39:03] iter = 0120, loss = 41.7394
2024-11-21 22:41:20: [2024-11-21 22:41:20] iter = 0130, loss = 41.4180
2024-11-21 22:43:42: [2024-11-21 22:43:42] iter = 0140, loss = 41.0205
2024-11-21 22:46:04: [2024-11-21 22:46:04] iter = 0150, loss = 41.3323
2024-11-21 22:48:26: [2024-11-21 22:48:26] iter = 0160, loss = 38.6604
2024-11-21 22:50:54: [2024-11-21 22:50:54] iter = 0170, loss = 37.8280
2024-11-21 22:53:17: [2024-11-21 22:53:17] iter = 0180, loss = 38.5105
2024-11-21 22:55:39: [2024-11-21 22:55:39] iter = 0190, loss = 35.5018
2024-11-21 22:58:06: [2024-11-21 22:58:06] iter = 0200, loss = 36.5125
2024-11-21 23:00:32: [2024-11-21 23:00:32] iter = 0210, loss = 35.5991
2024-11-21 23:02:55: [2024-11-21 23:02:55] iter = 0220, loss = 36.1516
2024-11-21 23:05:22: [2024-11-21 23:05:22] iter = 0230, loss = 34.7435
2024-11-21 23:07:47: [2024-11-21 23:07:47] iter = 0240, loss = 36.3392
2024-11-21 23:10:15: [2024-11-21 23:10:15] iter = 0250, loss = 34.3339
2024-11-21 23:12:44: [2024-11-21 23:12:44] iter = 0260, loss = 33.3510
2024-11-21 23:15:07: [2024-11-21 23:15:07] iter = 0270, loss = 33.1128
2024-11-21 23:17:35: [2024-11-21 23:17:35] iter = 0280, loss = 32.5585
2024-11-21 23:20:02: [2024-11-21 23:20:02] iter = 0290, loss = 30.0156
2024-11-21 23:22:35: [2024-11-21 23:22:35] iter = 0300, loss = 29.8757
2024-11-21 23:25:04: [2024-11-21 23:25:04] iter = 0310, loss = 31.4603
2024-11-21 23:27:35: [2024-11-21 23:27:35] iter = 0320, loss = 31.8439
2024-11-21 23:30:01: [2024-11-21 23:30:01] iter = 0330, loss = 32.0421
2024-11-21 23:32:27: [2024-11-21 23:32:27] iter = 0340, loss = 30.8321
2024-11-21 23:34:57: [2024-11-21 23:34:57] iter = 0350, loss = 29.3636
2024-11-21 23:37:24: [2024-11-21 23:37:24] iter = 0360, loss = 32.5626
2024-11-21 23:39:44: [2024-11-21 23:39:44] iter = 0370, loss = 31.7681
2024-11-21 23:42:17: [2024-11-21 23:42:17] iter = 0380, loss = 31.5479
2024-11-21 23:44:44: [2024-11-21 23:44:44] iter = 0390, loss = 30.4594
2024-11-21 23:47:19: [2024-11-21 23:47:19] iter = 0400, loss = 31.1752
2024-11-21 23:49:47: [2024-11-21 23:49:47] iter = 0410, loss = 31.6447
2024-11-21 23:52:17: [2024-11-21 23:52:17] iter = 0420, loss = 28.5330
2024-11-21 23:54:49: [2024-11-21 23:54:49] iter = 0430, loss = 29.1474
2024-11-21 23:57:27: [2024-11-21 23:57:27] iter = 0440, loss = 29.8578
2024-11-21 23:59:59: [2024-11-21 23:59:59] iter = 0450, loss = 30.5180
2024-11-22 00:02:37: [2024-11-22 00:02:37] iter = 0460, loss = 29.4722
2024-11-22 00:05:15: [2024-11-22 00:05:15] iter = 0470, loss = 30.8123
2024-11-22 00:07:45: [2024-11-22 00:07:45] iter = 0480, loss = 31.4064
2024-11-22 00:10:17: [2024-11-22 00:10:17] iter = 0490, loss = 28.8366
2024-11-22 00:12:32: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 00:12:32: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 00:13:06: Evaluate 5 random ConvNet, ACCmean = 0.7943 ACCstd = 0.0024
-------------------------
2024-11-22 00:13:06: Evaluate 5 random ConvNet, F1mean = 0.7731 F!std = 0.0021
-------------------------
2024-11-22 00:13:21: [2024-11-22 00:13:21] iter = 0500, loss = 28.7327
2024-11-22 00:15:47: [2024-11-22 00:15:47] iter = 0510, loss = 31.7280
2024-11-22 00:18:16: [2024-11-22 00:18:16] iter = 0520, loss = 28.7151
2024-11-22 00:20:46: [2024-11-22 00:20:46] iter = 0530, loss = 32.1318
2024-11-22 00:23:17: [2024-11-22 00:23:17] iter = 0540, loss = 28.5193
2024-11-22 00:25:51: [2024-11-22 00:25:51] iter = 0550, loss = 31.1290
2024-11-22 00:28:24: [2024-11-22 00:28:24] iter = 0560, loss = 30.6272
2024-11-22 00:30:44: [2024-11-22 00:30:44] iter = 0570, loss = 31.3109
2024-11-22 00:33:15: [2024-11-22 00:33:15] iter = 0580, loss = 31.0561
2024-11-22 00:35:48: [2024-11-22 00:35:48] iter = 0590, loss = 32.6825
2024-11-22 00:38:14: [2024-11-22 00:38:14] iter = 0600, loss = 29.9832
2024-11-22 00:40:42: [2024-11-22 00:40:42] iter = 0610, loss = 30.3777
2024-11-22 00:43:12: [2024-11-22 00:43:12] iter = 0620, loss = 31.8920
2024-11-22 00:45:41: [2024-11-22 00:45:41] iter = 0630, loss = 28.9942
2024-11-22 00:48:22: [2024-11-22 00:48:22] iter = 0640, loss = 29.5879
2024-11-22 00:51:07: [2024-11-22 00:51:07] iter = 0650, loss = 27.3282
2024-11-22 00:53:33: [2024-11-22 00:53:33] iter = 0660, loss = 30.3219
2024-11-22 00:56:02: [2024-11-22 00:56:02] iter = 0670, loss = 29.7314
2024-11-22 00:58:27: [2024-11-22 00:58:27] iter = 0680, loss = 29.3159
2024-11-22 01:01:04: [2024-11-22 01:01:04] iter = 0690, loss = 29.6136
2024-11-22 01:03:39: [2024-11-22 01:03:39] iter = 0700, loss = 27.9675
2024-11-22 01:06:17: [2024-11-22 01:06:17] iter = 0710, loss = 30.5102
2024-11-22 01:08:59: [2024-11-22 01:08:59] iter = 0720, loss = 30.1838
2024-11-22 01:11:27: [2024-11-22 01:11:27] iter = 0730, loss = 30.6556
2024-11-22 01:14:08: [2024-11-22 01:14:08] iter = 0740, loss = 29.4948
2024-11-22 01:16:51: [2024-11-22 01:16:51] iter = 0750, loss = 28.2376
2024-11-22 01:19:36: [2024-11-22 01:19:36] iter = 0760, loss = 28.4835
2024-11-22 01:22:12: [2024-11-22 01:22:12] iter = 0770, loss = 29.3446
2024-11-22 01:24:54: [2024-11-22 01:24:54] iter = 0780, loss = 31.2825
2024-11-22 01:27:24: [2024-11-22 01:27:24] iter = 0790, loss = 29.4557
2024-11-22 01:29:50: [2024-11-22 01:29:50] iter = 0800, loss = 28.8767
2024-11-22 01:32:20: [2024-11-22 01:32:20] iter = 0810, loss = 29.1037
2024-11-22 01:34:49: [2024-11-22 01:34:49] iter = 0820, loss = 28.2203
2024-11-22 01:37:18: [2024-11-22 01:37:18] iter = 0830, loss = 29.1672
2024-11-22 01:39:45: [2024-11-22 01:39:45] iter = 0840, loss = 29.9979
2024-11-22 01:42:20: [2024-11-22 01:42:20] iter = 0850, loss = 30.6834
2024-11-22 01:44:57: [2024-11-22 01:44:57] iter = 0860, loss = 30.2217
2024-11-22 01:47:35: [2024-11-22 01:47:35] iter = 0870, loss = 29.3284
2024-11-22 01:50:07: [2024-11-22 01:50:07] iter = 0880, loss = 28.5311
2024-11-22 01:52:40: [2024-11-22 01:52:40] iter = 0890, loss = 30.2784
2024-11-22 01:55:10: [2024-11-22 01:55:10] iter = 0900, loss = 29.8654
2024-11-22 01:57:53: [2024-11-22 01:57:53] iter = 0910, loss = 30.2592
2024-11-22 02:00:27: [2024-11-22 02:00:27] iter = 0920, loss = 29.0943
2024-11-22 02:02:55: [2024-11-22 02:02:55] iter = 0930, loss = 28.9855
2024-11-22 02:05:18: [2024-11-22 02:05:18] iter = 0940, loss = 29.2366
2024-11-22 02:07:49: [2024-11-22 02:07:49] iter = 0950, loss = 29.2465
2024-11-22 02:10:29: [2024-11-22 02:10:29] iter = 0960, loss = 28.6247
2024-11-22 02:12:58: [2024-11-22 02:12:58] iter = 0970, loss = 30.0459
2024-11-22 02:15:30: [2024-11-22 02:15:30] iter = 0980, loss = 30.2074
2024-11-22 02:17:48: [2024-11-22 02:17:48] iter = 0990, loss = 28.9817
2024-11-22 02:20:02: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 02:20:02: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 02:20:35: Evaluate 5 random ConvNet, ACCmean = 0.8015 ACCstd = 0.0015
-------------------------
2024-11-22 02:20:35: Evaluate 5 random ConvNet, F1mean = 0.7809 F!std = 0.0018
-------------------------
2024-11-22 02:20:51: [2024-11-22 02:20:51] iter = 1000, loss = 26.5589
2024-11-22 02:20:51: 
================== Exp 2 ==================
 
2024-11-22 02:20:51: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganCMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fc239531a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganCMNIST (INFO)>, 'dc_aug_param': None}
2024-11-22 02:20:51: Evaluation model pool: ['ConvNet']
2024-11-22 02:20:52: class c = 0: 1148 real images
2024-11-22 02:20:52: class c = 1: 619 real images
2024-11-22 02:20:52: class c = 2: 595 real images
2024-11-22 02:20:52: class c = 3: 600 real images
2024-11-22 02:20:52: class c = 4: 1088 real images
2024-11-22 02:20:52: class c = 5: 1170 real images
2024-11-22 02:20:52: class c = 6: 2986 real images
2024-11-22 02:20:52: class c = 7: 1002 real images
2024-11-22 02:20:52: class c = 8: 1022 real images
2024-11-22 02:20:52: class c = 9: 1173 real images
2024-11-22 02:20:52: class c = 10: 1572 real images
2024-11-22 02:20:52: real images channel 0, mean = 0.4942, std = 0.2834
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-22 02:20:52: initialize synthetic data from random noise
2024-11-22 02:20:52: [2024-11-22 02:20:52] training begins
2024-11-22 02:20:52: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-22 02:20:52: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 02:21:26: Evaluate 5 random ConvNet, ACCmean = 0.1166 ACCstd = 0.0225
-------------------------
2024-11-22 02:21:26: Evaluate 5 random ConvNet, F1mean = 0.0753 F!std = 0.0115
-------------------------
2024-11-22 02:21:41: [2024-11-22 02:21:41] iter = 0000, loss = 303.7151
2024-11-22 02:24:07: [2024-11-22 02:24:06] iter = 0010, loss = 147.4181
2024-11-22 02:26:41: [2024-11-22 02:26:41] iter = 0020, loss = 93.9728
2024-11-22 02:29:23: [2024-11-22 02:29:23] iter = 0030, loss = 76.1399
2024-11-22 02:31:35: [2024-11-22 02:31:35] iter = 0040, loss = 66.8516
2024-11-22 02:33:57: [2024-11-22 02:33:57] iter = 0050, loss = 61.5614
2024-11-22 02:36:20: [2024-11-22 02:36:20] iter = 0060, loss = 56.7190
2024-11-22 02:38:49: [2024-11-22 02:38:49] iter = 0070, loss = 50.8300
2024-11-22 02:41:19: [2024-11-22 02:41:19] iter = 0080, loss = 46.4285
2024-11-22 02:43:41: [2024-11-22 02:43:41] iter = 0090, loss = 45.4652
2024-11-22 02:46:12: [2024-11-22 02:46:12] iter = 0100, loss = 44.8420
2024-11-22 02:48:42: [2024-11-22 02:48:42] iter = 0110, loss = 43.1433
2024-11-22 02:51:09: [2024-11-22 02:51:09] iter = 0120, loss = 43.0115
2024-11-22 02:53:28: [2024-11-22 02:53:28] iter = 0130, loss = 42.6848
2024-11-22 02:55:53: [2024-11-22 02:55:53] iter = 0140, loss = 41.1486
2024-11-22 02:58:14: [2024-11-22 02:58:14] iter = 0150, loss = 41.0253
2024-11-22 03:00:36: [2024-11-22 03:00:36] iter = 0160, loss = 38.1097
2024-11-22 03:02:49: [2024-11-22 03:02:49] iter = 0170, loss = 38.7624
2024-11-22 03:05:14: [2024-11-22 03:05:14] iter = 0180, loss = 37.7306
2024-11-22 03:07:32: [2024-11-22 03:07:32] iter = 0190, loss = 38.1455
2024-11-22 03:10:02: [2024-11-22 03:10:02] iter = 0200, loss = 36.4927
2024-11-22 03:12:18: [2024-11-22 03:12:18] iter = 0210, loss = 35.5600
2024-11-22 03:14:31: [2024-11-22 03:14:31] iter = 0220, loss = 36.3516
2024-11-22 03:16:58: [2024-11-22 03:16:58] iter = 0230, loss = 36.2960
2024-11-22 03:19:24: [2024-11-22 03:19:24] iter = 0240, loss = 34.3131
2024-11-22 03:22:03: [2024-11-22 03:22:03] iter = 0250, loss = 35.7645
2024-11-22 03:24:35: [2024-11-22 03:24:35] iter = 0260, loss = 34.6076
2024-11-22 03:26:59: [2024-11-22 03:26:59] iter = 0270, loss = 34.6842
2024-11-22 03:29:22: [2024-11-22 03:29:22] iter = 0280, loss = 32.9693
2024-11-22 03:31:49: [2024-11-22 03:31:49] iter = 0290, loss = 33.4648
2024-11-22 03:34:13: [2024-11-22 03:34:13] iter = 0300, loss = 34.6347
2024-11-22 03:36:49: [2024-11-22 03:36:49] iter = 0310, loss = 32.9185
2024-11-22 03:39:24: [2024-11-22 03:39:24] iter = 0320, loss = 31.4431
2024-11-22 03:41:57: [2024-11-22 03:41:57] iter = 0330, loss = 32.3229
2024-11-22 03:44:24: [2024-11-22 03:44:24] iter = 0340, loss = 33.1185
2024-11-22 03:46:55: [2024-11-22 03:46:55] iter = 0350, loss = 32.2573
2024-11-22 03:49:13: [2024-11-22 03:49:13] iter = 0360, loss = 31.4614
2024-11-22 03:51:37: [2024-11-22 03:51:37] iter = 0370, loss = 31.2203
2024-11-22 03:54:09: [2024-11-22 03:54:09] iter = 0380, loss = 31.9748
2024-11-22 03:56:37: [2024-11-22 03:56:37] iter = 0390, loss = 31.0181
2024-11-22 03:59:08: [2024-11-22 03:59:08] iter = 0400, loss = 32.7810
2024-11-22 04:01:39: [2024-11-22 04:01:39] iter = 0410, loss = 30.1054
2024-11-22 04:04:09: [2024-11-22 04:04:09] iter = 0420, loss = 30.2844
2024-11-22 04:06:23: [2024-11-22 04:06:23] iter = 0430, loss = 30.4812
2024-11-22 04:08:44: [2024-11-22 04:08:44] iter = 0440, loss = 30.9305
2024-11-22 04:11:11: [2024-11-22 04:11:11] iter = 0450, loss = 30.2546
2024-11-22 04:13:44: [2024-11-22 04:13:44] iter = 0460, loss = 30.1388
2024-11-22 04:16:12: [2024-11-22 04:16:12] iter = 0470, loss = 30.9076
2024-11-22 04:18:40: [2024-11-22 04:18:40] iter = 0480, loss = 30.2705
2024-11-22 04:21:12: [2024-11-22 04:21:12] iter = 0490, loss = 29.6040
2024-11-22 04:23:30: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 04:23:30: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 04:24:00: Evaluate 5 random ConvNet, ACCmean = 0.7929 ACCstd = 0.0020
-------------------------
2024-11-22 04:24:00: Evaluate 5 random ConvNet, F1mean = 0.7711 F!std = 0.0018
-------------------------
2024-11-22 04:24:17: [2024-11-22 04:24:17] iter = 0500, loss = 29.0677
2024-11-22 04:26:39: [2024-11-22 04:26:39] iter = 0510, loss = 31.7956
2024-11-22 04:29:08: [2024-11-22 04:29:08] iter = 0520, loss = 29.5237
2024-11-22 04:31:40: [2024-11-22 04:31:40] iter = 0530, loss = 29.4807
2024-11-22 04:34:05: [2024-11-22 04:34:05] iter = 0540, loss = 31.6681
2024-11-22 04:36:26: [2024-11-22 04:36:26] iter = 0550, loss = 29.8627
2024-11-22 04:38:56: [2024-11-22 04:38:56] iter = 0560, loss = 30.0943
2024-11-22 04:41:21: [2024-11-22 04:41:21] iter = 0570, loss = 29.8397
2024-11-22 04:43:46: [2024-11-22 04:43:46] iter = 0580, loss = 30.8350
2024-11-22 04:46:17: [2024-11-22 04:46:17] iter = 0590, loss = 31.1699
2024-11-22 04:48:47: [2024-11-22 04:48:47] iter = 0600, loss = 29.0061
2024-11-22 04:51:15: [2024-11-22 04:51:15] iter = 0610, loss = 31.8678
2024-11-22 04:53:40: [2024-11-22 04:53:40] iter = 0620, loss = 30.1541
2024-11-22 04:56:17: [2024-11-22 04:56:17] iter = 0630, loss = 30.1784
2024-11-22 04:58:50: [2024-11-22 04:58:50] iter = 0640, loss = 31.1214
2024-11-22 05:01:21: [2024-11-22 05:01:21] iter = 0650, loss = 29.6447
2024-11-22 05:03:53: [2024-11-22 05:03:53] iter = 0660, loss = 31.6551
2024-11-22 05:06:22: [2024-11-22 05:06:22] iter = 0670, loss = 31.2400
2024-11-22 05:08:45: [2024-11-22 05:08:45] iter = 0680, loss = 28.7505
2024-11-22 05:11:14: [2024-11-22 05:11:14] iter = 0690, loss = 29.1750
2024-11-22 05:13:36: [2024-11-22 05:13:36] iter = 0700, loss = 28.6682
2024-11-22 05:15:58: [2024-11-22 05:15:58] iter = 0710, loss = 29.3719
2024-11-22 05:18:23: [2024-11-22 05:18:23] iter = 0720, loss = 29.0790
2024-11-22 05:20:44: [2024-11-22 05:20:44] iter = 0730, loss = 30.4693
2024-11-22 05:23:21: [2024-11-22 05:23:21] iter = 0740, loss = 28.9427
2024-11-22 05:25:39: [2024-11-22 05:25:39] iter = 0750, loss = 28.7544
2024-11-22 05:28:13: [2024-11-22 05:28:13] iter = 0760, loss = 29.0162
2024-11-22 05:30:49: [2024-11-22 05:30:49] iter = 0770, loss = 30.5597
2024-11-22 05:33:23: [2024-11-22 05:33:23] iter = 0780, loss = 29.3390
2024-11-22 05:35:53: [2024-11-22 05:35:53] iter = 0790, loss = 31.3142
2024-11-22 05:38:26: [2024-11-22 05:38:26] iter = 0800, loss = 29.4240
2024-11-22 05:40:56: [2024-11-22 05:40:56] iter = 0810, loss = 28.6678
2024-11-22 05:43:25: [2024-11-22 05:43:25] iter = 0820, loss = 29.4597
2024-11-22 05:45:48: [2024-11-22 05:45:48] iter = 0830, loss = 27.7898
2024-11-22 05:48:13: [2024-11-22 05:48:13] iter = 0840, loss = 31.2947
2024-11-22 05:50:19: [2024-11-22 05:50:19] iter = 0850, loss = 29.6021
2024-11-22 05:52:19: [2024-11-22 05:52:19] iter = 0860, loss = 29.9857
2024-11-22 05:54:29: [2024-11-22 05:54:29] iter = 0870, loss = 28.8094
2024-11-22 05:56:41: [2024-11-22 05:56:41] iter = 0880, loss = 28.2949
2024-11-22 05:58:50: [2024-11-22 05:58:50] iter = 0890, loss = 30.3901
2024-11-22 06:01:02: [2024-11-22 06:01:02] iter = 0900, loss = 29.0755
2024-11-22 06:03:17: [2024-11-22 06:03:17] iter = 0910, loss = 30.4256
2024-11-22 06:05:37: [2024-11-22 06:05:37] iter = 0920, loss = 29.4050
2024-11-22 06:07:42: [2024-11-22 06:07:42] iter = 0930, loss = 30.7290
2024-11-22 06:09:57: [2024-11-22 06:09:57] iter = 0940, loss = 28.3914
2024-11-22 06:12:12: [2024-11-22 06:12:12] iter = 0950, loss = 28.7954
2024-11-22 06:14:22: [2024-11-22 06:14:22] iter = 0960, loss = 29.0801
2024-11-22 06:16:31: [2024-11-22 06:16:31] iter = 0970, loss = 27.9005
2024-11-22 06:18:38: [2024-11-22 06:18:38] iter = 0980, loss = 28.5253
2024-11-22 06:20:47: [2024-11-22 06:20:47] iter = 0990, loss = 29.5972
2024-11-22 06:22:47: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 06:22:47: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
Using downloaded and verified file: /data/users/xiongyuxuan/.medmnist/organcmnist.npz
Using downloaded and verified file: /data/users/xiongyuxuan/.medmnist/organcmnist.npz
Loaded the dataset:OrganCMNIST
[2024-11-21 18:29:35] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.005515 train acc = 1.0000, test acc = 0.0999, test_sen =0.0981, test_spe =0.9056, test_f1 =0.0769
[2024-11-21 18:29:38] Evaluate_01: epoch = 0300 train time = 2 s train loss = 0.005355 train acc = 1.0000, test acc = 0.1075, test_sen =0.0941, test_spe =0.9073, test_f1 =0.0710
[2024-11-21 18:29:41] Evaluate_02: epoch = 0300 train time = 2 s train loss = 0.005363 train acc = 1.0000, test acc = 0.0764, test_sen =0.0952, test_spe =0.9076, test_f1 =0.0752
[2024-11-21 18:29:44] Evaluate_03: epoch = 0300 train time = 2 s train loss = 0.005509 train acc = 1.0000, test acc = 0.0908, test_sen =0.0759, test_spe =0.9073, test_f1 =0.0729
[2024-11-21 18:29:47] Evaluate_04: epoch = 0300 train time = 2 s train loss = 0.005293 train acc = 1.0000, test acc = 0.1103, test_sen =0.1205, test_spe =0.9086, test_f1 =0.0887
[2024-11-21 20:07:21] Evaluate_00: epoch = 0300 train time = 3 s train loss = 0.010095 train acc = 1.0000, test acc = 0.8005, test_sen =0.7916, test_spe =0.9801, test_f1 =0.7782
[2024-11-21 20:07:25] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.009829 train acc = 1.0000, test acc = 0.8001, test_sen =0.7896, test_spe =0.9800, test_f1 =0.7778
[2024-11-21 20:07:29] Evaluate_02: epoch = 0300 train time = 3 s train loss = 0.010316 train acc = 1.0000, test acc = 0.7941, test_sen =0.7840, test_spe =0.9795, test_f1 =0.7711
[2024-11-21 20:07:34] Evaluate_03: epoch = 0300 train time = 3 s train loss = 0.010590 train acc = 1.0000, test acc = 0.7969, test_sen =0.7874, test_spe =0.9797, test_f1 =0.7751
[2024-11-21 20:07:38] Evaluate_04: epoch = 0300 train time = 3 s train loss = 0.010269 train acc = 1.0000, test acc = 0.7930, test_sen =0.7837, test_spe =0.9794, test_f1 =0.7702
[2024-11-21 22:07:55] Evaluate_00: epoch = 0300 train time = 6 s train loss = 0.011461 train acc = 1.0000, test acc = 0.7930, test_sen =0.7854, test_spe =0.9793, test_f1 =0.7724
[2024-11-21 22:08:01] Evaluate_01: epoch = 0300 train time = 5 s train loss = 0.010761 train acc = 1.0000, test acc = 0.7977, test_sen =0.7894, test_spe =0.9798, test_f1 =0.7762
[2024-11-21 22:08:09] Evaluate_02: epoch = 0300 train time = 6 s train loss = 0.011380 train acc = 1.0000, test acc = 0.7994, test_sen =0.7940, test_spe =0.9799, test_f1 =0.7796
[2024-11-21 22:08:16] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.010183 train acc = 1.0000, test acc = 0.7998, test_sen =0.7937, test_spe =0.9800, test_f1 =0.7792
[2024-11-21 22:08:23] Evaluate_04: epoch = 0300 train time = 6 s train loss = 0.010803 train acc = 1.0000, test acc = 0.7927, test_sen =0.7865, test_spe =0.9793, test_f1 =0.7719
[2024-11-21 22:08:47] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.005178 train acc = 1.0000, test acc = 0.0960, test_sen =0.1090, test_spe =0.9108, test_f1 =0.0884
[2024-11-21 22:08:54] Evaluate_01: epoch = 0300 train time = 6 s train loss = 0.005450 train acc = 1.0000, test acc = 0.0828, test_sen =0.1052, test_spe =0.9099, test_f1 =0.0736
[2024-11-21 22:09:00] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.005370 train acc = 1.0000, test acc = 0.1145, test_sen =0.1448, test_spe =0.9138, test_f1 =0.0861
[2024-11-21 22:09:05] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.005537 train acc = 1.0000, test acc = 0.1323, test_sen =0.1684, test_spe =0.9151, test_f1 =0.1148
[2024-11-21 22:09:10] Evaluate_04: epoch = 0300 train time = 3 s train loss = 0.005339 train acc = 1.0000, test acc = 0.1136, test_sen =0.1426, test_spe =0.9125, test_f1 =0.1060
[2024-11-22 00:12:38] Evaluate_00: epoch = 0300 train time = 5 s train loss = 0.009857 train acc = 1.0000, test acc = 0.7975, test_sen =0.7903, test_spe =0.9798, test_f1 =0.7755
[2024-11-22 00:12:45] Evaluate_01: epoch = 0300 train time = 5 s train loss = 0.009558 train acc = 1.0000, test acc = 0.7970, test_sen =0.7889, test_spe =0.9797, test_f1 =0.7757
[2024-11-22 00:12:52] Evaluate_02: epoch = 0300 train time = 6 s train loss = 0.010378 train acc = 1.0000, test acc = 0.7917, test_sen =0.7859, test_spe =0.9792, test_f1 =0.7705
[2024-11-22 00:13:00] Evaluate_03: epoch = 0300 train time = 6 s train loss = 0.010285 train acc = 1.0000, test acc = 0.7926, test_sen =0.7858, test_spe =0.9793, test_f1 =0.7715
[2024-11-22 00:13:06] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.010317 train acc = 1.0000, test acc = 0.7927, test_sen =0.7869, test_spe =0.9793, test_f1 =0.7722
[2024-11-22 02:20:08] Evaluate_00: epoch = 0300 train time = 5 s train loss = 0.010741 train acc = 1.0000, test acc = 0.7998, test_sen =0.7945, test_spe =0.9800, test_f1 =0.7793
[2024-11-22 02:20:15] Evaluate_01: epoch = 0300 train time = 6 s train loss = 0.010543 train acc = 1.0000, test acc = 0.8008, test_sen =0.7945, test_spe =0.9801, test_f1 =0.7798
[2024-11-22 02:20:22] Evaluate_02: epoch = 0300 train time = 5 s train loss = 0.010205 train acc = 1.0000, test acc = 0.8005, test_sen =0.7952, test_spe =0.9801, test_f1 =0.7792
[2024-11-22 02:20:29] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.010664 train acc = 1.0000, test acc = 0.8033, test_sen =0.7982, test_spe =0.9804, test_f1 =0.7832
[2024-11-22 02:20:35] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.010217 train acc = 1.0000, test acc = 0.8033, test_sen =0.7975, test_spe =0.9804, test_f1 =0.7828
[2024-11-22 02:20:59] Evaluate_00: epoch = 0300 train time = 5 s train loss = 0.005266 train acc = 1.0000, test acc = 0.1157, test_sen =0.1227, test_spe =0.9117, test_f1 =0.0926
[2024-11-22 02:21:05] Evaluate_01: epoch = 0300 train time = 5 s train loss = 0.005433 train acc = 1.0000, test acc = 0.0959, test_sen =0.0836, test_spe =0.9087, test_f1 =0.0702
[2024-11-22 02:21:12] Evaluate_02: epoch = 0300 train time = 5 s train loss = 0.005321 train acc = 1.0000, test acc = 0.0908, test_sen =0.0676, test_spe =0.9086, test_f1 =0.0594
[2024-11-22 02:21:20] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.005265 train acc = 1.0000, test acc = 0.1529, test_sen =0.0994, test_spe =0.9116, test_f1 =0.0833
[2024-11-22 02:21:26] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.005439 train acc = 1.0000, test acc = 0.1274, test_sen =0.0780, test_spe =0.9096, test_f1 =0.0710
[2024-11-22 04:23:37] Evaluate_00: epoch = 0300 train time = 5 s train loss = 0.010430 train acc = 1.0000, test acc = 0.7897, test_sen =0.7845, test_spe =0.9790, test_f1 =0.7685
[2024-11-22 04:23:42] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.010262 train acc = 1.0000, test acc = 0.7955, test_sen =0.7865, test_spe =0.9796, test_f1 =0.7721
[2024-11-22 04:23:47] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.010167 train acc = 1.0000, test acc = 0.7916, test_sen =0.7874, test_spe =0.9792, test_f1 =0.7706
[2024-11-22 04:23:54] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.010313 train acc = 1.0000, test acc = 0.7935, test_sen =0.7875, test_spe =0.9794, test_f1 =0.7705
[2024-11-22 04:24:00] Evaluate_04: epoch = 0300 train time = 6 s train loss = 0.010352 train acc = 1.0000, test acc = 0.7941, test_sen =0.7899, test_spe =0.9794, test_f1 =0.7740
[2024-11-22 06:22:53] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.010683 train acc = 1.0000, test acc = 0.7948, test_sen =0.7922, test_spe =0.9796, test_f1 =0.7748
[2024-11-22 06:22:59] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.010148 train acc = 1.0000, test acc = 0.8021, test_sen =0.7980, test_spe =0.9803, test_f1 =0.7803
[2024-11-22 06:23:04] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.010775 train acc = 1.0000, test acc = 0.7981, test_sen =0.7928, test_spe =0.9799, test_f1 =0.7766
[2024-11-22 06:23:09] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.011418 train acc = 1.0000, test acc = 0.7997, test_sen =0.7943, test_spe =0.9800, test_f1 =0.7804
[2024-11-22 06:23:14] Evaluate_04: epoch = 0300 train time = 3 s train loss = 0.010812 train acc = 1.0000, test acc = 0.7942, test_sen =0.7884, test_spe =0.9794, test_f1 =0.77322024-11-22 06:23:14: Evaluate 5 random ConvNet, ACCmean = 0.7978 ACCstd = 0.0030
-------------------------
2024-11-22 06:23:14: Evaluate 5 random ConvNet, F1mean = 0.7771 F!std = 0.0029
-------------------------
2024-11-22 06:23:27: [2024-11-22 06:23:27] iter = 1000, loss = 28.6691
2024-11-22 06:23:27: 
================== Exp 3 ==================
 
2024-11-22 06:23:27: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganCMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fc239531a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganCMNIST (INFO)>, 'dc_aug_param': None}
2024-11-22 06:23:27: Evaluation model pool: ['ConvNet']
2024-11-22 06:23:27: class c = 0: 1148 real images
2024-11-22 06:23:27: class c = 1: 619 real images
2024-11-22 06:23:27: class c = 2: 595 real images
2024-11-22 06:23:27: class c = 3: 600 real images
2024-11-22 06:23:27: class c = 4: 1088 real images
2024-11-22 06:23:27: class c = 5: 1170 real images
2024-11-22 06:23:27: class c = 6: 2986 real images
2024-11-22 06:23:27: class c = 7: 1002 real images
2024-11-22 06:23:27: class c = 8: 1022 real images
2024-11-22 06:23:27: class c = 9: 1173 real images
2024-11-22 06:23:27: class c = 10: 1572 real images
2024-11-22 06:23:27: real images channel 0, mean = 0.4942, std = 0.2834
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-22 06:23:27: initialize synthetic data from random noise
2024-11-22 06:23:27: [2024-11-22 06:23:27] training begins
2024-11-22 06:23:27: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-22 06:23:27: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 06:23:58: Evaluate 5 random ConvNet, ACCmean = 0.0664 ACCstd = 0.0140
-------------------------
2024-11-22 06:23:58: Evaluate 5 random ConvNet, F1mean = 0.0516 F!std = 0.0062
-------------------------
2024-11-22 06:24:08: [2024-11-22 06:24:08] iter = 0000, loss = 300.7695
2024-11-22 06:26:18: [2024-11-22 06:26:18] iter = 0010, loss = 155.4346
2024-11-22 06:28:28: [2024-11-22 06:28:28] iter = 0020, loss = 96.9566
2024-11-22 06:30:39: [2024-11-22 06:30:39] iter = 0030, loss = 72.6038
2024-11-22 06:32:51: [2024-11-22 06:32:51] iter = 0040, loss = 62.1064
2024-11-22 06:34:59: [2024-11-22 06:34:59] iter = 0050, loss = 56.4060
2024-11-22 06:37:07: [2024-11-22 06:37:07] iter = 0060, loss = 56.0223
2024-11-22 06:39:17: [2024-11-22 06:39:17] iter = 0070, loss = 53.5849
2024-11-22 06:41:30: [2024-11-22 06:41:30] iter = 0080, loss = 48.6678
2024-11-22 06:43:39: [2024-11-22 06:43:39] iter = 0090, loss = 46.7884
2024-11-22 06:45:53: [2024-11-22 06:45:53] iter = 0100, loss = 45.8295
2024-11-22 06:47:51: [2024-11-22 06:47:51] iter = 0110, loss = 43.2899
2024-11-22 06:50:03: [2024-11-22 06:50:03] iter = 0120, loss = 43.7603
2024-11-22 06:52:15: [2024-11-22 06:52:15] iter = 0130, loss = 41.0062
2024-11-22 06:54:24: [2024-11-22 06:54:24] iter = 0140, loss = 39.5434
2024-11-22 06:56:35: [2024-11-22 06:56:35] iter = 0150, loss = 39.0595
2024-11-22 06:58:47: [2024-11-22 06:58:47] iter = 0160, loss = 37.9533
2024-11-22 07:01:02: [2024-11-22 07:01:02] iter = 0170, loss = 37.7094
2024-11-22 07:03:17: [2024-11-22 07:03:17] iter = 0180, loss = 37.4512
2024-11-22 07:05:26: [2024-11-22 07:05:26] iter = 0190, loss = 36.2900
2024-11-22 07:07:36: [2024-11-22 07:07:36] iter = 0200, loss = 35.2461
2024-11-22 07:09:43: [2024-11-22 07:09:43] iter = 0210, loss = 34.1700
2024-11-22 07:11:53: [2024-11-22 07:11:53] iter = 0220, loss = 36.1511
2024-11-22 07:14:03: [2024-11-22 07:14:03] iter = 0230, loss = 35.2051
2024-11-22 07:16:04: [2024-11-22 07:16:04] iter = 0240, loss = 33.5893
2024-11-22 07:18:08: [2024-11-22 07:18:08] iter = 0250, loss = 32.5734
2024-11-22 07:20:16: [2024-11-22 07:20:16] iter = 0260, loss = 34.2794
2024-11-22 07:22:28: [2024-11-22 07:22:28] iter = 0270, loss = 34.9556
2024-11-22 07:24:37: [2024-11-22 07:24:37] iter = 0280, loss = 33.7863
2024-11-22 07:26:43: [2024-11-22 07:26:43] iter = 0290, loss = 32.4862
2024-11-22 07:28:51: [2024-11-22 07:28:51] iter = 0300, loss = 33.0347
2024-11-22 07:30:55: [2024-11-22 07:30:55] iter = 0310, loss = 32.2332
2024-11-22 07:33:09: [2024-11-22 07:33:09] iter = 0320, loss = 32.2339
2024-11-22 07:35:17: [2024-11-22 07:35:17] iter = 0330, loss = 31.9411
2024-11-22 07:37:23: [2024-11-22 07:37:23] iter = 0340, loss = 32.5340
2024-11-22 07:39:31: [2024-11-22 07:39:31] iter = 0350, loss = 30.6012
2024-11-22 07:41:43: [2024-11-22 07:41:43] iter = 0360, loss = 35.1629
2024-11-22 07:43:51: [2024-11-22 07:43:51] iter = 0370, loss = 30.9178
2024-11-22 07:46:03: [2024-11-22 07:46:03] iter = 0380, loss = 31.2628
2024-11-22 07:48:21: [2024-11-22 07:48:21] iter = 0390, loss = 28.9769
2024-11-22 07:50:36: [2024-11-22 07:50:36] iter = 0400, loss = 29.1142
2024-11-22 07:52:41: [2024-11-22 07:52:41] iter = 0410, loss = 30.0330
2024-11-22 07:54:49: [2024-11-22 07:54:49] iter = 0420, loss = 30.6157
2024-11-22 07:57:00: [2024-11-22 07:57:00] iter = 0430, loss = 30.0236
2024-11-22 07:59:09: [2024-11-22 07:59:09] iter = 0440, loss = 30.1716
2024-11-22 08:01:23: [2024-11-22 08:01:23] iter = 0450, loss = 29.4426
2024-11-22 08:03:32: [2024-11-22 08:03:32] iter = 0460, loss = 30.6935
2024-11-22 08:05:45: [2024-11-22 08:05:45] iter = 0470, loss = 29.9479
2024-11-22 08:07:55: [2024-11-22 08:07:55] iter = 0480, loss = 30.2664
2024-11-22 08:10:00: [2024-11-22 08:10:00] iter = 0490, loss = 30.3610
2024-11-22 08:11:56: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 08:11:56: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 08:12:25: Evaluate 5 random ConvNet, ACCmean = 0.7900 ACCstd = 0.0014
-------------------------
2024-11-22 08:12:25: Evaluate 5 random ConvNet, F1mean = 0.7695 F!std = 0.0016
-------------------------
2024-11-22 08:12:36: [2024-11-22 08:12:36] iter = 0500, loss = 29.3247
2024-11-22 08:14:47: [2024-11-22 08:14:47] iter = 0510, loss = 30.7118
2024-11-22 08:17:02: [2024-11-22 08:17:02] iter = 0520, loss = 31.3120
2024-11-22 08:19:05: [2024-11-22 08:19:05] iter = 0530, loss = 31.5503
2024-11-22 08:21:08: [2024-11-22 08:21:08] iter = 0540, loss = 28.7576
2024-11-22 08:23:09: [2024-11-22 08:23:09] iter = 0550, loss = 29.2140
2024-11-22 08:25:14: [2024-11-22 08:25:14] iter = 0560, loss = 29.9443
2024-11-22 08:27:22: [2024-11-22 08:27:22] iter = 0570, loss = 30.4959
2024-11-22 08:29:28: [2024-11-22 08:29:28] iter = 0580, loss = 30.1572
2024-11-22 08:31:30: [2024-11-22 08:31:30] iter = 0590, loss = 29.4263
2024-11-22 08:33:36: [2024-11-22 08:33:36] iter = 0600, loss = 31.2344
2024-11-22 08:35:37: [2024-11-22 08:35:37] iter = 0610, loss = 29.7831
2024-11-22 08:37:40: [2024-11-22 08:37:40] iter = 0620, loss = 29.1779
2024-11-22 08:39:44: [2024-11-22 08:39:44] iter = 0630, loss = 28.8178
2024-11-22 08:41:49: [2024-11-22 08:41:49] iter = 0640, loss = 31.1967
2024-11-22 08:43:49: [2024-11-22 08:43:49] iter = 0650, loss = 29.5346
2024-11-22 08:45:52: [2024-11-22 08:45:52] iter = 0660, loss = 29.1693
2024-11-22 08:47:58: [2024-11-22 08:47:58] iter = 0670, loss = 28.2232
2024-11-22 08:50:05: [2024-11-22 08:50:05] iter = 0680, loss = 29.7724
2024-11-22 08:52:07: [2024-11-22 08:52:07] iter = 0690, loss = 30.0013
2024-11-22 08:54:14: [2024-11-22 08:54:14] iter = 0700, loss = 30.8526
2024-11-22 08:56:18: [2024-11-22 08:56:18] iter = 0710, loss = 28.9117
2024-11-22 08:58:24: [2024-11-22 08:58:24] iter = 0720, loss = 30.4427
2024-11-22 09:00:32: [2024-11-22 09:00:32] iter = 0730, loss = 29.2108
2024-11-22 09:02:42: [2024-11-22 09:02:42] iter = 0740, loss = 29.8048
2024-11-22 09:04:42: [2024-11-22 09:04:42] iter = 0750, loss = 29.3500
2024-11-22 09:06:47: [2024-11-22 09:06:47] iter = 0760, loss = 29.4839
2024-11-22 09:08:51: [2024-11-22 09:08:51] iter = 0770, loss = 29.2406
2024-11-22 09:10:55: [2024-11-22 09:10:55] iter = 0780, loss = 28.8711
2024-11-22 09:12:59: [2024-11-22 09:12:59] iter = 0790, loss = 29.5042
2024-11-22 09:14:59: [2024-11-22 09:14:59] iter = 0800, loss = 30.2031
2024-11-22 09:17:08: [2024-11-22 09:17:08] iter = 0810, loss = 29.0008
2024-11-22 09:19:08: [2024-11-22 09:19:08] iter = 0820, loss = 27.5364
2024-11-22 09:21:11: [2024-11-22 09:21:11] iter = 0830, loss = 29.3172
2024-11-22 09:23:19: [2024-11-22 09:23:19] iter = 0840, loss = 29.9116
2024-11-22 09:25:19: [2024-11-22 09:25:19] iter = 0850, loss = 29.8989
2024-11-22 09:27:24: [2024-11-22 09:27:24] iter = 0860, loss = 29.5657
2024-11-22 09:29:31: [2024-11-22 09:29:31] iter = 0870, loss = 29.9070
2024-11-22 09:31:36: [2024-11-22 09:31:36] iter = 0880, loss = 29.3038
2024-11-22 09:33:42: [2024-11-22 09:33:42] iter = 0890, loss = 30.4570
2024-11-22 09:35:43: [2024-11-22 09:35:43] iter = 0900, loss = 28.1326
2024-11-22 09:37:52: [2024-11-22 09:37:52] iter = 0910, loss = 28.7955
2024-11-22 09:39:53: [2024-11-22 09:39:53] iter = 0920, loss = 29.0811
2024-11-22 09:41:58: [2024-11-22 09:41:58] iter = 0930, loss = 28.8268
2024-11-22 09:44:05: [2024-11-22 09:44:05] iter = 0940, loss = 30.4929
2024-11-22 09:46:06: [2024-11-22 09:46:06] iter = 0950, loss = 29.1851
2024-11-22 09:48:14: [2024-11-22 09:48:14] iter = 0960, loss = 28.0432
2024-11-22 09:50:17: [2024-11-22 09:50:17] iter = 0970, loss = 28.9252
2024-11-22 09:52:21: [2024-11-22 09:52:21] iter = 0980, loss = 30.0537
2024-11-22 09:54:25: [2024-11-22 09:54:25] iter = 0990, loss = 28.9060
2024-11-22 09:56:18: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 09:56:18: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 09:56:45: Evaluate 5 random ConvNet, ACCmean = 0.7987 ACCstd = 0.0011
-------------------------
2024-11-22 09:56:45: Evaluate 5 random ConvNet, F1mean = 0.7771 F!std = 0.0009
-------------------------
2024-11-22 09:56:57: [2024-11-22 09:56:57] iter = 1000, loss = 29.2545
2024-11-22 09:56:57: 
================== Exp 4 ==================
 
2024-11-22 09:56:57: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganCMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fc239531a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganCMNIST (INFO)>, 'dc_aug_param': None}
2024-11-22 09:56:57: Evaluation model pool: ['ConvNet']
2024-11-22 09:56:58: class c = 0: 1148 real images
2024-11-22 09:56:58: class c = 1: 619 real images
2024-11-22 09:56:58: class c = 2: 595 real images
2024-11-22 09:56:58: class c = 3: 600 real images
2024-11-22 09:56:58: class c = 4: 1088 real images
2024-11-22 09:56:58: class c = 5: 1170 real images
2024-11-22 09:56:58: class c = 6: 2986 real images
2024-11-22 09:56:58: class c = 7: 1002 real images
2024-11-22 09:56:58: class c = 8: 1022 real images
2024-11-22 09:56:58: class c = 9: 1173 real images
2024-11-22 09:56:58: class c = 10: 1572 real images
2024-11-22 09:56:58: real images channel 0, mean = 0.4942, std = 0.2834
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-22 09:56:58: initialize synthetic data from random noise
2024-11-22 09:56:58: [2024-11-22 09:56:58] training begins
2024-11-22 09:56:58: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-22 09:56:58: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 09:57:24: Evaluate 5 random ConvNet, ACCmean = 0.0594 ACCstd = 0.0126
-------------------------
2024-11-22 09:57:24: Evaluate 5 random ConvNet, F1mean = 0.0455 F!std = 0.0060
-------------------------
2024-11-22 09:57:37: [2024-11-22 09:57:37] iter = 0000, loss = 302.4086
2024-11-22 09:59:45: [2024-11-22 09:59:45] iter = 0010, loss = 150.5040
2024-11-22 10:01:47: [2024-11-22 10:01:47] iter = 0020, loss = 88.3638
2024-11-22 10:03:54: [2024-11-22 10:03:54] iter = 0030, loss = 74.0326
2024-11-22 10:06:00: [2024-11-22 10:06:00] iter = 0040, loss = 67.7253
2024-11-22 10:08:06: [2024-11-22 10:08:06] iter = 0050, loss = 58.2520
2024-11-22 10:10:05: [2024-11-22 10:10:05] iter = 0060, loss = 51.4228
2024-11-22 10:12:05: [2024-11-22 10:12:05] iter = 0070, loss = 50.0622
2024-11-22 10:14:05: [2024-11-22 10:14:05] iter = 0080, loss = 49.9541
2024-11-22 10:16:06: [2024-11-22 10:16:06] iter = 0090, loss = 46.7331
2024-11-22 10:18:07: [2024-11-22 10:18:07] iter = 0100, loss = 46.2349
2024-11-22 10:20:03: [2024-11-22 10:20:03] iter = 0110, loss = 41.9986
2024-11-22 10:22:06: [2024-11-22 10:22:06] iter = 0120, loss = 40.8300
2024-11-22 10:24:07: [2024-11-22 10:24:07] iter = 0130, loss = 41.8804
2024-11-22 10:26:09: [2024-11-22 10:26:09] iter = 0140, loss = 40.1027
2024-11-22 10:28:13: [2024-11-22 10:28:13] iter = 0150, loss = 37.8868
2024-11-22 10:30:17: [2024-11-22 10:30:17] iter = 0160, loss = 39.3749
2024-11-22 10:32:21: [2024-11-22 10:32:21] iter = 0170, loss = 37.0696
2024-11-22 10:34:27: [2024-11-22 10:34:27] iter = 0180, loss = 35.9196
2024-11-22 10:36:32: [2024-11-22 10:36:32] iter = 0190, loss = 37.3212
2024-11-22 10:38:35: [2024-11-22 10:38:35] iter = 0200, loss = 36.2256
2024-11-22 10:40:39: [2024-11-22 10:40:39] iter = 0210, loss = 35.6773
2024-11-22 10:42:45: [2024-11-22 10:42:45] iter = 0220, loss = 33.2775
2024-11-22 10:44:48: [2024-11-22 10:44:48] iter = 0230, loss = 37.3538
2024-11-22 10:46:51: [2024-11-22 10:46:51] iter = 0240, loss = 33.7254
2024-11-22 10:48:55: [2024-11-22 10:48:55] iter = 0250, loss = 34.4188
2024-11-22 10:50:58: [2024-11-22 10:50:58] iter = 0260, loss = 32.0211
2024-11-22 10:53:04: [2024-11-22 10:53:04] iter = 0270, loss = 33.7295
2024-11-22 10:55:06: [2024-11-22 10:55:06] iter = 0280, loss = 33.0617
2024-11-22 10:57:14: [2024-11-22 10:57:14] iter = 0290, loss = 33.8070
2024-11-22 10:59:19: [2024-11-22 10:59:19] iter = 0300, loss = 33.0112
2024-11-22 11:01:14: [2024-11-22 11:01:13] iter = 0310, loss = 32.7510
2024-11-22 11:03:15: [2024-11-22 11:03:15] iter = 0320, loss = 32.0967
2024-11-22 11:05:22: [2024-11-22 11:05:22] iter = 0330, loss = 30.5695
2024-11-22 11:07:24: [2024-11-22 11:07:24] iter = 0340, loss = 31.4813
2024-11-22 11:09:23: [2024-11-22 11:09:23] iter = 0350, loss = 33.0651
2024-11-22 11:11:23: [2024-11-22 11:11:23] iter = 0360, loss = 32.4510
2024-11-22 11:13:19: [2024-11-22 11:13:19] iter = 0370, loss = 30.6171
2024-11-22 11:15:19: [2024-11-22 11:15:19] iter = 0380, loss = 29.8634
2024-11-22 11:17:21: [2024-11-22 11:17:21] iter = 0390, loss = 32.6729
2024-11-22 11:19:23: [2024-11-22 11:19:23] iter = 0400, loss = 30.9059
2024-11-22 11:21:24: [2024-11-22 11:21:24] iter = 0410, loss = 28.7952
2024-11-22 11:23:30: [2024-11-22 11:23:30] iter = 0420, loss = 32.5647
2024-11-22 11:25:35: [2024-11-22 11:25:35] iter = 0430, loss = 30.5110
2024-11-22 11:27:38: [2024-11-22 11:27:38] iter = 0440, loss = 30.6694
2024-11-22 11:29:38: [2024-11-22 11:29:38] iter = 0450, loss = 31.3056
2024-11-22 11:31:44: [2024-11-22 11:31:44] iter = 0460, loss = 32.1227
2024-11-22 11:33:46: [2024-11-22 11:33:46] iter = 0470, loss = 30.0803
2024-11-22 11:35:45: [2024-11-22 11:35:45] iter = 0480, loss = 30.4873
2024-11-22 11:37:54: [2024-11-22 11:37:54] iter = 0490, loss = 28.4964
2024-11-22 11:39:46: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 11:39:46: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 11:40:12: Evaluate 5 random ConvNet, ACCmean = 0.7961 ACCstd = 0.0020
-------------------------
2024-11-22 11:40:12: Evaluate 5 random ConvNet, F1mean = 0.7731 F!std = 0.0022
-------------------------
2024-11-22 11:40:25: [2024-11-22 11:40:25] iter = 0500, loss = 31.3181
2024-11-22 11:42:26: [2024-11-22 11:42:26] iter = 0510, loss = 30.5675
2024-11-22 11:44:30: [2024-11-22 11:44:30] iter = 0520, loss = 30.2315
2024-11-22 11:46:30: [2024-11-22 11:46:30] iter = 0530, loss = 31.0363
2024-11-22 11:48:30: [2024-11-22 11:48:30] iter = 0540, loss = 30.9917
2024-11-22 11:50:30: [2024-11-22 11:50:30] iter = 0550, loss = 29.4549
2024-11-22 11:52:34: [2024-11-22 11:52:34] iter = 0560, loss = 30.6478
2024-11-22 11:54:38: [2024-11-22 11:54:38] iter = 0570, loss = 31.3665
2024-11-22 11:56:46: [2024-11-22 11:56:46] iter = 0580, loss = 29.8018
2024-11-22 11:58:47: [2024-11-22 11:58:47] iter = 0590, loss = 30.5718
2024-11-22 12:00:48: [2024-11-22 12:00:48] iter = 0600, loss = 29.7768
2024-11-22 12:02:53: [2024-11-22 12:02:53] iter = 0610, loss = 30.4229
2024-11-22 12:04:59: [2024-11-22 12:04:59] iter = 0620, loss = 30.7291
2024-11-22 12:06:58: [2024-11-22 12:06:58] iter = 0630, loss = 29.0472
2024-11-22 12:09:01: [2024-11-22 12:09:01] iter = 0640, loss = 29.1815
2024-11-22 12:11:02: [2024-11-22 12:11:02] iter = 0650, loss = 28.7287
2024-11-22 12:12:59: [2024-11-22 12:12:59] iter = 0660, loss = 30.6650
2024-11-22 12:14:56: [2024-11-22 12:14:56] iter = 0670, loss = 29.4187
2024-11-22 12:16:57: [2024-11-22 12:16:57] iter = 0680, loss = 29.8699
2024-11-22 12:18:59: [2024-11-22 12:18:59] iter = 0690, loss = 31.7236
2024-11-22 12:20:59: [2024-11-22 12:20:59] iter = 0700, loss = 28.9633
2024-11-22 12:22:57: [2024-11-22 12:22:57] iter = 0710, loss = 31.1300
2024-11-22 12:24:55: [2024-11-22 12:24:55] iter = 0720, loss = 31.0573
2024-11-22 12:26:50: [2024-11-22 12:26:50] iter = 0730, loss = 27.7245
2024-11-22 12:28:47: [2024-11-22 12:28:47] iter = 0740, loss = 28.5327
2024-11-22 12:30:45: [2024-11-22 12:30:45] iter = 0750, loss = 31.0241
2024-11-22 12:32:46: [2024-11-22 12:32:46] iter = 0760, loss = 30.4712
2024-11-22 12:34:42: [2024-11-22 12:34:42] iter = 0770, loss = 30.3722
2024-11-22 12:36:41: [2024-11-22 12:36:41] iter = 0780, loss = 30.9472
2024-11-22 12:38:39: [2024-11-22 12:38:39] iter = 0790, loss = 30.4874
2024-11-22 12:40:39: [2024-11-22 12:40:39] iter = 0800, loss = 29.6282
2024-11-22 12:42:39: [2024-11-22 12:42:39] iter = 0810, loss = 30.0318
2024-11-22 12:44:40: [2024-11-22 12:44:40] iter = 0820, loss = 29.5261
2024-11-22 12:46:38: [2024-11-22 12:46:38] iter = 0830, loss = 28.1263
2024-11-22 12:48:34: [2024-11-22 12:48:34] iter = 0840, loss = 29.4404
2024-11-22 12:50:21: [2024-11-22 12:50:21] iter = 0850, loss = 28.4025
2024-11-22 12:52:10: [2024-11-22 12:52:10] iter = 0860, loss = 29.1485
2024-11-22 12:54:07: [2024-11-22 12:54:07] iter = 0870, loss = 28.4391
2024-11-22 12:56:05: [2024-11-22 12:56:05] iter = 0880, loss = 27.5267
2024-11-22 12:58:04: [2024-11-22 12:58:04] iter = 0890, loss = 29.8596
2024-11-22 13:00:00: [2024-11-22 13:00:00] iter = 0900, loss = 29.4522
2024-11-22 13:01:58: [2024-11-22 13:01:58] iter = 0910, loss = 27.8225
2024-11-22 13:03:55: [2024-11-22 13:03:55] iter = 0920, loss = 27.9456
2024-11-22 13:05:53: [2024-11-22 13:05:53] iter = 0930, loss = 30.4168
2024-11-22 13:07:50: [2024-11-22 13:07:50] iter = 0940, loss = 30.9909
2024-11-22 13:09:30: [2024-11-22 13:09:30] iter = 0950, loss = 28.7903
2024-11-22 13:10:51: [2024-11-22 13:10:51] iter = 0960, loss = 29.1791
2024-11-22 13:12:10: [2024-11-22 13:12:10] iter = 0970, loss = 30.1094
2024-11-22 13:13:29: [2024-11-22 13:13:29] iter = 0980, loss = 31.1985
2024-11-22 13:14:49: [2024-11-22 13:14:49] iter = 0990, loss = 28.7090
2024-11-22 13:16:02: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 13:16:02: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 13:16:18: Evaluate 5 random ConvNet, ACCmean = 0.8003 ACCstd = 0.0041
-------------------------
2024-11-22 13:16:18: Evaluate 5 random ConvNet, F1mean = 0.7808 F!std = 0.0046
-------------------------
2024-11-22 13:16:26: [2024-11-22 13:16:26] iter = 1000, loss = 28.0713
2024-11-22 13:16:26: 
==================== Final Results ====================

2024-11-22 13:16:26: Run 5 experiments, train on ConvNet, evaluate 25 random ConvNet, mean  = 79.90%  std = 0.33%

[2024-11-22 06:23:33] Evaluate_00: epoch = 0300 train time = 5 s train loss = 0.005393 train acc = 1.0000, test acc = 0.0486, test_sen =0.0486, test_spe =0.9057, test_f1 =0.0412
[2024-11-22 06:23:39] Evaluate_01: epoch = 0300 train time = 5 s train loss = 0.005393 train acc = 1.0000, test acc = 0.0623, test_sen =0.0628, test_spe =0.9075, test_f1 =0.0561
[2024-11-22 06:23:45] Evaluate_02: epoch = 0300 train time = 5 s train loss = 0.005331 train acc = 1.0000, test acc = 0.0744, test_sen =0.0812, test_spe =0.9089, test_f1 =0.0579
[2024-11-22 06:23:52] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.005390 train acc = 1.0000, test acc = 0.0578, test_sen =0.0608, test_spe =0.9072, test_f1 =0.0548
[2024-11-22 06:23:58] Evaluate_04: epoch = 0300 train time = 4 s train loss = 0.005348 train acc = 1.0000, test acc = 0.0889, test_sen =0.0775, test_spe =0.9090, test_f1 =0.0477
[2024-11-22 08:12:02] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.010461 train acc = 1.0000, test acc = 0.7896, test_sen =0.7853, test_spe =0.9790, test_f1 =0.7683
[2024-11-22 08:12:07] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.010306 train acc = 1.0000, test acc = 0.7899, test_sen =0.7843, test_spe =0.9790, test_f1 =0.7691
[2024-11-22 08:12:13] Evaluate_02: epoch = 0300 train time = 5 s train loss = 0.010515 train acc = 1.0000, test acc = 0.7877, test_sen =0.7805, test_spe =0.9787, test_f1 =0.7675
[2024-11-22 08:12:19] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.010218 train acc = 1.0000, test acc = 0.7917, test_sen =0.7870, test_spe =0.9792, test_f1 =0.7719
[2024-11-22 08:12:25] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.010469 train acc = 1.0000, test acc = 0.7911, test_sen =0.7853, test_spe =0.9791, test_f1 =0.7709
[2024-11-22 09:56:23] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.010637 train acc = 1.0000, test acc = 0.7978, test_sen =0.7899, test_spe =0.9798, test_f1 =0.7760
[2024-11-22 09:56:29] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.010829 train acc = 1.0000, test acc = 0.7976, test_sen =0.7914, test_spe =0.9798, test_f1 =0.7768
[2024-11-22 09:56:34] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.010766 train acc = 1.0000, test acc = 0.7983, test_sen =0.7901, test_spe =0.9798, test_f1 =0.7776
[2024-11-22 09:56:40] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.010387 train acc = 1.0000, test acc = 0.8006, test_sen =0.7917, test_spe =0.9801, test_f1 =0.7784
[2024-11-22 09:56:45] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.010430 train acc = 1.0000, test acc = 0.7989, test_sen =0.7892, test_spe =0.9799, test_f1 =0.7764
[2024-11-22 09:57:03] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.005394 train acc = 1.0000, test acc = 0.0417, test_sen =0.0431, test_spe =0.9053, test_f1 =0.0370
[2024-11-22 09:57:08] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.005402 train acc = 1.0000, test acc = 0.0719, test_sen =0.0678, test_spe =0.9077, test_f1 =0.0489
[2024-11-22 09:57:14] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.005190 train acc = 1.0000, test acc = 0.0469, test_sen =0.0438, test_spe =0.9059, test_f1 =0.0397
[2024-11-22 09:57:19] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.005186 train acc = 1.0000, test acc = 0.0701, test_sen =0.0707, test_spe =0.9076, test_f1 =0.0493
[2024-11-22 09:57:24] Evaluate_04: epoch = 0300 train time = 4 s train loss = 0.005504 train acc = 1.0000, test acc = 0.0665, test_sen =0.0866, test_spe =0.9086, test_f1 =0.0525
[2024-11-22 11:39:52] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.010435 train acc = 1.0000, test acc = 0.7992, test_sen =0.7916, test_spe =0.9800, test_f1 =0.7769
[2024-11-22 11:39:57] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.010274 train acc = 1.0000, test acc = 0.7942, test_sen =0.7864, test_spe =0.9795, test_f1 =0.7716
[2024-11-22 11:40:02] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.010076 train acc = 1.0000, test acc = 0.7952, test_sen =0.7855, test_spe =0.9796, test_f1 =0.7718
[2024-11-22 11:40:08] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.010406 train acc = 1.0000, test acc = 0.7944, test_sen =0.7852, test_spe =0.9795, test_f1 =0.7712
[2024-11-22 11:40:12] Evaluate_04: epoch = 0300 train time = 3 s train loss = 0.010392 train acc = 1.0000, test acc = 0.7977, test_sen =0.7881, test_spe =0.9798, test_f1 =0.7742
[2024-11-22 13:16:05] Evaluate_00: epoch = 0300 train time = 2 s train loss = 0.010423 train acc = 1.0000, test acc = 0.7933, test_sen =0.7883, test_spe =0.9794, test_f1 =0.7732
[2024-11-22 13:16:09] Evaluate_01: epoch = 0300 train time = 2 s train loss = 0.010732 train acc = 1.0000, test acc = 0.8020, test_sen =0.7967, test_spe =0.9802, test_f1 =0.7825
[2024-11-22 13:16:12] Evaluate_02: epoch = 0300 train time = 3 s train loss = 0.010467 train acc = 1.0000, test acc = 0.8032, test_sen =0.7945, test_spe =0.9803, test_f1 =0.7827
[2024-11-22 13:16:15] Evaluate_03: epoch = 0300 train time = 2 s train loss = 0.010530 train acc = 1.0000, test acc = 0.8049, test_sen =0.7994, test_spe =0.9805, test_f1 =0.7867
[2024-11-22 13:16:18] Evaluate_04: epoch = 0300 train time = 2 s train loss = 0.010599 train acc = 1.0000, test acc = 0.7982, test_sen =0.7926, test_spe =0.9798, test_f1 =0.7786
