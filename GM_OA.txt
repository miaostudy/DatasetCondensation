nohup: ignoring input
2024-11-21 18:29:06: eval_it_pool: [0, 500, 1000]
2024-11-21 18:29:07: 
================== Exp 0 ==================
 
2024-11-21 18:29:07: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganAMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f12754f3a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganAMNIST (INFO)>}
2024-11-21 18:29:07: Evaluation model pool: ['ConvNet']
2024-11-21 18:29:09: class c = 0: 1956 real images
2024-11-21 18:29:09: class c = 1: 1390 real images
2024-11-21 18:29:09: class c = 2: 1357 real images
2024-11-21 18:29:09: class c = 3: 1474 real images
2024-11-21 18:29:09: class c = 4: 3963 real images
2024-11-21 18:29:09: class c = 5: 3817 real images
2024-11-21 18:29:09: class c = 6: 6164 real images
2024-11-21 18:29:09: class c = 7: 3919 real images
2024-11-21 18:29:09: class c = 8: 3929 real images
2024-11-21 18:29:09: class c = 9: 3031 real images
2024-11-21 18:29:09: class c = 10: 3561 real images
2024-11-21 18:29:09: real images channel 0, mean = 0.4680, std = 0.2974
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
main_base.py:125: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975312/work/torch/csrc/utils/tensor_new.cpp:230.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 18:29:09: initialize synthetic data from random noise
2024-11-21 18:29:09: [2024-11-21 18:29:09] training begins
2024-11-21 18:29:09: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 18:29:09: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 18:29:20: Evaluate 5 random ConvNet, ACCmean = 0.1234 ACCstd = 0.0113
-------------------------
2024-11-21 18:29:20: Evaluate 5 random ConvNet, F1mean = 0.0809 F!std = 0.0109
-------------------------
2024-11-21 18:29:24: [2024-11-21 18:29:24] iter = 0000, loss = 294.3377
2024-11-21 18:30:56: [2024-11-21 18:30:56] iter = 0010, loss = 157.6937
2024-11-21 18:32:56: [2024-11-21 18:32:56] iter = 0020, loss = 93.2015
2024-11-21 18:34:54: [2024-11-21 18:34:54] iter = 0030, loss = 74.4149
2024-11-21 18:36:56: [2024-11-21 18:36:56] iter = 0040, loss = 64.7610
2024-11-21 18:38:57: [2024-11-21 18:38:57] iter = 0050, loss = 58.5096
2024-11-21 18:40:58: [2024-11-21 18:40:58] iter = 0060, loss = 59.8320
2024-11-21 18:42:59: [2024-11-21 18:42:59] iter = 0070, loss = 51.7639
2024-11-21 18:44:59: [2024-11-21 18:44:59] iter = 0080, loss = 49.3531
2024-11-21 18:47:01: [2024-11-21 18:47:01] iter = 0090, loss = 48.5863
2024-11-21 18:49:02: [2024-11-21 18:49:02] iter = 0100, loss = 46.4598
2024-11-21 18:51:05: [2024-11-21 18:51:05] iter = 0110, loss = 46.1572
2024-11-21 18:53:08: [2024-11-21 18:53:08] iter = 0120, loss = 44.8590
2024-11-21 18:55:08: [2024-11-21 18:55:08] iter = 0130, loss = 42.1889
2024-11-21 18:57:07: [2024-11-21 18:57:07] iter = 0140, loss = 42.1866
2024-11-21 18:59:05: [2024-11-21 18:59:05] iter = 0150, loss = 40.4724
2024-11-21 19:01:05: [2024-11-21 19:01:05] iter = 0160, loss = 39.6392
2024-11-21 19:03:03: [2024-11-21 19:03:03] iter = 0170, loss = 39.5125
2024-11-21 19:05:00: [2024-11-21 19:05:00] iter = 0180, loss = 38.9648
2024-11-21 19:07:01: [2024-11-21 19:07:01] iter = 0190, loss = 40.5217
2024-11-21 19:09:04: [2024-11-21 19:09:04] iter = 0200, loss = 38.2795
2024-11-21 19:11:04: [2024-11-21 19:11:04] iter = 0210, loss = 35.9523
2024-11-21 19:13:07: [2024-11-21 19:13:07] iter = 0220, loss = 36.1909
2024-11-21 19:15:05: [2024-11-21 19:15:05] iter = 0230, loss = 36.1919
2024-11-21 19:17:07: [2024-11-21 19:17:07] iter = 0240, loss = 36.3850
2024-11-21 19:19:08: [2024-11-21 19:19:08] iter = 0250, loss = 33.6769
2024-11-21 19:21:11: [2024-11-21 19:21:11] iter = 0260, loss = 34.6397
2024-11-21 19:23:12: [2024-11-21 19:23:12] iter = 0270, loss = 36.0838
2024-11-21 19:25:10: [2024-11-21 19:25:10] iter = 0280, loss = 33.0307
2024-11-21 19:27:11: [2024-11-21 19:27:11] iter = 0290, loss = 34.4970
2024-11-21 19:29:12: [2024-11-21 19:29:12] iter = 0300, loss = 32.5516
2024-11-21 19:31:11: [2024-11-21 19:31:11] iter = 0310, loss = 36.3848
2024-11-21 19:33:09: [2024-11-21 19:33:09] iter = 0320, loss = 34.7420
2024-11-21 19:35:08: [2024-11-21 19:35:08] iter = 0330, loss = 35.5887
2024-11-21 19:37:07: [2024-11-21 19:37:07] iter = 0340, loss = 33.2775
2024-11-21 19:39:07: [2024-11-21 19:39:07] iter = 0350, loss = 34.9679
2024-11-21 19:41:07: [2024-11-21 19:41:07] iter = 0360, loss = 32.6410
2024-11-21 19:43:08: [2024-11-21 19:43:08] iter = 0370, loss = 34.5794
2024-11-21 19:45:06: [2024-11-21 19:45:06] iter = 0380, loss = 33.1088
2024-11-21 19:47:06: [2024-11-21 19:47:06] iter = 0390, loss = 34.0521
2024-11-21 19:49:05: [2024-11-21 19:49:05] iter = 0400, loss = 31.9788
2024-11-21 19:51:03: [2024-11-21 19:51:03] iter = 0410, loss = 32.1334
2024-11-21 19:53:02: [2024-11-21 19:53:02] iter = 0420, loss = 31.6004
2024-11-21 19:55:01: [2024-11-21 19:55:01] iter = 0430, loss = 32.2346
2024-11-21 19:57:00: [2024-11-21 19:57:00] iter = 0440, loss = 33.1952
2024-11-21 19:58:59: [2024-11-21 19:58:59] iter = 0450, loss = 31.1828
2024-11-21 20:00:57: [2024-11-21 20:00:57] iter = 0460, loss = 32.2830
2024-11-21 20:02:56: [2024-11-21 20:02:56] iter = 0470, loss = 32.5660
2024-11-21 20:04:55: [2024-11-21 20:04:55] iter = 0480, loss = 31.8856
2024-11-21 20:06:53: [2024-11-21 20:06:53] iter = 0490, loss = 31.9569
2024-11-21 20:08:43: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-21 20:08:43: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 20:09:06: Evaluate 5 random ConvNet, ACCmean = 0.7569 ACCstd = 0.0032
-------------------------
2024-11-21 20:09:06: Evaluate 5 random ConvNet, F1mean = 0.7360 F!std = 0.0035
-------------------------
2024-11-21 20:09:18: [2024-11-21 20:09:18] iter = 0500, loss = 34.3475
2024-11-21 20:11:19: [2024-11-21 20:11:19] iter = 0510, loss = 31.2946
2024-11-21 20:13:20: [2024-11-21 20:13:20] iter = 0520, loss = 32.9552
2024-11-21 20:15:19: [2024-11-21 20:15:19] iter = 0530, loss = 30.6139
2024-11-21 20:17:18: [2024-11-21 20:17:18] iter = 0540, loss = 32.8919
2024-11-21 20:19:20: [2024-11-21 20:19:20] iter = 0550, loss = 31.7603
2024-11-21 20:21:22: [2024-11-21 20:21:22] iter = 0560, loss = 33.0083
2024-11-21 20:23:24: [2024-11-21 20:23:24] iter = 0570, loss = 30.7628
2024-11-21 20:25:27: [2024-11-21 20:25:27] iter = 0580, loss = 32.3547
2024-11-21 20:27:29: [2024-11-21 20:27:29] iter = 0590, loss = 30.8399
2024-11-21 20:29:31: [2024-11-21 20:29:31] iter = 0600, loss = 30.5673
2024-11-21 20:31:32: [2024-11-21 20:31:32] iter = 0610, loss = 32.6647
2024-11-21 20:33:31: [2024-11-21 20:33:31] iter = 0620, loss = 31.6579
2024-11-21 20:35:30: [2024-11-21 20:35:30] iter = 0630, loss = 31.1316
2024-11-21 20:37:29: [2024-11-21 20:37:29] iter = 0640, loss = 31.0905
2024-11-21 20:39:31: [2024-11-21 20:39:31] iter = 0650, loss = 32.1888
2024-11-21 20:41:29: [2024-11-21 20:41:29] iter = 0660, loss = 33.2077
2024-11-21 20:43:30: [2024-11-21 20:43:30] iter = 0670, loss = 32.2037
2024-11-21 20:45:39: [2024-11-21 20:45:39] iter = 0680, loss = 31.2003
2024-11-21 20:47:59: [2024-11-21 20:47:59] iter = 0690, loss = 30.7808
2024-11-21 20:50:33: [2024-11-21 20:50:33] iter = 0700, loss = 33.0182
2024-11-21 20:52:54: [2024-11-21 20:52:54] iter = 0710, loss = 31.2037
2024-11-21 20:55:07: [2024-11-21 20:55:07] iter = 0720, loss = 31.3836
2024-11-21 20:57:35: [2024-11-21 20:57:35] iter = 0730, loss = 32.5623
2024-11-21 20:59:58: [2024-11-21 20:59:58] iter = 0740, loss = 31.6739
2024-11-21 21:02:20: [2024-11-21 21:02:20] iter = 0750, loss = 29.4603
2024-11-21 21:04:54: [2024-11-21 21:04:54] iter = 0760, loss = 31.3686
2024-11-21 21:07:26: [2024-11-21 21:07:26] iter = 0770, loss = 31.6644
2024-11-21 21:09:39: [2024-11-21 21:09:39] iter = 0780, loss = 31.9023
2024-11-21 21:12:12: [2024-11-21 21:12:12] iter = 0790, loss = 31.2011
2024-11-21 21:14:38: [2024-11-21 21:14:38] iter = 0800, loss = 31.6998
2024-11-21 21:16:54: [2024-11-21 21:16:54] iter = 0810, loss = 30.2302
2024-11-21 21:19:22: [2024-11-21 21:19:22] iter = 0820, loss = 30.7668
2024-11-21 21:21:51: [2024-11-21 21:21:51] iter = 0830, loss = 31.0679
2024-11-21 21:24:19: [2024-11-21 21:24:19] iter = 0840, loss = 31.8845
2024-11-21 21:26:54: [2024-11-21 21:26:54] iter = 0850, loss = 30.4389
2024-11-21 21:29:19: [2024-11-21 21:29:19] iter = 0860, loss = 31.7839
2024-11-21 21:31:44: [2024-11-21 21:31:44] iter = 0870, loss = 32.0100
2024-11-21 21:34:03: [2024-11-21 21:34:03] iter = 0880, loss = 30.7151
2024-11-21 21:36:31: [2024-11-21 21:36:31] iter = 0890, loss = 31.1032
2024-11-21 21:38:49: [2024-11-21 21:38:49] iter = 0900, loss = 30.6020
2024-11-21 21:41:12: [2024-11-21 21:41:12] iter = 0910, loss = 31.2244
2024-11-21 21:43:45: [2024-11-21 21:43:45] iter = 0920, loss = 30.7345
2024-11-21 21:46:06: [2024-11-21 21:46:05] iter = 0930, loss = 30.4973
2024-11-21 21:48:32: [2024-11-21 21:48:32] iter = 0940, loss = 30.9601
2024-11-21 21:50:57: [2024-11-21 21:50:57] iter = 0950, loss = 30.5490
2024-11-21 21:53:28: [2024-11-21 21:53:28] iter = 0960, loss = 30.0985
2024-11-21 21:55:58: [2024-11-21 21:55:57] iter = 0970, loss = 32.4450
2024-11-21 21:58:31: [2024-11-21 21:58:31] iter = 0980, loss = 30.5466
2024-11-21 22:01:08: [2024-11-21 22:01:08] iter = 0990, loss = 30.4139
2024-11-21 22:03:20: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-21 22:03:20: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 22:03:57: Evaluate 5 random ConvNet, ACCmean = 0.7652 ACCstd = 0.0011
-------------------------
2024-11-21 22:03:57: Evaluate 5 random ConvNet, F1mean = 0.7458 F!std = 0.0015
-------------------------
2024-11-21 22:04:13: [2024-11-21 22:04:13] iter = 1000, loss = 31.0948
2024-11-21 22:04:13: 
================== Exp 1 ==================
 
2024-11-21 22:04:13: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganAMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f12754f3a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganAMNIST (INFO)>, 'dc_aug_param': None}
2024-11-21 22:04:13: Evaluation model pool: ['ConvNet']
2024-11-21 22:04:18: class c = 0: 1956 real images
2024-11-21 22:04:18: class c = 1: 1390 real images
2024-11-21 22:04:18: class c = 2: 1357 real images
2024-11-21 22:04:18: class c = 3: 1474 real images
2024-11-21 22:04:18: class c = 4: 3963 real images
2024-11-21 22:04:18: class c = 5: 3817 real images
2024-11-21 22:04:18: class c = 6: 6164 real images
2024-11-21 22:04:18: class c = 7: 3919 real images
2024-11-21 22:04:18: class c = 8: 3929 real images
2024-11-21 22:04:18: class c = 9: 3031 real images
2024-11-21 22:04:18: class c = 10: 3561 real images
2024-11-21 22:04:18: real images channel 0, mean = 0.4680, std = 0.2974
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 22:04:18: initialize synthetic data from random noise
2024-11-21 22:04:18: [2024-11-21 22:04:18] training begins
2024-11-21 22:04:18: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 22:04:18: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 22:04:59: Evaluate 5 random ConvNet, ACCmean = 0.0818 ACCstd = 0.0147
-------------------------
2024-11-21 22:04:59: Evaluate 5 random ConvNet, F1mean = 0.0527 F!std = 0.0065
-------------------------
2024-11-21 22:05:15: [2024-11-21 22:05:15] iter = 0000, loss = 299.3118
2024-11-21 22:07:46: [2024-11-21 22:07:46] iter = 0010, loss = 143.1728
2024-11-21 22:10:11: [2024-11-21 22:10:11] iter = 0020, loss = 96.6867
2024-11-21 22:12:31: [2024-11-21 22:12:31] iter = 0030, loss = 74.5256
2024-11-21 22:15:02: [2024-11-21 22:15:02] iter = 0040, loss = 65.4739
2024-11-21 22:17:33: [2024-11-21 22:17:33] iter = 0050, loss = 63.2661
2024-11-21 22:20:00: [2024-11-21 22:20:00] iter = 0060, loss = 54.8852
2024-11-21 22:22:31: [2024-11-21 22:22:31] iter = 0070, loss = 52.3015
2024-11-21 22:25:02: [2024-11-21 22:25:02] iter = 0080, loss = 50.8019
2024-11-21 22:27:33: [2024-11-21 22:27:33] iter = 0090, loss = 46.9754
2024-11-21 22:30:08: [2024-11-21 22:30:08] iter = 0100, loss = 45.7560
2024-11-21 22:32:40: [2024-11-21 22:32:40] iter = 0110, loss = 46.0790
2024-11-21 22:35:13: [2024-11-21 22:35:13] iter = 0120, loss = 43.6316
2024-11-21 22:37:38: [2024-11-21 22:37:38] iter = 0130, loss = 42.4345
2024-11-21 22:40:12: [2024-11-21 22:40:12] iter = 0140, loss = 41.0096
2024-11-21 22:42:50: [2024-11-21 22:42:50] iter = 0150, loss = 41.6056
2024-11-21 22:45:18: [2024-11-21 22:45:18] iter = 0160, loss = 40.3244
2024-11-21 22:47:56: [2024-11-21 22:47:56] iter = 0170, loss = 40.0661
2024-11-21 22:50:20: [2024-11-21 22:50:20] iter = 0180, loss = 38.4961
2024-11-21 22:52:53: [2024-11-21 22:52:53] iter = 0190, loss = 37.9425
2024-11-21 22:55:25: [2024-11-21 22:55:25] iter = 0200, loss = 38.2329
2024-11-21 22:57:57: [2024-11-21 22:57:57] iter = 0210, loss = 37.7653
2024-11-21 23:00:28: [2024-11-21 23:00:28] iter = 0220, loss = 36.3596
2024-11-21 23:02:47: [2024-11-21 23:02:47] iter = 0230, loss = 35.9969
2024-11-21 23:05:13: [2024-11-21 23:05:13] iter = 0240, loss = 35.6806
2024-11-21 23:07:49: [2024-11-21 23:07:49] iter = 0250, loss = 36.9791
2024-11-21 23:10:14: [2024-11-21 23:10:14] iter = 0260, loss = 38.1232
2024-11-21 23:12:41: [2024-11-21 23:12:41] iter = 0270, loss = 34.4859
2024-11-21 23:15:10: [2024-11-21 23:15:10] iter = 0280, loss = 34.0944
2024-11-21 23:17:40: [2024-11-21 23:17:40] iter = 0290, loss = 35.3390
2024-11-21 23:20:12: [2024-11-21 23:20:12] iter = 0300, loss = 34.8177
2024-11-21 23:22:29: [2024-11-21 23:22:29] iter = 0310, loss = 35.4786
2024-11-21 23:24:58: [2024-11-21 23:24:58] iter = 0320, loss = 34.1713
2024-11-21 23:27:26: [2024-11-21 23:27:26] iter = 0330, loss = 32.3215
2024-11-21 23:29:49: [2024-11-21 23:29:49] iter = 0340, loss = 33.4467
2024-11-21 23:32:21: [2024-11-21 23:32:21] iter = 0350, loss = 35.5874
2024-11-21 23:34:48: [2024-11-21 23:34:48] iter = 0360, loss = 34.4156
2024-11-21 23:37:21: [2024-11-21 23:37:21] iter = 0370, loss = 33.2811
2024-11-21 23:39:53: [2024-11-21 23:39:53] iter = 0380, loss = 34.2053
2024-11-21 23:42:17: [2024-11-21 23:42:17] iter = 0390, loss = 33.7885
2024-11-21 23:44:53: [2024-11-21 23:44:53] iter = 0400, loss = 34.6662
2024-11-21 23:47:28: [2024-11-21 23:47:28] iter = 0410, loss = 32.7588
2024-11-21 23:50:03: [2024-11-21 23:50:03] iter = 0420, loss = 33.1221
2024-11-21 23:52:47: [2024-11-21 23:52:47] iter = 0430, loss = 33.3990
2024-11-21 23:55:19: [2024-11-21 23:55:19] iter = 0440, loss = 34.9379
2024-11-21 23:57:53: [2024-11-21 23:57:53] iter = 0450, loss = 32.8020
2024-11-22 00:00:35: [2024-11-22 00:00:35] iter = 0460, loss = 32.1002
2024-11-22 00:03:12: [2024-11-22 00:03:12] iter = 0470, loss = 32.3040
2024-11-22 00:05:47: [2024-11-22 00:05:47] iter = 0480, loss = 31.6609
2024-11-22 00:08:14: [2024-11-22 00:08:14] iter = 0490, loss = 32.1235
2024-11-22 00:10:26: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 00:10:26: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 00:11:05: Evaluate 5 random ConvNet, ACCmean = 0.7587 ACCstd = 0.0027
-------------------------
2024-11-22 00:11:05: Evaluate 5 random ConvNet, F1mean = 0.7384 F!std = 0.0032
-------------------------
2024-11-22 00:11:19: [2024-11-22 00:11:19] iter = 0500, loss = 31.2249
2024-11-22 00:13:52: [2024-11-22 00:13:52] iter = 0510, loss = 32.5522
2024-11-22 00:16:25: [2024-11-22 00:16:25] iter = 0520, loss = 31.2517
2024-11-22 00:18:50: [2024-11-22 00:18:50] iter = 0530, loss = 34.8410
2024-11-22 00:21:19: [2024-11-22 00:21:19] iter = 0540, loss = 32.6626
2024-11-22 00:23:43: [2024-11-22 00:23:43] iter = 0550, loss = 33.1853
2024-11-22 00:26:17: [2024-11-22 00:26:17] iter = 0560, loss = 31.5000
2024-11-22 00:28:47: [2024-11-22 00:28:47] iter = 0570, loss = 30.9361
2024-11-22 00:31:19: [2024-11-22 00:31:19] iter = 0580, loss = 32.1180
2024-11-22 00:33:49: [2024-11-22 00:33:49] iter = 0590, loss = 31.4805
2024-11-22 00:36:16: [2024-11-22 00:36:16] iter = 0600, loss = 32.6055
2024-11-22 00:38:53: [2024-11-22 00:38:53] iter = 0610, loss = 32.2844
2024-11-22 00:41:23: [2024-11-22 00:41:23] iter = 0620, loss = 29.9819
2024-11-22 00:43:54: [2024-11-22 00:43:54] iter = 0630, loss = 31.3295
2024-11-22 00:46:34: [2024-11-22 00:46:34] iter = 0640, loss = 33.1688
2024-11-22 00:49:01: [2024-11-22 00:49:01] iter = 0650, loss = 29.8430
2024-11-22 00:51:26: [2024-11-22 00:51:26] iter = 0660, loss = 31.4801
2024-11-22 00:53:50: [2024-11-22 00:53:50] iter = 0670, loss = 32.8658
2024-11-22 00:56:16: [2024-11-22 00:56:16] iter = 0680, loss = 30.5608
2024-11-22 00:58:41: [2024-11-22 00:58:41] iter = 0690, loss = 32.2524
2024-11-22 01:01:16: [2024-11-22 01:01:16] iter = 0700, loss = 33.3310
2024-11-22 01:03:41: [2024-11-22 01:03:41] iter = 0710, loss = 31.1934
2024-11-22 01:06:09: [2024-11-22 01:06:09] iter = 0720, loss = 32.3356
2024-11-22 01:08:37: [2024-11-22 01:08:37] iter = 0730, loss = 31.0959
2024-11-22 01:11:10: [2024-11-22 01:11:10] iter = 0740, loss = 31.6084
2024-11-22 01:13:36: [2024-11-22 01:13:36] iter = 0750, loss = 31.0619
2024-11-22 01:16:02: [2024-11-22 01:16:02] iter = 0760, loss = 30.4199
2024-11-22 01:18:20: [2024-11-22 01:18:20] iter = 0770, loss = 32.7505
2024-11-22 01:20:55: [2024-11-22 01:20:55] iter = 0780, loss = 32.2478
2024-11-22 01:23:24: [2024-11-22 01:23:24] iter = 0790, loss = 29.0977
2024-11-22 01:25:49: [2024-11-22 01:25:49] iter = 0800, loss = 32.1006
2024-11-22 01:28:21: [2024-11-22 01:28:21] iter = 0810, loss = 32.1959
2024-11-22 01:30:56: [2024-11-22 01:30:56] iter = 0820, loss = 31.0688
2024-11-22 01:33:23: [2024-11-22 01:33:23] iter = 0830, loss = 30.6552
2024-11-22 01:35:58: [2024-11-22 01:35:58] iter = 0840, loss = 30.2188
2024-11-22 01:38:34: [2024-11-22 01:38:34] iter = 0850, loss = 31.7094
2024-11-22 01:41:00: [2024-11-22 01:41:00] iter = 0860, loss = 29.6289
2024-11-22 01:43:29: [2024-11-22 01:43:29] iter = 0870, loss = 31.1627
2024-11-22 01:46:02: [2024-11-22 01:46:02] iter = 0880, loss = 31.4954
2024-11-22 01:48:34: [2024-11-22 01:48:34] iter = 0890, loss = 32.5049
2024-11-22 01:51:00: [2024-11-22 01:51:00] iter = 0900, loss = 30.4967
2024-11-22 01:53:31: [2024-11-22 01:53:31] iter = 0910, loss = 30.7914
2024-11-22 01:56:04: [2024-11-22 01:56:04] iter = 0920, loss = 31.0376
2024-11-22 01:58:32: [2024-11-22 01:58:32] iter = 0930, loss = 31.2141
2024-11-22 02:01:02: [2024-11-22 02:01:02] iter = 0940, loss = 30.4898
2024-11-22 02:03:31: [2024-11-22 02:03:31] iter = 0950, loss = 29.9578
2024-11-22 02:06:02: [2024-11-22 02:06:02] iter = 0960, loss = 30.3216
2024-11-22 02:08:42: [2024-11-22 02:08:42] iter = 0970, loss = 31.5141
2024-11-22 02:11:22: [2024-11-22 02:11:22] iter = 0980, loss = 30.6440
2024-11-22 02:13:48: [2024-11-22 02:13:48] iter = 0990, loss = 31.7608
2024-11-22 02:16:05: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 02:16:05: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 02:16:44: Evaluate 5 random ConvNet, ACCmean = 0.7580 ACCstd = 0.0031
-------------------------
2024-11-22 02:16:44: Evaluate 5 random ConvNet, F1mean = 0.7355 F!std = 0.0034
-------------------------
2024-11-22 02:16:57: [2024-11-22 02:16:57] iter = 1000, loss = 32.1735
2024-11-22 02:16:57: 
================== Exp 2 ==================
 
2024-11-22 02:16:57: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganAMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f12754f3a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganAMNIST (INFO)>, 'dc_aug_param': None}
2024-11-22 02:16:57: Evaluation model pool: ['ConvNet']
2024-11-22 02:17:02: class c = 0: 1956 real images
2024-11-22 02:17:02: class c = 1: 1390 real images
2024-11-22 02:17:02: class c = 2: 1357 real images
2024-11-22 02:17:02: class c = 3: 1474 real images
2024-11-22 02:17:02: class c = 4: 3963 real images
2024-11-22 02:17:02: class c = 5: 3817 real images
2024-11-22 02:17:02: class c = 6: 6164 real images
2024-11-22 02:17:02: class c = 7: 3919 real images
2024-11-22 02:17:02: class c = 8: 3929 real images
2024-11-22 02:17:02: class c = 9: 3031 real images
2024-11-22 02:17:02: class c = 10: 3561 real images
2024-11-22 02:17:02: real images channel 0, mean = 0.4680, std = 0.2974
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-22 02:17:02: initialize synthetic data from random noise
2024-11-22 02:17:02: [2024-11-22 02:17:02] training begins
2024-11-22 02:17:02: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-22 02:17:02: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 02:17:42: Evaluate 5 random ConvNet, ACCmean = 0.1978 ACCstd = 0.0225
-------------------------
2024-11-22 02:17:42: Evaluate 5 random ConvNet, F1mean = 0.1082 F!std = 0.0044
-------------------------
2024-11-22 02:17:56: [2024-11-22 02:17:56] iter = 0000, loss = 290.8170
2024-11-22 02:20:28: [2024-11-22 02:20:28] iter = 0010, loss = 160.4355
2024-11-22 02:22:57: [2024-11-22 02:22:57] iter = 0020, loss = 103.1772
2024-11-22 02:25:21: [2024-11-22 02:25:21] iter = 0030, loss = 77.6938
2024-11-22 02:28:07: [2024-11-22 02:28:07] iter = 0040, loss = 65.8412
2024-11-22 02:30:54: [2024-11-22 02:30:54] iter = 0050, loss = 58.3157
2024-11-22 02:33:40: [2024-11-22 02:33:40] iter = 0060, loss = 53.7768
2024-11-22 02:36:24: [2024-11-22 02:36:24] iter = 0070, loss = 50.1975
2024-11-22 02:39:11: [2024-11-22 02:39:11] iter = 0080, loss = 49.9960
2024-11-22 02:41:57: [2024-11-22 02:41:57] iter = 0090, loss = 45.9460
2024-11-22 02:44:41: [2024-11-22 02:44:41] iter = 0100, loss = 45.7978
2024-11-22 02:47:21: [2024-11-22 02:47:21] iter = 0110, loss = 44.7144
2024-11-22 02:50:02: [2024-11-22 02:50:02] iter = 0120, loss = 42.1890
2024-11-22 02:52:48: [2024-11-22 02:52:48] iter = 0130, loss = 40.4441
2024-11-22 02:55:32: [2024-11-22 02:55:32] iter = 0140, loss = 39.3789
2024-11-22 02:58:19: [2024-11-22 02:58:19] iter = 0150, loss = 40.7404
2024-11-22 03:01:01: [2024-11-22 03:01:01] iter = 0160, loss = 40.2620
2024-11-22 03:03:49: [2024-11-22 03:03:49] iter = 0170, loss = 39.0236
2024-11-22 03:06:32: [2024-11-22 03:06:32] iter = 0180, loss = 40.4711
2024-11-22 03:09:12: [2024-11-22 03:09:12] iter = 0190, loss = 37.0886
2024-11-22 03:11:56: [2024-11-22 03:11:56] iter = 0200, loss = 37.1256
2024-11-22 03:14:46: [2024-11-22 03:14:46] iter = 0210, loss = 34.6370
2024-11-22 03:17:30: [2024-11-22 03:17:30] iter = 0220, loss = 36.5264
2024-11-22 03:20:08: [2024-11-22 03:20:08] iter = 0230, loss = 36.8441
2024-11-22 03:22:34: [2024-11-22 03:22:34] iter = 0240, loss = 34.6780
2024-11-22 03:25:10: [2024-11-22 03:25:10] iter = 0250, loss = 34.9777
2024-11-22 03:27:46: [2024-11-22 03:27:46] iter = 0260, loss = 35.2899
2024-11-22 03:30:16: [2024-11-22 03:30:16] iter = 0270, loss = 36.5806
2024-11-22 03:32:52: [2024-11-22 03:32:52] iter = 0280, loss = 34.5024
2024-11-22 03:35:20: [2024-11-22 03:35:20] iter = 0290, loss = 35.6347
2024-11-22 03:37:55: [2024-11-22 03:37:55] iter = 0300, loss = 33.9368
2024-11-22 03:40:20: [2024-11-22 03:40:20] iter = 0310, loss = 33.2329
2024-11-22 03:42:52: [2024-11-22 03:42:52] iter = 0320, loss = 33.4805
2024-11-22 03:45:19: [2024-11-22 03:45:19] iter = 0330, loss = 34.0687
2024-11-22 03:47:47: [2024-11-22 03:47:47] iter = 0340, loss = 32.5008
2024-11-22 03:50:19: [2024-11-22 03:50:19] iter = 0350, loss = 34.1252
2024-11-22 03:52:46: [2024-11-22 03:52:46] iter = 0360, loss = 33.5526
2024-11-22 03:55:21: [2024-11-22 03:55:21] iter = 0370, loss = 31.4428
2024-11-22 03:57:36: [2024-11-22 03:57:36] iter = 0380, loss = 34.7598
2024-11-22 04:00:05: [2024-11-22 04:00:05] iter = 0390, loss = 33.3521
2024-11-22 04:02:41: [2024-11-22 04:02:41] iter = 0400, loss = 35.3626
2024-11-22 04:05:13: [2024-11-22 04:05:13] iter = 0410, loss = 33.2866
2024-11-22 04:07:40: [2024-11-22 04:07:40] iter = 0420, loss = 31.4480
2024-11-22 04:10:11: [2024-11-22 04:10:11] iter = 0430, loss = 33.7872
2024-11-22 04:12:43: [2024-11-22 04:12:43] iter = 0440, loss = 32.3687
2024-11-22 04:15:13: [2024-11-22 04:15:13] iter = 0450, loss = 32.6993
2024-11-22 04:17:44: [2024-11-22 04:17:44] iter = 0460, loss = 30.2636
2024-11-22 04:20:14: [2024-11-22 04:20:14] iter = 0470, loss = 30.7782
2024-11-22 04:22:51: [2024-11-22 04:22:51] iter = 0480, loss = 33.2531
2024-11-22 04:25:14: [2024-11-22 04:25:14] iter = 0490, loss = 31.7121
2024-11-22 04:27:31: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 04:27:31: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 04:28:10: Evaluate 5 random ConvNet, ACCmean = 0.7577 ACCstd = 0.0038
-------------------------
2024-11-22 04:28:10: Evaluate 5 random ConvNet, F1mean = 0.7362 F!std = 0.0039
-------------------------
2024-11-22 04:28:26: [2024-11-22 04:28:26] iter = 0500, loss = 33.0824
2024-11-22 04:30:53: [2024-11-22 04:30:53] iter = 0510, loss = 33.6901
2024-11-22 04:33:20: [2024-11-22 04:33:20] iter = 0520, loss = 31.2129
2024-11-22 04:35:56: [2024-11-22 04:35:56] iter = 0530, loss = 31.6142
2024-11-22 04:38:26: [2024-11-22 04:38:26] iter = 0540, loss = 31.8048
2024-11-22 04:40:58: [2024-11-22 04:40:58] iter = 0550, loss = 33.2611
2024-11-22 04:43:24: [2024-11-22 04:43:24] iter = 0560, loss = 30.7833
2024-11-22 04:45:59: [2024-11-22 04:45:59] iter = 0570, loss = 31.9662
2024-11-22 04:48:28: [2024-11-22 04:48:28] iter = 0580, loss = 30.0826
2024-11-22 04:50:56: [2024-11-22 04:50:56] iter = 0590, loss = 32.5697
2024-11-22 04:53:30: [2024-11-22 04:53:30] iter = 0600, loss = 32.9272
2024-11-22 04:55:59: [2024-11-22 04:55:59] iter = 0610, loss = 32.6119
2024-11-22 04:58:33: [2024-11-22 04:58:33] iter = 0620, loss = 30.6748
2024-11-22 05:01:01: [2024-11-22 05:01:01] iter = 0630, loss = 31.8019
2024-11-22 05:03:33: [2024-11-22 05:03:33] iter = 0640, loss = 31.1776
2024-11-22 05:06:06: [2024-11-22 05:06:06] iter = 0650, loss = 31.9406
2024-11-22 05:08:35: [2024-11-22 05:08:35] iter = 0660, loss = 32.0741
2024-11-22 05:11:04: [2024-11-22 05:11:04] iter = 0670, loss = 29.2183
2024-11-22 05:13:33: [2024-11-22 05:13:33] iter = 0680, loss = 32.9585
2024-11-22 05:16:07: [2024-11-22 05:16:07] iter = 0690, loss = 32.6428
2024-11-22 05:18:33: [2024-11-22 05:18:33] iter = 0700, loss = 30.7194
2024-11-22 05:21:04: [2024-11-22 05:21:04] iter = 0710, loss = 30.6866
2024-11-22 05:23:19: [2024-11-22 05:23:19] iter = 0720, loss = 31.0773
2024-11-22 05:25:52: [2024-11-22 05:25:52] iter = 0730, loss = 31.6756
2024-11-22 05:28:22: [2024-11-22 05:28:22] iter = 0740, loss = 31.3560
2024-11-22 05:30:52: [2024-11-22 05:30:52] iter = 0750, loss = 29.6725
2024-11-22 05:33:32: [2024-11-22 05:33:32] iter = 0760, loss = 31.4729
2024-11-22 05:36:06: [2024-11-22 05:36:06] iter = 0770, loss = 32.6648
2024-11-22 05:38:42: [2024-11-22 05:38:42] iter = 0780, loss = 32.3834
2024-11-22 05:41:15: [2024-11-22 05:41:15] iter = 0790, loss = 30.1189
2024-11-22 05:43:48: [2024-11-22 05:43:48] iter = 0800, loss = 30.4552
2024-11-22 05:46:13: [2024-11-22 05:46:13] iter = 0810, loss = 32.6814
2024-11-22 05:48:30: [2024-11-22 05:48:30] iter = 0820, loss = 30.3337
2024-11-22 05:50:51: [2024-11-22 05:50:51] iter = 0830, loss = 33.7396
2024-11-22 05:53:06: [2024-11-22 05:53:06] iter = 0840, loss = 30.0960
2024-11-22 05:55:06: [2024-11-22 05:55:06] iter = 0850, loss = 30.5399
2024-11-22 05:57:07: [2024-11-22 05:57:07] iter = 0860, loss = 30.8420
2024-11-22 05:59:23: [2024-11-22 05:59:23] iter = 0870, loss = 31.2707
2024-11-22 06:01:34: [2024-11-22 06:01:34] iter = 0880, loss = 30.4904
2024-11-22 06:03:42: [2024-11-22 06:03:41] iter = 0890, loss = 32.2227
2024-11-22 06:05:52: [2024-11-22 06:05:52] iter = 0900, loss = 31.2567
2024-11-22 06:08:05: [2024-11-22 06:08:05] iter = 0910, loss = 31.8468
2024-11-22 06:10:08: [2024-11-22 06:10:08] iter = 0920, loss = 32.4811
2024-11-22 06:12:20: [2024-11-22 06:12:20] iter = 0930, loss = 30.7234
2024-11-22 06:14:30: [2024-11-22 06:14:30] iter = 0940, loss = 29.8683
2024-11-22 06:16:42: [2024-11-22 06:16:42] iter = 0950, loss = 30.8772
2024-11-22 06:18:45: [2024-11-22 06:18:45] iter = 0960, loss = 30.0680
2024-11-22 06:20:45: [2024-11-22 06:20:45] iter = 0970, loss = 30.6443
2024-11-22 06:22:57: [2024-11-22 06:22:57] iter = 0980, loss = 31.3877
2024-11-22 06:25:08: [2024-11-22 06:25:08] iter = 0990, loss = 30.2321
2024-11-22 06:27:07: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 06:27:07: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
Using downloaded and verified file: /data/users/xiongyuxuan/.medmnist/organamnist.npz
Using downloaded and verified file: /data/users/xiongyuxuan/.medmnist/organamnist.npz
Loaded the dataset:OrganAMNIST
[2024-11-21 18:29:12] Evaluate_00: epoch = 0300 train time = 2 s train loss = 0.005281 train acc = 1.0000, test acc = 0.1204, test_sen =0.1015, test_spe =0.9095, test_f1 =0.0690
[2024-11-21 18:29:14] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.005314 train acc = 1.0000, test acc = 0.1408, test_sen =0.1112, test_spe =0.9110, test_f1 =0.0945
[2024-11-21 18:29:16] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.005248 train acc = 1.0000, test acc = 0.1252, test_sen =0.1152, test_spe =0.9102, test_f1 =0.0901
[2024-11-21 18:29:18] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.005200 train acc = 1.0000, test acc = 0.1252, test_sen =0.1068, test_spe =0.9098, test_f1 =0.0832
[2024-11-21 18:29:20] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.005304 train acc = 1.0000, test acc = 0.1056, test_sen =0.0788, test_spe =0.9071, test_f1 =0.0677
[2024-11-21 20:08:48] Evaluate_00: epoch = 0300 train time = 3 s train loss = 0.009991 train acc = 1.0000, test acc = 0.7626, test_sen =0.7663, test_spe =0.9763, test_f1 =0.7424
[2024-11-21 20:08:52] Evaluate_01: epoch = 0300 train time = 3 s train loss = 0.009855 train acc = 1.0000, test acc = 0.7557, test_sen =0.7580, test_spe =0.9756, test_f1 =0.7330
[2024-11-21 20:08:57] Evaluate_02: epoch = 0300 train time = 3 s train loss = 0.009896 train acc = 1.0000, test acc = 0.7573, test_sen =0.7578, test_spe =0.9757, test_f1 =0.7349
[2024-11-21 20:09:02] Evaluate_03: epoch = 0300 train time = 3 s train loss = 0.010312 train acc = 1.0000, test acc = 0.7528, test_sen =0.7599, test_spe =0.9753, test_f1 =0.7330
[2024-11-21 20:09:06] Evaluate_04: epoch = 0300 train time = 3 s train loss = 0.010635 train acc = 1.0000, test acc = 0.7562, test_sen =0.7606, test_spe =0.9756, test_f1 =0.7365
[2024-11-21 22:03:25] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.011056 train acc = 1.0000, test acc = 0.7646, test_sen =0.7662, test_spe =0.9764, test_f1 =0.7448
[2024-11-21 22:03:32] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.010420 train acc = 1.0000, test acc = 0.7639, test_sen =0.7690, test_spe =0.9764, test_f1 =0.7454
[2024-11-21 22:03:40] Evaluate_02: epoch = 0300 train time = 5 s train loss = 0.010594 train acc = 1.0000, test acc = 0.7648, test_sen =0.7686, test_spe =0.9765, test_f1 =0.7458
[2024-11-21 22:03:49] Evaluate_03: epoch = 0300 train time = 6 s train loss = 0.010084 train acc = 1.0000, test acc = 0.7656, test_sen =0.7665, test_spe =0.9766, test_f1 =0.7442
[2024-11-21 22:03:57] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.010580 train acc = 1.0000, test acc = 0.7672, test_sen =0.7708, test_spe =0.9767, test_f1 =0.7485
[2024-11-21 22:04:25] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.005464 train acc = 1.0000, test acc = 0.0796, test_sen =0.0765, test_spe =0.9065, test_f1 =0.0615
[2024-11-21 22:04:34] Evaluate_01: epoch = 0300 train time = 6 s train loss = 0.005595 train acc = 1.0000, test acc = 0.0974, test_sen =0.0802, test_spe =0.9081, test_f1 =0.0498
[2024-11-21 22:04:43] Evaluate_02: epoch = 0300 train time = 6 s train loss = 0.005385 train acc = 1.0000, test acc = 0.0547, test_sen =0.0441, test_spe =0.9047, test_f1 =0.0422
[2024-11-21 22:04:50] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.005426 train acc = 1.0000, test acc = 0.0902, test_sen =0.0684, test_spe =0.9072, test_f1 =0.0566
[2024-11-21 22:04:59] Evaluate_04: epoch = 0300 train time = 6 s train loss = 0.005479 train acc = 1.0000, test acc = 0.0874, test_sen =0.0747, test_spe =0.9070, test_f1 =0.0531
[2024-11-22 00:10:34] Evaluate_00: epoch = 0300 train time = 5 s train loss = 0.010186 train acc = 1.0000, test acc = 0.7580, test_sen =0.7655, test_spe =0.9758, test_f1 =0.7385
[2024-11-22 00:10:42] Evaluate_01: epoch = 0300 train time = 6 s train loss = 0.010235 train acc = 1.0000, test acc = 0.7571, test_sen =0.7596, test_spe =0.9757, test_f1 =0.7362
[2024-11-22 00:10:49] Evaluate_02: epoch = 0300 train time = 5 s train loss = 0.010380 train acc = 1.0000, test acc = 0.7592, test_sen =0.7613, test_spe =0.9760, test_f1 =0.7370
[2024-11-22 00:10:58] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.009997 train acc = 1.0000, test acc = 0.7636, test_sen =0.7673, test_spe =0.9764, test_f1 =0.7445
[2024-11-22 00:11:05] Evaluate_04: epoch = 0300 train time = 4 s train loss = 0.010445 train acc = 1.0000, test acc = 0.7557, test_sen =0.7581, test_spe =0.9756, test_f1 =0.7361
[2024-11-22 02:16:13] Evaluate_00: epoch = 0300 train time = 5 s train loss = 0.010126 train acc = 1.0000, test acc = 0.7603, test_sen =0.7644, test_spe =0.9760, test_f1 =0.7381
[2024-11-22 02:16:20] Evaluate_01: epoch = 0300 train time = 5 s train loss = 0.010345 train acc = 1.0000, test acc = 0.7598, test_sen =0.7630, test_spe =0.9760, test_f1 =0.7376
[2024-11-22 02:16:28] Evaluate_02: epoch = 0300 train time = 6 s train loss = 0.010489 train acc = 1.0000, test acc = 0.7611, test_sen =0.7663, test_spe =0.9762, test_f1 =0.7387
[2024-11-22 02:16:36] Evaluate_03: epoch = 0300 train time = 6 s train loss = 0.010868 train acc = 1.0000, test acc = 0.7528, test_sen =0.7541, test_spe =0.9753, test_f1 =0.7295
[2024-11-22 02:16:44] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.010742 train acc = 1.0000, test acc = 0.7562, test_sen =0.7578, test_spe =0.9756, test_f1 =0.7339
[2024-11-22 02:17:11] Evaluate_00: epoch = 0300 train time = 6 s train loss = 0.005312 train acc = 1.0000, test acc = 0.2013, test_sen =0.1369, test_spe =0.9167, test_f1 =0.1084
[2024-11-22 02:17:19] Evaluate_01: epoch = 0300 train time = 6 s train loss = 0.005247 train acc = 1.0000, test acc = 0.1666, test_sen =0.1164, test_spe =0.9147, test_f1 =0.1016
[2024-11-22 02:17:28] Evaluate_02: epoch = 0300 train time = 6 s train loss = 0.005261 train acc = 1.0000, test acc = 0.1831, test_sen =0.1227, test_spe =0.9161, test_f1 =0.1070
[2024-11-22 02:17:35] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.005156 train acc = 1.0000, test acc = 0.2046, test_sen =0.1391, test_spe =0.9175, test_f1 =0.1153
[2024-11-22 02:17:42] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.005270 train acc = 1.0000, test acc = 0.2335, test_sen =0.1448, test_spe =0.9193, test_f1 =0.1088
[2024-11-22 04:27:39] Evaluate_00: epoch = 0300 train time = 5 s train loss = 0.010595 train acc = 1.0000, test acc = 0.7625, test_sen =0.7672, test_spe =0.9763, test_f1 =0.7408
[2024-11-22 04:27:47] Evaluate_01: epoch = 0300 train time = 5 s train loss = 0.010179 train acc = 1.0000, test acc = 0.7511, test_sen =0.7532, test_spe =0.9751, test_f1 =0.7291
[2024-11-22 04:27:56] Evaluate_02: epoch = 0300 train time = 6 s train loss = 0.010326 train acc = 1.0000, test acc = 0.7565, test_sen =0.7602, test_spe =0.9756, test_f1 =0.7367
[2024-11-22 04:28:03] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.010295 train acc = 1.0000, test acc = 0.7583, test_sen =0.7643, test_spe =0.9759, test_f1 =0.7376
[2024-11-22 04:28:10] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.009799 train acc = 1.0000, test acc = 0.7602, test_sen =0.7615, test_spe =0.9760, test_f1 =0.7371
[2024-11-22 06:27:13] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.010061 train acc = 1.0000, test acc = 0.7661, test_sen =0.7695, test_spe =0.9766, test_f1 =0.7437
[2024-11-22 06:27:19] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.010377 train acc = 1.0000, test acc = 0.7650, test_sen =0.7684, test_spe =0.9765, test_f1 =0.7439
[2024-11-22 06:27:25] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.010439 train acc = 1.0000, test acc = 0.7540, test_sen =0.7568, test_spe =0.9754, test_f1 =0.7338
[2024-11-22 06:27:30] Evaluate_03: epoch = 0300 train time = 3 s train loss = 0.010259 train acc = 1.0000, test acc = 0.7568, test_sen =0.7626, test_spe =0.9757, test_f1 =0.7359
[2024-11-22 06:27:38] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.009831 train acc = 1.0000, test acc = 0.7528, test_sen =0.7589, test_spe =0.9753, test_f1 =0.73182024-11-22 06:27:38: Evaluate 5 random ConvNet, ACCmean = 0.7589 ACCstd = 0.0055
-------------------------
2024-11-22 06:27:38: Evaluate 5 random ConvNet, F1mean = 0.7378 F!std = 0.0051
-------------------------
2024-11-22 06:27:51: [2024-11-22 06:27:51] iter = 1000, loss = 29.8368
2024-11-22 06:27:51: 
================== Exp 3 ==================
 
2024-11-22 06:27:51: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganAMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f12754f3a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganAMNIST (INFO)>, 'dc_aug_param': None}
2024-11-22 06:27:51: Evaluation model pool: ['ConvNet']
2024-11-22 06:27:54: class c = 0: 1956 real images
2024-11-22 06:27:54: class c = 1: 1390 real images
2024-11-22 06:27:54: class c = 2: 1357 real images
2024-11-22 06:27:54: class c = 3: 1474 real images
2024-11-22 06:27:54: class c = 4: 3963 real images
2024-11-22 06:27:54: class c = 5: 3817 real images
2024-11-22 06:27:54: class c = 6: 6164 real images
2024-11-22 06:27:54: class c = 7: 3919 real images
2024-11-22 06:27:54: class c = 8: 3929 real images
2024-11-22 06:27:54: class c = 9: 3031 real images
2024-11-22 06:27:54: class c = 10: 3561 real images
2024-11-22 06:27:54: real images channel 0, mean = 0.4680, std = 0.2974
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-22 06:27:54: initialize synthetic data from random noise
2024-11-22 06:27:54: [2024-11-22 06:27:54] training begins
2024-11-22 06:27:54: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-22 06:27:54: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 06:28:26: Evaluate 5 random ConvNet, ACCmean = 0.0676 ACCstd = 0.0079
-------------------------
2024-11-22 06:28:26: Evaluate 5 random ConvNet, F1mean = 0.0420 F!std = 0.0038
-------------------------
2024-11-22 06:28:39: [2024-11-22 06:28:39] iter = 0000, loss = 297.3149
2024-11-22 06:30:50: [2024-11-22 06:30:50] iter = 0010, loss = 150.4270
2024-11-22 06:33:01: [2024-11-22 06:33:01] iter = 0020, loss = 94.4676
2024-11-22 06:35:08: [2024-11-22 06:35:08] iter = 0030, loss = 76.3724
2024-11-22 06:37:16: [2024-11-22 06:37:16] iter = 0040, loss = 63.5265
2024-11-22 06:39:27: [2024-11-22 06:39:27] iter = 0050, loss = 58.2229
2024-11-22 06:41:37: [2024-11-22 06:41:37] iter = 0060, loss = 54.5904
2024-11-22 06:43:44: [2024-11-22 06:43:44] iter = 0070, loss = 53.7731
2024-11-22 06:45:52: [2024-11-22 06:45:52] iter = 0080, loss = 48.9900
2024-11-22 06:48:04: [2024-11-22 06:48:04] iter = 0090, loss = 48.6993
2024-11-22 06:50:13: [2024-11-22 06:50:13] iter = 0100, loss = 45.9436
2024-11-22 06:52:24: [2024-11-22 06:52:24] iter = 0110, loss = 42.8006
2024-11-22 06:54:41: [2024-11-22 06:54:41] iter = 0120, loss = 41.6163
2024-11-22 06:56:52: [2024-11-22 06:56:52] iter = 0130, loss = 41.8604
2024-11-22 06:59:07: [2024-11-22 06:59:07] iter = 0140, loss = 41.5564
2024-11-22 07:01:17: [2024-11-22 07:01:17] iter = 0150, loss = 40.8292
2024-11-22 07:03:27: [2024-11-22 07:03:27] iter = 0160, loss = 37.7227
2024-11-22 07:05:40: [2024-11-22 07:05:40] iter = 0170, loss = 39.8991
2024-11-22 07:07:49: [2024-11-22 07:07:49] iter = 0180, loss = 38.6706
2024-11-22 07:10:05: [2024-11-22 07:10:05] iter = 0190, loss = 37.7066
2024-11-22 07:12:11: [2024-11-22 07:12:11] iter = 0200, loss = 35.5662
2024-11-22 07:14:20: [2024-11-22 07:14:20] iter = 0210, loss = 35.5892
2024-11-22 07:16:38: [2024-11-22 07:16:38] iter = 0220, loss = 36.1790
2024-11-22 07:18:50: [2024-11-22 07:18:50] iter = 0230, loss = 34.0460
2024-11-22 07:21:03: [2024-11-22 07:21:03] iter = 0240, loss = 35.9203
2024-11-22 07:23:16: [2024-11-22 07:23:16] iter = 0250, loss = 35.5691
2024-11-22 07:25:26: [2024-11-22 07:25:26] iter = 0260, loss = 34.8153
2024-11-22 07:27:42: [2024-11-22 07:27:42] iter = 0270, loss = 33.5477
2024-11-22 07:29:54: [2024-11-22 07:29:54] iter = 0280, loss = 34.2832
2024-11-22 07:32:07: [2024-11-22 07:32:07] iter = 0290, loss = 34.0580
2024-11-22 07:34:14: [2024-11-22 07:34:14] iter = 0300, loss = 34.3407
2024-11-22 07:36:26: [2024-11-22 07:36:26] iter = 0310, loss = 34.5021
2024-11-22 07:38:34: [2024-11-22 07:38:34] iter = 0320, loss = 31.3806
2024-11-22 07:40:42: [2024-11-22 07:40:42] iter = 0330, loss = 33.0994
2024-11-22 07:42:51: [2024-11-22 07:42:51] iter = 0340, loss = 33.0936
2024-11-22 07:45:02: [2024-11-22 07:45:02] iter = 0350, loss = 34.5879
2024-11-22 07:47:14: [2024-11-22 07:47:14] iter = 0360, loss = 33.2454
2024-11-22 07:49:30: [2024-11-22 07:49:30] iter = 0370, loss = 32.3179
2024-11-22 07:51:51: [2024-11-22 07:51:51] iter = 0380, loss = 33.5184
2024-11-22 07:54:14: [2024-11-22 07:54:14] iter = 0390, loss = 32.0699
2024-11-22 07:56:33: [2024-11-22 07:56:33] iter = 0400, loss = 29.9913
2024-11-22 07:58:39: [2024-11-22 07:58:39] iter = 0410, loss = 34.9879
2024-11-22 08:00:45: [2024-11-22 08:00:45] iter = 0420, loss = 33.1433
2024-11-22 08:02:55: [2024-11-22 08:02:55] iter = 0430, loss = 32.5645
2024-11-22 08:05:04: [2024-11-22 08:05:04] iter = 0440, loss = 33.1122
2024-11-22 08:07:19: [2024-11-22 08:07:19] iter = 0450, loss = 32.8951
2024-11-22 08:09:34: [2024-11-22 08:09:34] iter = 0460, loss = 31.5235
2024-11-22 08:11:45: [2024-11-22 08:11:45] iter = 0470, loss = 33.1715
2024-11-22 08:13:55: [2024-11-22 08:13:55] iter = 0480, loss = 30.1193
2024-11-22 08:16:07: [2024-11-22 08:16:07] iter = 0490, loss = 32.6942
2024-11-22 08:17:59: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 08:17:59: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 08:18:29: Evaluate 5 random ConvNet, ACCmean = 0.7570 ACCstd = 0.0028
-------------------------
2024-11-22 08:18:29: Evaluate 5 random ConvNet, F1mean = 0.7377 F!std = 0.0033
-------------------------
2024-11-22 08:18:41: [2024-11-22 08:18:41] iter = 0500, loss = 31.3661
2024-11-22 08:20:44: [2024-11-22 08:20:44] iter = 0510, loss = 32.2931
2024-11-22 08:22:54: [2024-11-22 08:22:54] iter = 0520, loss = 31.5740
2024-11-22 08:24:59: [2024-11-22 08:24:59] iter = 0530, loss = 30.1179
2024-11-22 08:27:05: [2024-11-22 08:27:05] iter = 0540, loss = 30.6911
2024-11-22 08:29:06: [2024-11-22 08:29:06] iter = 0550, loss = 31.2958
2024-11-22 08:31:10: [2024-11-22 08:31:10] iter = 0560, loss = 30.6225
2024-11-22 08:33:15: [2024-11-22 08:33:15] iter = 0570, loss = 33.3021
2024-11-22 08:35:18: [2024-11-22 08:35:18] iter = 0580, loss = 32.7249
2024-11-22 08:37:24: [2024-11-22 08:37:24] iter = 0590, loss = 32.4896
2024-11-22 08:39:33: [2024-11-22 08:39:33] iter = 0600, loss = 32.9178
2024-11-22 08:41:38: [2024-11-22 08:41:38] iter = 0610, loss = 31.5413
2024-11-22 08:43:51: [2024-11-22 08:43:51] iter = 0620, loss = 31.2835
2024-11-22 08:46:00: [2024-11-22 08:46:00] iter = 0630, loss = 32.7736
2024-11-22 08:48:05: [2024-11-22 08:48:05] iter = 0640, loss = 31.4933
2024-11-22 08:50:11: [2024-11-22 08:50:11] iter = 0650, loss = 31.6151
2024-11-22 08:52:22: [2024-11-22 08:52:22] iter = 0660, loss = 30.7556
2024-11-22 08:54:29: [2024-11-22 08:54:29] iter = 0670, loss = 32.5267
2024-11-22 08:56:39: [2024-11-22 08:56:39] iter = 0680, loss = 31.2417
2024-11-22 08:58:43: [2024-11-22 08:58:43] iter = 0690, loss = 34.3194
2024-11-22 09:00:45: [2024-11-22 09:00:45] iter = 0700, loss = 32.5885
2024-11-22 09:02:43: [2024-11-22 09:02:43] iter = 0710, loss = 30.9335
2024-11-22 09:04:47: [2024-11-22 09:04:47] iter = 0720, loss = 31.1650
2024-11-22 09:06:49: [2024-11-22 09:06:49] iter = 0730, loss = 31.9661
2024-11-22 09:08:48: [2024-11-22 09:08:48] iter = 0740, loss = 29.3306
2024-11-22 09:10:53: [2024-11-22 09:10:53] iter = 0750, loss = 29.8683
2024-11-22 09:12:53: [2024-11-22 09:12:53] iter = 0760, loss = 31.8262
2024-11-22 09:14:57: [2024-11-22 09:14:57] iter = 0770, loss = 31.9988
2024-11-22 09:17:04: [2024-11-22 09:17:04] iter = 0780, loss = 31.1419
2024-11-22 09:19:06: [2024-11-22 09:19:06] iter = 0790, loss = 32.0353
2024-11-22 09:21:09: [2024-11-22 09:21:09] iter = 0800, loss = 33.4027
2024-11-22 09:23:06: [2024-11-22 09:23:06] iter = 0810, loss = 31.3801
2024-11-22 09:25:10: [2024-11-22 09:25:10] iter = 0820, loss = 31.3218
2024-11-22 09:27:13: [2024-11-22 09:27:13] iter = 0830, loss = 29.9477
2024-11-22 09:29:19: [2024-11-22 09:29:19] iter = 0840, loss = 33.0578
2024-11-22 09:31:22: [2024-11-22 09:31:22] iter = 0850, loss = 32.3572
2024-11-22 09:33:26: [2024-11-22 09:33:26] iter = 0860, loss = 32.2512
2024-11-22 09:35:31: [2024-11-22 09:35:31] iter = 0870, loss = 31.2947
2024-11-22 09:37:31: [2024-11-22 09:37:31] iter = 0880, loss = 30.7742
2024-11-22 09:39:35: [2024-11-22 09:39:35] iter = 0890, loss = 32.3008
2024-11-22 09:41:38: [2024-11-22 09:41:38] iter = 0900, loss = 32.4437
2024-11-22 09:43:40: [2024-11-22 09:43:40] iter = 0910, loss = 32.5426
2024-11-22 09:45:44: [2024-11-22 09:45:44] iter = 0920, loss = 29.6903
2024-11-22 09:47:50: [2024-11-22 09:47:50] iter = 0930, loss = 31.3306
2024-11-22 09:49:53: [2024-11-22 09:49:53] iter = 0940, loss = 32.1467
2024-11-22 09:52:01: [2024-11-22 09:52:01] iter = 0950, loss = 31.7879
2024-11-22 09:54:07: [2024-11-22 09:54:07] iter = 0960, loss = 32.1199
2024-11-22 09:56:08: [2024-11-22 09:56:08] iter = 0970, loss = 30.6210
2024-11-22 09:58:11: [2024-11-22 09:58:11] iter = 0980, loss = 31.3130
2024-11-22 10:00:09: [2024-11-22 10:00:09] iter = 0990, loss = 30.5681
2024-11-22 10:02:07: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 10:02:07: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 10:02:36: Evaluate 5 random ConvNet, ACCmean = 0.7600 ACCstd = 0.0041
-------------------------
2024-11-22 10:02:36: Evaluate 5 random ConvNet, F1mean = 0.7392 F!std = 0.0040
-------------------------
2024-11-22 10:02:49: [2024-11-22 10:02:49] iter = 1000, loss = 33.4264
2024-11-22 10:02:49: 
================== Exp 4 ==================
 
2024-11-22 10:02:49: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganAMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f12754f3a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganAMNIST (INFO)>, 'dc_aug_param': None}
2024-11-22 10:02:49: Evaluation model pool: ['ConvNet']
2024-11-22 10:02:52: class c = 0: 1956 real images
2024-11-22 10:02:52: class c = 1: 1390 real images
2024-11-22 10:02:52: class c = 2: 1357 real images
2024-11-22 10:02:52: class c = 3: 1474 real images
2024-11-22 10:02:52: class c = 4: 3963 real images
2024-11-22 10:02:52: class c = 5: 3817 real images
2024-11-22 10:02:52: class c = 6: 6164 real images
2024-11-22 10:02:52: class c = 7: 3919 real images
2024-11-22 10:02:52: class c = 8: 3929 real images
2024-11-22 10:02:52: class c = 9: 3031 real images
2024-11-22 10:02:52: class c = 10: 3561 real images
2024-11-22 10:02:52: real images channel 0, mean = 0.4680, std = 0.2974
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-22 10:02:52: initialize synthetic data from random noise
2024-11-22 10:02:52: [2024-11-22 10:02:52] training begins
2024-11-22 10:02:52: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-22 10:02:52: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 10:03:23: Evaluate 5 random ConvNet, ACCmean = 0.1257 ACCstd = 0.0195
-------------------------
2024-11-22 10:03:23: Evaluate 5 random ConvNet, F1mean = 0.0928 F!std = 0.0068
-------------------------
2024-11-22 10:03:35: [2024-11-22 10:03:35] iter = 0000, loss = 300.1759
2024-11-22 10:05:42: [2024-11-22 10:05:42] iter = 0010, loss = 154.7189
2024-11-22 10:07:46: [2024-11-22 10:07:46] iter = 0020, loss = 103.1243
2024-11-22 10:09:53: [2024-11-22 10:09:53] iter = 0030, loss = 77.9630
2024-11-22 10:12:00: [2024-11-22 10:12:00] iter = 0040, loss = 68.6577
2024-11-22 10:14:10: [2024-11-22 10:14:10] iter = 0050, loss = 56.8430
2024-11-22 10:16:18: [2024-11-22 10:16:18] iter = 0060, loss = 52.7023
2024-11-22 10:18:27: [2024-11-22 10:18:27] iter = 0070, loss = 51.7569
2024-11-22 10:20:33: [2024-11-22 10:20:33] iter = 0080, loss = 47.2053
2024-11-22 10:22:41: [2024-11-22 10:22:41] iter = 0090, loss = 44.5852
2024-11-22 10:24:48: [2024-11-22 10:24:48] iter = 0100, loss = 45.1844
2024-11-22 10:26:49: [2024-11-22 10:26:49] iter = 0110, loss = 41.7828
2024-11-22 10:28:54: [2024-11-22 10:28:54] iter = 0120, loss = 42.2638
2024-11-22 10:31:03: [2024-11-22 10:31:03] iter = 0130, loss = 41.9491
2024-11-22 10:33:05: [2024-11-22 10:33:05] iter = 0140, loss = 41.2636
2024-11-22 10:35:09: [2024-11-22 10:35:09] iter = 0150, loss = 39.3616
2024-11-22 10:37:16: [2024-11-22 10:37:16] iter = 0160, loss = 38.4036
2024-11-22 10:39:22: [2024-11-22 10:39:22] iter = 0170, loss = 39.4955
2024-11-22 10:41:28: [2024-11-22 10:41:28] iter = 0180, loss = 39.8013
2024-11-22 10:43:30: [2024-11-22 10:43:30] iter = 0190, loss = 37.9605
2024-11-22 10:45:37: [2024-11-22 10:45:37] iter = 0200, loss = 35.4936
2024-11-22 10:47:43: [2024-11-22 10:47:43] iter = 0210, loss = 35.2039
2024-11-22 10:49:49: [2024-11-22 10:49:49] iter = 0220, loss = 36.7688
2024-11-22 10:51:55: [2024-11-22 10:51:55] iter = 0230, loss = 37.7223
2024-11-22 10:54:00: [2024-11-22 10:54:00] iter = 0240, loss = 35.2131
2024-11-22 10:56:02: [2024-11-22 10:56:02] iter = 0250, loss = 33.9287
2024-11-22 10:58:09: [2024-11-22 10:58:09] iter = 0260, loss = 33.6494
2024-11-22 11:00:21: [2024-11-22 11:00:21] iter = 0270, loss = 35.2935
2024-11-22 11:02:37: [2024-11-22 11:02:36] iter = 0280, loss = 33.1005
2024-11-22 11:04:37: [2024-11-22 11:04:37] iter = 0290, loss = 34.0855
2024-11-22 11:06:43: [2024-11-22 11:06:43] iter = 0300, loss = 33.5522
2024-11-22 11:08:55: [2024-11-22 11:08:55] iter = 0310, loss = 32.8348
2024-11-22 11:11:09: [2024-11-22 11:11:09] iter = 0320, loss = 35.1110
2024-11-22 11:13:17: [2024-11-22 11:13:17] iter = 0330, loss = 35.6198
2024-11-22 11:15:27: [2024-11-22 11:15:27] iter = 0340, loss = 33.9011
2024-11-22 11:17:37: [2024-11-22 11:17:37] iter = 0350, loss = 33.1276
2024-11-22 11:19:45: [2024-11-22 11:19:45] iter = 0360, loss = 35.3455
2024-11-22 11:21:55: [2024-11-22 11:21:55] iter = 0370, loss = 33.1860
2024-11-22 11:24:00: [2024-11-22 11:24:00] iter = 0380, loss = 32.7784
2024-11-22 11:26:09: [2024-11-22 11:26:09] iter = 0390, loss = 32.9860
2024-11-22 11:28:17: [2024-11-22 11:28:17] iter = 0400, loss = 33.4405
2024-11-22 11:30:27: [2024-11-22 11:30:27] iter = 0410, loss = 32.5385
2024-11-22 11:32:33: [2024-11-22 11:32:33] iter = 0420, loss = 31.3631
2024-11-22 11:34:38: [2024-11-22 11:34:38] iter = 0430, loss = 33.5810
2024-11-22 11:36:43: [2024-11-22 11:36:43] iter = 0440, loss = 33.1844
2024-11-22 11:38:45: [2024-11-22 11:38:45] iter = 0450, loss = 32.1983
2024-11-22 11:40:50: [2024-11-22 11:40:50] iter = 0460, loss = 30.5821
2024-11-22 11:42:50: [2024-11-22 11:42:50] iter = 0470, loss = 31.7904
2024-11-22 11:44:52: [2024-11-22 11:44:52] iter = 0480, loss = 31.6881
2024-11-22 11:47:01: [2024-11-22 11:47:01] iter = 0490, loss = 31.7923
2024-11-22 11:48:53: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 11:48:53: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 11:49:23: Evaluate 5 random ConvNet, ACCmean = 0.7504 ACCstd = 0.0029
-------------------------
2024-11-22 11:49:23: Evaluate 5 random ConvNet, F1mean = 0.7297 F!std = 0.0037
-------------------------
2024-11-22 11:49:35: [2024-11-22 11:49:35] iter = 0500, loss = 31.5359
2024-11-22 11:51:41: [2024-11-22 11:51:41] iter = 0510, loss = 32.6905
2024-11-22 11:53:52: [2024-11-22 11:53:52] iter = 0520, loss = 33.6420
2024-11-22 11:55:56: [2024-11-22 11:55:56] iter = 0530, loss = 31.2047
2024-11-22 11:57:59: [2024-11-22 11:57:59] iter = 0540, loss = 31.6223
2024-11-22 12:00:06: [2024-11-22 12:00:06] iter = 0550, loss = 31.9285
2024-11-22 12:02:14: [2024-11-22 12:02:14] iter = 0560, loss = 31.1749
2024-11-22 12:04:21: [2024-11-22 12:04:21] iter = 0570, loss = 33.7654
2024-11-22 12:06:32: [2024-11-22 12:06:32] iter = 0580, loss = 32.6262
2024-11-22 12:08:37: [2024-11-22 12:08:37] iter = 0590, loss = 30.1665
2024-11-22 12:10:44: [2024-11-22 12:10:44] iter = 0600, loss = 34.4484
2024-11-22 12:12:46: [2024-11-22 12:12:46] iter = 0610, loss = 30.4827
2024-11-22 12:14:44: [2024-11-22 12:14:44] iter = 0620, loss = 30.6996
2024-11-22 12:16:45: [2024-11-22 12:16:45] iter = 0630, loss = 31.6975
2024-11-22 12:18:45: [2024-11-22 12:18:45] iter = 0640, loss = 32.5196
2024-11-22 12:20:44: [2024-11-22 12:20:44] iter = 0650, loss = 33.8921
2024-11-22 12:22:43: [2024-11-22 12:22:43] iter = 0660, loss = 32.6038
2024-11-22 12:24:44: [2024-11-22 12:24:44] iter = 0670, loss = 30.6945
2024-11-22 12:26:46: [2024-11-22 12:26:46] iter = 0680, loss = 31.7122
2024-11-22 12:28:46: [2024-11-22 12:28:46] iter = 0690, loss = 32.2125
2024-11-22 12:30:46: [2024-11-22 12:30:46] iter = 0700, loss = 32.3120
2024-11-22 12:32:46: [2024-11-22 12:32:46] iter = 0710, loss = 30.9954
2024-11-22 12:34:46: [2024-11-22 12:34:46] iter = 0720, loss = 31.1395
2024-11-22 12:36:46: [2024-11-22 12:36:46] iter = 0730, loss = 30.5230
2024-11-22 12:38:47: [2024-11-22 12:38:47] iter = 0740, loss = 29.6963
2024-11-22 12:40:44: [2024-11-22 12:40:44] iter = 0750, loss = 31.1706
2024-11-22 12:42:44: [2024-11-22 12:42:44] iter = 0760, loss = 31.8983
2024-11-22 12:44:43: [2024-11-22 12:44:43] iter = 0770, loss = 31.7103
2024-11-22 12:46:43: [2024-11-22 12:46:43] iter = 0780, loss = 30.2009
2024-11-22 12:48:43: [2024-11-22 12:48:43] iter = 0790, loss = 31.7330
2024-11-22 12:50:54: [2024-11-22 12:50:54] iter = 0800, loss = 30.1217
2024-11-22 12:52:54: [2024-11-22 12:52:54] iter = 0810, loss = 31.9587
2024-11-22 12:54:49: [2024-11-22 12:54:49] iter = 0820, loss = 31.6339
2024-11-22 12:56:46: [2024-11-22 12:56:46] iter = 0830, loss = 31.7697
2024-11-22 12:58:45: [2024-11-22 12:58:45] iter = 0840, loss = 29.6712
2024-11-22 13:00:43: [2024-11-22 13:00:43] iter = 0850, loss = 32.4904
2024-11-22 13:02:43: [2024-11-22 13:02:43] iter = 0860, loss = 31.4979
2024-11-22 13:04:41: [2024-11-22 13:04:41] iter = 0870, loss = 32.1793
2024-11-22 13:06:40: [2024-11-22 13:06:40] iter = 0880, loss = 32.2835
2024-11-22 13:08:39: [2024-11-22 13:08:39] iter = 0890, loss = 31.2346
2024-11-22 13:10:02: [2024-11-22 13:10:02] iter = 0900, loss = 32.1071
2024-11-22 13:11:23: [2024-11-22 13:11:23] iter = 0910, loss = 29.8797
2024-11-22 13:12:46: [2024-11-22 13:12:46] iter = 0920, loss = 30.6042
2024-11-22 13:14:07: [2024-11-22 13:14:07] iter = 0930, loss = 29.5500
2024-11-22 13:15:28: [2024-11-22 13:15:28] iter = 0940, loss = 32.4237
2024-11-22 13:16:40: [2024-11-22 13:16:40] iter = 0950, loss = 30.5688
2024-11-22 13:17:30: [2024-11-22 13:17:30] iter = 0960, loss = 31.9237
2024-11-22 13:18:18: [2024-11-22 13:18:18] iter = 0970, loss = 30.0683
2024-11-22 13:19:06: [2024-11-22 13:19:06] iter = 0980, loss = 30.7389
2024-11-22 13:19:54: [2024-11-22 13:19:54] iter = 0990, loss = 30.4515
2024-11-22 13:20:37: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 13:20:37: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 13:20:48: Evaluate 5 random ConvNet, ACCmean = 0.7621 ACCstd = 0.0023
-------------------------
2024-11-22 13:20:48: Evaluate 5 random ConvNet, F1mean = 0.7412 F!std = 0.0026
-------------------------
2024-11-22 13:20:53: [2024-11-22 13:20:53] iter = 1000, loss = 31.3257
2024-11-22 13:20:53: 
==================== Final Results ====================

2024-11-22 13:20:53: Run 5 experiments, train on ConvNet, evaluate 25 random ConvNet, mean  = 76.09%  std = 0.44%

[2024-11-22 06:28:00] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.005381 train acc = 1.0000, test acc = 0.0739, test_sen =0.0700, test_spe =0.9076, test_f1 =0.0459
[2024-11-22 06:28:07] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.005318 train acc = 1.0000, test acc = 0.0760, test_sen =0.0556, test_spe =0.9062, test_f1 =0.0412
[2024-11-22 06:28:13] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.005406 train acc = 1.0000, test acc = 0.0722, test_sen =0.0627, test_spe =0.9072, test_f1 =0.0421
[2024-11-22 06:28:19] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.005355 train acc = 1.0000, test acc = 0.0571, test_sen =0.0501, test_spe =0.9057, test_f1 =0.0352
[2024-11-22 06:28:26] Evaluate_04: epoch = 0300 train time = 4 s train loss = 0.005410 train acc = 1.0000, test acc = 0.0591, test_sen =0.0586, test_spe =0.9070, test_f1 =0.0454
[2024-11-22 08:18:05] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.010356 train acc = 1.0000, test acc = 0.7575, test_sen =0.7592, test_spe =0.9757, test_f1 =0.7381
[2024-11-22 08:18:11] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.010463 train acc = 1.0000, test acc = 0.7586, test_sen =0.7610, test_spe =0.9758, test_f1 =0.7375
[2024-11-22 08:18:17] Evaluate_02: epoch = 0300 train time = 3 s train loss = 0.010715 train acc = 1.0000, test acc = 0.7611, test_sen =0.7649, test_spe =0.9760, test_f1 =0.7436
[2024-11-22 08:18:23] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.010585 train acc = 1.0000, test acc = 0.7545, test_sen =0.7601, test_spe =0.9754, test_f1 =0.7347
[2024-11-22 08:18:29] Evaluate_04: epoch = 0300 train time = 4 s train loss = 0.010271 train acc = 1.0000, test acc = 0.7532, test_sen =0.7593, test_spe =0.9753, test_f1 =0.7345
[2024-11-22 10:02:13] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.010999 train acc = 1.0000, test acc = 0.7587, test_sen =0.7618, test_spe =0.9759, test_f1 =0.7375
[2024-11-22 10:02:19] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.010481 train acc = 1.0000, test acc = 0.7603, test_sen =0.7645, test_spe =0.9760, test_f1 =0.7388
[2024-11-22 10:02:25] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.010539 train acc = 1.0000, test acc = 0.7666, test_sen =0.7719, test_spe =0.9767, test_f1 =0.7455
[2024-11-22 10:02:30] Evaluate_03: epoch = 0300 train time = 3 s train loss = 0.010962 train acc = 1.0000, test acc = 0.7539, test_sen =0.7591, test_spe =0.9754, test_f1 =0.7334
[2024-11-22 10:02:36] Evaluate_04: epoch = 0300 train time = 3 s train loss = 0.010441 train acc = 1.0000, test acc = 0.7606, test_sen =0.7681, test_spe =0.9761, test_f1 =0.7407
[2024-11-22 10:02:58] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.005417 train acc = 1.0000, test acc = 0.1000, test_sen =0.0996, test_spe =0.9090, test_f1 =0.0881
[2024-11-22 10:03:04] Evaluate_01: epoch = 0300 train time = 3 s train loss = 0.005269 train acc = 1.0000, test acc = 0.1227, test_sen =0.0951, test_spe =0.9103, test_f1 =0.0912
[2024-11-22 10:03:11] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.005275 train acc = 1.0000, test acc = 0.1597, test_sen =0.1134, test_spe =0.9130, test_f1 =0.1011
[2024-11-22 10:03:18] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.005199 train acc = 1.0000, test acc = 0.1285, test_sen =0.1281, test_spe =0.9125, test_f1 =0.0999
[2024-11-22 10:03:23] Evaluate_04: epoch = 0300 train time = 3 s train loss = 0.005178 train acc = 1.0000, test acc = 0.1178, test_sen =0.0984, test_spe =0.9108, test_f1 =0.0836
[2024-11-22 11:48:59] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.010155 train acc = 1.0000, test acc = 0.7524, test_sen =0.7571, test_spe =0.9753, test_f1 =0.7304
[2024-11-22 11:49:05] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.010466 train acc = 1.0000, test acc = 0.7548, test_sen =0.7604, test_spe =0.9755, test_f1 =0.7360
[2024-11-22 11:49:11] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.009926 train acc = 1.0000, test acc = 0.7488, test_sen =0.7527, test_spe =0.9749, test_f1 =0.7254
[2024-11-22 11:49:18] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.010877 train acc = 1.0000, test acc = 0.7464, test_sen =0.7505, test_spe =0.9746, test_f1 =0.7265
[2024-11-22 11:49:23] Evaluate_04: epoch = 0300 train time = 4 s train loss = 0.010100 train acc = 1.0000, test acc = 0.7497, test_sen =0.7562, test_spe =0.9750, test_f1 =0.7300
[2024-11-22 13:20:40] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.010577 train acc = 1.0000, test acc = 0.7635, test_sen =0.7692, test_spe =0.9764, test_f1 =0.7435
[2024-11-22 13:20:42] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.010539 train acc = 1.0000, test acc = 0.7594, test_sen =0.7662, test_spe =0.9760, test_f1 =0.7382
[2024-11-22 13:20:44] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.010292 train acc = 1.0000, test acc = 0.7601, test_sen =0.7644, test_spe =0.9760, test_f1 =0.7380
[2024-11-22 13:20:46] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.011009 train acc = 1.0000, test acc = 0.7620, test_sen =0.7656, test_spe =0.9762, test_f1 =0.7422
[2024-11-22 13:20:48] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.010886 train acc = 1.0000, test acc = 0.7657, test_sen =0.7693, test_spe =0.9766, test_f1 =0.7442
