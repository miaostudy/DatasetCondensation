nohup: ignoring input
2024-11-21 12:12:07: eval_it_pool: [0, 500, 1000]
2024-11-21 12:12:09: 
================== Exp 0 ==================
 
2024-11-21 12:12:09: Hyper-parameters: 
{'method': 'DC', 'dataset': 'TissueMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f2a40641a60>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_TissueMNIST (INFO)>}
2024-11-21 12:12:09: Evaluation model pool: ['ConvNet']
2024-11-21 12:12:14: class c = 0: 53075 real images
2024-11-21 12:12:14: class c = 1: 7814 real images
2024-11-21 12:12:14: class c = 2: 5866 real images
2024-11-21 12:12:14: class c = 3: 15406 real images
2024-11-21 12:12:14: class c = 4: 11789 real images
2024-11-21 12:12:14: class c = 5: 7705 real images
2024-11-21 12:12:14: class c = 6: 39203 real images
2024-11-21 12:12:14: class c = 7: 24608 real images
2024-11-21 12:12:14: real images channel 0, mean = 0.1020, std = 0.1000
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
main_base.py:125: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975312/work/torch/csrc/utils/tensor_new.cpp:230.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 12:12:14: initialize synthetic data from random noise
2024-11-21 12:12:14: [2024-11-21 12:12:14] training begins
2024-11-21 12:12:14: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 12:12:14: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 12:12:28: Evaluate 5 random ConvNet, ACCmean = 0.0965 ACCstd = 0.0178
-------------------------
2024-11-21 12:12:28: Evaluate 5 random ConvNet, F1mean = 0.0706 F!std = 0.0077
-------------------------
2024-11-21 12:12:31: [2024-11-21 12:12:31] iter = 0000, loss = 266.0850
2024-11-21 12:13:10: [2024-11-21 12:13:10] iter = 0010, loss = 131.8112
2024-11-21 12:13:49: [2024-11-21 12:13:49] iter = 0020, loss = 102.1406
2024-11-21 12:14:28: [2024-11-21 12:14:28] iter = 0030, loss = 78.3442
2024-11-21 12:15:07: [2024-11-21 12:15:07] iter = 0040, loss = 68.7761
2024-11-21 12:15:45: [2024-11-21 12:15:45] iter = 0050, loss = 61.9444
2024-11-21 12:16:24: [2024-11-21 12:16:24] iter = 0060, loss = 58.6022
2024-11-21 12:17:03: [2024-11-21 12:17:03] iter = 0070, loss = 56.0102
2024-11-21 12:17:42: [2024-11-21 12:17:42] iter = 0080, loss = 54.9641
2024-11-21 12:18:21: [2024-11-21 12:18:21] iter = 0090, loss = 50.1433
2024-11-21 12:18:59: [2024-11-21 12:18:59] iter = 0100, loss = 49.5008
2024-11-21 12:19:38: [2024-11-21 12:19:38] iter = 0110, loss = 50.1625
2024-11-21 12:20:17: [2024-11-21 12:20:17] iter = 0120, loss = 49.6300
2024-11-21 12:20:55: [2024-11-21 12:20:55] iter = 0130, loss = 47.5687
2024-11-21 12:21:34: [2024-11-21 12:21:34] iter = 0140, loss = 46.2180
2024-11-21 12:22:13: [2024-11-21 12:22:13] iter = 0150, loss = 45.7790
2024-11-21 12:22:52: [2024-11-21 12:22:52] iter = 0160, loss = 47.7837
2024-11-21 12:23:31: [2024-11-21 12:23:31] iter = 0170, loss = 44.1050
2024-11-21 12:24:09: [2024-11-21 12:24:09] iter = 0180, loss = 46.1647
2024-11-21 12:24:48: [2024-11-21 12:24:48] iter = 0190, loss = 44.2553
2024-11-21 12:25:27: [2024-11-21 12:25:27] iter = 0200, loss = 43.0559
2024-11-21 12:26:06: [2024-11-21 12:26:06] iter = 0210, loss = 46.0324
2024-11-21 12:26:45: [2024-11-21 12:26:45] iter = 0220, loss = 45.7148
2024-11-21 12:27:23: [2024-11-21 12:27:23] iter = 0230, loss = 44.9299
2024-11-21 12:28:02: [2024-11-21 12:28:02] iter = 0240, loss = 44.0145
2024-11-21 12:28:41: [2024-11-21 12:28:41] iter = 0250, loss = 40.9391
2024-11-21 12:29:20: [2024-11-21 12:29:20] iter = 0260, loss = 42.6899
2024-11-21 12:29:58: [2024-11-21 12:29:58] iter = 0270, loss = 42.2862
2024-11-21 12:30:37: [2024-11-21 12:30:37] iter = 0280, loss = 43.5507
2024-11-21 12:31:16: [2024-11-21 12:31:16] iter = 0290, loss = 41.7180
2024-11-21 12:31:55: [2024-11-21 12:31:55] iter = 0300, loss = 40.0592
2024-11-21 12:32:34: [2024-11-21 12:32:34] iter = 0310, loss = 43.7793
2024-11-21 12:33:13: [2024-11-21 12:33:13] iter = 0320, loss = 41.7383
2024-11-21 12:33:51: [2024-11-21 12:33:51] iter = 0330, loss = 42.4000
2024-11-21 12:34:30: [2024-11-21 12:34:30] iter = 0340, loss = 42.3662
2024-11-21 12:35:09: [2024-11-21 12:35:09] iter = 0350, loss = 44.6403
2024-11-21 12:35:48: [2024-11-21 12:35:48] iter = 0360, loss = 43.8053
2024-11-21 12:36:26: [2024-11-21 12:36:26] iter = 0370, loss = 44.2780
2024-11-21 12:37:04: [2024-11-21 12:37:04] iter = 0380, loss = 45.4665
2024-11-21 12:37:42: [2024-11-21 12:37:42] iter = 0390, loss = 44.3592
2024-11-21 12:38:20: [2024-11-21 12:38:20] iter = 0400, loss = 42.7781
2024-11-21 12:38:58: [2024-11-21 12:38:58] iter = 0410, loss = 44.0029
2024-11-21 12:39:36: [2024-11-21 12:39:36] iter = 0420, loss = 43.1717
2024-11-21 12:40:14: [2024-11-21 12:40:14] iter = 0430, loss = 42.0092
2024-11-21 12:40:52: [2024-11-21 12:40:52] iter = 0440, loss = 43.4161
2024-11-21 12:41:30: [2024-11-21 12:41:30] iter = 0450, loss = 43.1710
2024-11-21 12:42:08: [2024-11-21 12:42:08] iter = 0460, loss = 45.0807
2024-11-21 12:42:46: [2024-11-21 12:42:46] iter = 0470, loss = 43.5314
2024-11-21 12:43:24: [2024-11-21 12:43:24] iter = 0480, loss = 42.4773
2024-11-21 12:44:02: [2024-11-21 12:44:02] iter = 0490, loss = 46.3410
2024-11-21 12:44:36: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-21 12:44:36: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 12:44:48: Evaluate 5 random ConvNet, ACCmean = 0.3681 ACCstd = 0.0102
-------------------------
2024-11-21 12:44:48: Evaluate 5 random ConvNet, F1mean = 0.2868 F!std = 0.0049
-------------------------
2024-11-21 12:44:52: [2024-11-21 12:44:52] iter = 0500, loss = 43.6947
2024-11-21 12:45:30: [2024-11-21 12:45:30] iter = 0510, loss = 43.6651
2024-11-21 12:46:08: [2024-11-21 12:46:08] iter = 0520, loss = 45.4667
2024-11-21 12:46:45: [2024-11-21 12:46:45] iter = 0530, loss = 43.4796
2024-11-21 12:47:24: [2024-11-21 12:47:23] iter = 0540, loss = 44.5949
2024-11-21 12:48:02: [2024-11-21 12:48:02] iter = 0550, loss = 44.1141
2024-11-21 12:48:40: [2024-11-21 12:48:40] iter = 0560, loss = 42.4278
2024-11-21 12:49:17: [2024-11-21 12:49:17] iter = 0570, loss = 41.9956
2024-11-21 12:49:55: [2024-11-21 12:49:55] iter = 0580, loss = 43.4278
2024-11-21 12:50:33: [2024-11-21 12:50:33] iter = 0590, loss = 46.2683
2024-11-21 12:51:11: [2024-11-21 12:51:11] iter = 0600, loss = 44.1854
2024-11-21 12:51:49: [2024-11-21 12:51:49] iter = 0610, loss = 43.3475
2024-11-21 12:52:27: [2024-11-21 12:52:27] iter = 0620, loss = 42.0339
2024-11-21 12:53:05: [2024-11-21 12:53:05] iter = 0630, loss = 42.2138
2024-11-21 12:53:43: [2024-11-21 12:53:43] iter = 0640, loss = 45.2697
2024-11-21 12:54:20: [2024-11-21 12:54:20] iter = 0650, loss = 44.1493
2024-11-21 12:54:58: [2024-11-21 12:54:58] iter = 0660, loss = 42.8512
2024-11-21 12:55:36: [2024-11-21 12:55:36] iter = 0670, loss = 44.9466
2024-11-21 12:56:14: [2024-11-21 12:56:14] iter = 0680, loss = 42.2723
2024-11-21 12:56:52: [2024-11-21 12:56:52] iter = 0690, loss = 42.9264
2024-11-21 12:57:30: [2024-11-21 12:57:30] iter = 0700, loss = 43.5336
2024-11-21 12:58:08: [2024-11-21 12:58:08] iter = 0710, loss = 42.8925
2024-11-21 12:58:46: [2024-11-21 12:58:46] iter = 0720, loss = 43.1251
2024-11-21 12:59:23: [2024-11-21 12:59:23] iter = 0730, loss = 43.9589
2024-11-21 13:00:02: [2024-11-21 13:00:02] iter = 0740, loss = 45.8083
2024-11-21 13:00:39: [2024-11-21 13:00:39] iter = 0750, loss = 44.4304
2024-11-21 13:01:17: [2024-11-21 13:01:17] iter = 0760, loss = 42.8592
2024-11-21 13:01:55: [2024-11-21 13:01:55] iter = 0770, loss = 45.5659
2024-11-21 13:02:33: [2024-11-21 13:02:33] iter = 0780, loss = 45.4389
2024-11-21 13:03:11: [2024-11-21 13:03:11] iter = 0790, loss = 44.5507
2024-11-21 13:03:49: [2024-11-21 13:03:49] iter = 0800, loss = 45.3480
2024-11-21 13:04:27: [2024-11-21 13:04:27] iter = 0810, loss = 43.1065
2024-11-21 13:05:04: [2024-11-21 13:05:04] iter = 0820, loss = 43.1319
2024-11-21 13:05:42: [2024-11-21 13:05:42] iter = 0830, loss = 41.6997
2024-11-21 13:06:20: [2024-11-21 13:06:20] iter = 0840, loss = 44.8426
2024-11-21 13:06:58: [2024-11-21 13:06:58] iter = 0850, loss = 43.3574
2024-11-21 13:07:36: [2024-11-21 13:07:36] iter = 0860, loss = 40.6974
2024-11-21 13:08:14: [2024-11-21 13:08:14] iter = 0870, loss = 44.3886
2024-11-21 13:08:52: [2024-11-21 13:08:52] iter = 0880, loss = 43.5757
2024-11-21 13:09:30: [2024-11-21 13:09:30] iter = 0890, loss = 44.1017
2024-11-21 13:10:07: [2024-11-21 13:10:07] iter = 0900, loss = 43.2936
2024-11-21 13:10:45: [2024-11-21 13:10:45] iter = 0910, loss = 41.7793
2024-11-21 13:11:23: [2024-11-21 13:11:23] iter = 0920, loss = 43.3870
2024-11-21 13:12:01: [2024-11-21 13:12:01] iter = 0930, loss = 42.4447
2024-11-21 13:12:38: [2024-11-21 13:12:38] iter = 0940, loss = 44.1619
2024-11-21 13:13:16: [2024-11-21 13:13:16] iter = 0950, loss = 43.8973
2024-11-21 13:13:54: [2024-11-21 13:13:54] iter = 0960, loss = 45.0134
2024-11-21 13:14:31: [2024-11-21 13:14:31] iter = 0970, loss = 43.6948
2024-11-21 13:15:09: [2024-11-21 13:15:09] iter = 0980, loss = 44.0966
2024-11-21 13:15:47: [2024-11-21 13:15:47] iter = 0990, loss = 44.8048
2024-11-21 13:16:21: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-21 13:16:21: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 13:16:33: Evaluate 5 random ConvNet, ACCmean = 0.3677 ACCstd = 0.0113
-------------------------
2024-11-21 13:16:33: Evaluate 5 random ConvNet, F1mean = 0.2862 F!std = 0.0062
-------------------------
2024-11-21 13:16:37: [2024-11-21 13:16:37] iter = 1000, loss = 44.2742
2024-11-21 13:16:37: 
================== Exp 1 ==================
 
2024-11-21 13:16:37: Hyper-parameters: 
{'method': 'DC', 'dataset': 'TissueMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f2a40641a60>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_TissueMNIST (INFO)>, 'dc_aug_param': None}
2024-11-21 13:16:37: Evaluation model pool: ['ConvNet']
2024-11-21 13:16:42: class c = 0: 53075 real images
2024-11-21 13:16:42: class c = 1: 7814 real images
2024-11-21 13:16:42: class c = 2: 5866 real images
2024-11-21 13:16:42: class c = 3: 15406 real images
2024-11-21 13:16:42: class c = 4: 11789 real images
2024-11-21 13:16:42: class c = 5: 7705 real images
2024-11-21 13:16:42: class c = 6: 39203 real images
2024-11-21 13:16:42: class c = 7: 24608 real images
2024-11-21 13:16:42: real images channel 0, mean = 0.1020, std = 0.1000
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 13:16:42: initialize synthetic data from random noise
2024-11-21 13:16:42: [2024-11-21 13:16:42] training begins
2024-11-21 13:16:42: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 13:16:42: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 13:16:54: Evaluate 5 random ConvNet, ACCmean = 0.1703 ACCstd = 0.0151
-------------------------
2024-11-21 13:16:54: Evaluate 5 random ConvNet, F1mean = 0.0966 F!std = 0.0053
-------------------------
2024-11-21 13:16:58: [2024-11-21 13:16:58] iter = 0000, loss = 265.4648
2024-11-21 13:17:36: [2024-11-21 13:17:36] iter = 0010, loss = 127.2235
2024-11-21 13:18:14: [2024-11-21 13:18:14] iter = 0020, loss = 107.2283
2024-11-21 13:18:53: [2024-11-21 13:18:53] iter = 0030, loss = 75.5764
2024-11-21 13:19:31: [2024-11-21 13:19:31] iter = 0040, loss = 67.8965
2024-11-21 13:20:08: [2024-11-21 13:20:08] iter = 0050, loss = 62.9162
2024-11-21 13:20:47: [2024-11-21 13:20:47] iter = 0060, loss = 57.8082
2024-11-21 13:21:24: [2024-11-21 13:21:24] iter = 0070, loss = 55.3930
2024-11-21 13:22:02: [2024-11-21 13:22:02] iter = 0080, loss = 53.2595
2024-11-21 13:22:40: [2024-11-21 13:22:40] iter = 0090, loss = 51.4780
2024-11-21 13:23:18: [2024-11-21 13:23:18] iter = 0100, loss = 53.9004
2024-11-21 13:23:56: [2024-11-21 13:23:56] iter = 0110, loss = 50.9458
2024-11-21 13:24:34: [2024-11-21 13:24:34] iter = 0120, loss = 48.1152
2024-11-21 13:25:12: [2024-11-21 13:25:12] iter = 0130, loss = 48.9881
2024-11-21 13:25:50: [2024-11-21 13:25:50] iter = 0140, loss = 48.0349
2024-11-21 13:26:28: [2024-11-21 13:26:28] iter = 0150, loss = 44.7676
2024-11-21 13:27:06: [2024-11-21 13:27:06] iter = 0160, loss = 49.8485
2024-11-21 13:27:44: [2024-11-21 13:27:44] iter = 0170, loss = 46.8367
2024-11-21 13:28:22: [2024-11-21 13:28:22] iter = 0180, loss = 45.6866
2024-11-21 13:29:00: [2024-11-21 13:29:00] iter = 0190, loss = 45.4096
2024-11-21 13:29:38: [2024-11-21 13:29:38] iter = 0200, loss = 46.5275
2024-11-21 13:30:16: [2024-11-21 13:30:16] iter = 0210, loss = 45.7757
2024-11-21 13:30:58: [2024-11-21 13:30:58] iter = 0220, loss = 45.1381
2024-11-21 13:31:40: [2024-11-21 13:31:40] iter = 0230, loss = 45.3470
2024-11-21 13:32:18: [2024-11-21 13:32:18] iter = 0240, loss = 42.9403
2024-11-21 13:32:58: [2024-11-21 13:32:58] iter = 0250, loss = 42.3904
2024-11-21 13:33:36: [2024-11-21 13:33:36] iter = 0260, loss = 44.4780
2024-11-21 13:34:14: [2024-11-21 13:34:14] iter = 0270, loss = 42.6928
2024-11-21 13:34:52: [2024-11-21 13:34:52] iter = 0280, loss = 45.4306
2024-11-21 13:35:29: [2024-11-21 13:35:29] iter = 0290, loss = 43.4369
2024-11-21 13:36:06: [2024-11-21 13:36:06] iter = 0300, loss = 43.4163
2024-11-21 13:36:44: [2024-11-21 13:36:44] iter = 0310, loss = 41.7482
2024-11-21 13:37:22: [2024-11-21 13:37:22] iter = 0320, loss = 42.9888
2024-11-21 13:38:04: [2024-11-21 13:38:04] iter = 0330, loss = 44.1096
2024-11-21 13:38:47: [2024-11-21 13:38:47] iter = 0340, loss = 43.4785
2024-11-21 13:39:29: [2024-11-21 13:39:29] iter = 0350, loss = 45.0340
2024-11-21 13:40:13: [2024-11-21 13:40:13] iter = 0360, loss = 44.6289
2024-11-21 13:40:56: [2024-11-21 13:40:56] iter = 0370, loss = 42.0084
2024-11-21 13:41:39: [2024-11-21 13:41:39] iter = 0380, loss = 42.5207
2024-11-21 13:42:23: [2024-11-21 13:42:23] iter = 0390, loss = 44.9926
2024-11-21 13:43:09: [2024-11-21 13:43:09] iter = 0400, loss = 40.3931
2024-11-21 13:43:53: [2024-11-21 13:43:53] iter = 0410, loss = 43.3055
2024-11-21 13:44:37: [2024-11-21 13:44:37] iter = 0420, loss = 44.2268
2024-11-21 13:45:21: [2024-11-21 13:45:21] iter = 0430, loss = 43.2145
2024-11-21 13:46:06: [2024-11-21 13:46:06] iter = 0440, loss = 43.2391
2024-11-21 13:46:50: [2024-11-21 13:46:50] iter = 0450, loss = 41.7164
2024-11-21 13:47:36: [2024-11-21 13:47:36] iter = 0460, loss = 44.7059
2024-11-21 13:48:18: [2024-11-21 13:48:18] iter = 0470, loss = 43.0410
2024-11-21 13:49:02: [2024-11-21 13:49:02] iter = 0480, loss = 40.0625
2024-11-21 13:49:45: [2024-11-21 13:49:45] iter = 0490, loss = 41.4814
2024-11-21 13:50:24: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-21 13:50:24: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 13:50:41: Evaluate 5 random ConvNet, ACCmean = 0.3925 ACCstd = 0.0114
-------------------------
2024-11-21 13:50:41: Evaluate 5 random ConvNet, F1mean = 0.2947 F!std = 0.0076
-------------------------
2024-11-21 13:50:46: [2024-11-21 13:50:46] iter = 0500, loss = 42.9414
2024-11-21 13:51:29: [2024-11-21 13:51:29] iter = 0510, loss = 44.3240
2024-11-21 13:52:14: [2024-11-21 13:52:14] iter = 0520, loss = 41.3976
2024-11-21 13:52:58: [2024-11-21 13:52:58] iter = 0530, loss = 41.5459
2024-11-21 13:53:42: [2024-11-21 13:53:42] iter = 0540, loss = 46.6243
2024-11-21 13:54:26: [2024-11-21 13:54:26] iter = 0550, loss = 42.8391
2024-11-21 13:55:04: [2024-11-21 13:55:04] iter = 0560, loss = 42.6580
2024-11-21 13:55:42: [2024-11-21 13:55:42] iter = 0570, loss = 44.6219
2024-11-21 13:56:20: [2024-11-21 13:56:20] iter = 0580, loss = 42.5313
2024-11-21 13:56:58: [2024-11-21 13:56:58] iter = 0590, loss = 45.7029
2024-11-21 13:57:36: [2024-11-21 13:57:36] iter = 0600, loss = 42.7957
2024-11-21 13:58:14: [2024-11-21 13:58:14] iter = 0610, loss = 43.8192
2024-11-21 13:58:52: [2024-11-21 13:58:52] iter = 0620, loss = 42.9981
2024-11-21 13:59:30: [2024-11-21 13:59:30] iter = 0630, loss = 44.4090
2024-11-21 14:00:08: [2024-11-21 14:00:08] iter = 0640, loss = 42.9383
2024-11-21 14:00:46: [2024-11-21 14:00:46] iter = 0650, loss = 43.2187
2024-11-21 14:01:24: [2024-11-21 14:01:24] iter = 0660, loss = 42.3202
2024-11-21 14:02:02: [2024-11-21 14:02:02] iter = 0670, loss = 43.0501
2024-11-21 14:02:40: [2024-11-21 14:02:40] iter = 0680, loss = 41.8230
2024-11-21 14:03:18: [2024-11-21 14:03:18] iter = 0690, loss = 41.8781
2024-11-21 14:03:56: [2024-11-21 14:03:56] iter = 0700, loss = 42.1984
2024-11-21 14:04:34: [2024-11-21 14:04:34] iter = 0710, loss = 42.3061
2024-11-21 14:05:11: [2024-11-21 14:05:11] iter = 0720, loss = 40.5836
2024-11-21 14:05:49: [2024-11-21 14:05:49] iter = 0730, loss = 43.3010
2024-11-21 14:06:27: [2024-11-21 14:06:27] iter = 0740, loss = 42.9878
2024-11-21 14:07:10: [2024-11-21 14:07:10] iter = 0750, loss = 43.3495
2024-11-21 14:07:48: [2024-11-21 14:07:48] iter = 0760, loss = 45.2946
2024-11-21 14:08:26: [2024-11-21 14:08:26] iter = 0770, loss = 45.0590
2024-11-21 14:09:04: [2024-11-21 14:09:04] iter = 0780, loss = 43.3091
2024-11-21 14:09:41: [2024-11-21 14:09:41] iter = 0790, loss = 43.2164
2024-11-21 14:10:19: [2024-11-21 14:10:19] iter = 0800, loss = 42.2388
2024-11-21 14:10:57: [2024-11-21 14:10:57] iter = 0810, loss = 45.0278
2024-11-21 14:11:35: [2024-11-21 14:11:35] iter = 0820, loss = 44.7928
2024-11-21 14:12:13: [2024-11-21 14:12:13] iter = 0830, loss = 45.5264
2024-11-21 14:12:51: [2024-11-21 14:12:51] iter = 0840, loss = 44.4757
2024-11-21 14:13:29: [2024-11-21 14:13:29] iter = 0850, loss = 43.0424
2024-11-21 14:14:06: [2024-11-21 14:14:06] iter = 0860, loss = 43.6521
2024-11-21 14:14:44: [2024-11-21 14:14:44] iter = 0870, loss = 43.3701
2024-11-21 14:15:22: [2024-11-21 14:15:22] iter = 0880, loss = 44.7154
2024-11-21 14:16:00: [2024-11-21 14:16:00] iter = 0890, loss = 43.2182
2024-11-21 14:16:38: [2024-11-21 14:16:38] iter = 0900, loss = 44.7596
2024-11-21 14:17:16: [2024-11-21 14:17:16] iter = 0910, loss = 44.4979
2024-11-21 14:17:54: [2024-11-21 14:17:54] iter = 0920, loss = 43.0181
2024-11-21 14:18:32: [2024-11-21 14:18:32] iter = 0930, loss = 43.1777
2024-11-21 14:19:09: [2024-11-21 14:19:09] iter = 0940, loss = 42.4848
2024-11-21 14:19:47: [2024-11-21 14:19:47] iter = 0950, loss = 44.8845
2024-11-21 14:20:25: [2024-11-21 14:20:25] iter = 0960, loss = 43.1569
2024-11-21 14:21:03: [2024-11-21 14:21:03] iter = 0970, loss = 43.6615
2024-11-21 14:21:41: [2024-11-21 14:21:41] iter = 0980, loss = 44.4157
2024-11-21 14:22:19: [2024-11-21 14:22:19] iter = 0990, loss = 42.9479
2024-11-21 14:22:54: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-21 14:22:54: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 14:23:06: Evaluate 5 random ConvNet, ACCmean = 0.3936 ACCstd = 0.0062
-------------------------
2024-11-21 14:23:06: Evaluate 5 random ConvNet, F1mean = 0.2953 F!std = 0.0025
-------------------------
2024-11-21 14:23:10: [2024-11-21 14:23:10] iter = 1000, loss = 45.5384
2024-11-21 14:23:10: 
================== Exp 2 ==================
 
2024-11-21 14:23:10: Hyper-parameters: 
{'method': 'DC', 'dataset': 'TissueMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f2a40641a60>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_TissueMNIST (INFO)>, 'dc_aug_param': None}
2024-11-21 14:23:10: Evaluation model pool: ['ConvNet']
2024-11-21 14:23:16: class c = 0: 53075 real images
2024-11-21 14:23:16: class c = 1: 7814 real images
2024-11-21 14:23:16: class c = 2: 5866 real images
2024-11-21 14:23:16: class c = 3: 15406 real images
2024-11-21 14:23:16: class c = 4: 11789 real images
2024-11-21 14:23:16: class c = 5: 7705 real images
2024-11-21 14:23:16: class c = 6: 39203 real images
2024-11-21 14:23:16: class c = 7: 24608 real images
2024-11-21 14:23:16: real images channel 0, mean = 0.1020, std = 0.1000
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 14:23:16: initialize synthetic data from random noise
2024-11-21 14:23:16: [2024-11-21 14:23:16] training begins
2024-11-21 14:23:16: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 14:23:16: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 14:23:28: Evaluate 5 random ConvNet, ACCmean = 0.2234 ACCstd = 0.0280
-------------------------
2024-11-21 14:23:28: Evaluate 5 random ConvNet, F1mean = 0.0989 F!std = 0.0023
-------------------------
2024-11-21 14:23:32: [2024-11-21 14:23:32] iter = 0000, loss = 267.6862
2024-11-21 14:24:10: [2024-11-21 14:24:10] iter = 0010, loss = 140.1615
2024-11-21 14:24:48: [2024-11-21 14:24:48] iter = 0020, loss = 101.6612
2024-11-21 14:25:26: [2024-11-21 14:25:26] iter = 0030, loss = 73.7822
2024-11-21 14:26:04: [2024-11-21 14:26:04] iter = 0040, loss = 71.0525
2024-11-21 14:26:42: [2024-11-21 14:26:42] iter = 0050, loss = 61.2847
2024-11-21 14:27:19: [2024-11-21 14:27:19] iter = 0060, loss = 58.5513
2024-11-21 14:27:57: [2024-11-21 14:27:57] iter = 0070, loss = 53.4795
2024-11-21 14:28:35: [2024-11-21 14:28:35] iter = 0080, loss = 53.4619
2024-11-21 14:29:13: [2024-11-21 14:29:13] iter = 0090, loss = 52.0013
2024-11-21 14:29:51: [2024-11-21 14:29:51] iter = 0100, loss = 53.0116
2024-11-21 14:30:29: [2024-11-21 14:30:29] iter = 0110, loss = 47.1765
2024-11-21 14:31:06: [2024-11-21 14:31:06] iter = 0120, loss = 48.2974
2024-11-21 14:31:44: [2024-11-21 14:31:44] iter = 0130, loss = 47.9605
2024-11-21 14:32:22: [2024-11-21 14:32:22] iter = 0140, loss = 45.9695
2024-11-21 14:33:00: [2024-11-21 14:33:00] iter = 0150, loss = 46.1730
2024-11-21 14:33:38: [2024-11-21 14:33:38] iter = 0160, loss = 44.9162
2024-11-21 14:34:16: [2024-11-21 14:34:16] iter = 0170, loss = 44.3434
2024-11-21 14:34:54: [2024-11-21 14:34:53] iter = 0180, loss = 43.7872
2024-11-21 14:35:31: [2024-11-21 14:35:31] iter = 0190, loss = 43.4275
2024-11-21 14:36:09: [2024-11-21 14:36:09] iter = 0200, loss = 43.2498
2024-11-21 14:36:47: [2024-11-21 14:36:47] iter = 0210, loss = 43.4823
2024-11-21 14:37:25: [2024-11-21 14:37:25] iter = 0220, loss = 44.5106
2024-11-21 14:38:03: [2024-11-21 14:38:03] iter = 0230, loss = 43.0352
2024-11-21 14:38:41: [2024-11-21 14:38:41] iter = 0240, loss = 43.0389
2024-11-21 14:39:19: [2024-11-21 14:39:19] iter = 0250, loss = 41.7013
2024-11-21 14:39:57: [2024-11-21 14:39:57] iter = 0260, loss = 41.6509
2024-11-21 14:40:35: [2024-11-21 14:40:35] iter = 0270, loss = 42.3003
2024-11-21 14:41:13: [2024-11-21 14:41:13] iter = 0280, loss = 41.9926
2024-11-21 14:41:50: [2024-11-21 14:41:50] iter = 0290, loss = 41.8772
2024-11-21 14:42:28: [2024-11-21 14:42:28] iter = 0300, loss = 43.7104
2024-11-21 14:43:06: [2024-11-21 14:43:06] iter = 0310, loss = 41.1787
2024-11-21 14:43:44: [2024-11-21 14:43:44] iter = 0320, loss = 42.1443
2024-11-21 14:44:22: [2024-11-21 14:44:22] iter = 0330, loss = 43.0191
2024-11-21 14:45:00: [2024-11-21 14:45:00] iter = 0340, loss = 41.5963
2024-11-21 14:45:38: [2024-11-21 14:45:38] iter = 0350, loss = 42.8233
2024-11-21 14:46:15: [2024-11-21 14:46:15] iter = 0360, loss = 42.0955
2024-11-21 14:46:53: [2024-11-21 14:46:53] iter = 0370, loss = 42.1328
2024-11-21 14:47:31: [2024-11-21 14:47:31] iter = 0380, loss = 43.0056
2024-11-21 14:48:09: [2024-11-21 14:48:09] iter = 0390, loss = 46.9529
2024-11-21 14:48:47: [2024-11-21 14:48:46] iter = 0400, loss = 40.8308
2024-11-21 14:49:25: [2024-11-21 14:49:25] iter = 0410, loss = 44.0866
2024-11-21 14:50:02: [2024-11-21 14:50:02] iter = 0420, loss = 43.5812
2024-11-21 14:50:40: [2024-11-21 14:50:40] iter = 0430, loss = 43.5731
2024-11-21 14:51:18: [2024-11-21 14:51:18] iter = 0440, loss = 42.6439
2024-11-21 14:51:56: [2024-11-21 14:51:56] iter = 0450, loss = 42.1072
2024-11-21 14:52:34: [2024-11-21 14:52:34] iter = 0460, loss = 40.8678
2024-11-21 14:53:12: [2024-11-21 14:53:12] iter = 0470, loss = 41.0864
2024-11-21 14:53:50: [2024-11-21 14:53:50] iter = 0480, loss = 41.1595
2024-11-21 14:54:27: [2024-11-21 14:54:27] iter = 0490, loss = 44.9440
2024-11-21 14:55:01: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-21 14:55:01: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 14:55:13: Evaluate 5 random ConvNet, ACCmean = 0.3788 ACCstd = 0.0062
-------------------------
2024-11-21 14:55:13: Evaluate 5 random ConvNet, F1mean = 0.2895 F!std = 0.0045
-------------------------
2024-11-21 14:55:17: [2024-11-21 14:55:17] iter = 0500, loss = 41.9911
2024-11-21 14:55:55: [2024-11-21 14:55:55] iter = 0510, loss = 43.3862
2024-11-21 14:56:33: [2024-11-21 14:56:33] iter = 0520, loss = 42.5137
2024-11-21 14:57:11: [2024-11-21 14:57:11] iter = 0530, loss = 43.9803
2024-11-21 14:57:49: [2024-11-21 14:57:49] iter = 0540, loss = 42.1797
2024-11-21 14:58:27: [2024-11-21 14:58:27] iter = 0550, loss = 43.4830
2024-11-21 14:59:05: [2024-11-21 14:59:05] iter = 0560, loss = 45.8520
2024-11-21 14:59:43: [2024-11-21 14:59:43] iter = 0570, loss = 39.8903
2024-11-21 15:00:20: [2024-11-21 15:00:20] iter = 0580, loss = 44.6365
2024-11-21 15:00:58: [2024-11-21 15:00:58] iter = 0590, loss = 41.8018
2024-11-21 15:01:36: [2024-11-21 15:01:36] iter = 0600, loss = 43.4012
2024-11-21 15:02:14: [2024-11-21 15:02:14] iter = 0610, loss = 42.5560
2024-11-21 15:02:51: [2024-11-21 15:02:51] iter = 0620, loss = 43.0231
2024-11-21 15:03:29: [2024-11-21 15:03:29] iter = 0630, loss = 43.4941
2024-11-21 15:04:06: [2024-11-21 15:04:06] iter = 0640, loss = 45.1166
2024-11-21 15:04:44: [2024-11-21 15:04:44] iter = 0650, loss = 43.3902
2024-11-21 15:05:21: [2024-11-21 15:05:21] iter = 0660, loss = 42.8213
2024-11-21 15:05:59: [2024-11-21 15:05:59] iter = 0670, loss = 43.9749
2024-11-21 15:06:37: [2024-11-21 15:06:36] iter = 0680, loss = 43.7538
2024-11-21 15:07:14: [2024-11-21 15:07:14] iter = 0690, loss = 42.8764
2024-11-21 15:07:52: [2024-11-21 15:07:52] iter = 0700, loss = 43.8948
2024-11-21 15:08:30: [2024-11-21 15:08:30] iter = 0710, loss = 43.1824
2024-11-21 15:09:08: [2024-11-21 15:09:08] iter = 0720, loss = 44.2358
2024-11-21 15:09:45: [2024-11-21 15:09:45] iter = 0730, loss = 44.3419
2024-11-21 15:10:23: [2024-11-21 15:10:23] iter = 0740, loss = 44.5267
2024-11-21 15:11:01: [2024-11-21 15:11:01] iter = 0750, loss = 42.5894
2024-11-21 15:11:39: [2024-11-21 15:11:39] iter = 0760, loss = 45.4164
2024-11-21 15:12:17: [2024-11-21 15:12:17] iter = 0770, loss = 42.3788
2024-11-21 15:12:54: [2024-11-21 15:12:54] iter = 0780, loss = 45.1139
2024-11-21 15:13:32: [2024-11-21 15:13:32] iter = 0790, loss = 43.2386
2024-11-21 15:14:10: [2024-11-21 15:14:10] iter = 0800, loss = 44.1703
2024-11-21 15:14:48: [2024-11-21 15:14:48] iter = 0810, loss = 41.8962
2024-11-21 15:15:26: [2024-11-21 15:15:26] iter = 0820, loss = 45.9235
2024-11-21 15:16:04: [2024-11-21 15:16:04] iter = 0830, loss = 44.7113
2024-11-21 15:16:41: [2024-11-21 15:16:41] iter = 0840, loss = 42.8489
2024-11-21 15:17:19: [2024-11-21 15:17:19] iter = 0850, loss = 43.7807
2024-11-21 15:17:57: [2024-11-21 15:17:57] iter = 0860, loss = 43.6989
2024-11-21 15:18:35: [2024-11-21 15:18:35] iter = 0870, loss = 44.5087
2024-11-21 15:19:13: [2024-11-21 15:19:13] iter = 0880, loss = 43.9388
2024-11-21 15:19:51: [2024-11-21 15:19:51] iter = 0890, loss = 44.8414
2024-11-21 15:20:29: [2024-11-21 15:20:29] iter = 0900, loss = 44.4503
2024-11-21 15:21:07: [2024-11-21 15:21:07] iter = 0910, loss = 44.7388
2024-11-21 15:21:45: [2024-11-21 15:21:45] iter = 0920, loss = 44.5928
2024-11-21 15:22:22: [2024-11-21 15:22:22] iter = 0930, loss = 40.5006
2024-11-21 15:23:00: [2024-11-21 15:23:00] iter = 0940, loss = 43.0399
2024-11-21 15:23:38: [2024-11-21 15:23:38] iter = 0950, loss = 43.7512
2024-11-21 15:24:16: [2024-11-21 15:24:16] iter = 0960, loss = 43.4737
2024-11-21 15:24:53: [2024-11-21 15:24:53] iter = 0970, loss = 42.8082
2024-11-21 15:25:31: [2024-11-21 15:25:31] iter = 0980, loss = 44.7796
2024-11-21 15:26:09: [2024-11-21 15:26:09] iter = 0990, loss = 41.4579
2024-11-21 15:26:43: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-21 15:26:43: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
Using downloaded and verified file: /data/users/xiongyuxuan/.medmnist/tissuemnist.npz
Using downloaded and verified file: /data/users/xiongyuxuan/.medmnist/tissuemnist.npz
Loaded the dataset:TissueMNIST
[2024-11-21 12:12:18] Evaluate_00: epoch = 0300 train time = 2 s train loss = 0.003156 train acc = 1.0000, test acc = 0.0620, test_sen =0.1162, test_spe =0.8746, test_f1 =0.0563
[2024-11-21 12:12:20] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.003141 train acc = 1.0000, test acc = 0.1009, test_sen =0.1225, test_spe =0.8746, test_f1 =0.0712
[2024-11-21 12:12:23] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.003138 train acc = 1.0000, test acc = 0.1004, test_sen =0.1276, test_spe =0.8754, test_f1 =0.0711
[2024-11-21 12:12:25] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.003429 train acc = 1.0000, test acc = 0.1071, test_sen =0.1338, test_spe =0.8760, test_f1 =0.0765
[2024-11-21 12:12:28] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.003220 train acc = 1.0000, test acc = 0.1122, test_sen =0.1313, test_spe =0.8754, test_f1 =0.0780
[2024-11-21 12:44:38] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.008268 train acc = 1.0000, test acc = 0.3865, test_sen =0.3134, test_spe =0.9093, test_f1 =0.2956
[2024-11-21 12:44:41] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.007415 train acc = 1.0000, test acc = 0.3671, test_sen =0.3109, test_spe =0.9080, test_f1 =0.2871
[2024-11-21 12:44:43] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.007309 train acc = 1.0000, test acc = 0.3550, test_sen =0.3060, test_spe =0.9065, test_f1 =0.2830
[2024-11-21 12:44:45] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.007939 train acc = 1.0000, test acc = 0.3648, test_sen =0.2999, test_spe =0.9072, test_f1 =0.2815
[2024-11-21 12:44:48] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.007583 train acc = 1.0000, test acc = 0.3673, test_sen =0.3051, test_spe =0.9075, test_f1 =0.2869
[2024-11-21 13:16:24] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.008155 train acc = 1.0000, test acc = 0.3875, test_sen =0.3114, test_spe =0.9097, test_f1 =0.2917
[2024-11-21 13:16:26] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.008448 train acc = 1.0000, test acc = 0.3732, test_sen =0.3194, test_spe =0.9095, test_f1 =0.2898
[2024-11-21 13:16:29] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.009409 train acc = 1.0000, test acc = 0.3585, test_sen =0.3031, test_spe =0.9073, test_f1 =0.2767
[2024-11-21 13:16:31] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.008476 train acc = 1.0000, test acc = 0.3607, test_sen =0.3119, test_spe =0.9079, test_f1 =0.2916
[2024-11-21 13:16:33] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.009457 train acc = 1.0000, test acc = 0.3588, test_sen =0.3055, test_spe =0.9073, test_f1 =0.2810
[2024-11-21 13:16:44] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.003170 train acc = 1.0000, test acc = 0.1627, test_sen =0.1180, test_spe =0.8726, test_f1 =0.1034
[2024-11-21 13:16:47] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.003216 train acc = 1.0000, test acc = 0.1604, test_sen =0.1190, test_spe =0.8713, test_f1 =0.1016
[2024-11-21 13:16:49] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.003335 train acc = 1.0000, test acc = 0.1576, test_sen =0.1112, test_spe =0.8699, test_f1 =0.0890
[2024-11-21 13:16:52] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.003124 train acc = 1.0000, test acc = 0.1989, test_sen =0.1163, test_spe =0.8697, test_f1 =0.0944
[2024-11-21 13:16:54] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.003105 train acc = 1.0000, test acc = 0.1720, test_sen =0.1124, test_spe =0.8697, test_f1 =0.0944
[2024-11-21 13:50:28] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.007497 train acc = 1.0000, test acc = 0.4114, test_sen =0.3208, test_spe =0.9123, test_f1 =0.3056
[2024-11-21 13:50:31] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.007851 train acc = 1.0000, test acc = 0.3977, test_sen =0.3128, test_spe =0.9111, test_f1 =0.2972
[2024-11-21 13:50:35] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.007645 train acc = 1.0000, test acc = 0.3815, test_sen =0.3094, test_spe =0.9093, test_f1 =0.2892
[2024-11-21 13:50:38] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.007260 train acc = 1.0000, test acc = 0.3912, test_sen =0.3182, test_spe =0.9108, test_f1 =0.2980
[2024-11-21 13:50:41] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.007834 train acc = 1.0000, test acc = 0.3806, test_sen =0.3098, test_spe =0.9094, test_f1 =0.2836
[2024-11-21 14:22:56] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.008803 train acc = 1.0000, test acc = 0.3862, test_sen =0.3119, test_spe =0.9098, test_f1 =0.2950
[2024-11-21 14:22:58] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.008449 train acc = 1.0000, test acc = 0.3892, test_sen =0.3122, test_spe =0.9100, test_f1 =0.2943
[2024-11-21 14:23:01] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.008727 train acc = 1.0000, test acc = 0.4010, test_sen =0.3138, test_spe =0.9109, test_f1 =0.2997
[2024-11-21 14:23:03] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.008590 train acc = 1.0000, test acc = 0.3905, test_sen =0.3105, test_spe =0.9102, test_f1 =0.2921
[2024-11-21 14:23:06] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.007730 train acc = 1.0000, test acc = 0.4009, test_sen =0.3063, test_spe =0.9105, test_f1 =0.2954
[2024-11-21 14:23:18] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.003098 train acc = 1.0000, test acc = 0.2452, test_sen =0.1346, test_spe =0.8777, test_f1 =0.1021
[2024-11-21 14:23:21] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.003130 train acc = 1.0000, test acc = 0.2341, test_sen =0.1387, test_spe =0.8795, test_f1 =0.1005
[2024-11-21 14:23:23] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.003196 train acc = 1.0000, test acc = 0.2259, test_sen =0.1359, test_spe =0.8783, test_f1 =0.0978
[2024-11-21 14:23:26] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.003132 train acc = 1.0000, test acc = 0.2427, test_sen =0.1390, test_spe =0.8795, test_f1 =0.0985
[2024-11-21 14:23:28] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.003197 train acc = 1.0000, test acc = 0.1691, test_sen =0.1260, test_spe =0.8747, test_f1 =0.0954
[2024-11-21 14:55:04] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.008268 train acc = 1.0000, test acc = 0.3799, test_sen =0.3120, test_spe =0.9092, test_f1 =0.2940
[2024-11-21 14:55:06] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.007634 train acc = 1.0000, test acc = 0.3718, test_sen =0.3047, test_spe =0.9075, test_f1 =0.2828
[2024-11-21 14:55:09] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.008112 train acc = 1.0000, test acc = 0.3730, test_sen =0.3087, test_spe =0.9082, test_f1 =0.2877
[2024-11-21 14:55:11] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.008139 train acc = 1.0000, test acc = 0.3801, test_sen =0.3108, test_spe =0.9089, test_f1 =0.2881
[2024-11-21 14:55:13] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.008202 train acc = 1.0000, test acc = 0.3890, test_sen =0.3151, test_spe =0.9097, test_f1 =0.2950
[2024-11-21 15:26:46] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.008738 train acc = 1.0000, test acc = 0.3682, test_sen =0.3078, test_spe =0.9079, test_f1 =0.2829
[2024-11-21 15:26:48] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.008076 train acc = 1.0000, test acc = 0.3770, test_sen =0.3122, test_spe =0.9094, test_f1 =0.2897
[2024-11-21 15:26:51] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.009132 train acc = 1.0000, test acc = 0.3795, test_sen =0.3152, test_spe =0.9091, test_f1 =0.2973
[2024-11-21 15:26:53] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.008855 train acc = 1.0000, test acc = 0.3898, test_sen =0.3114, test_spe =0.9102, test_f1 =0.2964
[2024-11-21 15:26:55] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.008974 train acc = 1.0000, test acc = 0.3621, test_sen =0.3121, test_spe =0.9085, test_f1 =0.28412024-11-21 15:26:55: Evaluate 5 random ConvNet, ACCmean = 0.3753 ACCstd = 0.0095
-------------------------
2024-11-21 15:26:55: Evaluate 5 random ConvNet, F1mean = 0.2901 F!std = 0.0060
-------------------------
2024-11-21 15:26:59: [2024-11-21 15:26:59] iter = 1000, loss = 44.2209
2024-11-21 15:26:59: 
================== Exp 3 ==================
 
2024-11-21 15:26:59: Hyper-parameters: 
{'method': 'DC', 'dataset': 'TissueMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f2a40641a60>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_TissueMNIST (INFO)>, 'dc_aug_param': None}
2024-11-21 15:26:59: Evaluation model pool: ['ConvNet']
2024-11-21 15:27:03: class c = 0: 53075 real images
2024-11-21 15:27:03: class c = 1: 7814 real images
2024-11-21 15:27:03: class c = 2: 5866 real images
2024-11-21 15:27:03: class c = 3: 15406 real images
2024-11-21 15:27:03: class c = 4: 11789 real images
2024-11-21 15:27:03: class c = 5: 7705 real images
2024-11-21 15:27:03: class c = 6: 39203 real images
2024-11-21 15:27:03: class c = 7: 24608 real images
2024-11-21 15:27:03: real images channel 0, mean = 0.1020, std = 0.1000
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 15:27:03: initialize synthetic data from random noise
2024-11-21 15:27:03: [2024-11-21 15:27:03] training begins
2024-11-21 15:27:03: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 15:27:03: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 15:27:15: Evaluate 5 random ConvNet, ACCmean = 0.0909 ACCstd = 0.0072
-------------------------
2024-11-21 15:27:15: Evaluate 5 random ConvNet, F1mean = 0.0714 F!std = 0.0046
-------------------------
2024-11-21 15:27:19: [2024-11-21 15:27:19] iter = 0000, loss = 259.6819
2024-11-21 15:27:57: [2024-11-21 15:27:57] iter = 0010, loss = 126.6743
2024-11-21 15:28:35: [2024-11-21 15:28:35] iter = 0020, loss = 100.0664
2024-11-21 15:29:13: [2024-11-21 15:29:13] iter = 0030, loss = 75.8863
2024-11-21 15:29:51: [2024-11-21 15:29:51] iter = 0040, loss = 66.7297
2024-11-21 15:30:28: [2024-11-21 15:30:28] iter = 0050, loss = 63.1162
2024-11-21 15:31:06: [2024-11-21 15:31:06] iter = 0060, loss = 62.6496
2024-11-21 15:31:44: [2024-11-21 15:31:44] iter = 0070, loss = 55.5343
2024-11-21 15:32:22: [2024-11-21 15:32:22] iter = 0080, loss = 56.0704
2024-11-21 15:33:00: [2024-11-21 15:33:00] iter = 0090, loss = 55.8228
2024-11-21 15:33:38: [2024-11-21 15:33:38] iter = 0100, loss = 50.8407
2024-11-21 15:34:16: [2024-11-21 15:34:16] iter = 0110, loss = 50.5563
2024-11-21 15:34:54: [2024-11-21 15:34:54] iter = 0120, loss = 51.5689
2024-11-21 15:35:32: [2024-11-21 15:35:32] iter = 0130, loss = 47.9879
2024-11-21 15:36:10: [2024-11-21 15:36:10] iter = 0140, loss = 48.3488
2024-11-21 15:36:48: [2024-11-21 15:36:48] iter = 0150, loss = 46.2429
2024-11-21 15:37:26: [2024-11-21 15:37:26] iter = 0160, loss = 46.5001
2024-11-21 15:38:03: [2024-11-21 15:38:03] iter = 0170, loss = 43.2797
2024-11-21 15:38:41: [2024-11-21 15:38:41] iter = 0180, loss = 42.6180
2024-11-21 15:39:19: [2024-11-21 15:39:19] iter = 0190, loss = 45.4312
2024-11-21 15:39:57: [2024-11-21 15:39:57] iter = 0200, loss = 43.9814
2024-11-21 15:40:35: [2024-11-21 15:40:35] iter = 0210, loss = 45.4014
2024-11-21 15:41:13: [2024-11-21 15:41:13] iter = 0220, loss = 42.1584
2024-11-21 15:41:51: [2024-11-21 15:41:51] iter = 0230, loss = 44.1300
2024-11-21 15:42:29: [2024-11-21 15:42:29] iter = 0240, loss = 41.3820
2024-11-21 15:43:07: [2024-11-21 15:43:07] iter = 0250, loss = 41.9459
2024-11-21 15:43:44: [2024-11-21 15:43:44] iter = 0260, loss = 45.7625
2024-11-21 15:44:22: [2024-11-21 15:44:22] iter = 0270, loss = 45.1302
2024-11-21 15:45:00: [2024-11-21 15:45:00] iter = 0280, loss = 42.2802
2024-11-21 15:45:38: [2024-11-21 15:45:38] iter = 0290, loss = 41.6778
2024-11-21 15:46:16: [2024-11-21 15:46:16] iter = 0300, loss = 43.0101
2024-11-21 15:46:54: [2024-11-21 15:46:54] iter = 0310, loss = 41.3372
2024-11-21 15:47:32: [2024-11-21 15:47:32] iter = 0320, loss = 42.5416
2024-11-21 15:48:09: [2024-11-21 15:48:09] iter = 0330, loss = 41.0373
2024-11-21 15:48:47: [2024-11-21 15:48:47] iter = 0340, loss = 41.3791
2024-11-21 15:49:25: [2024-11-21 15:49:25] iter = 0350, loss = 41.3211
2024-11-21 15:50:03: [2024-11-21 15:50:03] iter = 0360, loss = 42.8251
2024-11-21 15:50:41: [2024-11-21 15:50:41] iter = 0370, loss = 42.4018
2024-11-21 15:51:19: [2024-11-21 15:51:19] iter = 0380, loss = 41.7089
2024-11-21 15:51:57: [2024-11-21 15:51:57] iter = 0390, loss = 42.6514
2024-11-21 15:52:35: [2024-11-21 15:52:35] iter = 0400, loss = 44.3650
2024-11-21 15:53:13: [2024-11-21 15:53:13] iter = 0410, loss = 43.1057
2024-11-21 15:53:50: [2024-11-21 15:53:50] iter = 0420, loss = 40.3893
2024-11-21 15:54:28: [2024-11-21 15:54:28] iter = 0430, loss = 43.5809
2024-11-21 15:55:06: [2024-11-21 15:55:06] iter = 0440, loss = 41.4169
2024-11-21 15:55:44: [2024-11-21 15:55:44] iter = 0450, loss = 42.4551
2024-11-21 15:56:22: [2024-11-21 15:56:22] iter = 0460, loss = 43.7121
2024-11-21 15:57:00: [2024-11-21 15:57:00] iter = 0470, loss = 39.5924
2024-11-21 15:57:38: [2024-11-21 15:57:38] iter = 0480, loss = 43.8319
2024-11-21 15:58:16: [2024-11-21 15:58:16] iter = 0490, loss = 41.7559
2024-11-21 15:58:50: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-21 15:58:50: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 15:59:02: Evaluate 5 random ConvNet, ACCmean = 0.3966 ACCstd = 0.0142
-------------------------
2024-11-21 15:59:02: Evaluate 5 random ConvNet, F1mean = 0.2989 F!std = 0.0061
-------------------------
2024-11-21 15:59:06: [2024-11-21 15:59:06] iter = 0500, loss = 43.8349
2024-11-21 15:59:43: [2024-11-21 15:59:43] iter = 0510, loss = 43.7351
2024-11-21 16:00:21: [2024-11-21 16:00:21] iter = 0520, loss = 43.4041
2024-11-21 16:00:58: [2024-11-21 16:00:58] iter = 0530, loss = 44.3415
2024-11-21 16:01:36: [2024-11-21 16:01:36] iter = 0540, loss = 42.3814
2024-11-21 16:02:14: [2024-11-21 16:02:14] iter = 0550, loss = 41.6269
2024-11-21 16:02:52: [2024-11-21 16:02:52] iter = 0560, loss = 41.4215
2024-11-21 16:03:30: [2024-11-21 16:03:30] iter = 0570, loss = 43.9246
2024-11-21 16:04:08: [2024-11-21 16:04:08] iter = 0580, loss = 42.6718
2024-11-21 16:04:46: [2024-11-21 16:04:46] iter = 0590, loss = 42.8083
2024-11-21 16:05:24: [2024-11-21 16:05:24] iter = 0600, loss = 42.5908
2024-11-21 16:06:02: [2024-11-21 16:06:02] iter = 0610, loss = 42.6026
2024-11-21 16:06:40: [2024-11-21 16:06:40] iter = 0620, loss = 44.8138
2024-11-21 16:07:18: [2024-11-21 16:07:18] iter = 0630, loss = 41.1032
2024-11-21 16:07:55: [2024-11-21 16:07:55] iter = 0640, loss = 43.2238
2024-11-21 16:08:33: [2024-11-21 16:08:33] iter = 0650, loss = 44.4320
2024-11-21 16:09:11: [2024-11-21 16:09:11] iter = 0660, loss = 44.2012
2024-11-21 16:09:49: [2024-11-21 16:09:49] iter = 0670, loss = 41.3881
2024-11-21 16:10:27: [2024-11-21 16:10:27] iter = 0680, loss = 44.5456
2024-11-21 16:11:05: [2024-11-21 16:11:05] iter = 0690, loss = 41.2411
2024-11-21 16:11:43: [2024-11-21 16:11:43] iter = 0700, loss = 44.4511
2024-11-21 16:12:21: [2024-11-21 16:12:21] iter = 0710, loss = 44.5082
2024-11-21 16:12:59: [2024-11-21 16:12:59] iter = 0720, loss = 42.9636
2024-11-21 16:13:37: [2024-11-21 16:13:37] iter = 0730, loss = 42.7231
2024-11-21 16:14:14: [2024-11-21 16:14:14] iter = 0740, loss = 41.5823
2024-11-21 16:14:52: [2024-11-21 16:14:52] iter = 0750, loss = 44.1836
2024-11-21 16:15:30: [2024-11-21 16:15:30] iter = 0760, loss = 42.3739
2024-11-21 16:16:08: [2024-11-21 16:16:08] iter = 0770, loss = 46.2838
2024-11-21 16:16:46: [2024-11-21 16:16:46] iter = 0780, loss = 45.0671
2024-11-21 16:17:24: [2024-11-21 16:17:24] iter = 0790, loss = 42.3776
2024-11-21 16:18:01: [2024-11-21 16:18:01] iter = 0800, loss = 44.2648
2024-11-21 16:18:39: [2024-11-21 16:18:39] iter = 0810, loss = 42.6289
2024-11-21 16:19:17: [2024-11-21 16:19:17] iter = 0820, loss = 44.2559
2024-11-21 16:19:55: [2024-11-21 16:19:55] iter = 0830, loss = 43.7024
2024-11-21 16:20:33: [2024-11-21 16:20:33] iter = 0840, loss = 43.7214
2024-11-21 16:21:11: [2024-11-21 16:21:11] iter = 0850, loss = 45.3416
2024-11-21 16:21:48: [2024-11-21 16:21:48] iter = 0860, loss = 42.7700
2024-11-21 16:22:26: [2024-11-21 16:22:26] iter = 0870, loss = 44.2826
2024-11-21 16:23:04: [2024-11-21 16:23:04] iter = 0880, loss = 45.2372
2024-11-21 16:23:42: [2024-11-21 16:23:42] iter = 0890, loss = 42.4790
2024-11-21 16:24:20: [2024-11-21 16:24:20] iter = 0900, loss = 43.7307
2024-11-21 16:24:58: [2024-11-21 16:24:57] iter = 0910, loss = 43.2953
2024-11-21 16:25:36: [2024-11-21 16:25:36] iter = 0920, loss = 44.4278
2024-11-21 16:26:13: [2024-11-21 16:26:13] iter = 0930, loss = 43.2150
2024-11-21 16:26:51: [2024-11-21 16:26:51] iter = 0940, loss = 43.6870
2024-11-21 16:27:29: [2024-11-21 16:27:29] iter = 0950, loss = 44.4849
2024-11-21 16:28:07: [2024-11-21 16:28:07] iter = 0960, loss = 44.1102
2024-11-21 16:28:45: [2024-11-21 16:28:45] iter = 0970, loss = 44.6039
2024-11-21 16:29:23: [2024-11-21 16:29:23] iter = 0980, loss = 42.8801
2024-11-21 16:30:01: [2024-11-21 16:30:01] iter = 0990, loss = 41.8196
2024-11-21 16:30:35: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-21 16:30:35: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 16:30:47: Evaluate 5 random ConvNet, ACCmean = 0.3621 ACCstd = 0.0135
-------------------------
2024-11-21 16:30:47: Evaluate 5 random ConvNet, F1mean = 0.2879 F!std = 0.0046
-------------------------
2024-11-21 16:30:51: [2024-11-21 16:30:51] iter = 1000, loss = 44.9236
2024-11-21 16:30:51: 
================== Exp 4 ==================
 
2024-11-21 16:30:51: Hyper-parameters: 
{'method': 'DC', 'dataset': 'TissueMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f2a40641a60>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_TissueMNIST (INFO)>, 'dc_aug_param': None}
2024-11-21 16:30:51: Evaluation model pool: ['ConvNet']
2024-11-21 16:30:55: class c = 0: 53075 real images
2024-11-21 16:30:55: class c = 1: 7814 real images
2024-11-21 16:30:55: class c = 2: 5866 real images
2024-11-21 16:30:55: class c = 3: 15406 real images
2024-11-21 16:30:55: class c = 4: 11789 real images
2024-11-21 16:30:55: class c = 5: 7705 real images
2024-11-21 16:30:55: class c = 6: 39203 real images
2024-11-21 16:30:55: class c = 7: 24608 real images
2024-11-21 16:30:55: real images channel 0, mean = 0.1020, std = 0.1000
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 16:30:55: initialize synthetic data from random noise
2024-11-21 16:30:55: [2024-11-21 16:30:55] training begins
2024-11-21 16:30:55: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 16:30:55: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 16:31:07: Evaluate 5 random ConvNet, ACCmean = 0.0839 ACCstd = 0.0103
-------------------------
2024-11-21 16:31:07: Evaluate 5 random ConvNet, F1mean = 0.0631 F!std = 0.0062
-------------------------
2024-11-21 16:31:11: [2024-11-21 16:31:11] iter = 0000, loss = 251.2523
2024-11-21 16:31:49: [2024-11-21 16:31:49] iter = 0010, loss = 130.4635
2024-11-21 16:32:26: [2024-11-21 16:32:26] iter = 0020, loss = 95.6118
2024-11-21 16:33:04: [2024-11-21 16:33:04] iter = 0030, loss = 81.5178
2024-11-21 16:33:41: [2024-11-21 16:33:41] iter = 0040, loss = 64.4513
2024-11-21 16:34:19: [2024-11-21 16:34:19] iter = 0050, loss = 64.8297
2024-11-21 16:34:57: [2024-11-21 16:34:57] iter = 0060, loss = 61.3933
2024-11-21 16:35:34: [2024-11-21 16:35:34] iter = 0070, loss = 56.7122
2024-11-21 16:36:11: [2024-11-21 16:36:11] iter = 0080, loss = 52.9228
2024-11-21 16:36:49: [2024-11-21 16:36:49] iter = 0090, loss = 49.5139
2024-11-21 16:37:27: [2024-11-21 16:37:27] iter = 0100, loss = 52.0141
2024-11-21 16:38:05: [2024-11-21 16:38:05] iter = 0110, loss = 50.5515
2024-11-21 16:38:42: [2024-11-21 16:38:42] iter = 0120, loss = 50.4167
2024-11-21 16:39:19: [2024-11-21 16:39:19] iter = 0130, loss = 50.6468
2024-11-21 16:39:57: [2024-11-21 16:39:57] iter = 0140, loss = 48.5632
2024-11-21 16:40:35: [2024-11-21 16:40:35] iter = 0150, loss = 46.5781
2024-11-21 16:41:13: [2024-11-21 16:41:13] iter = 0160, loss = 48.9475
2024-11-21 16:41:50: [2024-11-21 16:41:50] iter = 0170, loss = 46.6172
2024-11-21 16:42:28: [2024-11-21 16:42:28] iter = 0180, loss = 45.6301
2024-11-21 16:43:05: [2024-11-21 16:43:05] iter = 0190, loss = 40.9409
2024-11-21 16:43:42: [2024-11-21 16:43:42] iter = 0200, loss = 45.7221
2024-11-21 16:44:20: [2024-11-21 16:44:20] iter = 0210, loss = 43.2435
2024-11-21 16:44:58: [2024-11-21 16:44:58] iter = 0220, loss = 45.7892
2024-11-21 16:45:36: [2024-11-21 16:45:36] iter = 0230, loss = 44.0276
2024-11-21 16:46:14: [2024-11-21 16:46:14] iter = 0240, loss = 42.3106
2024-11-21 16:46:51: [2024-11-21 16:46:51] iter = 0250, loss = 42.0491
2024-11-21 16:47:29: [2024-11-21 16:47:29] iter = 0260, loss = 44.0851
2024-11-21 16:48:07: [2024-11-21 16:48:07] iter = 0270, loss = 42.7933
2024-11-21 16:48:45: [2024-11-21 16:48:45] iter = 0280, loss = 44.9073
2024-11-21 16:49:23: [2024-11-21 16:49:23] iter = 0290, loss = 45.2103
2024-11-21 16:50:01: [2024-11-21 16:50:01] iter = 0300, loss = 43.4933
2024-11-21 16:50:39: [2024-11-21 16:50:39] iter = 0310, loss = 43.5965
2024-11-21 16:51:17: [2024-11-21 16:51:17] iter = 0320, loss = 41.9890
2024-11-21 16:51:55: [2024-11-21 16:51:55] iter = 0330, loss = 43.1143
2024-11-21 16:52:32: [2024-11-21 16:52:32] iter = 0340, loss = 42.0313
2024-11-21 16:53:10: [2024-11-21 16:53:10] iter = 0350, loss = 44.9624
2024-11-21 16:53:48: [2024-11-21 16:53:48] iter = 0360, loss = 42.2016
2024-11-21 16:54:26: [2024-11-21 16:54:26] iter = 0370, loss = 43.5149
2024-11-21 16:55:04: [2024-11-21 16:55:04] iter = 0380, loss = 43.1134
2024-11-21 16:55:41: [2024-11-21 16:55:41] iter = 0390, loss = 41.3745
2024-11-21 16:56:19: [2024-11-21 16:56:19] iter = 0400, loss = 42.7359
2024-11-21 16:56:57: [2024-11-21 16:56:57] iter = 0410, loss = 42.0973
2024-11-21 16:57:35: [2024-11-21 16:57:35] iter = 0420, loss = 43.6310
2024-11-21 16:58:13: [2024-11-21 16:58:13] iter = 0430, loss = 42.4079
2024-11-21 16:58:49: [2024-11-21 16:58:49] iter = 0440, loss = 41.4431
2024-11-21 16:59:27: [2024-11-21 16:59:27] iter = 0450, loss = 41.2870
2024-11-21 17:00:04: [2024-11-21 17:00:04] iter = 0460, loss = 41.9440
2024-11-21 17:00:42: [2024-11-21 17:00:42] iter = 0470, loss = 43.1740
2024-11-21 17:01:20: [2024-11-21 17:01:20] iter = 0480, loss = 44.6251
2024-11-21 17:01:57: [2024-11-21 17:01:57] iter = 0490, loss = 42.0631
2024-11-21 17:02:31: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-21 17:02:31: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 17:02:43: Evaluate 5 random ConvNet, ACCmean = 0.3810 ACCstd = 0.0032
-------------------------
2024-11-21 17:02:43: Evaluate 5 random ConvNet, F1mean = 0.2923 F!std = 0.0026
-------------------------
2024-11-21 17:02:47: [2024-11-21 17:02:47] iter = 0500, loss = 41.1487
2024-11-21 17:03:25: [2024-11-21 17:03:25] iter = 0510, loss = 42.4686
2024-11-21 17:04:02: [2024-11-21 17:04:02] iter = 0520, loss = 44.3365
2024-11-21 17:04:40: [2024-11-21 17:04:40] iter = 0530, loss = 44.9804
2024-11-21 17:05:18: [2024-11-21 17:05:18] iter = 0540, loss = 43.8110
2024-11-21 17:05:55: [2024-11-21 17:05:55] iter = 0550, loss = 43.0013
2024-11-21 17:06:33: [2024-11-21 17:06:33] iter = 0560, loss = 44.1361
2024-11-21 17:07:11: [2024-11-21 17:07:11] iter = 0570, loss = 44.0755
2024-11-21 17:07:49: [2024-11-21 17:07:49] iter = 0580, loss = 42.2637
2024-11-21 17:08:26: [2024-11-21 17:08:26] iter = 0590, loss = 43.3205
2024-11-21 17:09:04: [2024-11-21 17:09:04] iter = 0600, loss = 43.2576
2024-11-21 17:09:42: [2024-11-21 17:09:42] iter = 0610, loss = 42.7596
2024-11-21 17:10:20: [2024-11-21 17:10:20] iter = 0620, loss = 42.9441
2024-11-21 17:10:58: [2024-11-21 17:10:58] iter = 0630, loss = 41.9354
2024-11-21 17:11:36: [2024-11-21 17:11:36] iter = 0640, loss = 41.6911

[2024-11-21 15:27:06] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.003208 train acc = 1.0000, test acc = 0.0839, test_sen =0.1189, test_spe =0.8751, test_f1 =0.0691
[2024-11-21 15:27:08] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.003296 train acc = 1.0000, test acc = 0.1042, test_sen =0.1175, test_spe =0.8750, test_f1 =0.0790
[2024-11-21 15:27:11] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.003199 train acc = 1.0000, test acc = 0.0924, test_sen =0.1183, test_spe =0.8762, test_f1 =0.0742
[2024-11-21 15:27:13] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.003174 train acc = 1.0000, test acc = 0.0864, test_sen =0.1207, test_spe =0.8763, test_f1 =0.0664
[2024-11-21 15:27:15] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.003187 train acc = 1.0000, test acc = 0.0879, test_sen =0.1215, test_spe =0.8756, test_f1 =0.0680
[2024-11-21 15:58:52] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.008037 train acc = 1.0000, test acc = 0.3961, test_sen =0.3220, test_spe =0.9113, test_f1 =0.3050
[2024-11-21 15:58:55] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.007681 train acc = 1.0000, test acc = 0.4208, test_sen =0.3109, test_spe =0.9120, test_f1 =0.3028
[2024-11-21 15:58:57] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.009370 train acc = 1.0000, test acc = 0.3762, test_sen =0.3050, test_spe =0.9085, test_f1 =0.2873
[2024-11-21 15:59:00] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.007658 train acc = 1.0000, test acc = 0.3941, test_sen =0.3138, test_spe =0.9102, test_f1 =0.3002
[2024-11-21 15:59:02] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.008238 train acc = 1.0000, test acc = 0.3958, test_sen =0.3123, test_spe =0.9103, test_f1 =0.2990
[2024-11-21 16:30:37] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.008903 train acc = 1.0000, test acc = 0.3838, test_sen =0.3129, test_spe =0.9084, test_f1 =0.2962
[2024-11-21 16:30:40] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.008617 train acc = 1.0000, test acc = 0.3413, test_sen =0.3089, test_spe =0.9053, test_f1 =0.2831
[2024-11-21 16:30:42] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.008588 train acc = 1.0000, test acc = 0.3641, test_sen =0.3025, test_spe =0.9066, test_f1 =0.2842
[2024-11-21 16:30:45] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.009219 train acc = 1.0000, test acc = 0.3593, test_sen =0.3108, test_spe =0.9068, test_f1 =0.2892
[2024-11-21 16:30:47] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.008319 train acc = 1.0000, test acc = 0.3622, test_sen =0.3068, test_spe =0.9067, test_f1 =0.2870
[2024-11-21 16:30:58] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.003167 train acc = 1.0000, test acc = 0.0814, test_sen =0.1147, test_spe =0.8731, test_f1 =0.0600
[2024-11-21 16:31:00] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.003244 train acc = 1.0000, test acc = 0.0757, test_sen =0.1158, test_spe =0.8736, test_f1 =0.0589
[2024-11-21 16:31:02] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.003166 train acc = 1.0000, test acc = 0.0904, test_sen =0.1148, test_spe =0.8731, test_f1 =0.0616
[2024-11-21 16:31:05] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.003171 train acc = 1.0000, test acc = 0.0718, test_sen =0.1160, test_spe =0.8734, test_f1 =0.0597
[2024-11-21 16:31:07] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.003104 train acc = 1.0000, test acc = 0.1004, test_sen =0.1173, test_spe =0.8733, test_f1 =0.0754
[2024-11-21 17:02:34] Evaluate_00: epoch = 0300 train time = 1 s train loss = 0.007345 train acc = 1.0000, test acc = 0.3852, test_sen =0.3092, test_spe =0.9093, test_f1 =0.2906
[2024-11-21 17:02:36] Evaluate_01: epoch = 0300 train time = 1 s train loss = 0.007393 train acc = 1.0000, test acc = 0.3792, test_sen =0.3099, test_spe =0.9093, test_f1 =0.2893
[2024-11-21 17:02:38] Evaluate_02: epoch = 0300 train time = 1 s train loss = 0.007340 train acc = 1.0000, test acc = 0.3790, test_sen =0.3072, test_spe =0.9090, test_f1 =0.2908
[2024-11-21 17:02:41] Evaluate_03: epoch = 0300 train time = 1 s train loss = 0.007889 train acc = 1.0000, test acc = 0.3773, test_sen =0.3119, test_spe =0.9089, test_f1 =0.2949
[2024-11-21 17:02:43] Evaluate_04: epoch = 0300 train time = 1 s train loss = 0.007499 train acc = 1.0000, test acc = 0.3843, test_sen =0.3126, test_spe =0.9097, test_f1 =0.2960
Traceback (most recent call last):
  File "main_base.py", line 283, in <module>
    main()
  File "main_base.py", line 249, in main
    loss.backward()
  File "/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/data/users/xiongyuxuan/anaconda3/envs/DD/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
