nohup: ignoring input
2024-11-21 18:29:58: eval_it_pool: [0, 500, 1000]
2024-11-21 18:29:58: 
================== Exp 0 ==================
 
2024-11-21 18:29:58: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganSMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fb9e6470a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganSMNIST (INFO)>}
2024-11-21 18:29:58: Evaluation model pool: ['ConvNet']
2024-11-21 18:30:01: class c = 0: 1148 real images
2024-11-21 18:30:01: class c = 1: 630 real images
2024-11-21 18:30:01: class c = 2: 614 real images
2024-11-21 18:30:01: class c = 3: 721 real images
2024-11-21 18:30:01: class c = 4: 1132 real images
2024-11-21 18:30:01: class c = 5: 1119 real images
2024-11-21 18:30:01: class c = 6: 3464 real images
2024-11-21 18:30:01: class c = 7: 741 real images
2024-11-21 18:30:01: class c = 8: 803 real images
2024-11-21 18:30:01: class c = 9: 2004 real images
2024-11-21 18:30:01: class c = 10: 1556 real images
2024-11-21 18:30:01: real images channel 0, mean = 0.4953, std = 0.2826
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
main_base.py:125: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642975312/work/torch/csrc/utils/tensor_new.cpp:230.)
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 18:30:01: initialize synthetic data from random noise
2024-11-21 18:30:01: [2024-11-21 18:30:01] training begins
2024-11-21 18:30:01: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 18:30:01: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 18:30:26: Evaluate 5 random ConvNet, ACCmean = 0.0880 ACCstd = 0.0217
-------------------------
2024-11-21 18:30:26: Evaluate 5 random ConvNet, F1mean = 0.0714 F!std = 0.0070
-------------------------
2024-11-21 18:30:38: [2024-11-21 18:30:38] iter = 0000, loss = 312.2333
2024-11-21 18:32:32: [2024-11-21 18:32:32] iter = 0010, loss = 158.6056
2024-11-21 18:34:30: [2024-11-21 18:34:30] iter = 0020, loss = 95.4765
2024-11-21 18:36:26: [2024-11-21 18:36:26] iter = 0030, loss = 68.3377
2024-11-21 18:38:22: [2024-11-21 18:38:22] iter = 0040, loss = 57.7389
2024-11-21 18:40:17: [2024-11-21 18:40:17] iter = 0050, loss = 53.3511
2024-11-21 18:42:13: [2024-11-21 18:42:13] iter = 0060, loss = 50.0182
2024-11-21 18:44:09: [2024-11-21 18:44:09] iter = 0070, loss = 47.9792
2024-11-21 18:46:04: [2024-11-21 18:46:04] iter = 0080, loss = 47.6420
2024-11-21 18:48:01: [2024-11-21 18:48:01] iter = 0090, loss = 45.1782
2024-11-21 18:49:57: [2024-11-21 18:49:57] iter = 0100, loss = 44.7706
2024-11-21 18:51:53: [2024-11-21 18:51:53] iter = 0110, loss = 41.4872
2024-11-21 18:53:49: [2024-11-21 18:53:49] iter = 0120, loss = 40.2075
2024-11-21 18:55:47: [2024-11-21 18:55:46] iter = 0130, loss = 39.4332
2024-11-21 18:57:43: [2024-11-21 18:57:43] iter = 0140, loss = 39.1931
2024-11-21 18:59:41: [2024-11-21 18:59:41] iter = 0150, loss = 38.6169
2024-11-21 19:01:38: [2024-11-21 19:01:38] iter = 0160, loss = 39.2144
2024-11-21 19:03:36: [2024-11-21 19:03:36] iter = 0170, loss = 35.7954
2024-11-21 19:05:36: [2024-11-21 19:05:36] iter = 0180, loss = 32.5639
2024-11-21 19:07:32: [2024-11-21 19:07:32] iter = 0190, loss = 34.3707
2024-11-21 19:09:28: [2024-11-21 19:09:28] iter = 0200, loss = 35.8427
2024-11-21 19:11:23: [2024-11-21 19:11:23] iter = 0210, loss = 33.8901
2024-11-21 19:13:19: [2024-11-21 19:13:19] iter = 0220, loss = 32.4470
2024-11-21 19:15:16: [2024-11-21 19:15:16] iter = 0230, loss = 32.6893
2024-11-21 19:17:12: [2024-11-21 19:17:12] iter = 0240, loss = 33.8670
2024-11-21 19:19:09: [2024-11-21 19:19:09] iter = 0250, loss = 32.6040
2024-11-21 19:21:04: [2024-11-21 19:21:04] iter = 0260, loss = 32.2096
2024-11-21 19:23:00: [2024-11-21 19:23:00] iter = 0270, loss = 31.9910
2024-11-21 19:24:58: [2024-11-21 19:24:58] iter = 0280, loss = 32.7819
2024-11-21 19:26:54: [2024-11-21 19:26:54] iter = 0290, loss = 31.8769
2024-11-21 19:28:51: [2024-11-21 19:28:51] iter = 0300, loss = 31.1562
2024-11-21 19:30:46: [2024-11-21 19:30:46] iter = 0310, loss = 31.2578
2024-11-21 19:32:43: [2024-11-21 19:32:43] iter = 0320, loss = 29.9318
2024-11-21 19:34:40: [2024-11-21 19:34:40] iter = 0330, loss = 30.5306
2024-11-21 19:36:37: [2024-11-21 19:36:37] iter = 0340, loss = 28.7211
2024-11-21 19:38:33: [2024-11-21 19:38:33] iter = 0350, loss = 30.8643
2024-11-21 19:40:29: [2024-11-21 19:40:29] iter = 0360, loss = 29.6435
2024-11-21 19:42:26: [2024-11-21 19:42:26] iter = 0370, loss = 30.3182
2024-11-21 19:44:24: [2024-11-21 19:44:24] iter = 0380, loss = 28.0921
2024-11-21 19:46:21: [2024-11-21 19:46:21] iter = 0390, loss = 26.9228
2024-11-21 19:48:17: [2024-11-21 19:48:17] iter = 0400, loss = 28.3900
2024-11-21 19:50:15: [2024-11-21 19:50:15] iter = 0410, loss = 29.0175
2024-11-21 19:52:12: [2024-11-21 19:52:12] iter = 0420, loss = 28.8730
2024-11-21 19:54:10: [2024-11-21 19:54:10] iter = 0430, loss = 29.6721
2024-11-21 19:56:07: [2024-11-21 19:56:07] iter = 0440, loss = 27.7868
2024-11-21 19:58:05: [2024-11-21 19:58:05] iter = 0450, loss = 29.2027
2024-11-21 20:00:02: [2024-11-21 20:00:02] iter = 0460, loss = 26.7260
2024-11-21 20:01:59: [2024-11-21 20:01:59] iter = 0470, loss = 29.3314
2024-11-21 20:03:57: [2024-11-21 20:03:57] iter = 0480, loss = 28.3474
2024-11-21 20:05:55: [2024-11-21 20:05:55] iter = 0490, loss = 27.0624
2024-11-21 20:07:43: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-21 20:07:43: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 20:08:05: Evaluate 5 random ConvNet, ACCmean = 0.5968 ACCstd = 0.0030
-------------------------
2024-11-21 20:08:05: Evaluate 5 random ConvNet, F1mean = 0.5616 F!std = 0.0028
-------------------------
2024-11-21 20:08:17: [2024-11-21 20:08:17] iter = 0500, loss = 27.5235
2024-11-21 20:10:16: [2024-11-21 20:10:15] iter = 0510, loss = 27.2498
2024-11-21 20:12:12: [2024-11-21 20:12:12] iter = 0520, loss = 27.8872
2024-11-21 20:14:09: [2024-11-21 20:14:09] iter = 0530, loss = 29.2023
2024-11-21 20:16:07: [2024-11-21 20:16:07] iter = 0540, loss = 29.6976
2024-11-21 20:18:05: [2024-11-21 20:18:05] iter = 0550, loss = 28.0102
2024-11-21 20:20:03: [2024-11-21 20:20:03] iter = 0560, loss = 28.1029
2024-11-21 20:21:59: [2024-11-21 20:21:59] iter = 0570, loss = 29.2292
2024-11-21 20:23:56: [2024-11-21 20:23:56] iter = 0580, loss = 28.8081
2024-11-21 20:25:53: [2024-11-21 20:25:53] iter = 0590, loss = 26.5452
2024-11-21 20:27:50: [2024-11-21 20:27:50] iter = 0600, loss = 28.9762
2024-11-21 20:29:47: [2024-11-21 20:29:47] iter = 0610, loss = 28.6571
2024-11-21 20:31:46: [2024-11-21 20:31:46] iter = 0620, loss = 28.9402
2024-11-21 20:33:45: [2024-11-21 20:33:45] iter = 0630, loss = 28.1402
2024-11-21 20:35:43: [2024-11-21 20:35:43] iter = 0640, loss = 26.2796
2024-11-21 20:37:40: [2024-11-21 20:37:40] iter = 0650, loss = 28.5033
2024-11-21 20:39:37: [2024-11-21 20:39:37] iter = 0660, loss = 27.4453
2024-11-21 20:41:36: [2024-11-21 20:41:36] iter = 0670, loss = 26.2335
2024-11-21 20:43:38: [2024-11-21 20:43:38] iter = 0680, loss = 28.0918
2024-11-21 20:45:47: [2024-11-21 20:45:47] iter = 0690, loss = 29.8689
2024-11-21 20:47:59: [2024-11-21 20:47:59] iter = 0700, loss = 27.8546
2024-11-21 20:50:34: [2024-11-21 20:50:34] iter = 0710, loss = 30.2964
2024-11-21 20:53:01: [2024-11-21 20:53:01] iter = 0720, loss = 28.8027
2024-11-21 20:55:35: [2024-11-21 20:55:35] iter = 0730, loss = 26.6512
2024-11-21 20:57:54: [2024-11-21 20:57:54] iter = 0740, loss = 27.4506
2024-11-21 21:00:14: [2024-11-21 21:00:14] iter = 0750, loss = 29.7464
2024-11-21 21:02:38: [2024-11-21 21:02:38] iter = 0760, loss = 27.9447
2024-11-21 21:04:54: [2024-11-21 21:04:54] iter = 0770, loss = 29.0670
2024-11-21 21:07:12: [2024-11-21 21:07:12] iter = 0780, loss = 28.4173
2024-11-21 21:09:38: [2024-11-21 21:09:38] iter = 0790, loss = 30.3783
2024-11-21 21:12:02: [2024-11-21 21:12:02] iter = 0800, loss = 27.6053
2024-11-21 21:14:17: [2024-11-21 21:14:17] iter = 0810, loss = 30.1611
2024-11-21 21:16:44: [2024-11-21 21:16:44] iter = 0820, loss = 26.5185
2024-11-21 21:19:07: [2024-11-21 21:19:07] iter = 0830, loss = 28.0227
2024-11-21 21:21:22: [2024-11-21 21:21:22] iter = 0840, loss = 26.0953
2024-11-21 21:23:38: [2024-11-21 21:23:38] iter = 0850, loss = 29.4183
2024-11-21 21:25:58: [2024-11-21 21:25:58] iter = 0860, loss = 29.8519
2024-11-21 21:28:25: [2024-11-21 21:28:25] iter = 0870, loss = 28.5795
2024-11-21 21:30:47: [2024-11-21 21:30:47] iter = 0880, loss = 28.2084
2024-11-21 21:33:09: [2024-11-21 21:33:09] iter = 0890, loss = 27.9451
2024-11-21 21:35:33: [2024-11-21 21:35:33] iter = 0900, loss = 30.3340
2024-11-21 21:38:00: [2024-11-21 21:38:00] iter = 0910, loss = 28.1369
2024-11-21 21:40:25: [2024-11-21 21:40:25] iter = 0920, loss = 29.0857
2024-11-21 21:42:39: [2024-11-21 21:42:39] iter = 0930, loss = 29.8773
2024-11-21 21:45:06: [2024-11-21 21:45:06] iter = 0940, loss = 28.5467
2024-11-21 21:47:34: [2024-11-21 21:47:34] iter = 0950, loss = 29.2853
2024-11-21 21:50:02: [2024-11-21 21:50:02] iter = 0960, loss = 27.9663
2024-11-21 21:52:22: [2024-11-21 21:52:22] iter = 0970, loss = 27.4219
2024-11-21 21:54:49: [2024-11-21 21:54:49] iter = 0980, loss = 27.2459
2024-11-21 21:57:19: [2024-11-21 21:57:19] iter = 0990, loss = 28.1194
2024-11-21 21:59:24: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-21 21:59:24: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 21:59:52: Evaluate 5 random ConvNet, ACCmean = 0.5961 ACCstd = 0.0061
-------------------------
2024-11-21 21:59:52: Evaluate 5 random ConvNet, F1mean = 0.5657 F!std = 0.0057
-------------------------
2024-11-21 22:00:07: [2024-11-21 22:00:07] iter = 1000, loss = 27.3764
2024-11-21 22:00:07: 
================== Exp 1 ==================
 
2024-11-21 22:00:07: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganSMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fb9e6470a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganSMNIST (INFO)>, 'dc_aug_param': None}
2024-11-21 22:00:07: Evaluation model pool: ['ConvNet']
2024-11-21 22:00:08: class c = 0: 1148 real images
2024-11-21 22:00:08: class c = 1: 630 real images
2024-11-21 22:00:08: class c = 2: 614 real images
2024-11-21 22:00:08: class c = 3: 721 real images
2024-11-21 22:00:08: class c = 4: 1132 real images
2024-11-21 22:00:08: class c = 5: 1119 real images
2024-11-21 22:00:08: class c = 6: 3464 real images
2024-11-21 22:00:08: class c = 7: 741 real images
2024-11-21 22:00:08: class c = 8: 803 real images
2024-11-21 22:00:08: class c = 9: 2004 real images
2024-11-21 22:00:08: class c = 10: 1556 real images
2024-11-21 22:00:08: real images channel 0, mean = 0.4953, std = 0.2826
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-21 22:00:08: initialize synthetic data from random noise
2024-11-21 22:00:08: [2024-11-21 22:00:08] training begins
2024-11-21 22:00:08: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-21 22:00:08: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-21 22:00:41: Evaluate 5 random ConvNet, ACCmean = 0.0759 ACCstd = 0.0168
-------------------------
2024-11-21 22:00:41: Evaluate 5 random ConvNet, F1mean = 0.0504 F!std = 0.0087
-------------------------
2024-11-21 22:00:55: [2024-11-21 22:00:55] iter = 0000, loss = 312.7266
2024-11-21 22:03:25: [2024-11-21 22:03:25] iter = 0010, loss = 153.9443
2024-11-21 22:05:46: [2024-11-21 22:05:46] iter = 0020, loss = 80.2077
2024-11-21 22:08:13: [2024-11-21 22:08:13] iter = 0030, loss = 66.7588
2024-11-21 22:10:38: [2024-11-21 22:10:38] iter = 0040, loss = 60.1940
2024-11-21 22:13:05: [2024-11-21 22:13:05] iter = 0050, loss = 52.9053
2024-11-21 22:15:27: [2024-11-21 22:15:27] iter = 0060, loss = 53.1293
2024-11-21 22:17:52: [2024-11-21 22:17:52] iter = 0070, loss = 48.1992
2024-11-21 22:20:13: [2024-11-21 22:20:13] iter = 0080, loss = 44.7363
2024-11-21 22:22:40: [2024-11-21 22:22:40] iter = 0090, loss = 44.0877
2024-11-21 22:25:13: [2024-11-21 22:25:13] iter = 0100, loss = 41.3925
2024-11-21 22:27:43: [2024-11-21 22:27:43] iter = 0110, loss = 41.9405
2024-11-21 22:30:06: [2024-11-21 22:30:06] iter = 0120, loss = 39.0717
2024-11-21 22:32:28: [2024-11-21 22:32:28] iter = 0130, loss = 38.2309
2024-11-21 22:34:47: [2024-11-21 22:34:47] iter = 0140, loss = 37.6200
2024-11-21 22:37:16: [2024-11-21 22:37:16] iter = 0150, loss = 38.3237
2024-11-21 22:39:41: [2024-11-21 22:39:41] iter = 0160, loss = 36.7382
2024-11-21 22:42:09: [2024-11-21 22:42:09] iter = 0170, loss = 37.3449
2024-11-21 22:44:32: [2024-11-21 22:44:32] iter = 0180, loss = 36.0812
2024-11-21 22:47:01: [2024-11-21 22:47:01] iter = 0190, loss = 35.8218
2024-11-21 22:49:31: [2024-11-21 22:49:31] iter = 0200, loss = 36.4254
2024-11-21 22:51:54: [2024-11-21 22:51:54] iter = 0210, loss = 33.4323
2024-11-21 22:54:15: [2024-11-21 22:54:15] iter = 0220, loss = 34.2234
2024-11-21 22:56:38: [2024-11-21 22:56:38] iter = 0230, loss = 35.5581
2024-11-21 22:58:58: [2024-11-21 22:58:58] iter = 0240, loss = 33.7375
2024-11-21 23:01:30: [2024-11-21 23:01:30] iter = 0250, loss = 33.5860
2024-11-21 23:04:02: [2024-11-21 23:04:02] iter = 0260, loss = 33.3096
2024-11-21 23:06:34: [2024-11-21 23:06:34] iter = 0270, loss = 32.0424
2024-11-21 23:08:59: [2024-11-21 23:08:59] iter = 0280, loss = 29.6950
2024-11-21 23:11:32: [2024-11-21 23:11:32] iter = 0290, loss = 31.0740
2024-11-21 23:13:53: [2024-11-21 23:13:53] iter = 0300, loss = 31.6470
2024-11-21 23:16:19: [2024-11-21 23:16:19] iter = 0310, loss = 29.2627
2024-11-21 23:18:41: [2024-11-21 23:18:41] iter = 0320, loss = 30.8603
2024-11-21 23:21:00: [2024-11-21 23:21:00] iter = 0330, loss = 29.8939
2024-11-21 23:23:19: [2024-11-21 23:23:19] iter = 0340, loss = 31.3760
2024-11-21 23:25:52: [2024-11-21 23:25:52] iter = 0350, loss = 29.6059
2024-11-21 23:28:17: [2024-11-21 23:28:17] iter = 0360, loss = 29.8954
2024-11-21 23:30:38: [2024-11-21 23:30:38] iter = 0370, loss = 27.1894
2024-11-21 23:33:00: [2024-11-21 23:33:00] iter = 0380, loss = 28.3699
2024-11-21 23:35:23: [2024-11-21 23:35:23] iter = 0390, loss = 28.7399
2024-11-21 23:37:42: [2024-11-21 23:37:42] iter = 0400, loss = 28.3654
2024-11-21 23:40:08: [2024-11-21 23:40:08] iter = 0410, loss = 28.7520
2024-11-21 23:42:36: [2024-11-21 23:42:36] iter = 0420, loss = 30.7767
2024-11-21 23:45:03: [2024-11-21 23:45:03] iter = 0430, loss = 27.4734
2024-11-21 23:47:42: [2024-11-21 23:47:42] iter = 0440, loss = 29.7330
2024-11-21 23:50:15: [2024-11-21 23:50:15] iter = 0450, loss = 26.4742
2024-11-21 23:52:43: [2024-11-21 23:52:43] iter = 0460, loss = 28.8240
2024-11-21 23:55:19: [2024-11-21 23:55:19] iter = 0470, loss = 29.0449
2024-11-21 23:57:53: [2024-11-21 23:57:53] iter = 0480, loss = 29.1302
2024-11-22 00:00:24: [2024-11-22 00:00:24] iter = 0490, loss = 28.9926
2024-11-22 00:02:42: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 00:02:42: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 00:03:15: Evaluate 5 random ConvNet, ACCmean = 0.6040 ACCstd = 0.0053
-------------------------
2024-11-22 00:03:15: Evaluate 5 random ConvNet, F1mean = 0.5766 F!std = 0.0051
-------------------------
2024-11-22 00:03:29: [2024-11-22 00:03:29] iter = 0500, loss = 29.1350
2024-11-22 00:05:58: [2024-11-22 00:05:58] iter = 0510, loss = 28.2012
2024-11-22 00:08:30: [2024-11-22 00:08:30] iter = 0520, loss = 28.0603
2024-11-22 00:11:02: [2024-11-22 00:11:02] iter = 0530, loss = 27.9213
2024-11-22 00:13:36: [2024-11-22 00:13:36] iter = 0540, loss = 27.0764
2024-11-22 00:16:11: [2024-11-22 00:16:11] iter = 0550, loss = 28.9040
2024-11-22 00:18:41: [2024-11-22 00:18:41] iter = 0560, loss = 28.5849
2024-11-22 00:21:15: [2024-11-22 00:21:15] iter = 0570, loss = 28.7726
2024-11-22 00:23:44: [2024-11-22 00:23:44] iter = 0580, loss = 29.6928
2024-11-22 00:26:15: [2024-11-22 00:26:15] iter = 0590, loss = 29.8294
2024-11-22 00:28:40: [2024-11-22 00:28:40] iter = 0600, loss = 28.5737
2024-11-22 00:31:13: [2024-11-22 00:31:13] iter = 0610, loss = 27.5071
2024-11-22 00:33:41: [2024-11-22 00:33:41] iter = 0620, loss = 29.9139
2024-11-22 00:36:13: [2024-11-22 00:36:13] iter = 0630, loss = 28.9208
2024-11-22 00:38:42: [2024-11-22 00:38:42] iter = 0640, loss = 26.7323
2024-11-22 00:41:13: [2024-11-22 00:41:13] iter = 0650, loss = 27.6221
2024-11-22 00:43:44: [2024-11-22 00:43:44] iter = 0660, loss = 28.1388
2024-11-22 00:46:16: [2024-11-22 00:46:16] iter = 0670, loss = 27.6276
2024-11-22 00:49:00: [2024-11-22 00:49:00] iter = 0680, loss = 28.0385
2024-11-22 00:51:43: [2024-11-22 00:51:43] iter = 0690, loss = 28.1209
2024-11-22 00:54:28: [2024-11-22 00:54:28] iter = 0700, loss = 28.2564
2024-11-22 00:57:08: [2024-11-22 00:57:08] iter = 0710, loss = 28.2983
2024-11-22 00:59:49: [2024-11-22 00:59:49] iter = 0720, loss = 27.5551
2024-11-22 01:02:11: [2024-11-22 01:02:11] iter = 0730, loss = 29.5265
2024-11-22 01:04:35: [2024-11-22 01:04:35] iter = 0740, loss = 28.5159
2024-11-22 01:07:05: [2024-11-22 01:07:05] iter = 0750, loss = 28.2164
2024-11-22 01:09:35: [2024-11-22 01:09:35] iter = 0760, loss = 27.2457
2024-11-22 01:12:01: [2024-11-22 01:12:01] iter = 0770, loss = 28.0317
2024-11-22 01:14:43: [2024-11-22 01:14:43] iter = 0780, loss = 27.1812
2024-11-22 01:17:13: [2024-11-22 01:17:13] iter = 0790, loss = 28.6614
2024-11-22 01:19:30: [2024-11-22 01:19:30] iter = 0800, loss = 28.5379
2024-11-22 01:21:55: [2024-11-22 01:21:55] iter = 0810, loss = 28.7770
2024-11-22 01:24:14: [2024-11-22 01:24:14] iter = 0820, loss = 27.5090
2024-11-22 01:26:41: [2024-11-22 01:26:41] iter = 0830, loss = 27.4499
2024-11-22 01:29:12: [2024-11-22 01:29:12] iter = 0840, loss = 27.6828
2024-11-22 01:31:29: [2024-11-22 01:31:29] iter = 0850, loss = 26.1921
2024-11-22 01:33:52: [2024-11-22 01:33:52] iter = 0860, loss = 27.6497
2024-11-22 01:36:26: [2024-11-22 01:36:26] iter = 0870, loss = 27.4770
2024-11-22 01:39:03: [2024-11-22 01:39:03] iter = 0880, loss = 30.1117
2024-11-22 01:41:34: [2024-11-22 01:41:34] iter = 0890, loss = 28.5746
2024-11-22 01:44:02: [2024-11-22 01:44:02] iter = 0900, loss = 26.7202
2024-11-22 01:46:32: [2024-11-22 01:46:32] iter = 0910, loss = 29.2980
2024-11-22 01:49:03: [2024-11-22 01:49:03] iter = 0920, loss = 28.7367
2024-11-22 01:51:37: [2024-11-22 01:51:37] iter = 0930, loss = 29.0155
2024-11-22 01:54:07: [2024-11-22 01:54:07] iter = 0940, loss = 29.9056
2024-11-22 01:56:33: [2024-11-22 01:56:33] iter = 0950, loss = 27.7108
2024-11-22 01:58:57: [2024-11-22 01:58:57] iter = 0960, loss = 29.1189
2024-11-22 02:01:21: [2024-11-22 02:01:21] iter = 0970, loss = 29.7039
2024-11-22 02:03:56: [2024-11-22 02:03:56] iter = 0980, loss = 28.2254
2024-11-22 02:06:28: [2024-11-22 02:06:28] iter = 0990, loss = 27.1680
2024-11-22 02:08:48: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 02:08:48: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 02:09:21: Evaluate 5 random ConvNet, ACCmean = 0.6118 ACCstd = 0.0039
-------------------------
2024-11-22 02:09:21: Evaluate 5 random ConvNet, F1mean = 0.5780 F!std = 0.0047
-------------------------
2024-11-22 02:09:32: [2024-11-22 02:09:32] iter = 1000, loss = 26.6741
2024-11-22 02:09:32: 
================== Exp 2 ==================
 
2024-11-22 02:09:32: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganSMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fb9e6470a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganSMNIST (INFO)>, 'dc_aug_param': None}
2024-11-22 02:09:32: Evaluation model pool: ['ConvNet']
2024-11-22 02:09:34: class c = 0: 1148 real images
2024-11-22 02:09:34: class c = 1: 630 real images
2024-11-22 02:09:34: class c = 2: 614 real images
2024-11-22 02:09:34: class c = 3: 721 real images
2024-11-22 02:09:34: class c = 4: 1132 real images
2024-11-22 02:09:34: class c = 5: 1119 real images
2024-11-22 02:09:34: class c = 6: 3464 real images
2024-11-22 02:09:34: class c = 7: 741 real images
2024-11-22 02:09:34: class c = 8: 803 real images
2024-11-22 02:09:34: class c = 9: 2004 real images
2024-11-22 02:09:34: class c = 10: 1556 real images
2024-11-22 02:09:34: real images channel 0, mean = 0.4953, std = 0.2826
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-22 02:09:34: initialize synthetic data from random noise
2024-11-22 02:09:34: [2024-11-22 02:09:34] training begins
2024-11-22 02:09:34: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-22 02:09:34: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 02:10:06: Evaluate 5 random ConvNet, ACCmean = 0.0731 ACCstd = 0.0043
-------------------------
2024-11-22 02:10:06: Evaluate 5 random ConvNet, F1mean = 0.0640 F!std = 0.0018
-------------------------
2024-11-22 02:10:20: [2024-11-22 02:10:20] iter = 0000, loss = 310.2625
2024-11-22 02:12:42: [2024-11-22 02:12:42] iter = 0010, loss = 157.4405
2024-11-22 02:15:16: [2024-11-22 02:15:16] iter = 0020, loss = 88.5767
2024-11-22 02:17:51: [2024-11-22 02:17:51] iter = 0030, loss = 71.5620
2024-11-22 02:20:21: [2024-11-22 02:20:21] iter = 0040, loss = 60.4140
2024-11-22 02:22:53: [2024-11-22 02:22:53] iter = 0050, loss = 56.3619
2024-11-22 02:25:18: [2024-11-22 02:25:18] iter = 0060, loss = 51.3055
2024-11-22 02:27:30: [2024-11-22 02:27:30] iter = 0070, loss = 48.4040
2024-11-22 02:29:43: [2024-11-22 02:29:43] iter = 0080, loss = 47.3967
2024-11-22 02:32:12: [2024-11-22 02:32:12] iter = 0090, loss = 42.0973
2024-11-22 02:34:38: [2024-11-22 02:34:38] iter = 0100, loss = 44.2285
2024-11-22 02:37:03: [2024-11-22 02:37:03] iter = 0110, loss = 42.6279
2024-11-22 02:39:21: [2024-11-22 02:39:21] iter = 0120, loss = 39.7080
2024-11-22 02:41:41: [2024-11-22 02:41:41] iter = 0130, loss = 42.7573
2024-11-22 02:44:03: [2024-11-22 02:44:03] iter = 0140, loss = 41.2806
2024-11-22 02:46:28: [2024-11-22 02:46:28] iter = 0150, loss = 38.1158
2024-11-22 02:48:55: [2024-11-22 02:48:55] iter = 0160, loss = 37.6686
2024-11-22 02:51:18: [2024-11-22 02:51:18] iter = 0170, loss = 36.3888
2024-11-22 02:53:44: [2024-11-22 02:53:44] iter = 0180, loss = 35.7143
2024-11-22 02:56:09: [2024-11-22 02:56:09] iter = 0190, loss = 37.1155
2024-11-22 02:58:39: [2024-11-22 02:58:39] iter = 0200, loss = 35.2724
2024-11-22 03:00:56: [2024-11-22 03:00:56] iter = 0210, loss = 33.8354
2024-11-22 03:03:24: [2024-11-22 03:03:24] iter = 0220, loss = 35.6715
2024-11-22 03:05:49: [2024-11-22 03:05:49] iter = 0230, loss = 35.2034
2024-11-22 03:08:16: [2024-11-22 03:08:16] iter = 0240, loss = 32.0447
2024-11-22 03:10:40: [2024-11-22 03:10:40] iter = 0250, loss = 33.0040
2024-11-22 03:13:08: [2024-11-22 03:13:08] iter = 0260, loss = 32.2629
2024-11-22 03:15:33: [2024-11-22 03:15:33] iter = 0270, loss = 29.8411
2024-11-22 03:17:59: [2024-11-22 03:17:59] iter = 0280, loss = 32.0973
2024-11-22 03:20:29: [2024-11-22 03:20:29] iter = 0290, loss = 30.7244
2024-11-22 03:22:56: [2024-11-22 03:22:56] iter = 0300, loss = 31.7490
2024-11-22 03:25:25: [2024-11-22 03:25:25] iter = 0310, loss = 31.0996
2024-11-22 03:27:41: [2024-11-22 03:27:41] iter = 0320, loss = 30.9501
2024-11-22 03:30:18: [2024-11-22 03:30:18] iter = 0330, loss = 31.0430
2024-11-22 03:32:37: [2024-11-22 03:32:37] iter = 0340, loss = 31.8112
2024-11-22 03:34:59: [2024-11-22 03:34:59] iter = 0350, loss = 31.4713
2024-11-22 03:37:18: [2024-11-22 03:37:18] iter = 0360, loss = 31.1855
2024-11-22 03:39:37: [2024-11-22 03:39:37] iter = 0370, loss = 29.6284
2024-11-22 03:42:00: [2024-11-22 03:42:00] iter = 0380, loss = 29.7639
2024-11-22 03:44:26: [2024-11-22 03:44:26] iter = 0390, loss = 30.0264
2024-11-22 03:46:51: [2024-11-22 03:46:51] iter = 0400, loss = 29.8118
2024-11-22 03:49:25: [2024-11-22 03:49:25] iter = 0410, loss = 30.4575
2024-11-22 03:51:59: [2024-11-22 03:51:59] iter = 0420, loss = 29.3931
2024-11-22 03:54:23: [2024-11-22 03:54:23] iter = 0430, loss = 28.7541
2024-11-22 03:56:50: [2024-11-22 03:56:50] iter = 0440, loss = 28.7993
2024-11-22 03:59:13: [2024-11-22 03:59:13] iter = 0450, loss = 28.3706
2024-11-22 04:01:35: [2024-11-22 04:01:35] iter = 0460, loss = 29.0577
2024-11-22 04:04:00: [2024-11-22 04:04:00] iter = 0470, loss = 27.7091
2024-11-22 04:06:42: [2024-11-22 04:06:42] iter = 0480, loss = 28.6427
2024-11-22 04:09:06: [2024-11-22 04:09:06] iter = 0490, loss = 26.5817
2024-11-22 04:11:20: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 04:11:20: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 04:11:55: Evaluate 5 random ConvNet, ACCmean = 0.6091 ACCstd = 0.0049
-------------------------
2024-11-22 04:11:55: Evaluate 5 random ConvNet, F1mean = 0.5665 F!std = 0.0055
-------------------------
2024-11-22 04:12:08: [2024-11-22 04:12:08] iter = 0500, loss = 28.4876
2024-11-22 04:14:29: [2024-11-22 04:14:29] iter = 0510, loss = 28.1199
2024-11-22 04:16:59: [2024-11-22 04:16:59] iter = 0520, loss = 27.4700
2024-11-22 04:19:29: [2024-11-22 04:19:29] iter = 0530, loss = 28.0363
2024-11-22 04:21:57: [2024-11-22 04:21:57] iter = 0540, loss = 28.2606
2024-11-22 04:24:19: [2024-11-22 04:24:19] iter = 0550, loss = 27.3502
2024-11-22 04:26:51: [2024-11-22 04:26:51] iter = 0560, loss = 27.8934
2024-11-22 04:29:12: [2024-11-22 04:29:12] iter = 0570, loss = 27.1972
2024-11-22 04:31:39: [2024-11-22 04:31:39] iter = 0580, loss = 29.8980
2024-11-22 04:34:06: [2024-11-22 04:34:06] iter = 0590, loss = 27.8056
2024-11-22 04:36:29: [2024-11-22 04:36:29] iter = 0600, loss = 29.3843
2024-11-22 04:38:55: [2024-11-22 04:38:55] iter = 0610, loss = 27.3775
2024-11-22 04:41:24: [2024-11-22 04:41:24] iter = 0620, loss = 28.4477
2024-11-22 04:43:49: [2024-11-22 04:43:49] iter = 0630, loss = 27.4772
2024-11-22 04:46:00: [2024-11-22 04:46:00] iter = 0640, loss = 28.5297
2024-11-22 04:48:23: [2024-11-22 04:48:23] iter = 0650, loss = 26.7822
2024-11-22 04:50:56: [2024-11-22 04:50:56] iter = 0660, loss = 29.7544
2024-11-22 04:53:14: [2024-11-22 04:53:14] iter = 0670, loss = 27.0315
2024-11-22 04:55:37: [2024-11-22 04:55:37] iter = 0680, loss = 26.7296
2024-11-22 04:58:03: [2024-11-22 04:58:03] iter = 0690, loss = 27.0313
2024-11-22 05:00:30: [2024-11-22 05:00:30] iter = 0700, loss = 27.6628
2024-11-22 05:02:50: [2024-11-22 05:02:50] iter = 0710, loss = 27.2359
2024-11-22 05:05:10: [2024-11-22 05:05:10] iter = 0720, loss = 27.2550
2024-11-22 05:07:23: [2024-11-22 05:07:23] iter = 0730, loss = 28.7125
2024-11-22 05:09:51: [2024-11-22 05:09:51] iter = 0740, loss = 28.3364
2024-11-22 05:12:22: [2024-11-22 05:12:22] iter = 0750, loss = 27.1314
2024-11-22 05:14:50: [2024-11-22 05:14:50] iter = 0760, loss = 25.6627
2024-11-22 05:17:15: [2024-11-22 05:17:15] iter = 0770, loss = 27.1401
2024-11-22 05:19:42: [2024-11-22 05:19:42] iter = 0780, loss = 27.5756
2024-11-22 05:22:12: [2024-11-22 05:22:12] iter = 0790, loss = 30.6236
2024-11-22 05:24:36: [2024-11-22 05:24:36] iter = 0800, loss = 29.5794
2024-11-22 05:27:03: [2024-11-22 05:27:03] iter = 0810, loss = 29.0204
2024-11-22 05:29:32: [2024-11-22 05:29:32] iter = 0820, loss = 27.8871
2024-11-22 05:32:00: [2024-11-22 05:32:00] iter = 0830, loss = 28.6521
2024-11-22 05:34:24: [2024-11-22 05:34:24] iter = 0840, loss = 26.5374
2024-11-22 05:36:56: [2024-11-22 05:36:56] iter = 0850, loss = 29.5576
2024-11-22 05:39:30: [2024-11-22 05:39:30] iter = 0860, loss = 27.4635
2024-11-22 05:42:00: [2024-11-22 05:42:00] iter = 0870, loss = 28.9820
2024-11-22 05:44:25: [2024-11-22 05:44:25] iter = 0880, loss = 29.9844
2024-11-22 05:46:50: [2024-11-22 05:46:50] iter = 0890, loss = 28.6950
2024-11-22 05:49:14: [2024-11-22 05:49:14] iter = 0900, loss = 28.5795
2024-11-22 05:51:34: [2024-11-22 05:51:34] iter = 0910, loss = 27.8121
2024-11-22 05:53:52: [2024-11-22 05:53:52] iter = 0920, loss = 27.6532
2024-11-22 05:56:02: [2024-11-22 05:56:02] iter = 0930, loss = 27.4358
2024-11-22 05:58:13: [2024-11-22 05:58:13] iter = 0940, loss = 29.6073
2024-11-22 06:00:21: [2024-11-22 06:00:21] iter = 0950, loss = 27.6602
2024-11-22 06:02:32: [2024-11-22 06:02:32] iter = 0960, loss = 28.6739
2024-11-22 06:04:29: [2024-11-22 06:04:29] iter = 0970, loss = 29.4128
2024-11-22 06:06:41: [2024-11-22 06:06:41] iter = 0980, loss = 27.8860
2024-11-22 06:09:00: [2024-11-22 06:09:00] iter = 0990, loss = 28.2589
2024-11-22 06:10:58: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 06:10:58: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
Using downloaded and verified file: /data/users/xiongyuxuan/.medmnist/organsmnist.npz
Using downloaded and verified file: /data/users/xiongyuxuan/.medmnist/organsmnist.npz
Loaded the dataset:OrganSMNIST
[2024-11-21 18:30:09] Evaluate_00: epoch = 0300 train time = 8 s train loss = 0.005347 train acc = 1.0000, test acc = 0.0795, test_sen =0.0996, test_spe =0.9093, test_f1 =0.0732
[2024-11-21 18:30:13] Evaluate_01: epoch = 0300 train time = 3 s train loss = 0.005290 train acc = 1.0000, test acc = 0.1296, test_sen =0.0933, test_spe =0.9108, test_f1 =0.0837
[2024-11-21 18:30:18] Evaluate_02: epoch = 0300 train time = 3 s train loss = 0.005337 train acc = 1.0000, test acc = 0.0859, test_sen =0.0714, test_spe =0.9059, test_f1 =0.0688
[2024-11-21 18:30:22] Evaluate_03: epoch = 0300 train time = 3 s train loss = 0.005310 train acc = 1.0000, test acc = 0.0778, test_sen =0.0882, test_spe =0.9085, test_f1 =0.0627
[2024-11-21 18:30:26] Evaluate_04: epoch = 0300 train time = 3 s train loss = 0.005557 train acc = 1.0000, test acc = 0.0670, test_sen =0.0689, test_spe =0.9085, test_f1 =0.0685
[2024-11-21 20:07:48] Evaluate_00: epoch = 0300 train time = 3 s train loss = 0.013218 train acc = 1.0000, test acc = 0.5975, test_sen =0.5873, test_spe =0.9595, test_f1 =0.5609
[2024-11-21 20:07:52] Evaluate_01: epoch = 0300 train time = 3 s train loss = 0.012315 train acc = 1.0000, test acc = 0.5984, test_sen =0.5919, test_spe =0.9596, test_f1 =0.5595
[2024-11-21 20:07:56] Evaluate_02: epoch = 0300 train time = 3 s train loss = 0.012698 train acc = 1.0000, test acc = 0.5969, test_sen =0.5887, test_spe =0.9595, test_f1 =0.5618
[2024-11-21 20:08:01] Evaluate_03: epoch = 0300 train time = 3 s train loss = 0.013254 train acc = 1.0000, test acc = 0.5911, test_sen =0.5831, test_spe =0.9588, test_f1 =0.5590
[2024-11-21 20:08:05] Evaluate_04: epoch = 0300 train time = 3 s train loss = 0.012484 train acc = 1.0000, test acc = 0.6000, test_sen =0.5907, test_spe =0.9597, test_f1 =0.5670
[2024-11-21 21:59:28] Evaluate_00: epoch = 0300 train time = 2 s train loss = 0.014021 train acc = 1.0000, test acc = 0.5934, test_sen =0.5836, test_spe =0.9591, test_f1 =0.5595
[2024-11-21 21:59:32] Evaluate_01: epoch = 0300 train time = 3 s train loss = 0.014284 train acc = 1.0000, test acc = 0.6058, test_sen =0.5972, test_spe =0.9604, test_f1 =0.5756
[2024-11-21 21:59:37] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.013699 train acc = 1.0000, test acc = 0.5961, test_sen =0.5902, test_spe =0.9594, test_f1 =0.5655
[2024-11-21 21:59:45] Evaluate_03: epoch = 0300 train time = 6 s train loss = 0.013979 train acc = 1.0000, test acc = 0.5871, test_sen =0.5837, test_spe =0.9586, test_f1 =0.5607
[2024-11-21 21:59:52] Evaluate_04: epoch = 0300 train time = 6 s train loss = 0.014574 train acc = 1.0000, test acc = 0.5979, test_sen =0.5912, test_spe =0.9595, test_f1 =0.5672
[2024-11-21 22:00:14] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.005356 train acc = 1.0000, test acc = 0.0655, test_sen =0.0782, test_spe =0.9089, test_f1 =0.0463
[2024-11-21 22:00:21] Evaluate_01: epoch = 0300 train time = 6 s train loss = 0.005208 train acc = 1.0000, test acc = 0.0702, test_sen =0.1057, test_spe =0.9096, test_f1 =0.0461
[2024-11-21 22:00:26] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.005226 train acc = 1.0000, test acc = 0.1072, test_sen =0.0908, test_spe =0.9094, test_f1 =0.0676
[2024-11-21 22:00:34] Evaluate_03: epoch = 0300 train time = 6 s train loss = 0.005222 train acc = 1.0000, test acc = 0.0589, test_sen =0.0901, test_spe =0.9085, test_f1 =0.0447
[2024-11-21 22:00:41] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.005369 train acc = 1.0000, test acc = 0.0775, test_sen =0.0612, test_spe =0.9073, test_f1 =0.0472
[2024-11-22 00:02:48] Evaluate_00: epoch = 0300 train time = 5 s train loss = 0.012975 train acc = 1.0000, test acc = 0.5964, test_sen =0.5887, test_spe =0.9594, test_f1 =0.5713
[2024-11-22 00:02:56] Evaluate_01: epoch = 0300 train time = 5 s train loss = 0.012228 train acc = 1.0000, test acc = 0.6063, test_sen =0.5958, test_spe =0.9603, test_f1 =0.5777
[2024-11-22 00:03:03] Evaluate_02: epoch = 0300 train time = 6 s train loss = 0.012444 train acc = 1.0000, test acc = 0.5994, test_sen =0.5904, test_spe =0.9597, test_f1 =0.5706
[2024-11-22 00:03:09] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.013757 train acc = 1.0000, test acc = 0.6106, test_sen =0.5994, test_spe =0.9607, test_f1 =0.5838
[2024-11-22 00:03:15] Evaluate_04: epoch = 0300 train time = 4 s train loss = 0.012628 train acc = 1.0000, test acc = 0.6072, test_sen =0.5979, test_spe =0.9605, test_f1 =0.5799
[2024-11-22 02:08:55] Evaluate_00: epoch = 0300 train time = 6 s train loss = 0.015255 train acc = 1.0000, test acc = 0.6062, test_sen =0.5864, test_spe =0.9601, test_f1 =0.5714
[2024-11-22 02:09:03] Evaluate_01: epoch = 0300 train time = 6 s train loss = 0.014548 train acc = 1.0000, test acc = 0.6183, test_sen =0.6037, test_spe =0.9615, test_f1 =0.5855
[2024-11-22 02:09:08] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.012724 train acc = 1.0000, test acc = 0.6126, test_sen =0.5960, test_spe =0.9610, test_f1 =0.5754
[2024-11-22 02:09:15] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.013287 train acc = 1.0000, test acc = 0.6113, test_sen =0.5951, test_spe =0.9608, test_f1 =0.5783
[2024-11-22 02:09:21] Evaluate_04: epoch = 0300 train time = 4 s train loss = 0.013953 train acc = 1.0000, test acc = 0.6106, test_sen =0.5958, test_spe =0.9607, test_f1 =0.5795
[2024-11-22 02:09:40] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.005335 train acc = 1.0000, test acc = 0.0656, test_sen =0.0948, test_spe =0.9090, test_f1 =0.0630
[2024-11-22 02:09:46] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.005465 train acc = 1.0000, test acc = 0.0747, test_sen =0.1025, test_spe =0.9095, test_f1 =0.0619
[2024-11-22 02:09:52] Evaluate_02: epoch = 0300 train time = 5 s train loss = 0.005164 train acc = 1.0000, test acc = 0.0740, test_sen =0.0923, test_spe =0.9086, test_f1 =0.0631
[2024-11-22 02:09:59] Evaluate_03: epoch = 0300 train time = 6 s train loss = 0.005383 train acc = 1.0000, test acc = 0.0725, test_sen =0.0923, test_spe =0.9091, test_f1 =0.0671
[2024-11-22 02:10:06] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.005232 train acc = 1.0000, test acc = 0.0786, test_sen =0.1142, test_spe =0.9099, test_f1 =0.0650
[2024-11-22 04:11:27] Evaluate_00: epoch = 0300 train time = 6 s train loss = 0.012934 train acc = 1.0000, test acc = 0.6092, test_sen =0.5922, test_spe =0.9606, test_f1 =0.5700
[2024-11-22 04:11:33] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.012793 train acc = 1.0000, test acc = 0.6086, test_sen =0.5884, test_spe =0.9605, test_f1 =0.5659
[2024-11-22 04:11:40] Evaluate_02: epoch = 0300 train time = 6 s train loss = 0.014025 train acc = 1.0000, test acc = 0.6064, test_sen =0.5847, test_spe =0.9603, test_f1 =0.5575
[2024-11-22 04:11:47] Evaluate_03: epoch = 0300 train time = 6 s train loss = 0.012729 train acc = 1.0000, test acc = 0.6033, test_sen =0.5894, test_spe =0.9600, test_f1 =0.5651
[2024-11-22 04:11:55] Evaluate_04: epoch = 0300 train time = 6 s train loss = 0.012792 train acc = 1.0000, test acc = 0.6181, test_sen =0.6013, test_spe =0.9615, test_f1 =0.5742
[2024-11-22 06:11:03] Evaluate_00: epoch = 0300 train time = 3 s train loss = 0.013667 train acc = 1.0000, test acc = 0.6109, test_sen =0.6000, test_spe =0.9608, test_f1 =0.5830
[2024-11-22 06:11:08] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.013591 train acc = 1.0000, test acc = 0.5883, test_sen =0.5762, test_spe =0.9585, test_f1 =0.5577
[2024-11-22 06:11:13] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.013487 train acc = 1.0000, test acc = 0.5991, test_sen =0.5904, test_spe =0.9596, test_f1 =0.5708
[2024-11-22 06:11:19] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.014043 train acc = 1.0000, test acc = 0.6001, test_sen =0.5869, test_spe =0.9597, test_f1 =0.5697
[2024-11-22 06:11:26] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.013626 train acc = 1.0000, test acc = 0.5960, test_sen =0.5908, test_spe =0.9594, test_f1 =0.57142024-11-22 06:11:26: Evaluate 5 random ConvNet, ACCmean = 0.5989 ACCstd = 0.0073
-------------------------
2024-11-22 06:11:26: Evaluate 5 random ConvNet, F1mean = 0.5705 F!std = 0.0080
-------------------------
2024-11-22 06:11:40: [2024-11-22 06:11:40] iter = 1000, loss = 26.4677
2024-11-22 06:11:40: 
================== Exp 3 ==================
 
2024-11-22 06:11:40: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganSMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fb9e6470a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganSMNIST (INFO)>, 'dc_aug_param': None}
2024-11-22 06:11:40: Evaluation model pool: ['ConvNet']
2024-11-22 06:11:41: class c = 0: 1148 real images
2024-11-22 06:11:41: class c = 1: 630 real images
2024-11-22 06:11:41: class c = 2: 614 real images
2024-11-22 06:11:41: class c = 3: 721 real images
2024-11-22 06:11:41: class c = 4: 1132 real images
2024-11-22 06:11:41: class c = 5: 1119 real images
2024-11-22 06:11:41: class c = 6: 3464 real images
2024-11-22 06:11:41: class c = 7: 741 real images
2024-11-22 06:11:41: class c = 8: 803 real images
2024-11-22 06:11:41: class c = 9: 2004 real images
2024-11-22 06:11:41: class c = 10: 1556 real images
2024-11-22 06:11:41: real images channel 0, mean = 0.4953, std = 0.2826
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-22 06:11:41: initialize synthetic data from random noise
2024-11-22 06:11:41: [2024-11-22 06:11:41] training begins
2024-11-22 06:11:41: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-22 06:11:41: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 06:12:06: Evaluate 5 random ConvNet, ACCmean = 0.0733 ACCstd = 0.0089
-------------------------
2024-11-22 06:12:06: Evaluate 5 random ConvNet, F1mean = 0.0573 F!std = 0.0031
-------------------------
2024-11-22 06:12:21: [2024-11-22 06:12:21] iter = 0000, loss = 304.3434
2024-11-22 06:14:37: [2024-11-22 06:14:37] iter = 0010, loss = 160.2248
2024-11-22 06:16:51: [2024-11-22 06:16:51] iter = 0020, loss = 95.6323
2024-11-22 06:19:05: [2024-11-22 06:19:05] iter = 0030, loss = 69.0523
2024-11-22 06:21:24: [2024-11-22 06:21:24] iter = 0040, loss = 59.8128
2024-11-22 06:23:36: [2024-11-22 06:23:36] iter = 0050, loss = 53.2190
2024-11-22 06:25:53: [2024-11-22 06:25:53] iter = 0060, loss = 49.2970
2024-11-22 06:28:01: [2024-11-22 06:28:01] iter = 0070, loss = 45.9499
2024-11-22 06:30:13: [2024-11-22 06:30:13] iter = 0080, loss = 43.5710
2024-11-22 06:32:21: [2024-11-22 06:32:21] iter = 0090, loss = 42.5198
2024-11-22 06:34:30: [2024-11-22 06:34:30] iter = 0100, loss = 41.4177
2024-11-22 06:36:40: [2024-11-22 06:36:40] iter = 0110, loss = 38.6992
2024-11-22 06:38:52: [2024-11-22 06:38:52] iter = 0120, loss = 40.8868
2024-11-22 06:41:04: [2024-11-22 06:41:04] iter = 0130, loss = 38.2150
2024-11-22 06:43:11: [2024-11-22 06:43:11] iter = 0140, loss = 37.7357
2024-11-22 06:45:19: [2024-11-22 06:45:18] iter = 0150, loss = 36.4434
2024-11-22 06:47:31: [2024-11-22 06:47:31] iter = 0160, loss = 37.0882
2024-11-22 06:49:43: [2024-11-22 06:49:43] iter = 0170, loss = 34.9075
2024-11-22 06:51:49: [2024-11-22 06:51:49] iter = 0180, loss = 34.6342
2024-11-22 06:53:55: [2024-11-22 06:53:55] iter = 0190, loss = 32.8982
2024-11-22 06:56:07: [2024-11-22 06:56:07] iter = 0200, loss = 31.8721
2024-11-22 06:58:14: [2024-11-22 06:58:14] iter = 0210, loss = 33.3035
2024-11-22 07:00:21: [2024-11-22 07:00:21] iter = 0220, loss = 31.9460
2024-11-22 07:02:28: [2024-11-22 07:02:28] iter = 0230, loss = 31.8219
2024-11-22 07:04:37: [2024-11-22 07:04:37] iter = 0240, loss = 30.9737
2024-11-22 07:06:50: [2024-11-22 07:06:50] iter = 0250, loss = 30.5639
2024-11-22 07:08:59: [2024-11-22 07:08:59] iter = 0260, loss = 31.2085
2024-11-22 07:11:10: [2024-11-22 07:11:10] iter = 0270, loss = 30.8637
2024-11-22 07:13:19: [2024-11-22 07:13:19] iter = 0280, loss = 31.4961
2024-11-22 07:15:31: [2024-11-22 07:15:31] iter = 0290, loss = 30.5785
2024-11-22 07:17:43: [2024-11-22 07:17:43] iter = 0300, loss = 31.5398
2024-11-22 07:19:54: [2024-11-22 07:19:54] iter = 0310, loss = 30.9842
2024-11-22 07:22:05: [2024-11-22 07:22:05] iter = 0320, loss = 30.8436
2024-11-22 07:24:14: [2024-11-22 07:24:14] iter = 0330, loss = 29.9591
2024-11-22 07:26:30: [2024-11-22 07:26:30] iter = 0340, loss = 28.8835
2024-11-22 07:28:43: [2024-11-22 07:28:43] iter = 0350, loss = 29.1164
2024-11-22 07:30:55: [2024-11-22 07:30:55] iter = 0360, loss = 32.2536
2024-11-22 07:33:00: [2024-11-22 07:33:00] iter = 0370, loss = 28.8319
2024-11-22 07:35:08: [2024-11-22 07:35:08] iter = 0380, loss = 30.4393
2024-11-22 07:37:21: [2024-11-22 07:37:21] iter = 0390, loss = 29.6984
2024-11-22 07:39:28: [2024-11-22 07:39:28] iter = 0400, loss = 29.1712
2024-11-22 07:41:38: [2024-11-22 07:41:38] iter = 0410, loss = 28.8840
2024-11-22 07:43:48: [2024-11-22 07:43:48] iter = 0420, loss = 28.5783
2024-11-22 07:46:04: [2024-11-22 07:46:04] iter = 0430, loss = 29.8615
2024-11-22 07:48:23: [2024-11-22 07:48:23] iter = 0440, loss = 29.2832
2024-11-22 07:50:21: [2024-11-22 07:50:21] iter = 0450, loss = 29.0237
2024-11-22 07:52:28: [2024-11-22 07:52:28] iter = 0460, loss = 29.6892
2024-11-22 07:54:29: [2024-11-22 07:54:29] iter = 0470, loss = 28.9253
2024-11-22 07:56:35: [2024-11-22 07:56:35] iter = 0480, loss = 29.2693
2024-11-22 07:58:47: [2024-11-22 07:58:47] iter = 0490, loss = 28.5694
2024-11-22 08:00:35: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 08:00:35: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 08:01:05: Evaluate 5 random ConvNet, ACCmean = 0.6043 ACCstd = 0.0049
-------------------------
2024-11-22 08:01:05: Evaluate 5 random ConvNet, F1mean = 0.5734 F!std = 0.0060
-------------------------
2024-11-22 08:01:19: [2024-11-22 08:01:19] iter = 0500, loss = 28.9347
2024-11-22 08:03:31: [2024-11-22 08:03:31] iter = 0510, loss = 26.7582
2024-11-22 08:05:35: [2024-11-22 08:05:35] iter = 0520, loss = 27.4225
2024-11-22 08:07:38: [2024-11-22 08:07:38] iter = 0530, loss = 27.2129
2024-11-22 08:09:48: [2024-11-22 08:09:48] iter = 0540, loss = 28.5164
2024-11-22 08:12:00: [2024-11-22 08:12:00] iter = 0550, loss = 28.6256
2024-11-22 08:14:13: [2024-11-22 08:14:13] iter = 0560, loss = 28.9033
2024-11-22 08:16:17: [2024-11-22 08:16:17] iter = 0570, loss = 26.9852
2024-11-22 08:18:21: [2024-11-22 08:18:21] iter = 0580, loss = 28.1380
2024-11-22 08:20:26: [2024-11-22 08:20:26] iter = 0590, loss = 27.7424
2024-11-22 08:22:23: [2024-11-22 08:22:23] iter = 0600, loss = 28.7983
2024-11-22 08:24:27: [2024-11-22 08:24:27] iter = 0610, loss = 28.2998
2024-11-22 08:26:27: [2024-11-22 08:26:27] iter = 0620, loss = 28.2189
2024-11-22 08:28:27: [2024-11-22 08:28:27] iter = 0630, loss = 29.2599
2024-11-22 08:30:31: [2024-11-22 08:30:31] iter = 0640, loss = 27.5490
2024-11-22 08:32:34: [2024-11-22 08:32:34] iter = 0650, loss = 28.4124
2024-11-22 08:34:37: [2024-11-22 08:34:37] iter = 0660, loss = 29.2918
2024-11-22 08:36:43: [2024-11-22 08:36:43] iter = 0670, loss = 29.9345
2024-11-22 08:38:48: [2024-11-22 08:38:48] iter = 0680, loss = 27.7397
2024-11-22 08:40:46: [2024-11-22 08:40:46] iter = 0690, loss = 27.6005
2024-11-22 08:42:47: [2024-11-22 08:42:47] iter = 0700, loss = 28.0953
2024-11-22 08:44:43: [2024-11-22 08:44:43] iter = 0710, loss = 28.8505
2024-11-22 08:46:47: [2024-11-22 08:46:47] iter = 0720, loss = 28.5521
2024-11-22 08:48:48: [2024-11-22 08:48:48] iter = 0730, loss = 27.4023
2024-11-22 08:50:42: [2024-11-22 08:50:42] iter = 0740, loss = 27.4496
2024-11-22 08:52:42: [2024-11-22 08:52:42] iter = 0750, loss = 29.7811
2024-11-22 08:54:42: [2024-11-22 08:54:42] iter = 0760, loss = 28.5294
2024-11-22 08:56:41: [2024-11-22 08:56:41] iter = 0770, loss = 27.1435
2024-11-22 08:58:42: [2024-11-22 08:58:42] iter = 0780, loss = 26.4294
2024-11-22 09:00:44: [2024-11-22 09:00:44] iter = 0790, loss = 27.8627
2024-11-22 09:02:46: [2024-11-22 09:02:46] iter = 0800, loss = 28.3952
2024-11-22 09:04:50: [2024-11-22 09:04:50] iter = 0810, loss = 29.7945
2024-11-22 09:06:55: [2024-11-22 09:06:55] iter = 0820, loss = 28.5662
2024-11-22 09:09:01: [2024-11-22 09:09:01] iter = 0830, loss = 27.5991
2024-11-22 09:11:09: [2024-11-22 09:11:09] iter = 0840, loss = 27.3025
2024-11-22 09:13:15: [2024-11-22 09:13:15] iter = 0850, loss = 27.7827
2024-11-22 09:15:24: [2024-11-22 09:15:24] iter = 0860, loss = 27.7204
2024-11-22 09:17:21: [2024-11-22 09:17:21] iter = 0870, loss = 28.4113
2024-11-22 09:19:29: [2024-11-22 09:19:29] iter = 0880, loss = 28.7506
2024-11-22 09:21:36: [2024-11-22 09:21:36] iter = 0890, loss = 26.4561
2024-11-22 09:23:44: [2024-11-22 09:23:44] iter = 0900, loss = 27.3808
2024-11-22 09:25:48: [2024-11-22 09:25:48] iter = 0910, loss = 28.3371
2024-11-22 09:27:52: [2024-11-22 09:27:52] iter = 0920, loss = 27.7034
2024-11-22 09:29:55: [2024-11-22 09:29:55] iter = 0930, loss = 26.9585
2024-11-22 09:31:59: [2024-11-22 09:31:59] iter = 0940, loss = 26.3681
2024-11-22 09:33:59: [2024-11-22 09:33:59] iter = 0950, loss = 27.5001
2024-11-22 09:36:07: [2024-11-22 09:36:07] iter = 0960, loss = 28.2110
2024-11-22 09:38:11: [2024-11-22 09:38:11] iter = 0970, loss = 28.4546
2024-11-22 09:40:18: [2024-11-22 09:40:18] iter = 0980, loss = 28.0337
2024-11-22 09:42:25: [2024-11-22 09:42:25] iter = 0990, loss = 28.0266
2024-11-22 09:44:14: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 09:44:14: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 09:44:42: Evaluate 5 random ConvNet, ACCmean = 0.6023 ACCstd = 0.0062
-------------------------
2024-11-22 09:44:42: Evaluate 5 random ConvNet, F1mean = 0.5680 F!std = 0.0056
-------------------------
2024-11-22 09:44:54: [2024-11-22 09:44:54] iter = 1000, loss = 27.4205
2024-11-22 09:44:54: 
================== Exp 4 ==================
 
2024-11-22 09:44:54: Hyper-parameters: 
{'method': 'DC', 'dataset': 'OrganSMNIST', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 5, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7fb9e6470a90>, 'dsa': False, 'mode': 'GM', 'logger': <Logger GM_IPC_10_limg_0.1_Data_OrganSMNIST (INFO)>, 'dc_aug_param': None}
2024-11-22 09:44:54: Evaluation model pool: ['ConvNet']
2024-11-22 09:44:56: class c = 0: 1148 real images
2024-11-22 09:44:56: class c = 1: 630 real images
2024-11-22 09:44:56: class c = 2: 614 real images
2024-11-22 09:44:56: class c = 3: 721 real images
2024-11-22 09:44:56: class c = 4: 1132 real images
2024-11-22 09:44:56: class c = 5: 1119 real images
2024-11-22 09:44:56: class c = 6: 3464 real images
2024-11-22 09:44:56: class c = 7: 741 real images
2024-11-22 09:44:56: class c = 8: 803 real images
2024-11-22 09:44:56: class c = 9: 2004 real images
2024-11-22 09:44:56: class c = 10: 1556 real images
2024-11-22 09:44:56: real images channel 0, mean = 0.4953, std = 0.2826
main_base.py:125: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]
2024-11-22 09:44:56: initialize synthetic data from random noise
2024-11-22 09:44:56: [2024-11-22 09:44:56] training begins
2024-11-22 09:44:56: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 0
2024-11-22 09:44:56: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 09:45:24: Evaluate 5 random ConvNet, ACCmean = 0.0738 ACCstd = 0.0091
-------------------------
2024-11-22 09:45:24: Evaluate 5 random ConvNet, F1mean = 0.0618 F!std = 0.0087
-------------------------
2024-11-22 09:45:37: [2024-11-22 09:45:37] iter = 0000, loss = 299.8083
2024-11-22 09:47:39: [2024-11-22 09:47:39] iter = 0010, loss = 160.6032
2024-11-22 09:49:43: [2024-11-22 09:49:43] iter = 0020, loss = 98.4551
2024-11-22 09:51:48: [2024-11-22 09:51:48] iter = 0030, loss = 67.7771
2024-11-22 09:53:50: [2024-11-22 09:53:50] iter = 0040, loss = 59.5451
2024-11-22 09:55:54: [2024-11-22 09:55:54] iter = 0050, loss = 55.6865
2024-11-22 09:57:58: [2024-11-22 09:57:58] iter = 0060, loss = 50.0913
2024-11-22 10:00:06: [2024-11-22 10:00:06] iter = 0070, loss = 46.2959
2024-11-22 10:02:07: [2024-11-22 10:02:07] iter = 0080, loss = 43.9270
2024-11-22 10:04:10: [2024-11-22 10:04:10] iter = 0090, loss = 40.9949
2024-11-22 10:06:15: [2024-11-22 10:06:15] iter = 0100, loss = 40.7267
2024-11-22 10:08:18: [2024-11-22 10:08:18] iter = 0110, loss = 39.9902
2024-11-22 10:10:23: [2024-11-22 10:10:23] iter = 0120, loss = 40.1790
2024-11-22 10:12:29: [2024-11-22 10:12:29] iter = 0130, loss = 36.2931
2024-11-22 10:14:35: [2024-11-22 10:14:35] iter = 0140, loss = 35.6700
2024-11-22 10:16:41: [2024-11-22 10:16:41] iter = 0150, loss = 36.6968
2024-11-22 10:18:44: [2024-11-22 10:18:44] iter = 0160, loss = 35.9361
2024-11-22 10:20:50: [2024-11-22 10:20:50] iter = 0170, loss = 35.8737
2024-11-22 10:22:54: [2024-11-22 10:22:54] iter = 0180, loss = 34.0427
2024-11-22 10:25:00: [2024-11-22 10:25:00] iter = 0190, loss = 34.6058
2024-11-22 10:27:07: [2024-11-22 10:27:07] iter = 0200, loss = 35.8804
2024-11-22 10:29:10: [2024-11-22 10:29:10] iter = 0210, loss = 31.9302
2024-11-22 10:31:13: [2024-11-22 10:31:13] iter = 0220, loss = 34.3241
2024-11-22 10:33:15: [2024-11-22 10:33:15] iter = 0230, loss = 33.7671
2024-11-22 10:35:16: [2024-11-22 10:35:16] iter = 0240, loss = 32.2941
2024-11-22 10:37:18: [2024-11-22 10:37:18] iter = 0250, loss = 33.0449
2024-11-22 10:39:20: [2024-11-22 10:39:20] iter = 0260, loss = 31.5402
2024-11-22 10:41:22: [2024-11-22 10:41:22] iter = 0270, loss = 31.0787
2024-11-22 10:43:24: [2024-11-22 10:43:24] iter = 0280, loss = 30.4926
2024-11-22 10:45:26: [2024-11-22 10:45:26] iter = 0290, loss = 30.9692
2024-11-22 10:47:27: [2024-11-22 10:47:27] iter = 0300, loss = 30.3111
2024-11-22 10:49:28: [2024-11-22 10:49:28] iter = 0310, loss = 30.1416
2024-11-22 10:51:32: [2024-11-22 10:51:32] iter = 0320, loss = 29.0420
2024-11-22 10:53:30: [2024-11-22 10:53:30] iter = 0330, loss = 29.3493
2024-11-22 10:55:37: [2024-11-22 10:55:37] iter = 0340, loss = 32.2481
2024-11-22 10:57:37: [2024-11-22 10:57:37] iter = 0350, loss = 29.1596
2024-11-22 10:59:37: [2024-11-22 10:59:37] iter = 0360, loss = 31.5473
2024-11-22 11:01:44: [2024-11-22 11:01:44] iter = 0370, loss = 31.2608
2024-11-22 11:03:43: [2024-11-22 11:03:43] iter = 0380, loss = 30.3656
2024-11-22 11:05:49: [2024-11-22 11:05:49] iter = 0390, loss = 28.9568
2024-11-22 11:07:50: [2024-11-22 11:07:50] iter = 0400, loss = 28.3662
2024-11-22 11:09:52: [2024-11-22 11:09:52] iter = 0410, loss = 28.8537
2024-11-22 11:11:52: [2024-11-22 11:11:52] iter = 0420, loss = 28.1783
2024-11-22 11:13:58: [2024-11-22 11:13:57] iter = 0430, loss = 29.7118
2024-11-22 11:15:55: [2024-11-22 11:15:55] iter = 0440, loss = 29.1361
2024-11-22 11:17:55: [2024-11-22 11:17:55] iter = 0450, loss = 28.3530
2024-11-22 11:19:53: [2024-11-22 11:19:53] iter = 0460, loss = 28.4426
2024-11-22 11:21:51: [2024-11-22 11:21:51] iter = 0470, loss = 26.9027
2024-11-22 11:23:50: [2024-11-22 11:23:50] iter = 0480, loss = 28.4747
2024-11-22 11:25:48: [2024-11-22 11:25:48] iter = 0490, loss = 28.2376
2024-11-22 11:27:32: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 500
2024-11-22 11:27:32: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 11:28:01: Evaluate 5 random ConvNet, ACCmean = 0.5937 ACCstd = 0.0044
-------------------------
2024-11-22 11:28:01: Evaluate 5 random ConvNet, F1mean = 0.5626 F!std = 0.0061
-------------------------
2024-11-22 11:28:13: [2024-11-22 11:28:13] iter = 0500, loss = 28.0098
2024-11-22 11:30:10: [2024-11-22 11:30:10] iter = 0510, loss = 27.1521
2024-11-22 11:32:14: [2024-11-22 11:32:14] iter = 0520, loss = 29.6547
2024-11-22 11:34:17: [2024-11-22 11:34:17] iter = 0530, loss = 27.8781
2024-11-22 11:36:21: [2024-11-22 11:36:21] iter = 0540, loss = 27.1200
2024-11-22 11:38:25: [2024-11-22 11:38:25] iter = 0550, loss = 30.3526
2024-11-22 11:40:23: [2024-11-22 11:40:23] iter = 0560, loss = 27.1046
2024-11-22 11:42:28: [2024-11-22 11:42:28] iter = 0570, loss = 27.6431
2024-11-22 11:44:34: [2024-11-22 11:44:34] iter = 0580, loss = 28.9157
2024-11-22 11:46:37: [2024-11-22 11:46:37] iter = 0590, loss = 27.3454
2024-11-22 11:48:42: [2024-11-22 11:48:42] iter = 0600, loss = 27.6117
2024-11-22 11:50:41: [2024-11-22 11:50:41] iter = 0610, loss = 29.3147
2024-11-22 11:52:41: [2024-11-22 11:52:41] iter = 0620, loss = 27.0701
2024-11-22 11:54:41: [2024-11-22 11:54:41] iter = 0630, loss = 27.1516
2024-11-22 11:56:38: [2024-11-22 11:56:38] iter = 0640, loss = 28.1197
2024-11-22 11:58:41: [2024-11-22 11:58:41] iter = 0650, loss = 27.7925
2024-11-22 12:00:42: [2024-11-22 12:00:42] iter = 0660, loss = 26.3623
2024-11-22 12:02:42: [2024-11-22 12:02:42] iter = 0670, loss = 29.5304
2024-11-22 12:04:42: [2024-11-22 12:04:42] iter = 0680, loss = 29.0582
2024-11-22 12:06:44: [2024-11-22 12:06:44] iter = 0690, loss = 28.6496
2024-11-22 12:08:52: [2024-11-22 12:08:52] iter = 0700, loss = 27.9207
2024-11-22 12:10:58: [2024-11-22 12:10:58] iter = 0710, loss = 27.7274
2024-11-22 12:13:00: [2024-11-22 12:13:00] iter = 0720, loss = 25.7875
2024-11-22 12:15:01: [2024-11-22 12:15:01] iter = 0730, loss = 28.9112
2024-11-22 12:16:57: [2024-11-22 12:16:57] iter = 0740, loss = 28.4875
2024-11-22 12:18:54: [2024-11-22 12:18:54] iter = 0750, loss = 31.6845
2024-11-22 12:20:52: [2024-11-22 12:20:52] iter = 0760, loss = 29.1270
2024-11-22 12:22:52: [2024-11-22 12:22:52] iter = 0770, loss = 28.4385
2024-11-22 12:24:51: [2024-11-22 12:24:51] iter = 0780, loss = 27.8512
2024-11-22 12:26:49: [2024-11-22 12:26:49] iter = 0790, loss = 27.0619
2024-11-22 12:28:48: [2024-11-22 12:28:48] iter = 0800, loss = 28.7135
2024-11-22 12:30:46: [2024-11-22 12:30:46] iter = 0810, loss = 28.1651
2024-11-22 12:32:42: [2024-11-22 12:32:42] iter = 0820, loss = 27.5117
2024-11-22 12:34:41: [2024-11-22 12:34:41] iter = 0830, loss = 29.6068
2024-11-22 12:36:40: [2024-11-22 12:36:40] iter = 0840, loss = 28.1304
2024-11-22 12:38:37: [2024-11-22 12:38:37] iter = 0850, loss = 28.7151
2024-11-22 12:40:36: [2024-11-22 12:40:36] iter = 0860, loss = 27.8775
2024-11-22 12:42:34: [2024-11-22 12:42:34] iter = 0870, loss = 29.1662
2024-11-22 12:44:32: [2024-11-22 12:44:32] iter = 0880, loss = 28.1877
2024-11-22 12:46:30: [2024-11-22 12:46:30] iter = 0890, loss = 30.2330
2024-11-22 12:48:36: [2024-11-22 12:48:36] iter = 0900, loss = 29.3228
2024-11-22 12:50:40: [2024-11-22 12:50:40] iter = 0910, loss = 28.0941
2024-11-22 12:52:44: [2024-11-22 12:52:44] iter = 0920, loss = 27.0201
2024-11-22 12:54:46: [2024-11-22 12:54:46] iter = 0930, loss = 28.2156
2024-11-22 12:56:44: [2024-11-22 12:56:44] iter = 0940, loss = 28.3860
2024-11-22 12:58:41: [2024-11-22 12:58:41] iter = 0950, loss = 28.3061
2024-11-22 13:00:37: [2024-11-22 13:00:37] iter = 0960, loss = 28.2891
2024-11-22 13:02:34: [2024-11-22 13:02:34] iter = 0970, loss = 27.7497
2024-11-22 13:04:31: [2024-11-22 13:04:31] iter = 0980, loss = 27.0615
2024-11-22 13:06:27: [2024-11-22 13:06:27] iter = 0990, loss = 27.9898
2024-11-22 13:08:13: -------------------------
Evaluation
model_train = ConvNet, model_eval = ConvNet, iteration = 1000
2024-11-22 13:08:13: DC augmentation parameters: 
{'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'none'}
2024-11-22 13:08:35: Evaluate 5 random ConvNet, ACCmean = 0.6063 ACCstd = 0.0054
-------------------------
2024-11-22 13:08:35: Evaluate 5 random ConvNet, F1mean = 0.5723 F!std = 0.0077
-------------------------
2024-11-22 13:08:47: [2024-11-22 13:08:47] iter = 1000, loss = 28.2679
2024-11-22 13:08:47: 
==================== Final Results ====================

2024-11-22 13:08:47: Run 5 experiments, train on ConvNet, evaluate 25 random ConvNet, mean  = 60.31%  std = 0.81%

[2024-11-22 06:11:46] Evaluate_00: epoch = 0300 train time = 3 s train loss = 0.005461 train acc = 1.0000, test acc = 0.0740, test_sen =0.0993, test_spe =0.9091, test_f1 =0.0522
[2024-11-22 06:11:50] Evaluate_01: epoch = 0300 train time = 3 s train loss = 0.005230 train acc = 1.0000, test acc = 0.0616, test_sen =0.0794, test_spe =0.9075, test_f1 =0.0598
[2024-11-22 06:11:55] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.005379 train acc = 1.0000, test acc = 0.0872, test_sen =0.0843, test_spe =0.9062, test_f1 =0.0610
[2024-11-22 06:12:00] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.005437 train acc = 1.0000, test acc = 0.0663, test_sen =0.0943, test_spe =0.9088, test_f1 =0.0575
[2024-11-22 06:12:06] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.005299 train acc = 1.0000, test acc = 0.0771, test_sen =0.0932, test_spe =0.9076, test_f1 =0.0561
[2024-11-22 08:00:41] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.012971 train acc = 1.0000, test acc = 0.6028, test_sen =0.5815, test_spe =0.9600, test_f1 =0.5647
[2024-11-22 08:00:46] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.011937 train acc = 1.0000, test acc = 0.6010, test_sen =0.5913, test_spe =0.9599, test_f1 =0.5753
[2024-11-22 08:00:53] Evaluate_02: epoch = 0300 train time = 5 s train loss = 0.013400 train acc = 1.0000, test acc = 0.5991, test_sen =0.5857, test_spe =0.9596, test_f1 =0.5705
[2024-11-22 08:00:59] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.013815 train acc = 1.0000, test acc = 0.6052, test_sen =0.5890, test_spe =0.9601, test_f1 =0.5735
[2024-11-22 08:01:05] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.012323 train acc = 1.0000, test acc = 0.6132, test_sen =0.6000, test_spe =0.9610, test_f1 =0.5830
[2024-11-22 09:44:20] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.014749 train acc = 1.0000, test acc = 0.6021, test_sen =0.5845, test_spe =0.9599, test_f1 =0.5674
[2024-11-22 09:44:25] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.015173 train acc = 1.0000, test acc = 0.6140, test_sen =0.5946, test_spe =0.9610, test_f1 =0.5784
[2024-11-22 09:44:31] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.013296 train acc = 1.0000, test acc = 0.5979, test_sen =0.5804, test_spe =0.9595, test_f1 =0.5629
[2024-11-22 09:44:37] Evaluate_03: epoch = 0300 train time = 5 s train loss = 0.015029 train acc = 1.0000, test acc = 0.6009, test_sen =0.5817, test_spe =0.9597, test_f1 =0.5681
[2024-11-22 09:44:42] Evaluate_04: epoch = 0300 train time = 4 s train loss = 0.013916 train acc = 1.0000, test acc = 0.5967, test_sen =0.5811, test_spe =0.9594, test_f1 =0.5632
[2024-11-22 09:45:01] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.005405 train acc = 1.0000, test acc = 0.0802, test_sen =0.0760, test_spe =0.9080, test_f1 =0.0696
[2024-11-22 09:45:07] Evaluate_01: epoch = 0300 train time = 5 s train loss = 0.005285 train acc = 1.0000, test acc = 0.0861, test_sen =0.0891, test_spe =0.9084, test_f1 =0.0693
[2024-11-22 09:45:12] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.005437 train acc = 1.0000, test acc = 0.0715, test_sen =0.0732, test_spe =0.9083, test_f1 =0.0594
[2024-11-22 09:45:18] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.005420 train acc = 1.0000, test acc = 0.0718, test_sen =0.0765, test_spe =0.9081, test_f1 =0.0645
[2024-11-22 09:45:24] Evaluate_04: epoch = 0300 train time = 4 s train loss = 0.005391 train acc = 1.0000, test acc = 0.0593, test_sen =0.0613, test_spe =0.9065, test_f1 =0.0460
[2024-11-22 11:27:37] Evaluate_00: epoch = 0300 train time = 4 s train loss = 0.013117 train acc = 1.0000, test acc = 0.5911, test_sen =0.5797, test_spe =0.9589, test_f1 =0.5607
[2024-11-22 11:27:43] Evaluate_01: epoch = 0300 train time = 4 s train loss = 0.013360 train acc = 1.0000, test acc = 0.5931, test_sen =0.5773, test_spe =0.9590, test_f1 =0.5591
[2024-11-22 11:27:49] Evaluate_02: epoch = 0300 train time = 4 s train loss = 0.013992 train acc = 1.0000, test acc = 0.5908, test_sen =0.5788, test_spe =0.9589, test_f1 =0.5577
[2024-11-22 11:27:54] Evaluate_03: epoch = 0300 train time = 4 s train loss = 0.013473 train acc = 1.0000, test acc = 0.5913, test_sen =0.5789, test_spe =0.9589, test_f1 =0.5610
[2024-11-22 11:28:01] Evaluate_04: epoch = 0300 train time = 5 s train loss = 0.012898 train acc = 1.0000, test acc = 0.6024, test_sen =0.5899, test_spe =0.9599, test_f1 =0.5745
[2024-11-22 13:08:17] Evaluate_00: epoch = 0300 train time = 3 s train loss = 0.014499 train acc = 1.0000, test acc = 0.6092, test_sen =0.5955, test_spe =0.9606, test_f1 =0.5801
[2024-11-22 13:08:22] Evaluate_01: epoch = 0300 train time = 3 s train loss = 0.014006 train acc = 1.0000, test acc = 0.6063, test_sen =0.5829, test_spe =0.9602, test_f1 =0.5665
[2024-11-22 13:08:26] Evaluate_02: epoch = 0300 train time = 3 s train loss = 0.013240 train acc = 1.0000, test acc = 0.5959, test_sen =0.5812, test_spe =0.9594, test_f1 =0.5603
[2024-11-22 13:08:31] Evaluate_03: epoch = 0300 train time = 3 s train loss = 0.014013 train acc = 1.0000, test acc = 0.6104, test_sen =0.5923, test_spe =0.9607, test_f1 =0.5754
[2024-11-22 13:08:35] Evaluate_04: epoch = 0300 train time = 3 s train loss = 0.014760 train acc = 1.0000, test acc = 0.6099, test_sen =0.5954, test_spe =0.9607, test_f1 =0.5793
